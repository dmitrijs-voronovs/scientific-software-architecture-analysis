quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?. It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/277#issuecomment-427333649:376,install,install,376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427333649,1,['install'],['install']
Deployability,"> I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. Probably for another discussion -- I like jax as much as anyone, but it's not nearly as easy to install as pytorch, especially on windows and m1 mac.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081:199,install,install,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081,1,['install'],['install']
Deployability,"> If anyone is stuck waiting for the new release, you can edit your `.../lib/python3.7/site-packages/scanpy/tools/_louvain.py` with these changes:; > ; > Add: `partition_kwargs[""seed""] = random_state`; > Remove: `louvain.set_rng_seed(random_state)`; > ; > From:; > [b54d67b](https://github.com/theislab/scanpy/commit/b54d67b9d6b41269c1612df0242210d1279ede85). Thankx this worked",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-638817267:41,release,release,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-638817267,1,['release'],['release']
Deployability,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:739,release,releases,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569,1,['release'],['releases']
Deployability,"> If the problem is windows, it's possible it will be solved by numpy 2.0. Not sure how easy the upgrade path to numpy 2.0 will be, however.; > ; > * https://numpy.org/devdocs/numpy_2_0_migration_guide.html#windows-default-integer. I can reproduce the error using Numpy 2.0.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2326703284:97,upgrade,upgrade,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2326703284,1,['upgrade'],['upgrade']
Deployability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too – *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:489,release,release,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,3,"['install', 'release']","['install', 'release']"
Deployability,"> If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. . This isn't always true though, e.g., if you use scArches or seurat (which also seems to use this umap transform). On the other hand, the umap transform visualization can be quite deceiving. It can be the case that it qualitatively appears to have no batch effects even when there definitely has been no integration/correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744:14,integrat,integrated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744,2,['integrat'],"['integrated', 'integration']"
Deployability,"> In case anyone has this error again, here is what worked for me:; > ; > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl; > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded).; > * scanpy should work now; > ; > This worked on mine and also on a colleagues windows laptop.; > ; > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-966445217:296,install,install,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-966445217,3,['install'],"['install', 'installing']"
Deployability,"> In case anyone has this error again, here is what worked for me:; > ; > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl; > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded).; > * scanpy should work now; > ; > This worked on mine and also on a colleagues windows laptop.; > ; > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users. this is the only way I solve my error. I tried every else except reinstall system.; thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-871181214:296,install,install,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-871181214,3,['install'],"['install', 'installing']"
Deployability,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455:345,integrat,integration,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455,1,['integrat'],['integration']
Deployability,"> Is there a widely used processing pipeline which does not adhere to this file naming?. STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```; barcodes.tsv.gz; features.tsv.gz; matrix.mtx.gz; UniqueAndMult-EM.mtx.gz; UniqueAndMult-PropUnique.mtx.gz; UniqueAndMult-Rescue.mtx.gz; UniqueAndMult-Uniform.mtx.gz; ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593:36,pipeline,pipeline,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593,1,['pipeline'],['pipeline']
Deployability,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?. I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({; library(reticulate); library(SingleCellExperiment); library(glue); library(clustree); }); sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE); H5AD_PATH = args[1]; OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")); print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {; adata <- sc$read_h5ad(h5ad_path). return(adata); }. count_clusterings = function(adata){; # Ryan suggests:; # length(grep(""leiden"",names(adata$obs))). clusterings = c(); for (x in adata$obs_keys()){; if (startsWith(x, ""leiden"")){; clusterings = append(clusterings, x); }; }; ; return(length(clusterings)); }. set_fig_dimensions = function(num_clusterings){; width = 10; height = (0.6 * num_clusterings); ; if (height < 8){; height = 8; }; ; png(width = width, height = height); options(repr.plot.width = width, repr.plot.height = height); ; return(list(width=width,height=height)); }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)); # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(; x=adata$obs,; prefix=""leiden_"",; # suffix = NULL,; # metadata = NULL,; # count_filter = 0,; # prop_filter = 0.1,; # layout = ""sugiyama"",; # layout = ""tree"",; # use_core_edges = FALSE,; # highlight_core = FALSE,; # node_colour = prefix,; # node_colour_aggr = NULL,; # node_size = ""size"",; # node_size_aggr = NULL,; # node_size_range = c(4, 15),; # node_alpha = 1,; # node_alpha_aggr = NULL,; # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409:88,integrat,integrates,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409,1,['integrat'],['integrates']
Deployability,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap.; Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304:589,update,updated,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666#issuecomment-496848304,1,['update'],['updated']
Deployability,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:782,continuous,continuous,782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['continuous'],['continuous']
Deployability,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1212,pipeline,pipeline,1212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522,1,['pipeline'],['pipeline']
Deployability,> Let's update the notebook as well. Would be great to understand performance difference before merging + get rid of the horrible densifying operation in `dask.ipynb`. On it: https://github.com/scverse/scanpy-tutorials/pull/137,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999:8,update,update,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999,1,['update'],['update']
Deployability,"> Looking through these, I saw that the skmisc isn’t named after the feature it enables. Should we add a more descriptive copy?. I don't have strong feelings. I assume most people try to run the method and then it yells at them telling them exactly what to install and how to do it. The comment is sufficient for me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2087#issuecomment-999259300:257,install,install,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2087#issuecomment-999259300,1,['install'],['install']
Deployability,"> Looks good, thanks for all the work!; > ; > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that?; > ; > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: ; Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""?; Everything else might just be details that people will uncover anyways since the workflows might complain :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494:62,release,release,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494,1,['release'],['release']
Deployability,"> Maybe the only relevant issue would be to change the default color if this clashes with a color already assigned to a category. I'm not sure how to check if colors are similar. But I think @Hrovatin's suggestion could make this obvious enough to users:. > Also, should the NaN and its colour be added to the legend (if categorical) or besides it (if continuous)?. Any suggestions for how to handle this @fidelram?. > Now that you are looking into this, any chance that the missing_color can also be used as the default color when the color parameter is empty? The current default is light gray . Yes, but it took a little bit more work than I expected (https://github.com/matplotlib/matplotlib/issues/18294). Basically we can't just pass an array of nulls for the color values here, since matplotlib throws user visible warnings about this at plot time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675876471:352,continuous,continuous,352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675876471,1,['continuous'],['continuous']
Deployability,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:951,update,update,951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240,1,['update'],['update']
Deployability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:945,pipeline,pipelines,945,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['pipeline'],['pipelines']
Deployability,"> My idea was to have print_header output very little, plus an expandable region (as it’s the one called in notebooks), and to revert print_versions to just copyable text output. Could we deprecate `print_header` and instead suggest a way to call `session_info` for the equivalent?. If all we're doing is calling `session_info.show` with a couple default arguments, I'm not sure it's worth keeping here. I'd like users to call it directly because:. * Users get access to all of the session_info options without us having to mediate that; * If something doesn't work, it's not our problem. > session_info has no file argument. It has `write_req_file`, which is the same intent – right?. I assume `file` was there from a time when this wrote something that you could `pip install` from?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030:770,install,install,770,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030,1,['install'],['install']
Deployability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,integrat,integrate,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,2,['integrat'],['integrate']
Deployability,"> Not sure whether it is resolved, just put here another solution to read_mtx and add to anndata one by one; > ; > ```; > import pandas as pd; > import scanpy as sc; > ```; > ; > ```; > adata = sc.read_mtx('./matrix.mtx'); > adata_bc=pd.read_csv('./barcodes.tsv',header=None); > adata_features=pd.read_csv('./features.tsv',header=None); > adata= adata.T; > adata.obs['cell_id']= adata_bc; > adata.var['gene_name']= adata_features[0].tolist(); > adata.var.index= adata.var['gene_name']; > ```. set the delimiter for the features to tab, then use the second column which contains the gene names and not the gene ensembl id. Using the gene names is better for downstream qc since the scanpy recommended pipeline uses the gene name prefixes to identify mitochondrial genes. ```; adata_features = pd.read_csv('./barcodes.tsv', header = None, delimiter = '\t'); ... # technically don't need to use .values or tolist() since the mtx and features file should ; # have same number of rows resulting in same index in the adata.var and adata_features dataframes. adata.var['gene_name'] = adata_features[1].values; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568:700,pipeline,pipeline,700,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568,1,['pipeline'],['pipeline']
Deployability,"> OK! Please add a release note and we’re good to go I think. Added note, under features",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1944145708:19,release,release,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1944145708,1,['release'],['release']
Deployability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['integrat'],['integration']
Deployability,> One idea:; > ; > 1. Cluster the graph with leiden; > 2. Coarsen the graph (collapse cells in a single cluster into super nodes); > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color; > 4. Assign all cells in each cluster that cluster's color.; > ; > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277804:346,patch,patches,346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277804,1,['patch'],['patches']
Deployability,"> Please also add a release note!. sorry for the naive question, but how do I add a release note? As a comment in the PR? As a commit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746:20,release,release,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746,2,['release'],['release']
Deployability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:550,integrat,integrate,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['integrat'],['integrate']
Deployability,"> So is it OK if I go ahead and merge this before more PRs come in with conflicts? . No. There are already open PRs which I'm working on merging, and this will cause conflicts in those. > But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. I've only partially responded because I'm low on time. At first glance, there are a number of things I'm against here. But I'll be able to consider them more thoroughly, and tell you my arguments, once I've got more time – sometime after the 1.7.0 release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-744154930:548,release,release,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-744154930,1,['release'],['release']
Deployability,"> So it might be better to either switch to the numba kernel for larger datasets or take the compile hit for small datasets. The compiled versions should get cached, so it's a one time cost per install. No?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346:194,install,install,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346,1,['install'],['install']
Deployability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:943,install,installing,943,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['install'],['installing']
Deployability,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work; > ; > ```python; > fig, ax = plt.subplots(1,3, figsize=(20,6)); > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); > plt.tight_layout(pad=3.0); > plt.show(); > ```; > ; > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-1454679327:67,update,update,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-1454679327,1,['update'],['update']
Deployability,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949:60,update,update,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949,2,['update'],"['update', 'updated']"
Deployability,"> Thank you! With “tests” I mean “functions named `test_*` with `assert ...` statements inside”; ; Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, ; Khalid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243:230,update,updated,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-493362243,1,['update'],['updated']
Deployability,"> Thanks for opening the issue.; > ; > It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this.; > ; > Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy. Thank you for pointing out the issue with pytables. Tried a couple things and it works now.; I don't know how this matters. I uninstalled pytables > tried importing scanpy > doesn't work (says tables module not found, which is expected I guess). I reinstalled pytables - now it decides to work. I can't see how that makes a difference since I had the same pytables version before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057:187,update,update,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057,2,"['install', 'update']","['installation', 'update']"
Deployability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,1,['update'],['update']
Deployability,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1659#issuecomment-783241825:17,update,updates,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659#issuecomment-783241825,1,['update'],['updates']
Deployability,"> Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :). Hello, Massonix, was the problem resolved?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-1149904076:18,update,update,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-1149904076,1,['update'],['update']
Deployability,"> Thanks, I am running the version of `1.4.5.1`. Assuming that it is one of 1.4.1 or 1.5.1 I would suggest that you upgrade to the latest scanpy version. What you are using is quite old. If the problem still occurs we can discuss it further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2189#issuecomment-1077460049:116,upgrade,upgrade,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2189#issuecomment-1077460049,1,['upgrade'],['upgrade']
Deployability,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:707,release,release,707,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124,1,['release'],['release']
Deployability,"> That's fair. Might be worth asking the `conos` developers in this case?. Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808:177,install,install,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808,3,['install'],['install']
Deployability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:127,release,release,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['release'],['release']
Deployability,"> The only advantage of sort_order=order_array is that is explicit for the user. Another advantage is that it could be user specified per plot when there are multiple plots. -------------------. I think there is another issue, which is that `sort_order` currently just applies to numeric values while here we are trying to deal with issues around categorical values. To me this suggests a need to have separate arguments for the two cases (`order_categorical`, `order_continuous`), though this raises issues with ""vectorizing"" the argument. Docstrings for these arguments would look something like:. ```rst; order_continuous: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""ascending""; How to order points in plots colored by continuous values. Options include:; * ""current"": use current ordering of AnnData object; * ""random"": randomize the order; * ""ascending"": points with the highest value are plotted on top; * ""descending"": points with lowest value are plotted on top; order_categorical: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""random""; How to order non-null categorical points in the plot. Uses same options as order_continuous.; ```. In this case, `sort_order` would be deprecated, and tell the user to use `order_continuous` instead. ## Potential extensions. * We could also allow users to pass `Callable[Vector, Vector[int]]`s (e.g. function which takes color vector, returns vector of integers) as arguments. ## Possible issues. ### Vectorization could be complicated. Vectorization of argument unclear/ maybe not possible. That is, what if I want the same variable twice, but ordered differently? This would look like: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8""], order_continuous=[""ascending"", ""descending""]); ```. Now what if I wanted to also plot a categorical value? Is this: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8"", ""leiden""], order_continuous=[""ascending"", ""descending"", None]); ```. ### Null values. This solution assumes we still wa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554:737,continuous,continuous,737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554,1,['continuous'],['continuous']
Deployability,"> The tests don’t fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-438320501:137,install,install,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438320501,1,['install'],['install']
Deployability,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version.; > […](#); > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. ; Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/637#issuecomment-492577684:169,update,update,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637#issuecomment-492577684,1,['update'],['update']
Deployability,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649,1,['release'],['release']
Deployability,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:33,install,install,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220,2,['install'],['install']
Deployability,"> This might be due to updates in pandas >1.3.0. The command; > `pbmc.rename_categories('phase', new_cluster_names)`; > seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be; > `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; > I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified.; > ; > Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html. I have verified that this change works in the scanpy clustering tutorial. The exact change I made was; `adata.rename_categories('leiden', new_cluster_names)`; became; `adata.obs['leiden'] =adata.obs['leiden'].cat.rename_categories(new_cluster_names)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522:23,update,updates,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522,2,['update'],"['updated', 'updates']"
Deployability,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:102,install,install,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,10,"['install', 'integrat', 'release']","['install', 'installed', 'installs', 'integrated', 'release']"
Deployability,"> This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes. @ilan-gold , I didn't check that. I will check that and let you guys know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898:61,patch,patched,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898,2,['patch'],['patched']
Deployability,"> Trying to figure out how much of a problem this is, how much does PAGA use forceatlas2?; > ; > From my skimming of the code: it's optional – the default is to use one of the `igraph` layout algorithms. Options I see:; 1. We fork forceatlas2 and create our own release with its own name. Not a fan of this.; 2. Somebody finds a way to contact the author. I already failed through several channels. No response.; 3. We remove support for forceatlas2 since we have another option. @ivirshup how comparable are the forceatlas2 and the igraph implementations when it comes to results? Do you have any idea?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-991913417:262,release,release,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-991913417,1,['release'],['release']
Deployability,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`.; > ; > Would you guys say this is a legal move to make statistically speaking?; > ; > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: ; ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-1756713620:2,UPDATE,UPDATE,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-1756713620,1,['UPDATE'],['UPDATE']
Deployability,"> Unfortunately, we had a prerelease with a bug. the dangers of installing prereleases :wink:. the docker image is on 1.1a1 though :innocent:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-391253208:64,install,installing,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-391253208,1,['install'],['installing']
Deployability,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:341,install,install,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715,1,['install'],['install']
Deployability,"> Well, as that issue says, it’s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means it’s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:838,release,released,838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427,1,['release'],['released']
Deployability,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:45,release,release,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159,8,"['install', 'release']","['install', 'installation', 'installs', 'release']"
Deployability,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we don’t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:242,install,install,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200,2,['install'],['install']
Deployability,"> What the fair way to color this? If it were random, or purely by count this would look mostly like A. I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. And rare cell types shouldn't be up-weighted in that in an unbiased representation (if there is such a thing). In general I do like the idea of density being linked to transparency though. We could do a quick fix based on random order for now though, and then look into transparency for a larger update that would have to do with updating scanpy plotting to larger cell numbers?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761598626:518,update,update,518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761598626,1,['update'],['update']
Deployability,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. I’ve often seen things like this: completely unrelated packages “conflicting” containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, you’ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043:21,install,install,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043,4,['install'],['install']
Deployability,"> Why do we change Matplotlib ""font.sans-serif"" anyway?. No idea, I don’t even like Arial. Alex updated the fonts last in 6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2, and setting the fonts happened in the initial commit:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. @falexwolf do you recall why you did that? Can we just remove that line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-547000682:96,update,updated,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-547000682,1,['update'],['updated']
Deployability,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:267,configurat,configuration,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275,1,['configurat'],['configuration']
Deployability,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:96,install,install,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453,6,"['install', 'release', 'upgrade']","['install', 'installed', 'installs', 'release', 'upgraded', 'upgrades']"
Deployability,"> You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way. Thats a problem with this strategy. Since it's an undirected graph, a node is it's own second neighbor. Since it's a hexagonal grid my first neighbors are also my second neighbors as well as my nth neighbors (in most cases, if there are edges or missing cells this might not be the case). We would either have to do our own BFS which precludes back tracking (i.e. for each search from each node remove previously visited edges), or we could take the difference of the edge sets at each update. Taking the difference would probably be easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705360617:700,update,update,700,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705360617,1,['update'],['update']
Deployability,"> ```; > MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; > simultaneously is deprecated since 3.3 and will become an error two minor releases ; > later. Please pass vmin/vmax directly to the norm when creating it.; > ```. Yeah, that actually re-ignited my idea of adding support for vcenter after upgrading mpl :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208:155,release,releases,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208,1,['release'],['releases']
Deployability,"> ```python; > import scanpy as sc; > ; > em_adata = sc.datasets.pbmc3k(); > ; > sc.pp.pca(em_adata, n_comps=50); > sc.pp.neighbors(em_adata); > sc.tl.umap(em_adata); > sc.tl.leiden(em_adata,flavor='igraph',n_iterations=2,random_state=1653,directed=False); > ```. @melonora, would you mind running this on your windows machine with the latest scanpy release to see if you can reproduce it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2039960139:350,release,release,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2039960139,1,['release'],['release']
Deployability,"> `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False. I'm not seeing this behaviour, could you check what versions you're using?. <details>; <summary> My versions </summary>. ```; -----; sinfo 0.3.1; -----; IPython 7.23.1; jupyter_client 6.1.11; jupyter_core 4.7.0; jupyterlab 2.2.9; notebook 6.3.0; -----; Python 3.8.9 (default, Apr 3 2021, 01:50:09) [Clang 12.0.0 (clang-1200.0.32.29)]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-05-10 10:13; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024:498,update,updated,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024,1,['update'],['updated']
Deployability,"> `paul15` is downloaded automatically, very practical. Yeah, it’s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:132,continuous,continuous,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details. 🤦 remember having a look but not quite getting it the first time... thank you!. > Argument order. 👍 . > Minor things. 👍 🙏 . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-802659709:248,release,release,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802659709,1,['release'],['release']
Deployability,> conda install -c conda-forge pynndescent; > ; > This also fixed my problem. Thanks @FlyPythons for the hint. Thanks!This also work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1592461176:8,install,install,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1592461176,1,['install'],['install']
Deployability,"> coverage decreased, I think not detected because some pytest.parametrize were removed?. I think codecov just hadn’t updated its comment yet when you saw that. What you say can’t be, it doesn’t matter how a line was hit: If a line is run, it’ll be reported as hit, if your changes would have caused it to no longer be it, it would have been reported as a miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042#issuecomment-2196399245:118,update,updated,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042#issuecomment-2196399245,1,['update'],['updated']
Deployability,"> flit install --pth-file --deps=production doesn’t work with setuptools-scm (just in conda?); >; > Fix: it needs setuptools_scm, which is in the dev extra. We need to document this. I had run into a problem with. ```; flit install --pth-file --deps=develop; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174,2,['install'],['install']
Deployability,"> hi @yotamcons ,; > ; > thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,; > ; > Thank you!. Would love to starting November, ping me if thats still relevant",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531:162,update,updated,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1233189531,1,['update'],['updated']
Deployability,"> in the outs/spatial file, change the tissue_positions.csv to tissue_positions_list.csv, it should work out. Doing this will cause issues when you try to spatially plot the data because the coordinates dtype is a string instead of int. You have to also do this suggestion after: https://github.com/scverse/squidpy/issues/623#issuecomment-1339403351. It would be great if the next scanpy release included the read_visium() update or revisit the idea to [remove it in scanpy in favor of squidpy](https://github.com/scverse/scanpy/issues/2331).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488#issuecomment-1652910834:388,release,release,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488#issuecomment-1652910834,2,"['release', 'update']","['release', 'update']"
Deployability,"> it would still be the older conda-forge version that gets installed. This could be overcome by pinning the channel but I agree it could be an issue if not explicitly specified. > Also there is [no mention of singularity in the bioconda docs](https://bioconda.github.io/search.html?q=singularity). This is the Github repo used for all of the automation and is hosted by the Galaxy project:; https://github.com/BioContainers/singularity-build-bot. As others have mentioned here, it would be awesome to have the latest versions of `scanpy` on Bioconda because it is the primary channel for most Bioinformatics tools. This also allows other communities like nf-core to piggy back off their automation to make our lives easier when writing reproducible, standardised workflows. @grst did come up with a couple of workarounds like adding a mulled container with scanpy but that adds a maintenance overhead to keep things up-to-date. How much work would it be to make this happen @ivirshup and would you be willing to help?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160340283:60,install,installed,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160340283,1,['install'],['installed']
Deployability,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:829,pipeline,pipelines,829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179,1,['pipeline'],['pipelines']
Deployability,"> just mention this PR with brief description and your name here: https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst. ok, done. I put it in the ""on master"" section",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-744317801:118,release,release-latest,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744317801,1,['release'],['release-latest']
Deployability,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:11,install,install,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,1,['install'],['install']
Deployability,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138,8,['install'],"['install', 'installation', 'installing']"
Deployability,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:250,install,installed,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,11,"['configurat', 'install', 'update', 'upgrade']","['configuration', 'install', 'installed', 'update', 'upgrades']"
Deployability,"> see that it can process arrays now, i should check if it better to replace the custom implementation with the updated stats.mannwhitneyu. It still doesn't work with sparse arrays, and our version on a sparse array was much faster than theirs on a dense. I did not compare speeds for dense v dense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1892#issuecomment-864875522:112,update,updated,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892#issuecomment-864875522,1,['update'],['updated']
Deployability,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:980,release,release,980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984,1,['release'],['release']
Deployability,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text?. I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ?. > I think the .._pca function is missing from the release note. should I add it there?; The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944:310,release,release,310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944,1,['release'],['release']
Deployability,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:897,release,release,897,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245,1,['release'],['release']
Deployability,"> the newest versions of leidenalg and igraph should be compatible with each other. update both, e.g. `pip install -U leidenalg igraph`; > ; > `python-igraph` is no longer necessary, you can remove it. Thanks for your help!that‘s works！！！",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1647473441:84,update,update,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1647473441,2,"['install', 'update']","['install', 'update']"
Deployability,"> the problem is related to the palette being used. The color palette is taken from the scatter plots. A way to fix this is by running for example `sc.pl.umap(adata, palette='Blues')`. Then run the heatmap again. Does it work if I manually update the adata.uns['louvain_colors'] ?; It feels weird to run umap just to create the slot for colormap althought it worked for me.; Just want to double check.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/548#issuecomment-490092784:240,update,update,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548#issuecomment-490092784,1,['update'],['update']
Deployability,"> there is still a large amount of changes to the dataframe code here. Not really changes: it’s almost all refactoring, because the code was spaghetti with quite some duplication. I’m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and that’s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:1475,integrat,integrated,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,1,['integrat'],['integrated']
Deployability,"> wait to merge this until skmisc has a new release. Are we in a rush? We are upper bounding right now anyway on the current release, no? I would just wait.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182869254:44,release,release,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182869254,2,['release'],['release']
Deployability,"> we're not using pytest-xdist here. Ah. you do install it in the minimum-tests PR though. I guess that means that you intended the min-deps install script for local CI as well. anyway, LGTM!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2843#issuecomment-1934505176:48,install,install,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2843#issuecomment-1934505176,2,['install'],['install']
Deployability,"> what about `X_coords` ?. Ha, I was mostly just trying to get rid of the `X_`!. > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000:692,Update,Update,692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000,1,['Update'],['Update']
Deployability,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live; - [x] `.gitignore` update; - [x] Remove failing test (just there as an example); - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373:404,update,update,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373,1,['update'],['update']
Deployability,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/746#issuecomment-514121664:490,update,update,490,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-514121664,1,['update'],['update']
Deployability,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?. I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs; and check that:; - arguments and doc params match; - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:; - [x] add release note; - [ ] go over scanpy_tutorials and re run tutorial and merge; - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1054373951:748,release,release,748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1054373951,1,['release'],['release']
Deployability,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:; https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095:257,integrat,integrated,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095,1,['integrat'],['integrated']
Deployability,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3160,update,update,3160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['update'],['update']
Deployability,@BrianLohman ; I can't reproduce this. Could you please update scanpy and check?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1485#issuecomment-722581382:56,update,update,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722581382,1,['update'],['update']
Deployability,"@Intron7 FYI: the release notes were in the wrong file. this is in milestone 1.9.4, so they go in the 1.9.4 file",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2589#issuecomment-1668113430:18,release,release,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589#issuecomment-1668113430,1,['release'],['release']
Deployability,@Intron7 code for below link may work for only csr matrix. ; https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/blob/main/pipelines/single-cell-RNA-seq-analysis/notebooks/fastpp.py#L499-L522,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3135#issuecomment-2275907097:134,pipeline,pipelines,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3135#issuecomment-2275907097,1,['pipeline'],['pipelines']
Deployability,@Koncopd Any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1391#issuecomment-698869934:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-698869934,1,['update'],['update']
Deployability,"@Koncopd Hi, Thank you for the pointer. It seems to be a problem caused by pytables package. But I still couldn't import tables after installing and uninstalling pytables packages for many times. And I'm in Windows system.; (base) C:\Users\yuhong>python; ```; (base) C:\Users\yuhong>conda list | grep pytables; pytables 3.6.1 py37h14417ae_3 conda-forge ; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tables; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232:134,install,installing,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232,1,['install'],['installing']
Deployability,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:41,continuous,continuous,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263,4,['continuous'],['continuous']
Deployability,"@Koncopd, ah, this was as far back as I could trace the code initially: https://github.com/theislab/scanpy/commit/af4a82a2540eee65c732cb5e401d2145846e6d97. Now I've found an earlier commit from @falexwolf at https://github.com/theislab/scanpy/commit/43e71fe8577a8b3a51dc2117bd431911001d9869. @falexwolf, does this change look right to you?. @LuckyMD, I'm not sure if this would count as backward breaking if it's a bug. Should definitely go into the release notes, maybe as warning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705#issuecomment-507008710:450,release,release,450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-507008710,1,['release'],['release']
Deployability,"@Koncopd, could we have a meeting or VC on the progress here? Let's discuss via email. 🙂 . @tomwhite, any updates from @lmcinnes regarding integrating the distributed version of `pynndescent` into UMAP itself? I'm not 100% convinced to mingle with private functions within UMAP that might eventually break (another reason for why I copied over code from UMAP back in January/February 2018). It would be nice to have Leland's OK for adding distributed aspects to the package. Scanpy would then just make use of them... @tomwhite, by this, also many other single-cell packages (many of them use UMAP these days) would profit from the new distributed computing capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722:106,update,updates,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722,2,"['integrat', 'update']","['integrating', 'updates']"
Deployability,"@Koncopd, there was a bugged release of pip. I think this should work now that there has been a bugfix",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2255#issuecomment-1134725877:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2255#issuecomment-1134725877,1,['release'],['release']
Deployability,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:64,install,installing,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074,3,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,@LuckyMD Could you also add this fix to the release notes?; https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928:44,release,release,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928,2,['release'],"['release', 'release-latest']"
Deployability,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind.; I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376:84,release,release,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376,1,['release'],['release']
Deployability,@LuckyMD Ok. For me it seems that the packages can be differentiated by using `from gprofiler import GProfiler` for official package (for `python-gprofiler` this is `from gprofiler import gprofiler`). Possibly this allows to control if `gprofiler-official` is installed and used.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-504998754:260,install,installed,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-504998754,1,['install'],['installed']
Deployability,@LuckyMD is your fix (https://github.com/theislab/scanpy/pull/824) in the released scanpy or still only on master?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559305432:74,release,released,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559305432,1,['release'],['released']
Deployability,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:250,continuous,continuous-integration,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['continuous'],['continuous-integration']
Deployability,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332:61,update,updates,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332,1,['update'],['updates']
Deployability,"@LustigePerson, would you be able to add a quick test here? Then this could get into the next release",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2190#issuecomment-1081869351:94,release,release,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2190#issuecomment-1081869351,1,['release'],['release']
Deployability,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:24,install,install,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828,1,['install'],['install']
Deployability,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since; > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939:303,integrat,integrating,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939,3,['integrat'],"['integrating', 'integration']"
Deployability,"@PrimozGodec, probably don't add this to `requirements.txt`, since the requirement should be optional for install. I think instead you should mark it with something like:. ```python; from importlib.util import find_spec. @pytest.mark.skipif(find_spec('pointannotator') is None, reason=""pointannotator not installed""); ```. You can add a requirement for the package to this line in `setup.py`: https://github.com/theislab/scanpy/blob/d8f32c040f3a5f4fc07998b269796ca58de84b40/setup.py#L41. Maybe we should eventually have a second requirements file for CI testing, like we do for anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/812#issuecomment-537465652:106,install,install,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537465652,2,['install'],"['install', 'installed']"
Deployability,"@RicedeKrispy ; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495938081:61,install,install,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495938081,1,['install'],['install']
Deployability,"@RubenVanEsch Yes, and the issue there is that we're not the ones calling `randint`. We may be able to hack it. I'll have a look at how the pipeline errors out on our CI to maybe see where the call is coming from.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034529457:140,pipeline,pipeline,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034529457,1,['pipeline'],['pipeline']
Deployability,"@TheAustinator, can you replicate this in a fresh environment (e.g. conda)? It could help to limit the number of other installed packages. @flying-sheep, any ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496#issuecomment-729533125:119,install,installed,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-729533125,1,['install'],['installed']
Deployability,"@WeilerP would you be willing to write a tiny test and to add the release note, please?. Thanks! Happy to merge this then if you ping me. @ivirshup generally, I agree. Think that this tiny change doesn't harm though and deprecating the magic ""read"" is something bigger.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007:66,release,release,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007,1,['release'],['release']
Deployability,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:66,release,release,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184,1,['release'],['release']
Deployability,"@Zethson added a fix for this to #2566. if you install `scanpy[leiden]`, it’ll make sure the correct versions are installed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1645373691:47,install,install,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1645373691,2,['install'],"['install', 'installed']"
Deployability,"@Zethson re your comment (https://github.com/theislab/scanpy/pull/2028#issuecomment-956365435), what were you thinking for a patch?. Disallowing `0.5.2`? Or make a fix for that version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2026#issuecomment-959058930:125,patch,patch,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2026#issuecomment-959058930,1,['patch'],['patch']
Deployability,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427040589:401,update,update,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427040589,1,['update'],['update']
Deployability,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906:21,update,update,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906,1,['update'],['update']
Deployability,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:86,integrat,integration,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260,1,['integrat'],['integration']
Deployability,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763302767:245,release,release,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763302767,1,['release'],['release']
Deployability,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-601407528:81,install,install,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-601407528,1,['install'],['install']
Deployability,"@alam-shahul I'd suggest you to look into the squidpy plotting functionalities here: https://squidpy.readthedocs.io/en/latest/examples.html#plotting and here: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.spatial_scatter.html#squidpy.pl.spatial_scatter; they cointain an updated version of the scanpy plotting spatial, which will be deprecated. I'll close this comment and please re open one if relevant in squidpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2316#issuecomment-1249573153:281,update,updated,281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2316#issuecomment-1249573153,1,['update'],['updated']
Deployability,"@andrea-tango @MichaelPeibo To address the filtering of rank_genes_groups (eg. `min.pct = 0.25, logfc.threshold = 0.25`) I recently added a function called `sc.tl.filter_rank_genes_groups`. See https://github.com/theislab/scanpy/pull/425. @falexwolf I don't know why`sc.tl.filter_rank_genes_groups` does not show up in the docs. I will take a look. Also, I just noticed that this PR with updated examples is still open. I think it would be useful to merge: https://github.com/theislab/scanpy_usage/pull/11",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471531524:388,update,updated,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471531524,1,['update'],['updated']
Deployability,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/310#issuecomment-431115831:137,update,updates,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-431115831,2,['update'],"['update', 'updates']"
Deployability,"@bgruening I would be interested to hear your perspective on this, and hear any corrections. Apologies if any comments were unfair. My comments were definitely coloured by painful memories of debugging via CI for a release that just went live – which is always an emotional experience 😅. One thing I was wrong about: bioconda does have autoupdates. For whatever reason, it just looks like the scanpy recipe required a fair bit of manual intervention.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160759818:215,release,release,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160759818,1,['release'],['release']
Deployability,@brianpenghe Hi may I consult how you resolved the problem?. The comment says upgrade anndata to 0.8.0 but mine already is 0.8.0 and the error message remains.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394:78,upgrade,upgrade,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394,1,['upgrade'],['upgrade']
Deployability,"@brianpenghe I believe it'll work if you manually update `adata.uns[""louvain_colors""]`, at least it does for the scatter plots. It is weird. We've talked a bit about having a better API for this here #596.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/548#issuecomment-490100552:50,update,update,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548#issuecomment-490100552,1,['update'],['update']
Deployability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,integrat,integrate,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,1,['integrat'],['integrate']
Deployability,@charles-xu-ru you could also try installing numba from conda before installing scanpy. pytables better to install from conda-forge channel along with h5py.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1065067859:34,install,installing,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1065067859,3,['install'],"['install', 'installing']"
Deployability,@chris-rands Any update on this? Does the code from @ivirshup gives you any meaningful results?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-526496705:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-526496705,1,['update'],['update']
Deployability,"@davidsebfischer @falexwolf Also, where can we get this package? I tried doing a pip install, and while the package is listed the installation failed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420413750:85,install,install,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420413750,2,['install'],"['install', 'installation']"
Deployability,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:63,install,install,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004,9,['install'],"['install', 'install-', 'install-headers', 'install-record', 'installed']"
Deployability,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-822545856:14,update,updated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-822545856,1,['update'],['updated']
Deployability,"@doncarlos999 , can you please confirm that you have the latest version of MAGIC? You can do this by running; ```; import magic; magic.__version__; ```; MAGIC was only very recently (four days ago!) updated to support scanpy, so if you don't have version 1.1.0, I recommend reinstalling:; ```; pip install --upgrade --user git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python; ```; Thanks for using MAGIC!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/206#issuecomment-405262289:199,update,updated,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/206#issuecomment-405262289,3,"['install', 'update', 'upgrade']","['install', 'updated', 'upgrade']"
Deployability,@dorzhey can you reproduce this with the latest scanpy release? I believe it should be fixed there.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2967#issuecomment-2022656658:55,release,release,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2967#issuecomment-2022656658,1,['release'],['release']
Deployability,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1086#issuecomment-615129230:104,release,release,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086#issuecomment-615129230,1,['release'],['release']
Deployability,"@falexwolf , I used '0.4.2' version, I will update to the latest.; Thanks, scanpy is powerful tool for single cell RNASeq data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754:44,update,update,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754,1,['update'],['update']
Deployability,@falexwolf - feedback here would be appreciated. We are weary of rolling our own solution when a standard may be in place or planned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095:65,rolling,rolling,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378689095,1,['rolling'],['rolling']
Deployability,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no “move fast and break things” but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they won’t announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that!. ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b] → a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]` → `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesn’t). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelram’s last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874:431,patch,patching,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441590874,1,['patch'],['patching']
Deployability,"@falexwolf @pwl i used this one, works fine: https://gist.github.com/flying-sheep/0e003ae3398dd543638955a55c031c8d. i wonder why you didn’t have to install the dev packages though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344278619:148,install,install,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344278619,1,['install'],['install']
Deployability,"@falexwolf Can you take a look at the updated gist: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe. Most of the previous and new plots are quite similar. For diffmap I see different results but I suspect that there is a bug in the previous code. Also, I don't have any example with arrows. Do you have any?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-420917087:38,update,updated,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420917087,1,['update'],['updated']
Deployability,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:298,install,installed,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827,4,"['install', 'integrat', 'update']","['installed', 'integration', 'update']"
Deployability,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021:643,update,update,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021,1,['update'],['update']
Deployability,@falexwolf I updated the visualizations example notebook to reflect the changes: https://github.com/theislab/scanpy_usage/pull/11,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528#issuecomment-471660400:13,update,updated,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528#issuecomment-471660400,1,['update'],['updated']
Deployability,"@falexwolf any update on the plug-ins idea for scanpy?. On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532,1,['update'],['update']
Deployability,"@fbrundu I just had the exact same error after installing from cloned master, so just wondering if it's working for you now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585#issuecomment-480647524:47,install,installing,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585#issuecomment-480647524,1,['install'],['installing']
Deployability,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊; * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991:410,update,update,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991,1,['update'],['update']
Deployability,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511257594:528,release,release,528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511257594,1,['release'],['release']
Deployability,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754:319,integrat,integrate,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754,1,['integrat'],['integrate']
Deployability,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:993,update,updated,993,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676,1,['update'],['updated']
Deployability,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-759554768:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759554768,1,['update'],['update']
Deployability,"@fidelram Thank you for pointing this out. I did miss the `n_neighbors` parameter for `sc.pp.neighbors` function. It has default 15, while Seurat's have default 20 (and yes, they then prune the kNN). Adding this parameter did solve the discrepancy in the first dataset, but not the second, in the investigation (I have not updated the notebook). @LuckyMD Thank you! I really liked applying scArches, and it's also a very natural approach: having a reference, mapping to it. I hope we're moving towards that direction generally. Thank you for pointing out that Leiden is stochastic, I didn't realize that, and the fixed default random seed obscures it a little. I'll try to look at different seeds and assess the distribution of clustering. Can't estimate to which degree different runs would disagree. Indeed, it appears that scanpy does kNN and doesn't do any pruning (judging from my brief glance at the code). I honestly expected that some kind of pruning of the kNN graph would be there. I remember two talks, one from Dana Pe'er and one from Dominic Grün, that mentioned kNN pruning as a strategy to improve analysis. @dawe Thank you for linking to the resolution limit. However, I don't think it's the case here, because 2 of the 3 strategies that I tried did resolve those populations. . If we focus on dataset 2 (SC167) in the investigation, obviously, there's some small kNN topology difference between the strategies tried, that leads to SCT+scanpy strategy being slow to separate DC1 cells from B cells. I am mostly surprised that vanilla (log-norm) strategy does separate those cells. . I wonder how to go about investigating what drives that behaviour?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787:323,update,updated,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787,1,['update'],['updated']
Deployability,"@fidelram Thanks!. > I noticed two parameters in the embedding that I think belong only to the spatial.. Those are bw and alpha_img. In embeddings they do nothing. Yeah, those probably shouldn't get documented for functions like `sc.pl.umap`. > Other issue, that I don't expect to address at the moment, is the increase in parameters is becoming difficult to go through. I agree (related: #956). There are so many that I'm not sure alphabetical always makes sense? Perhaps they could be grouped into sections of related parameters? This would require some work on how the docs are generated. It would definitely be good to note when features were added. Related to this, I want to discuss versioning at the next meeting to figure out when this should go in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678115710:759,release,release,759,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678115710,1,['release'],['release']
Deployability,@fidelram i'll update this soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1391#issuecomment-698883595:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-698883595,1,['update'],['update']
Deployability,"@fidelram thanks for letting me know. I am using the command you suggested and still getting an error:. CCBR-KULKAMILT:~ kulkarnia2$ pip install git+https://github.com/theislab/scanpy.git; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /private/var/folders/56/g1b462hs0d3816ktcj6twm8hmsw5ch/T/pip-req-build-o4lkd6tp; xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun; Command ""git clone -q https://github.com/theislab/scanpy.git /private/var/folders/56/g1b462hs0d3816ktcj6twm8hmsw5ch/T/pip-req-build-o4lkd6tp"" failed with error code 1 in None",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544#issuecomment-475325377:137,install,install,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544#issuecomment-475325377,1,['install'],['install']
Deployability,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public].; The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376:362,pipeline,pipeline,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376,1,['pipeline'],['pipeline']
Deployability,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:108,integrat,integrated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177,1,['integrat'],['integrated']
Deployability,"@fidelram, I've updated this so the tests pass, and think I've caught a few more bugs. Hopefully I didn't misinterpret your intent here, but I'm merging as we'd like to get a release out. Please let me know if I've messed anything up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325:16,update,updated,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325,2,"['release', 'update']","['release', 'updated']"
Deployability,@flying-sheep @ivirshup The default continuous color map is usually set using `sc.set_figure_params` in the example tutorials. I am not totally sure but I think that internally the `rcParams` are modified. Maybe you can also mention this on the improved documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/477#issuecomment-462743341:36,continuous,continuous,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477#issuecomment-462743341,1,['continuous'],['continuous']
Deployability,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027,3,['install'],['install']
Deployability,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:449,install,install,449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090,1,['install'],['install']
Deployability,"@flying-sheep I think the [code added in that PR](https://github.com/scverse/scanpy/pull/2816/files#diff-5d0e683154209be7830f09b5389551bf9700a4184d08e97c46c23e2e4beb54a0) is minimally relevant to what happened here. > when user specifies an order, we use that. Right, so here the issue is that the category ordering is used for the labelling but we were not imposing it on the data itself when the violin plots render (separate from the axis labels, as the actual violin plots are added row-by-row). > if not, we rely on the DataFrame order for plotting, we don’t store this implicit order explicitly. In some sense the above also applies. If we want to add some sort of user-facing part of the API to allow for ordering, that is fine, but I think that should be separate as it would go into the next minor release and this is a fairly large bug. I'm fine not testing this because I genuinely don't know how and I spent a few hours yesterday trying different things to no avail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3196#issuecomment-2271132251:807,release,release,807,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3196#issuecomment-2271132251,1,['release'],['release']
Deployability,"@flying-sheep I tried running `pip install 'scanpy @ git+https://github.com/scverse/scanpy@modern-rng' ` in anaconda prompt and got ERROR: Invalid requirement: ""'scanpy"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085623955:35,install,install,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085623955,1,['install'],['install']
Deployability,"@flying-sheep Thought it'd be minor enough to skip a release note, but I added one now. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2798#issuecomment-1884401677:53,release,release,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2798#issuecomment-1884401677,1,['release'],['release']
Deployability,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:543,integrat,integrates,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729,1,['integrat'],['integrates']
Deployability,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302#issuecomment-447622248:448,update,updated,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302#issuecomment-447622248,1,['update'],['updated']
Deployability,@flying-sheep do you need a release note here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3328#issuecomment-2449687504:28,release,release,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328#issuecomment-2449687504,1,['release'],['release']
Deployability,"@flying-sheep no worries! We'll steadily increase test coverage. I assume that almost no one should have run into the bug in the past 22 days. Among those that updated their version, only very few will have run the PCA with sparse data... @Koncopd, I'm very happy if you move forward with a proper sparse implementation of PCA! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-447614569:160,update,updated,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-447614569,1,['update'],['updated']
Deployability,"@flying-sheep sorry for the trouble, I have updated my code and output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/850#issuecomment-532633516:44,update,updated,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532633516,1,['update'],['updated']
Deployability,"@flying-sheep, I see you're doing some updates here. Are you doing anything to narrow the scope of the PR as requested last go-around?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1597177367:39,update,updates,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1597177367,1,['update'],['updates']
Deployability,"@flying-sheep, figured out (part of) why I'm seeing the wrong version for the wheel, but the right version for the sdist via `flit build`. When the wheel is built, it hits this case:. https://github.com/theislab/scanpy/blob/3c7256560f53374ecf960bc329405912da8d6efe/scanpy/_metadata.py#L35-L41. Where it sees the currently installed version, which may not be aware of new tag (since I'm making the build in a clean directory). I imagine this is why using `python -m build` get's it right, since it's building everything in a fresh environment. However, getting this issue with the METADATA for the sdist generating warnings, but not having this problem with the wheel: . ```; $ twine check dist/*; Checking dist/scanpy-1.8.1-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-875415985:322,install,installed,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-875415985,1,['install'],['installed']
Deployability,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both?. I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886:46,update,update,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886,1,['update'],['update']
Deployability,"@flying-sheep, was this fixed by the recent update?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1011#issuecomment-1959463716:44,update,update,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1011#issuecomment-1959463716,1,['update'],['update']
Deployability,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:44,install,install,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344,3,"['Update', 'install']","['Update', 'install', 'installed']"
Deployability,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789707448:99,install,installed,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789707448,1,['install'],['installed']
Deployability,@giovp . I solved the problem its scvelo and I have installed the package and now it works fine. Thank you,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1433#issuecomment-704255621:52,install,installed,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1433#issuecomment-704255621,1,['install'],['installed']
Deployability,"@giovp I haven't been able to work around the issue. When I rebuild a container with different versions of scanorama, scanpy, numpy, scikit-learn I end up with errors. Most of the time it's this ValueError about the wrong shape. The only different error I noticed is when I tried scanorama 1.6 and there was an error about `concatenate()` not being an available function. . Whenever you find time to update the tutorial, that will be greatly appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633:400,update,update,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1054575633,1,['update'],['update']
Deployability,@giovp I want to make correlation plot between cell types and the continuous variables stored in .obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091:66,continuous,continuous,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091,1,['continuous'],['continuous']
Deployability,@giovp do we have an update on merging this yet? Anything left on my end?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2296#issuecomment-1314154765:21,update,update,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2296#issuecomment-1314154765,1,['update'],['update']
Deployability,"@giovp looking at this again, it seems you drop the mixed up columns anyway (https://github.com/scverse/squidpy/blob/fb069ded7515e8a1386224d32344a8657cbecd2e/squidpy/read/_read.py#L100) so I'd suggest having one code path with the correct column names. Please let me know if you need more clarity and help getting this over the hump for release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2296#issuecomment-1265541010:337,release,release,337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2296#issuecomment-1265541010,1,['release'],['release']
Deployability,"@giovp, I'll merge this. I'm merging a couple other things first though. I'm not super happy with the logic flow here at the moment. Could we aim for separating out the code for scatter plots, and overlaying grids on-top of images in the next release cycle?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238:243,release,release,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238,1,['release'],['release']
Deployability,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-628371370:131,update,updates,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-628371370,1,['update'],['updates']
Deployability,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:253,release,releases,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512,1,['release'],['releases']
Deployability,"@giovp, that does seem bad, but we can consider that a separate issue, right?. Is the dendrogram bug important for the `squidgy` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1603#issuecomment-768033082:129,release,release,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603#issuecomment-768033082,1,['release'],['release']
Deployability,@gokceneraslan - thanks for the fast response. This broke our (cellxgene) travis pipeline as well. Do you have any info on eta for a fix/workaround other than pinning the module version? TY!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-530530037:81,pipeline,pipeline,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530530037,1,['pipeline'],['pipeline']
Deployability,@gokceneraslan . A fix is provided here https://github.com/theislab/scanpy/pull/1245. In the meanwhile until this PR is merged into a new release you can follow this https://github.com/dpeerlab/Palantir/issues/34#issuecomment-632933449,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1237#issuecomment-638408390:138,release,release,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-638408390,1,['release'],['release']
Deployability,"@gokceneraslan @ivirshup just checking in on this. Will this be part of the next release? Do you need anything else from me? It might be nice to note somewhere that when `batch_key` is not None, results aren't absolutely consistent with Seurat. > The problem appears to be due to the fact that many genes have the same normalized variance in a given batch and the merging method uses ranks. So I believe the difference is due to genes being sorted differently with the same normalized variance. Perhaps this merging scheme is not ideal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-663879113:81,release,release,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-663879113,1,['release'],['release']
Deployability,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages.; 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users?. What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673:88,configurat,configurations,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673,1,['configurat'],['configurations']
Deployability,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:568,install,installed,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523,2,"['install', 'update']","['installed', 'updated']"
Deployability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:493,install,installing,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['install'],['installing']
Deployability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:917,Integrat,Integration,917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['Integrat'],['Integration']
Deployability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:41,release,release,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,9,"['install', 'release', 'update', 'upgrade']","['install', 'installation', 'release', 'released', 'updated', 'upgrade']"
Deployability,@hl324 @glycoaddict can you try the fix here? https://github.com/scverse/scanpy/pull/3041. ```; pip install 'scanpy @ git+https://github.com/scverse/scanpy@modern-rng'; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085152773:100,install,install,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2085152773,1,['install'],['install']
Deployability,"@hyjforesight no, that is a mess.; Could you try please 3 things:. 1. create fresh environment with python 3.8, install only pytables; `conda install pytables`; then run python and check; `import tables`; 2. create fresh environment with python 3.8, install only pytables from conda-forge; `conda install -c conda-forge pytables`; then run python and check; `import tables`; 3. create fresh environment with python 3.8 and install just scanpy (and nothing else); `conda install -c conda-forge scanpy`; then run python and check; `import scanpy`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013450589:112,install,install,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013450589,6,['install'],['install']
Deployability,@icml-compbio The `draw_graph` function calls out to `forceatlas2` if you have it installed. This does seem slower than using UMAP. @YubinXie I see some multithreading being used on my machine when I run `neighbors`. Is there none on yours? One thing I'd check first is to make sure UMAP is up to date and install `pynndescent`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507:82,install,installed,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507,2,['install'],"['install', 'installed']"
Deployability,"@ilan-gold If you want to try it out, I give instructions for how to reproduce the error with a Docker container for Windows in the cross-referenced issue. I also have tried it on WSL, and the problem is *not* present on WSL, so this is a workaround for Windows users. However, I am organizing a Python workshop in a few weeks, and I think it would add some additional administrative burden/overhead to the workshop to coordinate installing and setting up WSL (as we see in #3041, Ruben had trouble installing WSL and others might as well.) So, for me, using WSL is a suboptimal workaround.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2332444655:430,install,installing,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2332444655,2,['install'],['installing']
Deployability,"@ilan-gold turns out i cant install WSL on my laptop after all, so unfortunately i cant check this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3041#issuecomment-2090827588:28,install,install,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3041#issuecomment-2090827588,1,['install'],['install']
Deployability,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323:527,release,release,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323,3,"['release', 'update']","['release', 'update', 'updates']"
Deployability,"@ivirshup ; > * Could you show some examples of the new additions/ let me know where you are on tutorials?. I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: ; * embedding scatter plots which are separated already; * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this?. > * Could you run `black` over this?. Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723:207,update,update,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723,1,['update'],['update']
Deployability,"@ivirshup ; Hello ivirshup, thanks for the solution. I upgrade to py 3.8 and install scanpy 1.8.2, problem solved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967:55,upgrade,upgrade,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"@ivirshup ; Hello ivirshup, these error comes from the below environments. When I upgrade to py3.8 and use scanpy 1.8.2, everything works well. Thanks a lot!. <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; ble",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:82,upgrade,upgrade,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['upgrade'],['upgrade']
Deployability,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:266,pipeline,pipeline,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:222,release,release,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944,2,"['release', 'update']","['release', 'updated']"
Deployability,"@ivirshup @fidelram ; updated, this should be ready for merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1391#issuecomment-703691873:22,update,updated,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-703691873,1,['update'],['updated']
Deployability,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:391,integrat,integration,391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079,2,['integrat'],['integration']
Deployability,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium.; https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-693389468:24,update,updated,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693389468,1,['update'],['updated']
Deployability,"@ivirshup @ilan-gold just got back to this, thought i could not install wsl as I am on a somewhat company restricted laptop, but turns out i can. installing it now (and probably using that from here on out). will run the tester in a bit and let you know",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2039290843:64,install,install,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2039290843,2,['install'],"['install', 'installing']"
Deployability,@ivirshup Can we stick this in `1.10`? It seems small enough and IMO basically constitutes a breaking change anyway. then @theJasonFan can add a release note and we can merge if we don't notice anything else wrong here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2889#issuecomment-1968612083:145,release,release,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2889#issuecomment-1968612083,1,['release'],['release']
Deployability,"@ivirshup I can also confirm the updated tutorial works, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1072908371:33,update,updated,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1072908371,1,['update'],['updated']
Deployability,"@ivirshup I find it useful, since we're still changing our API a lot, so I don't forget to update the tutorials. Our main issue is just runtime of some functions.; I'm using https://github.com/chrisjsewell/pytest-notebook, which can compare expected output of certain cells (or completely) ignore them, but it's still a very small library. I haven't gotten around pushing the updates, since I figured if the notebooks run, it's fine if there are some small discrepencies in output (like images/printed stuff) - though maybe I will update this soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026:91,update,update,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026,3,['update'],"['update', 'updates']"
Deployability,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:65,configurat,configuration,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430,2,['configurat'],['configuration']
Deployability,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. We’ll have to think about naming though. Maybe we want to namespace our settings like matplotlib’s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:108,pipeline,pipeline,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481,1,['pipeline'],['pipeline']
Deployability,"@ivirshup Thank you for the feedback. I will add a release note soon. I also thought about the naming of the parameter. However, if we assume that also in the future it is mostly used to subset the number of PCs in PCA arrays stored under different names, it should be fine?. I can not comment further on what these changes may break, at least ideally they should make the use of n_pcs more consistent and as expected. Would you suggest, I implement some further, more comprehensive tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076393928,1,['release'],['release']
Deployability,@ivirshup Thanks for letting me know. It will also update for me once 1.10.1 is out. I use the logic from scanpy for this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964#issuecomment-2021388747:51,update,update,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964#issuecomment-2021388747,1,['update'],['update']
Deployability,"@ivirshup Thanks for pointing it up. I updated all to newest version, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942#issuecomment-560068082:39,update,updated,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-560068082,1,['update'],['updated']
Deployability,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950:318,update,updated,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950,1,['update'],['updated']
Deployability,"@ivirshup We estimate that the package will be released around 15th of April. So, in a month or so. ; If this is ok, then I'll let you know when it is out:)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-472857688:47,release,released,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-472857688,1,['release'],['released']
Deployability,@ivirshup We have updated gprofiler-official to version 1.0.0 that corresponds to the new API.; See the descriptions here: https://pypi.org/project/gprofiler-official/#description,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-481192068:18,update,updated,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-481192068,1,['update'],['updated']
Deployability,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy?. 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671:387,continuous,continuous,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671,1,['continuous'],['continuous']
Deployability,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)?. upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1589#issuecomment-762495449:143,release,release,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762495449,1,['release'],['release']
Deployability,"@ivirshup or @flying-sheep or any other active maintainers, if you get a chance, could you consider the associated PR for this issue? It'd be great to have this fixed in a future release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1196871672:179,release,release,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1196871672,1,['release'],['release']
Deployability,"@ivirshup this one should be good to go now, right? Do you require release notes for such small things or do you manually add those later?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2231#issuecomment-1158926683:67,release,release,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2231#issuecomment-1158926683,1,['release'],['release']
Deployability,"@karlann yeah, conda environments can run into these problems, especially if you've installed some packages with `pip` and some with `conda`. I generally try to create fresh environments very frequently instead of updating old ones. `mamba` (faster conda) definitely makes this less painful. @OnlyBelter have you tried the same solution of trying a fresh environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-862972961:84,install,installed,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-862972961,1,['install'],['installed']
Deployability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:157,patch,patch,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,2,['patch'],['patch']
Deployability,"@ktpolanski I re-created a new conda env and installed scanpy so now it's working. I'm still not sure where the problem lies, though I did realize it starts jumping error after I install another package called stereopy, I'm guessing maybe that package updated something which got in conflict with normal scanpy setting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1451305928:45,install,installed,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1451305928,3,"['install', 'update']","['install', 'installed', 'updated']"
Deployability,"@ktpolanski I thought I had the newest anndata version, but turns out 0.8.0 is not in Ubuntu repositiories. I had to manually download and install Python 3.8, anndata 0.8.0 and h5py, now it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1451566540:139,install,install,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1451566540,1,['install'],['install']
Deployability,@liiskolb The problem is that the `python-gprofiler` and `gprofiler-official` packages are both imported as `import gprofiler`. That means that someone who has installed one of them and then gets the other with scanpy won't know what they are importing if they just run `import gprofiler`. This is not ideal. I just experienced the same thing and decided to remove `python-gprofiler`. But we can't really mandate that everyone does this. @ivirshup maybe the solution is to detect which version people have and then parse according to their version? The format is quite similar. I've used both now and could probably convert inputs and outputs easily. And then I'd throw a warning if `python-gprofiler` is installed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-504960614:160,install,installed,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-504960614,2,['install'],['installed']
Deployability,"@liiskolb, any chance you have an estimate of when the python package will be released? I'd like to have this PR merge with up-to-date results, and am trying to figure out if I should write a little client. @fidelram Sure!. Just a heads up to everyone, I'm pretty swamped this week and probably won't get around to updating this PR until at least this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-472269046:78,release,released,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-472269046,1,['release'],['released']
Deployability,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:491,release,release,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714,1,['release'],['release']
Deployability,@macros29 try `pip install anndata --force-reinstall` then import your packages again and try saving.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515#issuecomment-469487061:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515#issuecomment-469487061,1,['install'],['install']
Deployability,"@massonix Latest version is available on PyPI, so you can try installing via pip install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942#issuecomment-577689480:62,install,installing,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577689480,2,['install'],"['install', 'installing']"
Deployability,"@maximilianh Sure! This [gist](https://gist.github.com/ivirshup/e7f0c435474d9b06b622c63d4221afe6) has a script to download a dataset and write it to a `.h5ad` file. I think having scanpy and tqdm installed should be sufficient to run it. Usage should be:. ```; python3 download_expression_atlas.py {accession}; ``` . where `accession` is something like: `E-EHCA-2` or `E-GEOD-98816`. . But what it does is just download and parse the expression `.zip` and ""Experiment design file"" from each dataset's download page ([example](https://www.ebi.ac.uk/gxa/sc/experiments/E-EHCA-2/downloads)). I think the experiment design files are a cleaned up version of the `.sdrf` file (some redundant fields are removed, field names can be less verbose).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476613271:196,install,installed,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476613271,1,['install'],['installed']
Deployability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:229,configurat,configurations,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,"['configurat', 'release']","['configurations', 'release']"
Deployability,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669#issuecomment-804067298:94,update,updates,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-804067298,1,['update'],['updates']
Deployability,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:450,install,install,450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692,2,['install'],['install']
Deployability,"@nahanoo ; Hi, there are 3 options for now:. 1. downgrading umap to 0.39; 2. installing scanpy from github; 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413:77,install,installing,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413,2,"['install', 'release']","['installing', 'release']"
Deployability,"@ontsilla We can take a look at this, but I'm not sure if there will be a solution soon. Have you tried using [conda](https://conda.io/en/latest/miniconda.html) on this system? I think it might be your best bet here. @flying-sheep I can recreate with:. ```; conda create -yn testenv python=3.5.2; conda activate testenv; pip install scanpy; python -c ""import scanpy"" ; ```. It looks like there were a lot of bug fixes to python's `typing` module between v3.5.2 and v3.5.4 ([changelog](https://docs.python.org/3.5/whatsnew/changelog.html#python-3-5-4rc1)). I don't get this error with v3.5.4. Are pre-bugfix versions of python supported?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-476952168:325,install,install,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-476952168,1,['install'],['install']
Deployability,"@outlace Curiously, your change causes an error. Without your change I can run the tests correctly without a problem. I remember that I fixed a bug similar to this one that was recently integrated into master (see https://github.com/theislab/scanpy/pull/425/files#diff-b5175ed1415cdbf853646e523cbe8ae0L902). Could it be that you didn't have the latest pull from scanpy and that was causing the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072:186,integrat,integrated,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072,1,['integrat'],['integrated']
Deployability,@outlace I will check the problem with the test and integrate your changes in a new PR that addresses #512 and #524 if this is OK with you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638,1,['integrat'],['integrate']
Deployability,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:41,install,installing,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843,4,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:75,install,installed,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707,3,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,@salwanbutrus This should be fixed from Scanpy version 1.4.5.1. You may need to update your Scanpy version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1017#issuecomment-645824317:80,update,update,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1017#issuecomment-645824317,1,['update'],['update']
Deployability,@sjfleming Is there a GIST or repo url to use this code? Might take time to integrate into scanpy/anndata but people can benefit from the code if it already lives somewhere...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615:76,integrat,integrate,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615,1,['integrat'],['integrate']
Deployability,"@taopeng1100: This is a numba bug. Please report this there, but only if you use the newest numba version (otherwise install it and try to reproduce this with the newest version). Give them this link so they see the code that triggers their bug:. https://github.com/theislab/scanpy/blob/2f160ea403d124d237fc2138c0aa0d175fbad22a/scanpy/preprocessing/_qc.py#L402-L428. @team: We should include numba in the package versions list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-666246043:117,install,install,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-666246043,1,['install'],['install']
Deployability,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:34,release,release,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889,1,['release'],['release']
Deployability,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084:112,integrat,integration-scanorama,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084,1,['integrat'],['integration-scanorama']
Deployability,"@vladie0, would you mind pulling again and checking if it works now?. @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/601#issuecomment-482107875:114,install,install,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601#issuecomment-482107875,1,['install'],['install']
Deployability,"@vtraag FWIW, `pip install leidenalg` worked without a hitch for me (CentOS 6.5, Python 3.6.6 in a relatively empty conda env: scanpy + scikit stack).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437074497:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437074497,1,['install'],['install']
Deployability,"@wniu721 We had similar issues, but everything seems to be solved by installing recent versions of UMAP. I have to say we were working with python 3.8 (IDK if 3.9 has other issues)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1840#issuecomment-844133832:69,install,installing,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-844133832,1,['install'],['installing']
Deployability,"@zappuf @ivirshup I think an update to the tutorial would be needed but don't have time right now, @zappuf did you manage to work around the issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1053612098:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1053612098,1,['update'],['update']
Deployability,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920:343,update,update,343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920,1,['update'],['update']
Deployability,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-541384867:145,install,installable,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541384867,1,['install'],['installable']
Deployability,"AFAICT `ubuntu 16.04` is what they use in most of their examples. Considering many of our users will be on academic clusters, I think old-ish versions of linux are reasonable to test against. The alternative would probably be `ubuntu-18.04`, which we should switch to when `16` is out of support. [Here are the options](https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#software).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948:366,pipeline,pipelines,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948,1,['pipeline'],['pipelines']
Deployability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:713,integrat,integrate,713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,2,['integrat'],"['integrate', 'integrated']"
Deployability,"About the version: That’s because `__file__` in `get_version(root='..', relative_to=__file__)` is a path where no component is in a git repository. Therefore the `setuptools_scm` code fails, we enter the `else` branch, and the (stale) metadata is used. Something like the following could make the `_metadata` module figure out we’re in a dev installation after all. (Needs some more as `here.readlink()` returns a relative path. I’m against `resolve()` as we really just want that one link to be replaced and `resolve()` is a sledge hammer). ```py; file_resolved = str(here.readlink() / f'{__name__}.py') if here.is_symlink() else __file__; ```. But the real issue is that dev installs are a hack and scanpy’s package metadata is stale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129:342,install,installation,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129,2,['install'],"['installation', 'installs']"
Deployability,"Actually I've always been using the conda package and never had any issues. ; Github releases are watched by the bioconda-bot, so it should never be out of date, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533874424:85,release,releases,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533874424,1,['release'],['releases']
Deployability,"Actually deploying this is probably blocked by correcting CPU affinities on the benchmarking machine, but writing the code for this should be manageable otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3013#issuecomment-2275866445:9,deploy,deploying,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3013#issuecomment-2275866445,1,['deploy'],['deploying']
Deployability,"Actually, I added two commits to my master branch and one was about the release notes. But then instead of pushing to my fork, I pushed these to the master branch of scanpy repo by mistake. Fortunately, there were just these two commits that I wanted to add to this PR, so everything should be all right. Sorry for the confusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-488295556:72,release,release,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-488295556,1,['release'],['release']
Deployability,"Advantage of networkx is that it's easily installed... But yes, we should remove it in the future. I think with anaconda, one gets all the igraph and louvain stuff to work very easily without compiling. Without using Grohlke's binaries... One just needs to document this probably. At the latest when igraph and louvain are easily installed, networkx can be removed... PS: I'm currently preparing scanpy 1.0; there will be some slight changes to make the API less redundant... So for now, please no big changes...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822:42,install,installed,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370144822,2,['install'],['installed']
Deployability,"After @ilan-gold mentioned that scanpy’s tutorials are actually not reproducible, I made an issue for that: https://github.com/scverse/scanpy-tutorials/issues/79. Maybe we need to address that before the release, that’ll also get rid of the warnings. If you need to suppress them, I think this extension could be an acceptable solution: https://github.com/picnixz/sphinx-zeta-suppress",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2636#issuecomment-1904033065:204,release,release,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636#issuecomment-1904033065,1,['release'],['release']
Deployability,After https://github.com/theislab/scanpy/pull/1156 I will update the function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1213#issuecomment-630597880:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630597880,1,['update'],['update']
Deployability,After my hard drive and computer crashed (which is why I respond so late) I was able to update my python env. So far it is working. Thanks a lot!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121#issuecomment-1025090712:88,update,update,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121#issuecomment-1025090712,1,['update'],['update']
Deployability,"Ah I see @aeisenbarth: https://github.com/scverse/scanpy/pull/2999 solved this, so this will be resolved in an upcoming release: https://github.com/scverse/scanpy/blob/main/docs/release-notes/1.10.2.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2865#issuecomment-2124170474:120,release,release,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2865#issuecomment-2124170474,2,['release'],"['release', 'release-notes']"
Deployability,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553087327:190,install,installed,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553087327,1,['install'],['installed']
Deployability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:167,install,installed,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['install'],['installed']
Deployability,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094:63,install,installs,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094,2,"['install', 'upgrade']","['installs', 'upgrade']"
Deployability,"Ah, my bad, I read the path wrong on my phone. In general, the most recent numba release cycle had a lot of deprecations, so many packages are throwing numba warnings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1242#issuecomment-632470036:81,release,release,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1242#issuecomment-632470036,1,['release'],['release']
Deployability,"Ah, thanks for the update!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1605#issuecomment-766288497:19,update,update,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605#issuecomment-766288497,1,['update'],['update']
Deployability,"Ah, yeah that can cause problems. I'm actually a little surprised you've gotten as far as ingest, so maybe the situation has improved. Do you have multiple installs of anndata if you `conda list | grep anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037697731:156,install,installs,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037697731,1,['install'],['installs']
Deployability,"Ah, yeah we need to fix the visibility of the builds. Hmm, so you can’t see the build [here](https://icb-scanpy--1306.com.readthedocs.build/en/1306/external/scanpy.external.pp.harmony_integrate.html)?. It still has issues:. ![grafik](https://user-images.githubusercontent.com/291575/88098047-6dc0ad00-cb99-11ea-8c30-f11ee3a820fb.png). you need to add blank lines before the `>>>` i guess. Also ![grafik](https://user-images.githubusercontent.com/291575/88098511-30a8ea80-cb9a-11ea-987b-c2fa929f36d6.png) again. You need to either give us acess to WarrenLab/scanpy@harmony or continuously merge master until you pressing the “merge upstream changes” button and us hitting the “squash & merge” button happens fast enough. forcing up-to-date branches this way is a bit annoying, but it’s the only way to be sure conflicts in changes don’t break everything. (well, the only way without switching to [bors](https://bors.tech/))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306#issuecomment-662064808:575,continuous,continuously,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662064808,1,['continuous'],['continuously']
Deployability,Aha okay. My problem was resolved when I updated the AnnData package for converting pandas dataframe into AnnData object using. '''adata = sc.AnnData(x)''',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475779409:41,update,updated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475779409,1,['update'],['updated']
Deployability,"All good, thanks for the update!; Glad to hear it works now as you'd expect it to :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2592#issuecomment-1743077209:25,update,update,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592#issuecomment-1743077209,1,['update'],['update']
Deployability,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:103,install,install,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434,4,['install'],['install']
Deployability,"Alright, I think I found the problem and hopefully fixed it, although I am not sure as I don't completely understand how the `_get_color_source_vector` function works. If you'd like to try my fix, it's in my fork:. ```; git clone https://github.com/WarrenLab/scanpy.git; cd scanpy; git checkout use_raw_fix; pip install .; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1277#issuecomment-703931637:312,install,install,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703931637,1,['install'],['install']
Deployability,Also looking forward to this update,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1552#issuecomment-1110180699:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1552#issuecomment-1110180699,1,['update'],['update']
Deployability,"Also running into this now with pertpy. . ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; h5py 3.9.0; igraph 0.9.7; joblib 1.3.1; kiwisolver 1.4.4; leidenalg 0.10.0; llvmlite 0.40.1; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; nvfuser NA; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.3; psutil 5.9.5; pyarrow 12.0.1; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.1; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; sphinxcontrib NA; sympy 1.12; texttable 1.6.7; threadpoolctl 3.2.0; torch 2.0.1+cu117; tqdm 4.65.0; typing_extensions NA; wcwidth 0.2.6; yaml 6.0.1; -----; Python 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]; Linux-6.4.3-arch1-2-x86_64-with-glibc2.37; -----; Session information updated at 2023-07-19 12:57; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1641870947:850,update,updated,850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1641870947,1,['update'],['updated']
Deployability,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-690964260:68,update,update,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690964260,1,['update'],['update']
Deployability,"Also, consider setting `flit >=3.4` for editable installs via [PEP 660](https://www.python.org/dev/peps/pep-0660/) 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1776#issuecomment-959756849:49,install,installs,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776#issuecomment-959756849,1,['install'],['installs']
Deployability,"Also, does the `enrich()` function run GProfiler as an ordered or unordered query? Can I toggle this using `gprofiler_kwargs`? (This question may be more appropriate for the gprofiler-official page, but I can't find one). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1901#issuecomment-869271735:89,toggle,toggle,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1901#issuecomment-869271735,1,['toggle'],['toggle']
Deployability,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:392,update,updated,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348,1,['update'],['updated']
Deployability,"And now I checked that the problem exists in the master branch. Versions used below:. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.10.0.dev57+g08be4e9a; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; google NA; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.4; invgauss_ufunc NA; ipykernel 6.22.0; ipywidgets 8.0.6; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; setuptools_scm NA; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; yaml 6.0; zmq 25.0.2; zoneinfo NA; -----; IPython 8.12.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-03 02:03. </p>; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531836933:1753,update,updated,1753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531836933,1,['update'],['updated']
Deployability,"And yes, should get a bug fix release note.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2231#issuecomment-1158944115:30,release,release,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2231#issuecomment-1158944115,1,['release'],['release']
Deployability,Any update on this ? I'm still getting the same error even with the development version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1944940981:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1944940981,1,['update'],['update']
Deployability,Any update on this? Can you add a test (probably reusing the example already in the method docstring)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/812#issuecomment-537020598:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537020598,1,['update'],['update']
Deployability,Any update on this? I can't figure out a way to run PCA on a specified layer & get the additional returns beyond the PCA plot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1308#issuecomment-943177025:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-943177025,1,['update'],['update']
Deployability,Any update on this? I encountered the same issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1549#issuecomment-1739649092:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1549#issuecomment-1739649092,1,['update'],['update']
Deployability,Any update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/637#issuecomment-495211738:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637#issuecomment-495211738,1,['update'],['update']
Deployability,"Any update? I have the same issue with scanpy==1.8.1. The tutorial data has different row lengths and it works, so that can't be the problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1103990647:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1103990647,1,['update'],['update']
Deployability,Any updates here? I'd love to add this to an analysis tool UI I'm working on (and presenting at a conference this weekend). Very happy to promote scanpy there.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-363210986:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-363210986,1,['update'],['updates']
Deployability,Any updates on this one @flying-sheep? I keep having the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/662#issuecomment-499112975:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662#issuecomment-499112975,1,['update'],['updates']
Deployability,Any updates on this thread?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-788017481:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-788017481,1,['update'],['updates']
Deployability,Any updates on this? Has `correlation_matrix()` been removed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-2271053765:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-2271053765,1,['update'],['updates']
Deployability,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after?; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1069#issuecomment-624821926:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069#issuecomment-624821926,1,['update'],['updates']
Deployability,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. 😒 All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347:210,release,releases,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347,1,['release'],['releases']
Deployability,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:21,continuous,continuous,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102,2,['continuous'],['continuous']
Deployability,Are there any updates concerning this PR? scTransform functionality in scanpy would be much appreciated :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-694955289:14,update,updates,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-694955289,1,['update'],['updates']
Deployability,"Are you able to run `import tables` in this environment? If not, I think the issue will be with installation of the `pytables` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1284#issuecomment-646467845:96,install,installation,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1284#issuecomment-646467845,1,['install'],['installation']
Deployability,"Are you sure about numba 0.43? This very much looks like a bug in numba. > It seems that `top_segment_proportions_sparse_csr` is new for scanpy 1.4.5. What makes you think that? It’s been there since @ivirshup added `calculate_qc_metrics` in #316. A second way for this to fail is:. ```pytb; NotImplementedError: No definition for lowering UniTuple(int64 x 2).shape; ...; numba.errors.LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); No definition for lowering UniTuple(int64 x 2).shape. File ""_qc_.py"", line 390:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; prev = 0; for j, n in enumerate(ns):; ^. [1] During: lowering ""$phi382.1_shape.158 = getattr(value=$380for_iter.2, attr=shape)"" at _qc.py (408); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978#issuecomment-572698263:424,pipeline,pipeline,424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572698263,1,['pipeline'],['pipeline']
Deployability,"Are you sure that the genes are in `adata.var_names` in the gene symbol format that you are using to subset the object? In other words, is `'Ada' in adata.var_names` `True`? I'd just like to check whether you don't have e.g., Ensembl IDs as your variable names by chance. Regarding normalization... there are other normalization methods. I believe a method was recently added to scanpy to use only a particular fraction of genes to calculate size factors (avoiding genes that make up >5% of the total counts). Otherwise, we have recently compiled a best practices pipeline in the group, which uses Scran's pooling strategy to normalize the data. This is implemented in R, but can easily be used in a python-based workflow via [`anndata2ri`](www.github.com/flying-sheep/anndata2ri). A case study using the best practices (with scran and anndata2ri) is available [here](www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785:564,pipeline,pipeline,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785,1,['pipeline'],['pipeline']
Deployability,Are you sure you haven't set wxpython as the matplotlib backend somehow? Many of us use scanpy on macOS but I never manually installed wxpython...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302#issuecomment-653022657:125,install,installed,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653022657,1,['install'],['installed']
Deployability,As a side-note: there isn't much harm in always returning the resulting object (rather than `None` when `copy=False`). It doesn't use memory (just another reference) and it allows for a more functional style of writing a processing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403163212:232,pipeline,pipeline,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403163212,1,['pipeline'],['pipeline']
Deployability,"As an added note, it would be great to see this 'gene_symbol' argument used uniformly across the plotting functions. We've had to handle it in pretty hacky ways to make it work throughout the pipeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/380#issuecomment-443101144:192,pipeline,pipeline,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/380#issuecomment-443101144,1,['pipeline'],['pipeline']
Deployability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,integrat,integration,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['integrat'],['integration']
Deployability,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:6,update,update,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919,1,['update'],['update']
Deployability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:35,integrat,integrate,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,1,['integrat'],['integrate']
Deployability,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906:42,install,install,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906,2,['install'],"['install', 'installation']"
Deployability,"As noted in [the installation docs](https://scanpy.readthedocs.io/en/stable/installation.html), scanpy is not distributed via bioconda. It should be installed with `conda install -c conda-forge scanpy`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2282#issuecomment-1160766533:17,install,installation,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2282#issuecomment-1160766533,4,['install'],"['install', 'installation', 'installed']"
Deployability,"As per the referencing issue, it looks like this should be fixed by the next bbknn release in a day or two. Thanks for the bug report @jipeifeng!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/632#issuecomment-490045550:83,release,release,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632#issuecomment-490045550,1,['release'],['release']
Deployability,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248,3,['install'],"['install', 'installation']"
Deployability,"As you can see from the error output, this is an error within loompy. I assume you don't have the most recent version of loompy - there used to be a few bugs in it. Try with an update installation of loompy. I'm running 0.2.8 and never had any problem with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091:177,update,update,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/154#issuecomment-389513091,2,"['install', 'update']","['installation', 'update']"
Deployability,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:161,install,installation,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516,4,['install'],"['install', 'installation', 'installed']"
Deployability,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625:76,update,update,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625,2,"['install', 'update']","['installed', 'update']"
Deployability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,integrat,integrate,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,1,['integrat'],['integrate']
Deployability,"At this point I'm not a big fan of moving back to bioconda either.; * anndata is not bio-specific and should go to conda-forge anyway; * it's debatable if it was a mistake to move scanpy, but moving it back causes confusion and more harm than good IMO. > Why have separate package registries for biology vs everything else?. probably because bioconda predates conda-forge? . > Just saw there's already a pr for this!; > ; > https://github.com/BioContainers/multi-package-containers/pull/2209. The only downside of this is that we need to update that file manually for every release of scanpy/anndata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160571955:538,update,update,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160571955,2,"['release', 'update']","['release', 'update']"
Deployability,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:61,release,release,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['release'],['release']
Deployability,"Awesome, thanks for this! I've added something to the release notes, let me know if you'd like to say more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-665509704:54,release,release,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-665509704,1,['release'],['release']
Deployability,"Based on [this conversation](https://discourse.scverse.org/t/scgen-generate-irreproducible-output/1622/3) on the scverse discourse site I found out that the behavior shown here does not happen if I install scanpy and pytorch (CUDA) from PyPI. It is however still happening if I install scanpy and pytorch (CUDA) using conda. I still do not know why this is happening but this shows it is not likely a problem with scanpy but some strange interaction. I will leave here the description of the packages installed in both cases for reference, and then close the issue:. 1. The behavior shown in this issue does not happen if I create an environment in conda with python installed, and then install all packages using pip like this:. ```; conda create -n scanpy_test1 python; pip install scanpy leidenalg scvi-tools; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118; ```. This allows me to have GPU support with scvi-tools and different runs are reproducible. The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; absl-py 1.4.0 pypi_0 pypi; adjusttext 0.8 pypi_0 pypi; aiohttp 3.8.5 pypi_0 pypi; aiosignal 1.3.1 pypi_0 pypi; airr 1.4.1 pypi_0 pypi; anndata 0.9.1 pypi_0 pypi; anyio 3.7.1 pypi_0 pypi; arrow 1.2.3 pypi_0 pypi; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; async-timeout 4.0.2 pypi_0 pypi; attrs 23.1.0 pypi_0 pypi; awkward 2.3.1 pypi_0 pypi; awkward-cpp 21 pypi_0 pypi; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backoff 2.2.1 pypi_0 pypi; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pypi_0 pypi; blessed 1.20.0 pypi_0 pypi; brotli-python 1.0.9 py311ha362b79_9 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2022.12.7 pypi_0 pypi; charset-normalizer 2.1.1 pypi_0 pypi; chex 0.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:198,install,install,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,7,['install'],"['install', 'installed']"
Deployability,Btw: I plan to release version 0.1 later today. Together with benchmarks and many examples for the 10x datasets. Any objections?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298884393:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298884393,1,['release'],['release']
Deployability,"Bumped so it's up to date, and added release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1936#issuecomment-961318640:37,release,release,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1936#issuecomment-961318640,1,['release'],['release']
Deployability,"Bumping matplotlib needs updating of google Colab's default matplotlib. As Colab imports matplotlib on start-up this means you have to restart the runtime after installing scanpy. For production I think it's fine, tutorials would require to restart after installing scanpy.; You can see this behavior in scverse tutorials using Colab, e.g. https://docs.scvi-tools.org/en/stable/tutorials/notebooks/api_overview.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208#issuecomment-1089493597:161,install,installing,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208#issuecomment-1089493597,2,['install'],['installing']
Deployability,But I am not sure about the readability and the order of operations. `a/b*c` is equal to `a/(b*c)` or `(a/b)*c`? . that why I did it with parenthesis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161:70,a/b,a/b,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161,2,['a/b'],['a/b']
Deployability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,integrat,integrate,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,1,['integrat'],['integrate']
Deployability,"Came across this, and just want to add we are using poetry on scvi-tools and it's been pretty painless thus far. > no good way to editably install into some env:. If you look at our pyproject file, you can add one line that allows `pip -e .` type installation. I don't actually use poetry to create the development environment. The only issue that I haven't quite figured out is how to get the `scvi.__version__` to work on editable install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130:139,install,install,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130,3,['install'],"['install', 'installation']"
Deployability,"Can confirm that `tissue_positions.csv` is not properly detected in 1.9.3 installed off pip, but everything works fine in 1.10.0.dev87+gd08518f5 installed off GitHub.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651665498:74,install,installed,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651665498,2,['install'],['installed']
Deployability,Can confirm the updated tutorial works. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1072774695:16,update,updated,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1072774695,1,['update'],['updated']
Deployability,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:212,update,update,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461,1,['update'],['update']
Deployability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:888,release,releases,888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,2,['release'],['releases']
Deployability,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430389498:296,continuous,continuous,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430389498,2,['continuous'],['continuous']
Deployability,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633#issuecomment-2283901014:344,update,update,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633#issuecomment-2283901014,1,['update'],['update']
Deployability,"Check out scanpy’s optional features:. https://github.com/scverse/scanpy/blob/21ca328672646d7d0ba42b64eee2823babc2d2ed/pyproject.toml#L133-L145. as you can see `scanpy[louvain]` will install it, while `scanpy[leiden]` will install the successor.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203:183,install,install,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203,2,['install'],['install']
Deployability,"Closing as:. * The original issue looks like a numba related problem, which I believe didn't work with python 3.9 at the time.; * We're past the 1.7 release series and aren't supporting it anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-963435024:149,release,release,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-963435024,1,['release'],['release']
Deployability,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624:35,integrat,integrated,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624,1,['integrat'],['integrated']
Deployability,"Completely agree, Gökcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:380,install,installation,380,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759,1,['install'],['installation']
Deployability,"Continuous color schemes are given with the `color_map` argument, categorical schemes are given with `palette`. All the scatter plots (`scatter`, `pca`, `tsne`, `umap`, etc...) share these arguments. Here's an example:. ```python; import scanpy as sc; import matplotlib as mpl; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color=[""louvain"", ""HES4""]); sc.pl.umap(adata, color=[""louvain"", ""HES4""], palette=""Set2"", color_map=mpl.cm.Reds); ```. It's not that clearly documented for `umap`, and is pretty easy to miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018:0,Continuous,Continuous,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476#issuecomment-462582018,1,['Continuous'],['Continuous']
Deployability,"Copying using the `copy` module is a bit ill defined for `AnnData` objects currently. This has to do with some internals of how we do views of arrays. In general I'd recommend doing copies via `adata.copy()`, which performs a deep copy. But it looks like there might be another problem with the PCA not being exactly reproducible. After a fair amount of checking that it was exactly reproducible, it looks like we forgot to actually pass the random seed... There has been fixed, and there will be a bug-fix release soon (#1240). This still does not fix the issue of reproducibility if you've made a shallow copy of a AnnData view with `copy`. I'll have to look into this a bit more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443:507,release,release,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443,1,['release'],['release']
Deployability,Could this get a notice in the docs/ something in the release notes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540#issuecomment-748754780:54,release,release,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540#issuecomment-748754780,1,['release'],['release']
Deployability,"Could you give examples of how you'd like scanpy to work with MetaCell? It looks like a pretty comprehensive pipeline, so I'm a little unsure of at what point in an analysis you'd want to use scanpy with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1016#issuecomment-583249005:109,pipeline,pipeline,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1016#issuecomment-583249005,1,['pipeline'],['pipeline']
Deployability,"Could you install the newest bugfix release of anndata and try again? Failing that, I'd recommend upgrading `h5py` and seeing if that works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987,2,"['install', 'release']","['install', 'release']"
Deployability,Could you please add a release note?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3220#issuecomment-2324786748:23,release,release,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220#issuecomment-2324786748,1,['release'],['release']
Deployability,Could you pull the current version (0.2.7) from github or install it via pip? Everything should be back to normal now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324379251:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324379251,1,['install'],['install']
Deployability,Could you update your scipy with `pip install -U scipy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967:10,update,update,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967,2,"['install', 'update']","['install', 'update']"
Deployability,"Could you update your version of scanpy and see if the issue persists? I believe this issue was an incompatibility with the 1.3.0 release of pandas https://github.com/pandas-dev/pandas/issues/42376, which was fixed for scanpy 1.8.1 (#1917)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084:10,update,update,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084,2,"['release', 'update']","['release', 'update']"
Deployability,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:31,install,install,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883,6,['install'],"['install', 'installed', 'installs', 'installs-runtime-layout-not-hooks']"
Deployability,"Currently matplotlibs colors maps are dealing with things, it's just their default ""bad color"" is often transparent. We currently don't do any handling of color maps, which makes dealing with continuous values a little more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1355#issuecomment-674744108:192,continuous,continuous,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-674744108,1,['continuous'],['continuous']
Deployability,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**; ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244:568,update,update,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244,1,['update'],['update']
Deployability,Dear @jfnavarro . we recommend that you install the latest scanpy version from conda-forge. ; We are currently updating our installation instructions https://github.com/theislab/scanpy/pull/1974,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620:40,install,install,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620,2,['install'],"['install', 'installation']"
Deployability,"Dear Bo, sorry for the late response. I just became the father of twins a few days ago and couldn't respond earlier. If you still need an answer, I will look into this tomorrow. If you have `h5ls` installed on the command line, it would be great to show me the output of running `h5ls your_file.h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56#issuecomment-354681849:197,install,installed,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354681849,1,['install'],['installed']
Deployability,"Dear Davide, again sorry for the late response... I'll, of course, merge this and you can directly push such small things on the master... . In the meanwhile, I've made another release (see notes on https://scanpy.readthedocs.io). It would be cool to further work on the gene scoring with the small amendments (default gene list class, a small notebook with a use case) to get this in a new release. Also, in this context. I know think that `score_genes` instead of `score_gene_lists` would be better. The `_lists` does not contain actual information and is self-understood. It would also fit nicely with other functions (`cluster_genes`, e.g.). Until all of this is fixed, I removed this stuff from the API docs... Have a good start into the week!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/82#issuecomment-364915958:177,release,release,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-364915958,2,['release'],['release']
Deployability,"Dear scverse authors (@flying-sheep, @LucaMarconato et al.), because of issues like these, maybe think about a sophisticated staging environment a la Bioconductor to ensure that packages are compatible with each other. If you do a `conda install scanpy` that finishes without any error, it cannot be that `import scanpy` fails with an error resulting from an incompatible matplotlib version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2362123395:238,install,install,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2362123395,1,['install'],['install']
Deployability,"Did you fix this already? The `CheckBuild` job’s task “Build & Twine check” looks fine to me:. For master:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.9.0.dev2+gc9b59137-py3-none-any.whl: PASSED; Checking dist/scanpy-1.9.0.dev2+gc9b59137.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```. For the 1.8.x branch:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090:113,Install,Installing,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090,4,"['Install', 'install']","['Installing', 'installed']"
Deployability,"Difficult to say something from the trace, it might be related to old setuptools version. You may try updating it. Alternatively, you can set up a [miniconda installation](https://scanpy.readthedocs.io/en/latest/installation.html#installing-miniconda) in your home folder to have more up-to-date packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148#issuecomment-386885473:158,install,installation,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386885473,3,['install'],"['installation', 'installing-miniconda']"
Deployability,"Do we know when UMAP 0.4 is due for a release? I wouldn't want to put too much effort into ensuring compatibility with something unstable. If it's really useful now, maybe it's worth it, but it could change again before something gets released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779#issuecomment-524177435:38,release,release,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524177435,2,['release'],"['release', 'released']"
Deployability,Do you also mean minimal dependency versions? Because Rust’s cargo e.g. has `carg update -Z minimum-versions` which allows people to figure out if their minimum version bounds are truthful or lies (i.e. to be raised),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942:82,update,update,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942,1,['update'],['update']
Deployability,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:107,install,install,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295,3,['install'],"['install', 'install-failure']"
Deployability,Do you want to add a release note entry?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2733#issuecomment-1799255825:21,release,release,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2733#issuecomment-1799255825,1,['release'],['release']
Deployability,Does pytest track their releases somewhere public? Curious what the timeline is here,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2050152233:24,release,releases,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2050152233,1,['release'],['releases']
Deployability,"Does running `pip install psutil --upgrade` help? If yes, I will update the requirements with a minimal version of psutil so that others don't run into the same problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324465215:18,install,install,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324465215,3,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']"
Deployability,Doing `pip install --user scikit-misc` as the last line says should solve this issue - I hope it worked out!; Will close this as no more followups seen here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352#issuecomment-2090072902:11,install,install,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352#issuecomment-2090072902,1,['install'],['install']
Deployability,"Don't know what this ""review"" process is , but basically it is ready. . De: ""Lukas Heumos"" ***@***.***> ; À: ""theislab/scanpy"" ***@***.***> ; Cc: ""Yves Le Feuvre"" ***@***.***>, ""Mention"" ***@***.***> ; Envoyé: Jeudi 6 Janvier 2022 20:11:35 ; Objet: Re: [theislab/scanpy] Pca loadings n points patch (PR #2075) . [ https://github.com/Yves33 | @Yves33 ] is this ready for review? . — ; Reply to this email directly, [ https://github.com/theislab/scanpy/pull/2075#issuecomment-1006847333 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/ACEYIQUT75OTZC3MUGVAT3DUUXSOPANCNFSM5JWG2IZQ | unsubscribe ] . ; Triage notifications on the go with GitHub Mobile for [ https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675 | iOS ] or [ https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub | Android ] . ; You are receiving this because you were mentioned. Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047:293,patch,patch,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047,1,['patch'],['patch']
Deployability,Done with release 1.3.6 and the warning: https://github.com/theislab/scanpy/commit/35030d28bb4e1e4559449bfe41238523bee0e616,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377138:10,release,release,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377138,1,['release'],['release']
Deployability,"Dot sizes now work, this is ready to be merged... assuming you are happy with how I integrated it into the documentation (I added a separate ""density"" subsection for the plotting tools as I felt it shouldn't really be put in the same category as embeddings.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551:84,integrat,integrated,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551,1,['integrat'],['integrated']
Deployability,"Duplicate of #2236, please follow there for updates",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3103#issuecomment-2413793255:44,update,updates,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3103#issuecomment-2413793255,1,['update'],['updates']
Deployability,"Emm, the code is just from the scanpy tutorial.; https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. I run ingest on pbmc dataset, then meet this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578:99,integrat,integrating-data-using-ingest,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578,1,['integrat'],['integrating-data-using-ingest']
Deployability,Even cooler!. Do you recall whether the memory usage for a million cells was anything prohibitive?. I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812:344,integrat,integration,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812,1,['integrat'],['integration']
Deployability,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:193,release,release,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872,3,"['continuous', 'integrat', 'release']","['continuous', 'integration', 'release']"
Deployability,"Exactly, the error was introduced by some third party update or so. Therefore there was no need for 6e797fa, and it even is is wrong, the second line *needs* to be. ```py; colors = cmap(normalize(mean_flat)); ```. It’s both faster and necessary: `Normalize` determines vmin and vmax from the first time it’s called when they’re not set / set to `None`. And when you call it with `normalize(mean_flat[0])` (what happens in the list comprehension), vmin gets set to `min(mean_flat[0]) == mean_flat[0]` instead of `min(mean_flat)`. please do. ```sh; git reset --hard a4b3ccd88f0412461813838d5435ce0cc0b10883; git push -f; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/390#issuecomment-446104893:54,update,update,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390#issuecomment-446104893,1,['update'],['update']
Deployability,"Excellent, thank you. I'll patch my code accordingly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1176#issuecomment-617141601:27,patch,patch,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1176#issuecomment-617141601,1,['patch'],['patch']
Deployability,"Extracting it would be great. What triggered the test to fail on other branches, I'd assume a matplotlib update?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1598724173:105,update,update,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1598724173,1,['update'],['update']
Deployability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:658,update,update,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,1,['update'],['update']
Deployability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:72,install,install,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,1,['install'],['install']
Deployability,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:670,integrat,integration,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020,2,['integrat'],"['integrated', 'integration']"
Deployability,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427037414:8,install,installed,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427037414,2,['install'],['installed']
Deployability,"Fixed by reverting the change on pynndescent, so users should just get the new release when they install",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508:79,release,release,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508,2,"['install', 'release']","['install', 'release']"
Deployability,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it?; <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935:293,configurat,configuration,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935,1,['configurat'],['configuration']
Deployability,Fixed in sphinx-autodoc-typehints [1.25.3](https://github.com/tox-dev/sphinx-autodoc-typehints/releases/tag/1.25.3),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2829#issuecomment-1912294688:95,release,releases,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2829#issuecomment-1912294688,1,['release'],['releases']
Deployability,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1746#issuecomment-801541188:62,update,update,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746#issuecomment-801541188,1,['update'],['update']
Deployability,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. ; - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to; - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/863#issuecomment-776459742:43,update,updated,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-776459742,2,['update'],['updated']
Deployability,"For context, the option was added in #334, and I think the scope for other feature types was much more limited at the time. > Would it already be worth either making gex_only a required input?. I'm not sure the `gex_only` argument even entirely makes sense anymore. I think a `feature_type` argument would make more sense. Erroring if nothing is passed and there are multiple kinds sounds reasonable to me, as multimodality should be handled explicitly. For backwards compatibility I think deprecation warnings for a release cycle when either `gex_only` is used or nothing is passed and there are multiple feature types present could work. -----------. Moving the 10x reading functions had been discussed in: #1387",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528:517,release,release,517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528,1,['release'],['release']
Deployability,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:4,continuous,continuous,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507,1,['continuous'],['continuous']
Deployability,For me `pip install anndata --upgrade` did the trick. You could also do `pip install anndata==0.8.0` if that's the specific version you want to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295:12,install,install,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:213,continuous,continuous,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302,2,['continuous'],['continuous']
Deployability,"For this PR, I'm thinking I'm going to wait on a response from @flying-sheep to figure out what to do about the sklearn intersphinx problem, then clean it up for a merge. A new issue will be opened up with all the functions that need examples, and those can be added through separate PRs as the release progresses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1632#issuecomment-775794535:295,release,release,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775794535,1,['release'],['release']
Deployability,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745,4,"['install', 'update']","['install', 'updated']"
Deployability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:148,patch,patches,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['patch'],['patches']
Deployability,FuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mo,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:36115,pipeline,pipeline,36115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,Getting the same bug. What's the status on merging this fix into a release? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-618739346:67,release,release,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-618739346,1,['release'],['release']
Deployability,"Glad that it worked, and thanks for reporting the issue. I realized that the requirement for scanpy was `matplotlib>=2.2.2`. I updated it to 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/480#issuecomment-463521833:127,update,updated,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480#issuecomment-463521833,1,['update'],['updated']
Deployability,"Glad to hear it! We've just released `v1.4.4` which has this fix in it, so you can use that if you don't want to be on the development branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-513444766:28,release,released,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-513444766,1,['release'],['released']
Deployability,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:38,Integrat,Integrating,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724,1,['Integrat'],['Integrating']
Deployability,Glad to see this is fixed in newer versions!. No worries about the changing install instructions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063294721:76,install,install,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063294721,1,['install'],['install']
Deployability,"Goal:. Add `dask` use-cases to the scanpy benchmarks so we can understand performance changes. . Nice links:. 1. Example benchmark: https://github.com/scverse/scanpy/blob/main/benchmarks/benchmarks/preprocessing_counts.py; 2. Project we use for benchmarking: https://asv.readthedocs.io/projects/asv-runner/en/latest/index.html; 3. Dask local cluster: https://distributed.dask.org/en/stable/api.html#cluster; 4. Using scanpy and dask: https://scanpy.readthedocs.io/en/stable/tutorials/experimental/dask.html. NOTE: this `read_elem_as_dask` function in the notebook is with anndata 0.11 i.e., `pip install --pre anndata`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519:596,install,install,596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519,1,['install'],['install']
Deployability,Going to implemented in future releases,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2982#issuecomment-2227468431:31,release,releases,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2982#issuecomment-2227468431,1,['release'],['releases']
Deployability,"Good point @ivirshup , by just asking around a bit it seems that no, that's not the case and it seems there is no prefix. From the space ranger [output](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview) it seems there shouldn't be such prefix added. However, to be honest not super confident that this is not gonna happen in the future, since so far there are not do many visum datasets around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1250#issuecomment-634537829:218,pipeline,pipelines,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1250#issuecomment-634537829,1,['pipeline'],['pipelines']
Deployability,Good to hear also positive installation results!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437077960:27,install,installation,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437077960,1,['install'],['installation']
Deployability,"Good, yes, in the meanwhile, test coverage should be high enough. I can't think of any major hole anymore. Still, it would be nice to briefly coordinate for Scanpy; at least, still these days. But yes, in this case, please make release 1.3.8!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460614412:228,release,release,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460614412,1,['release'],['release']
Deployability,Graph_tool library would be even better and it also implements SBM and many other things that may be useful in graph analysis of single cells. Unfortunately its installation is a painful experience.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370161196:161,install,installation,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370161196,1,['install'],['installation']
Deployability,"Great info about the figdir, thank you. I think it would be good to be able to access/save both plots, yes. I want this sort of pipeline script to be able to generate the same output for inspection as if the user were running the commands within Jupyter, and both display there. I don't think it's a critical thing though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/73#issuecomment-361946553:128,pipeline,pipeline,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/73#issuecomment-361946553,1,['pipeline'],['pipeline']
Deployability,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress!. @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345:68,integrat,integration,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345,1,['integrat'],['integration']
Deployability,"Great! :smile:. Sure, next Wednesday is fine. Also, we can always simply make 1.4.2. I just need to check one tiny thing before we actually make the release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355:149,release,release,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475598355,1,['release'],['release']
Deployability,"Great! And indeed, it hasn’t been released yet, so if you don’t want one, we don’t need a release note.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2743#issuecomment-1805786269:34,release,released,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2743#issuecomment-1805786269,2,['release'],"['release', 'released']"
Deployability,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests?. Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857:167,integrat,integration,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857,1,['integrat'],['integration']
Deployability,"Great! I'm going to close this then, since the discussion seems to be more about installation issues now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-614611920:81,install,installation,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-614611920,1,['install'],['installation']
Deployability,"Great! Yeah, coronavirus is pretty distracting. This last week has definitely felt like a month for me. So still to do:. * This should be rebased on master; * This should be the default, and this should be noted in the documentation; * Add this to the release notes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-601744153:252,release,release,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-601744153,1,['release'],['release']
Deployability,"Great!. Yes, I would have expected that the adjacency matrix will differ slightly and hence, `test_paga_paul15` fails. We'll need to rerun and upload https://scanpy-tutorials.readthedocs.io/en/latest/paga-paul15.html with the new version in that case and also update the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-479420690:260,update,update,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-479420690,1,['update'],['update']
Deployability,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1619#issuecomment-831156546:68,release,release,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-831156546,1,['release'],['release']
Deployability,"Great, but not in a patch release. We should do a feature release really soon anyway. Please add a release note, then this can be merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2798#issuecomment-1882964041:20,patch,patch,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2798#issuecomment-1882964041,4,"['patch', 'release']","['patch', 'release']"
Deployability,"Great, it is. what if I want to add multiple obs_keys (such as lovain, cell_type et al.,), and, when will the new version scanpy be released?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178#issuecomment-399032569:132,release,released,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399032569,1,['release'],['released']
Deployability,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:114,update,update,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047,1,['update'],['update']
Deployability,"Guys, I'll merge this for now so that I can conveniently play around with it in practical settings and potentially improve. We don't need to advertise for now and the next release might be a bit ahead anyway. I'm reading the latest comments as you being essentially positive. ☺️",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-557132848:172,release,release,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-557132848,1,['release'],['release']
Deployability,"Had this issue again recently using python 3.7, and the solution above wasn't enough to solve it. Turns out I also needed to download the tables .whl file: `pip install .\h5py-2.10.0-cp37-cp37m-win_amd64.whl .\tables-3.6.1-cp37-cp37m-win_amd64.whl numpy==1.20.0 --user --force-reinstall`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-954032488:161,install,install,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-954032488,1,['install'],['install']
Deployability,"Had this problem, followed the `scikit-misc` package [issue](https://github.com/has2k1/scikit-misc/issues/12) on a related problem and installed the recommended patch with ; ```; pip install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1""; ```. Seems to work now for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996:135,install,installed,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1489019996,3,"['install', 'patch']","['install', 'installed', 'patch']"
Deployability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,1,['integrat'],['integration']
Deployability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,integrat,integration,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['integrat'],['integration']
Deployability,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:; - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient.; - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced).; - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient); while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. ; Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802:29,integrat,integrated,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802,1,['integrat'],['integrated']
Deployability,"Has anyone found a solution for this? I run into segfault with the same message when trying to run `sc.pp.calculate_qc_metrics` on my M2. Latest clean installation. I have the core dump as well, but I don't know how to get useful information from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076:151,install,installation,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076,1,['install'],['installation']
Deployability,Has this been updated on the latest version of scanpy ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547#issuecomment-1164419928:14,update,updated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547#issuecomment-1164419928,1,['update'],['updated']
Deployability,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:66,install,install,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368,2,['install'],"['install', 'installation']"
Deployability,"Have you rebooted the python after updating anndata? Can you paste the exact error you're seeing? Are you allowed to share the object you're having trouble loading?. OP's error is bizarre, e.g. the last part seems to be pointing to an empty line. Like the package was updated but the python was not restarted. I can't recreate the exact one he's seeing, but I've managed to get other disjoint errors along those lines by updating anndata to 0.8.0 in a second terminal while the python in question is still running.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185:268,update,updated,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185,1,['update'],['updated']
Deployability,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:565,release,release,565,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498,1,['release'],['release']
Deployability,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:73,install,install,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,3,['install'],"['install', 'installed']"
Deployability,"Hej,. I stumbled upon your issue. Test for my PR #1440:. ```; python3 -m venv venv; source venv/bin/activate; pip install -e . ; pip install ""anndata<=0.7.3""; python3 -c ""import scanpy as sc""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510:114,install,install,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510,2,['install'],['install']
Deployability,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:601,integrat,integration,601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375,1,['integrat'],['integration']
Deployability,"Hello @Koncopd ,; 1. New py3.8.12 environment; ```python; conda install pytables; import tables # pass; ```; 2. New py3.8.12 environment; ```python; conda install pytables; import tables # pass; ```; 3. New py3.8.12 environment; ```python; conda install -c conda-forge scanpy; import scanpy # pass; ```; I can run the Scanpy pipeline now. However, I'm wondering whether your team did some slight changes for pca, neighboring, leiden, umap functions (not big enough to introduce a new release) inside of Scanpy v1.8.2.; Because now I rerun my data and I get the UMAP like below,; ![image](https://user-images.githubusercontent.com/75048821/149594142-fcd6b1d1-7b9a-4713-9850-8b2d86a1ff61.png). but it looks like this one month ago, run by the same version of Scanpy v1.8.2 with the same coding.; ![image](https://user-images.githubusercontent.com/75048821/149593173-fc6caf3d-09e4-4b92-89ad-8cdfc1173f3c.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013511702:64,install,install,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013511702,5,"['install', 'pipeline', 'release']","['install', 'pipeline', 'release']"
Deployability,"Hello @Koncopd ,; Thanks for the response!. Step 1: I created a fresh new environment (py3.8.12); ```python; !pip install scanpy[leiden]. Successfully installed anndata-0.7.8 cycler-0.11.0 fonttools-4.28.5 h5py-3.6.0 igraph-0.9.9 joblib-1.1.0 kiwisolver-1.3.2 leidenalg-0.8.8 llvmlite-0.38.0 matplotlib-3.5.1 natsort-8.0.2 networkx-2.6.3 numba-0.55.0 numexpr-2.8.1 numpy-1.21.5 pandas-1.3.5 patsy-0.5.2 pillow-9.0.0 pynndescent-0.5.5 python-igraph-0.9.9 scanpy-1.8.2 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.13.1 stdlib-list-0.8.0 tables-3.7.0 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:114,install,install,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],"['install', 'installed']"
Deployability,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,6,"['Install', 'install']","['Installing', 'install', 'installation']"
Deployability,"Hello @davidhbrann,; Thanks for the response.; I did pip install --user scikit-misc --force in the anaconda powershell, but this bug kept the same. Not solved.; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473:57,install,install,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473,1,['install'],['install']
Deployability,Hello @giovp. I used pip for the installation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210655230:33,install,installation,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210655230,1,['install'],['installation']
Deployability,"Hello @ivirshup , sorry for asking. Is there any update on this issue?; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1032179479:49,update,update,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1032179479,1,['update'],['update']
Deployability,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:213,install,install,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562,2,['install'],['install']
Deployability,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1759#issuecomment-806685821:56,install,installed,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759#issuecomment-806685821,1,['install'],['installed']
Deployability,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:14,update,updates,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,"Here is a patch that fixes the above problem... import matplotlib.colors. #if user defined, then use the vmax, vmin keywords, else use data to generate them...; if ('vmax' in kwds) and ('vmin' in kwds):; _vmax = kwds['vmax']; _vmin = kwds['vmin']; else: ; _vmax = max(mean_flat); _vmin = min(mean_flat) . #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)) ; normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax). I'll submit a pull request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444339817:10,patch,patch,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444339817,1,['patch'],['patch']
Deployability,"Here's a paper that compares various integration methods, with some nice figures showing which ones output corrected counts vs. corrected projections:. https://www.nature.com/articles/s41592-021-01336-8. You could use one of the methods that outputs corrected counts if you need corrected counts for some reason.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673:37,integrat,integration,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673,1,['integrat'],['integration']
Deployability,"Here's the commit that caused the issue: https://github.com/theislab/scanpy/commit/4cb8a61df2628f00ce7d1fff5a3b25dcbe2222ff. So the difference was switching from ColorBarBase to ColorBar, but also it looks like `ColorBarBase` only allows a single positional argument in the most recent `Matplotlib` release. Could do a conditional around matplotlib versions? Or could restrict which versions can be used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020337090:299,release,release,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020337090,1,['release'],['release']
Deployability,"Hey @LouisFaure,. During the Hackathlon last week we talked again about this PR. For the time being we will keep GPU computing functionality out of scanpy and in rapids-singlecell. RSC is now tested with a CI solution. If you want to contribute to rapids-singlecell I would be very happy. Missing functions like Umap and Neighbors are currently getting updated and also ported to RSC.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998:353,update,updated,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998,1,['update'],['updated']
Deployability,"Hey @atarashansky, what's your status with this? We're going for a larger release soon, and I'd really like to have this PR in it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-600122843:74,release,release,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-600122843,1,['release'],['release']
Deployability,"Hey @esrice, thanks for the PR! Those tests are failing due to a recent umap release. This should be fixed on master now, so just merge master into this branch and they should be fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946:77,release,release,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946,1,['release'],['release']
Deployability,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:843,release,release,843,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467,1,['release'],['release']
Deployability,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:237,integrat,integration,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714,1,['integrat'],['integration']
Deployability,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:56,update,updates,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007,1,['update'],['updates']
Deployability,"Hey @sebpott. That feature was implemented after the 1.3.7 release, so it should work if you use the development version or wait until the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-459584385:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-459584385,2,['release'],['release']
Deployability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,integrat,integration,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,2,['integrat'],['integration']
Deployability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1472,release,release,1472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,1,['release'],['release']
Deployability,"Hey Phil! I'm still a bit hesitant to adopt this option. As discussed before, I'm mainly planning on integrating c++ extensions. Then doing everything via Cython seems overhead. Even the latest Cython distributions recommends **not** doing it. See [here](http://cython.readthedocs.io/en/latest/src/reference/compilation.html) and search for *distributing cython modules*. They provide a solution very similar to the one fom stackoverflow that we have adopted right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803:101,integrat,integrating,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803,1,['integrat'],['integrating']
Deployability,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:444,integrat,integrate,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490,2,['integrat'],"['integrate', 'integration']"
Deployability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:173,integrat,integration,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['integrat'],['integration']
Deployability,"Hey guys, thanks for getting back to me. @flying-sheep `pip install -e` isn't working on the flit branch because it doesn't have a `setup.py` -- is there another way to install?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437:60,install,install,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437,2,['install'],['install']
Deployability,"Hey sorry for the delay: . ```; -----; anndata 0.7.5; scanpy 1.9.0; -----; PIL 8.1.2; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.1; dateutil 2.8.1; decorator 4.4.2; fsspec 0.8.7; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; leidenalg 0.8.3; llvmlite 0.34.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.51.2; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.16; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 0.16.0; pygments 2.8.0; pyparsing 2.4.7; pyrsistent NA; pytoml NA; pytz 2021.1; requests 2.25.1; ruamel NA; scipy 1.6.1; send2trash NA; session_info 1.0.0; setuptools_scm NA; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.3; wcwidth 0.2.5; yaml 5.3.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.9; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-4.15.0-112-generic-x86_64-with-glibc2.10; -----; Session information updated at 2022-04-08 14:58; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1092960373:1647,update,updated,1647,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1092960373,1,['update'],['updated']
Deployability,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:294,integrat,integration,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504,1,['integrat'],['integration']
Deployability,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it?. # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python; def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result; ```. <details>; <summary> An example showing this works </summary>. ```python; import networkx as nx; from scipy import sparse; from matplotlib import pyplot as plt; import numpy as np. def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result. # Test data (path graph). G = nx.Graph(); G.add_nodes_from([0,1,2,3]); G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]); adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3); # Fixed circle layout; pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):; nx.draw(; nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608:96,update,updates,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608,1,['update'],['updates']
Deployability,"Hey! Sorry for the late reply:; 1. Yes, a separate file, please.; 2. Put both in the same function. I wouldn't call it coexpression though. Something along the lines of `cell_selection_by_genes()` or just `cell_selection()`.; 3. There is a `sort_order` keyword for plotting which works for continuous covariates. I imagine that should work.; 4. That may be overkill... but it would definitely be interesting. I think MAGIC is in `sc.external` and DCA is also easily usable in this framework. They are not part of the core package though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328:290,continuous,continuous,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-589225328,1,['continuous'],['continuous']
Deployability,"Hey! 😄 . I'd in principle happy if we move the default `scanpy.settings.cachedir` from `./cache/` to `appdirs.user_cache_dir()`. . However, if then any Scanpy installation breaks, as _the main hpc I'm on 1gb of space where appdirs would put these files_, I would probably not make this the default, but choose something like `~/cache-scanpy/`, that is, a visible directory in home (if we really want, `~/.scanpy/` is also fine). Under https://scanpy.readthedocs.io/en/latest/api/index.html#settings, we could also talk about other alternatives. I second Isaac's concern. Like many others, I'm computing on AWS these days and there, the canonical way of making data locally accessible is via EBS volumes. Hence, I'm used to setting the cachedir to that mount point with a visible name, knowing that this can hold a lot of data. I'd manually clean it if something that I don't use often takes too much space. So, I fear that `appdirs.user_cache_dir()` is not smart enough to figure out locations on a Linux system that hold the size of data that we're typically talking about. It would for sure be the right solution for laptops and work stations etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843:159,install,installation,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843,1,['install'],['installation']
Deployability,"Hey!. You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:; `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602:24,integrat,integration,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602,1,['integrat'],['integration']
Deployability,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:82,integrat,integration,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256,5,"['install', 'integrat']","['install', 'installable', 'installed', 'integration']"
Deployability,"Hey!; I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,; Liis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205:189,update,update,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205,2,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:155,continuous,continuous,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527,2,['continuous'],['continuous']
Deployability,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:402,pipeline,pipelines,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859,1,['pipeline'],['pipelines']
Deployability,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:270,pipeline,pipeline,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:131,integrat,integrating,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402,1,['integrat'],['integrating']
Deployability,"Hey,. indeed there seems to be an [issue](https://github.com/mwaskom/seaborn/issues/3522) with our current usage of `seaborn`, not working with `seaborn 0.13.0`.; This has been fixed on the main branch [here](https://github.com/scverse/scanpy/pull/2661), and we'll eventually take over the newest `seaborn` version once this is cleared. For users running into this issue now ; - first check if you indeed have `seaborn 0.13.0`. If yes, then do; - `pip install seaborn==0.12.2` if using pip or; - `conda install seaborn=0.12.2` if using conda. this makes sure you are using the working version of seaborn. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215:452,install,install,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1764383215,2,['install'],['install']
Deployability,"Hi @ChineseBest, installing the following versions in google colab worked for me: `scanpy==1.7.1 pynndescent==0.4.8 numba==0.51.2`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-908461350:17,install,installing,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-908461350,1,['install'],['installing']
Deployability,"Hi @FADHLyemen,. You can export the raw count table (before calculating the percentage) into R for downstream analysis using `edgeR`. You can follow the following link to find further information: https://bioconductor.org/books/release/OSCA/multi-sample-comparisons.html#differential-abundance. Regards,; Mikhael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745:228,release,release,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745,1,['release'],['release']
Deployability,"Hi @GouQiao - it's been a while since this specific incident so I don't 100% remember / have the code anymore. However, I have run into this problem in general when using AnnData and it's usually resolved by one of two paths:. 1. Check the version of `h5py` that you have installed and perhaps it is too new and an older version resolves the issue. ; 2. Some components of AnnData are not implemented in the function, `.write_h5ad()`. One example that comes to mind is the umap or pca transformer. These objects are not handled well by `.h5py` (at least natively in my experience) and are better off saved independently as dictionaries using `pickle`. That being said, I think there is probably a more robust solution I am not aware of - I know in several instances transformers are able to be saved (e.g., the Scanpy tutorials). . Does this help at all? Happy to be of further assistance if possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713:272,install,installed,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713,1,['install'],['installed']
Deployability,"Hi @IfSumia, that’s a very different problem, see the last line:. ```pytb; ImportError: cannot import name 'colormaps' from 'matplotlib' (/opt/conda/lib/python3.9/site-packages/matplotlib/__init__.py); ```. This probably means that you should update matplotlib and try again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1693549285:243,update,update,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1693549285,1,['update'],['update']
Deployability,"Hi @KabitaBaral1 ,; You can update it using:; ```; pip install --upgrade scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832:28,update,update,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832,3,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']"
Deployability,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954:60,update,update,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954,2,['update'],['update']
Deployability,"Hi @Koncopd, any idea when the new release will be out?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1285#issuecomment-660627731:35,release,release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-660627731,1,['release'],['release']
Deployability,"Hi @LuckyMD ,; Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911:19,update,updates,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911,2,['update'],['updates']
Deployability,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:573,pipeline,pipeline,573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:308,integrat,integration,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916,3,"['integrat', 'pipeline']","['integrated', 'integration', 'pipeline']"
Deployability,"Hi @LuckyMD,. Sure, I'll work on it, as time allows. Before however, I have a couple of questions. . 1. Do you want it as a separate .py file in the tools module (similar to _dendrogram.py)?; 2. I also found interesting to look at exclusive expression of one gene and not the other. Would you be interested in adding a function for that as well and if so, should be a separate one or somehow integrated with coexpression?; 3. Turning values into categorical works, however now I have problem that the True (coexpressing) cells are not always plotted on top. Do you know how to do it in scanpy? I tried by setting `pd.Categorical(ordered = True)`, however, that doesn't help. ; 4. Could you elucidate on how you want to implement the imputation methods? I've never used them myself. Is there anything available in scanpy already?. And thanks @flying-sheep for showing how to remove the colourbar. I wanted to do it for some of my other plots, so that really helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560:392,integrat,integrated,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560,1,['integrat'],['integrated']
Deployability,"Hi @LuckyMD,. Thank you. Oddly, I went to sleep and tried installing 'python-igraph' again the next day and it worked just fine. I can only assume pip was being moody the first day. Thank you again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-640195237:58,install,installing,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-640195237,1,['install'],['installing']
Deployability,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626:55,install,installed,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626,2,['install'],"['install', 'installed']"
Deployability,"Hi @YiweiNiu,. Please install `python-igraph` and not `igraph` via pip. So run `pip install python-igraph` and get rid of the `igraph` package. You may have to update you `pip` as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-534457495:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534457495,3,"['install', 'update']","['install', 'update']"
Deployability,"Hi @Zethson,; I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-1102829655:52,update,update,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1102829655,1,['update'],['update']
Deployability,"Hi @Zethson,; Thank you so much for the response. Please let us know if any updates/changes are required to be done from our side in the above contribution, we will do them at the earliest. Also, please let us know if we need to do anything else for fixing the issues in the PR at the moment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2409#issuecomment-1432748647:76,update,updates,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409#issuecomment-1432748647,1,['update'],['updates']
Deployability,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:197,integrat,integration,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723,1,['integrat'],['integration']
Deployability,"Hi @cartal!. I believe the spatial branch has been merged into master now. So you no longer need the `@spatial` in there. . @giovp could you update this in the tutorial, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1104#issuecomment-599203000:141,update,update,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104#issuecomment-599203000,1,['update'],['update']
Deployability,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:351,integrat,integration,351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943,1,['integrat'],['integration']
Deployability,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773:328,update,update,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427339773,1,['update'],['update']
Deployability,"Hi @falexwolf, yes I will be making my method available. A [rough version](https://github.com/swolock/woublet) is already on github, and I also played around with adding it to my [scanpy fork](https://github.com/swolock/scanpy) (though not the right way -- I added it to `tl` rather than `pp`). I'll hopefully clean it up and release something more official when I have the chance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-400090424:326,release,release,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-400090424,1,['release'],['release']
Deployability,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:67,integrat,integration,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136,1,['integrat'],['integration']
Deployability,"Hi @flying-sheep , I’m using Scanpy on an HPC system, and even though the administrator updated it to the latest version, I'm still encountering the same error. -----; anndata 0.9.2; scanpy 1.10.2; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 2.1.0; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344:88,update,updated,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344,1,['update'],['updated']
Deployability,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:111,patch,patch,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,2,['patch'],"['patch', 'patched']"
Deployability,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:32,update,updates,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390,1,['update'],['updates']
Deployability,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:199,install,installable,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261,2,"['install', 'integrat']","['installable', 'integrate']"
Deployability,"Hi @geovp!. Yes, I mean the original image that was supplied to SpaceRanger pipeline.; It doesn't have to be a TIFF image - in my experience slide scanners save; JPEG images internally, so there is no value in converting that to TIFF.; Also, it would be cool to use sc.pl.spatial for other technologies - say to; overlay single cell spatial over the microscopy image image. Nice, I was using this hacky way before (if I remember correctly I also; changed spot size in the respective slot) - so it does work. I am wondering if you could add support for a fullres slot with size factor; 1 and explain which variables need to be set for it to work in the tutorial. On Thu, Oct 1, 2020 at 8:32 PM giovp <notifications@github.com> wrote:. > Hi @vitkl <https://github.com/vitkl> ,; > by fullres you mean the tiff image yes? This is not supported for now; > unfortunately, but we are working toward some extensions to make this; > possible (cc @hspitzer <https://github.com/hspitzer> ).; > One hacky way to go about this for now could be to:; >; > - assign the tiff to the hires slot in; > adata.uns['spatial]['library_id']['images']['hires']; > - change the hires scalefactor value to 1 in the respective slot; > This should work. also for plotting the spots in the right size. Of; > course, this is also possible if you replace the ""lowres"" instead.; > Let me know what you think about it and if it works.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1436#issuecomment-702351783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTV5FBT2DB4GKUIZUVNTSITKMBANCNFSM4R5XDYSQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732:76,pipeline,pipeline,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732,1,['pipeline'],['pipeline']
Deployability,"Hi @giovp ,; I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up!. Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065994090:165,release,release,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065994090,1,['release'],['release']
Deployability,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:164,integrat,integration,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277,1,['integrat'],['integration']
Deployability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,integrat,integration,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,1,['integrat'],['integration']
Deployability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,integrat,integrated,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['integrat'],['integrated']
Deployability,"Hi @grst,. We've been working on a best-practices workflow/tutorial for scRNA-sesq data for the past year. The tutorial is based on scanpy, but it also integrates tools from R. You can take a look at a case study which implements the workflow [here](https://github.com/theislab/single-cell-tutorial). The revised manuscript should be submitted this week as well, so I hope it will be out soon. If you like, I can send it to you, if you pass me your email address. Regarding regressing out covariates. @falexwolf has already mentioned that this is very dataset dependent. It is also dependent on the downstream analysis. This is especially true for the covariates you suggest. MT gene expression and cell cycle are both biological, rather than technical covariates. Regressing out biological covariates is generally done to isolate particular processes in the data that you are interested in, while losing global structure in the data. This is helpful especially for trajectory inference, but maybe less so for global exploratory analysis with clustering. For example, cell cycle stage can be a major determinant in the difference between two cell types (e.g. stem cells and proliferating cells like transit amplifying cells). Removing this effect, hides the distinction. MT gene expression is also a biological covariate (as well as a technical indicator of cell stress). Higher levels of MT gene expression can indicate increased respiration in e.g., asthmatic conditions. . An additional unwanted effect to regressing out biological covariates is that biological processes are not independent. Thus, regressing out one process, will partially remove effects of other, downstream processes as well. If you're interested, we can discuss this in more detail. Hope this helps a bit,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594:152,integrat,integrates,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594,1,['integrat'],['integrates']
Deployability,"Hi @helios,. You will have to install scanpy from github to use the fix for this. The latest release (1.3.7) does not yet include the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460306375:30,install,install,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460306375,2,"['install', 'release']","['install', 'release']"
Deployability,"Hi @hl324,. That argument is only available in scanpy 1.10, while you appear to have scanpy 1.9.8 installed. Could you try upgrading scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2981#issuecomment-2040195862:98,install,installed,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2981#issuecomment-2040195862,1,['install'],['installed']
Deployability,"Hi @honghh2018,. This might be an issue you have to raise with Seurat about their ReadH5AD function. From `AnnData` 0.7 the h5ad format has changed a little on disk, so maybe their function is not updated to this yet? Other ways you can go between Scanpy and Seurat are loom files or `anndata2ri` as shown [here](https://github.com/LuckyMD/Code_snippets/blob/master/Seurat_to_anndata.ipynb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1198#issuecomment-623974282:197,update,updated,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1198#issuecomment-623974282,1,['update'],['updated']
Deployability,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out.; I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688:296,release,release,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688,1,['release'],['release']
Deployability,"Hi @ivirshup ,; It just fixed when I installed the library directly from pip. Since I was following the documentation for library installation, the command mentioned in the documentation is downloading an outdated version. . Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903:37,install,installed,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903,2,['install'],"['installation', 'installed']"
Deployability,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:247,update,updated,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656,2,['update'],"['update', 'updated']"
Deployability,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:27,update,updates,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,1,['update'],['updates']
Deployability,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1703#issuecomment-788515341:127,release,releases,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703#issuecomment-788515341,1,['release'],['releases']
Deployability,"Hi @ivirshup, ; I am part of Intel Labs and we are trying to accelerate the genomics pipeline. We are trying to push some changes into scanpy details about which are mentioned in the blog : [https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715#:~:text=Intel%20Labs%20has%20accelerated%20a,of%20a%20single%20A100%20GPU.](url) ; We are facing some issues while pushing some changes in the leiden and louvain. The error states some issues with pca in the scanpy/tests/external/test_scrublet.py::test_scrublet_params when we have not made any changes for the same. Can you please help us to resolve this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2409#issuecomment-1429441613:85,pipeline,pipeline,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2409#issuecomment-1429441613,1,['pipeline'],['pipeline']
Deployability,"Hi @ivirshup, it used to work 6 months ago. As discussed in https://github.com/saezlab/omnipath/issues/54#issuecomment-1950265944, it seems it was an error with the package `requests_cache`. Installing the latest version from GitHub solves the issue so I'll close this then, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2861#issuecomment-1950276269:191,Install,Installing,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2861#issuecomment-1950276269,1,['Install'],['Installing']
Deployability,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144:31,install,install,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144,2,['install'],['install']
Deployability,"Hi @natalkon . If you mean to plot them as categories instead of a continuous scale, then the solution is to turn the values into `pd.Categorical` like LuckyMD mentioned. . ```; coex = (adata.raw[:,'{}'.format(gene1)].X.todense() > 0) &; (adata.raw[:,'{}'.format(gene2)].X.todense() > 0); coex_list = [item for sublist in coex.tolist() for item in sublist]; adata.obs['CoEx'] = pd.Categorical(coex_list, categories=[True, False]); ``` . Like I mentioned before, one problem is that the `True` (coexpressing) cells are not always plotted on top when plotting both categories with umap. A better way of visualising is to make use of the `groups` parameter:; ```; sc.pl.umap(adata, color='CoEx', groups=[True]); ```. It will then grey out all the `False` cells and put them in the background.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-768282049:67,continuous,continuous,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-768282049,1,['continuous'],['continuous']
Deployability,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:34,integrat,integration,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935,1,['integrat'],['integration']
Deployability,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:484,integrat,integration,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157,3,"['integrat', 'pipeline']","['integrated', 'integration', 'pipeline']"
Deployability,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:198,integrat,integrated,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147,1,['integrat'],['integrated']
Deployability,"Hi @vitkl !; that's a great idea yes, should be kept updated with most recent methods. ; Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo.; Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1574#issuecomment-757802073:53,update,updated,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574#issuecomment-757802073,1,['update'],['updated']
Deployability,"Hi Alex, ; The psutil issue by updating it has apparently gone away, however later on when I call `sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True)`; , I get the following error. The igraph I am using is V 0.1.11.; Many thanks; Hashem; `DeprecationWarning Traceback (most recent call last); <ipython-input-20-fb44185f2d28> in <module>(); 1 ; ----> 2 sc.tl.louvain(adata_corrected, n_neighbors=10, resolution=1.3, recompute_graph=True); 3 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, n_neighbors, resolution, n_pcs, random_state, flavor, directed, recompute_pca, recompute_distances, recompute_graph, n_dcs, n_jobs, copy); 78 directed = False; 79 if not directed: logg.m(' using the undirected graph', v=4); ---> 80 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 81 if flavor == 'vtraag':; 82 import louvain. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 41 def get_igraph_from_adjacency(adjacency, directed=None):; 42 """"""Get igraph graph from adjacency matrix.""""""; ---> 43 import igraph as ig; 44 sources, targets = adjacency.nonzero(); 45 weights = adjacency[sources, targets]. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457:1550,upgrade,upgrade,1550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324587457,2,['upgrade'],['upgrade']
Deployability,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/320#issuecomment-432491475:98,update,update,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320#issuecomment-432491475,1,['update'],['update']
Deployability,"Hi Alex,. Thanks for fixing this promptly. I will eagerly wait for the update.; I have another query related to usage of this function but I'll create a new issue for that. Parashar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/47#issuecomment-344538383:71,update,update,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344538383,1,['update'],['update']
Deployability,"Hi Benedikt!. Yes, we could make this automatic. Let me briefly think whether this will have any unwanted side effects. There are some advantages of letting the user fully control the annotation dataframes. PS: you could also call `mammary.obs['CellType'].cat.remove_unused_categories(inplace=True)`.; PPS: The mouse atlas example has been written by a first-time-python user, according to what he stated. It can be written a lot more elegantly. We will soon update it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/69#issuecomment-358298098:459,update,update,459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69#issuecomment-358298098,1,['update'],['update']
Deployability,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721:621,install,installation,621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721,1,['install'],['installation']
Deployability,"Hi Davide,. I like the preprint and the blog post. I agree that differential expression testing deserves a classification perspective. Coincidentally, we (with @tcallies) were also working on a little paper that makes this point but used neither logistic regression nor TCCs as covariates... unfortunately, we still haven't updated our benchmarks, but I'd assume that what Lior Pachter does works best. :smile:. Anyways, yes, we should include it at some point but let's still collect some experience... Until then, people can use your two-line workaround. :wink:. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454:324,update,updated,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369860454,1,['update'],['updated']
Deployability,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033:208,install,install,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033,2,"['install', 'update']","['install', 'update']"
Deployability,"Hi Dylan,. This is an issue with the new h5py package, which @ivirshup already fixed on master (https://github.com/theislab/scanpy/commit/928d475a8e2d2901c5744c3afc75e2d5a1b65f29). For now, you can downgrade your h5py package to 2.9.0 using `pip install h5py==2.9.0` as a workaround.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-530529513:246,install,install,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530529513,1,['install'],['install']
Deployability,"Hi Fidel,; Please note new pull request; dotplot can take vmin vmax arguments from user; <https://github.com/theislab/scanpy/pull/390>; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444592024:381,patch,patch,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444592024,1,['patch'],['patch']
Deployability,Hi Gökcen: makes sense!. Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207:129,continuous,continuous,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207,1,['continuous'],['continuous']
Deployability,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:1275,patch,patch,1275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952,1,['patch'],['patch']
Deployability,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693:247,integrat,integration,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693,1,['integrat'],['integration']
Deployability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:15,update,updated,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,1,['update'],['updated']
Deployability,"Hi Jorvis! This should be very easy. Use the text file reader:; ```; adata = sc.read_text(filename).transpose(); ```; or use the general purpose reader that writes cache files automatically; ```; adata = sc.read(filename, ext='txt').transpose() # 'tab', 'data', 'tsv' mean the same; ```; see the [API docs](https://scanpy.readthedocs.io/en/latest/api/index.html). The 'tsv' file ending is not yet in the latest release, I just commited that: https://github.com/theislab/scanpy/commit/884c5f8a6a39c43aef27c7398ec9c195b977a3d3. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056:411,release,release,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056,1,['release'],['release']
Deployability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:293,pipeline,pipeline,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['pipeline'],['pipeline']
Deployability,"Hi Pawel, sorry for the confusion, yes, we just did a major revision. The package is still in the testing phase even though everything should work fine. Any comments from your side would be greatly appreciated!. Packaging will start soon. Development will happen on a development branch from now on. The notebooks are currently being migrated to another repo, links will be updated tomorrow or day after tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-281458534:374,update,updated,374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-281458534,1,['update'],['updated']
Deployability,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i; analysed most of them are from previous code. Regards,; Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great!; >; > The only duplicated code left is that _prepare_weighted_dataframe is very; > similar to _prepare_dataframe. I think you can delete; > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return; > categories, obs_tidy, categorical. Then you can change each line like categories,; > obs_tidy = _prepare_dataframe(…) to categories, obs_tidy, _ =; > _prepare_dataframe(…); >; > Other than that, there’s only few things left:; >; > 1.; >; > The tests without plots should contain assertions. I.e. in; > test_genes_ranking() you should do assert; > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or; > so!; > 2.; >; > For the plot tests, you need to add these lines to the test file:; >; >; > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13; >; > And do each test like this (replace “xyz” with whatever you want):; >; > def test_xyz(image_comparer):; >; > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15); >; > […]; >; > sc.pl.xyz(adata, …); >; > save_and_compare_images('xyz'); >; > This will make the tests save your plots to scanpy/tests/figures and; > compare them to the images in scanpy/test/_images. The tests will fail; > because scanpy/test/_images/xyz.png doesn’t exist. You need to copy; > the pngs from scanpy/tests/figures→scanpy/test/_images and git commit; > them.; > 3.; >; > This needs to be fixed: #644 (comment); > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>; > 4.; >; > I think the test data might be too large. @falexwolf; > <https://github.com/falexwolf> do we have a recommended size for new; > test data?; >; > @Khalid-Usman <https://github.com/Khalid-Usman> I’m sorry if you find; > ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578:20,update,updated,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098578,1,['update'],['updated']
Deployability,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the; tools showed me duplications, which are mostly from other code and 1-2 from; my code. Please have a look into it. It's my first pull request and its; taking too much time :(. Thanks; Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have; > verified my code by keeping weights 1 and it has same values when; > observations has no weights or all weights equal to 1.; >; > I also suggest to update PCA for weighted sampled data.; >; > Thanks,; > Khalid Usman; >; > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; > wrote:; >; >> You can just open a new one, I’ll close this one then 🙂; >>; >> —; >> You are receiving this because you authored the thread.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >> .; >>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-493836074:577,update,update,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-493836074,1,['update'],['update']
Deployability,"Hi Quentin,. When you plot a categorical variable for the first time, scanpy stores the colors for each category in adata.uns, that's why it is modifying your adata. For continuous variables (like your adata.X), it does not do that, hence there is no warning there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526:170,continuous,continuous,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526,1,['continuous'],['continuous']
Deployability,"Hi Raphaël!. Thanks!. What do you mean with ""I just stumbled upon the same error, maybe due to the installation method."" - which error?. Regarding the typo: Hm, are you running Scanpy 0.4.4; if you run an early version, this was 'Phase' with a captical 'P'; since 0.4.3+7 it's 'phase'; like all the other annotations. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/82#issuecomment-368964431:99,install,installation,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368964431,1,['install'],['installation']
Deployability,"Hi Sarah,; thanks for the note and sorry about that; would you install a stable release from PyPi in the meanwhile `pip install scanpy`? I'm currently rewriting quite substantial parts and yes, this is clearly a bug I caused on the weekend; testing will also be more extensive in the future so that this stuff does happen anymore. This kind of stuff will also not happen on master branch in the future; but this rewriting goes along with building some [documentation](https://scanpy.readthedocs.io) and this builds from master... ; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498:63,install,install,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32#issuecomment-324116498,3,"['install', 'release']","['install', 'release']"
Deployability,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:97,release,release,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798,2,['release'],"['release', 'released']"
Deployability,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:114,install,installing,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861,3,"['install', 'release']","['install', 'installing', 'release']"
Deployability,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:271,install,installation,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101,4,['install'],"['installation', 'installed']"
Deployability,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:39,update,updated,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,2,"['Configurat', 'update']","['Configuration', 'updated']"
Deployability,"Hi all!. I just wanted to jump in with @sophietr and say that implementing a cell cycle classification function like Seurat's [CellCycleScoring](https://github.com/satijalab/seurat/blob/master/R/scoring.R) function would be a nice addition to the preprocessing options. Would be valuable to keep an eye on in downstream exploration and could then be easily regressed out if needed. Also, do you guys have any opinions about the inclusion of imputation/smoothing strategies? I've been messing around with including it in analysis pipelines, but still haven't really settled on when to include them. If there's interest, [MAGIC](https://github.com/pkathail/magic) seems like a great option and is currently implemented in Python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-356611882:529,pipeline,pipelines,529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-356611882,1,['pipeline'],['pipelines']
Deployability,"Hi all,. I am trying to use ScanPy for integrating multiple scRNA-Seq samples (~20). Doing so that I can look at RNA Velocity with SCVelo, and want to use MNN as I got good batch effect removal previously in monocle using MNN. Is it true - as stated above, that the current implementation of mnncorrect with ScanPy is only operating on expression values? I have run through a ScanPy MNN [tutorial ](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html) provided by NBI Sweden. The results are improved, but it doesn't appear to work as well as in monocle - some separation by batch is still going on. . I'm wondering what the difference might be? Whether it could be due to the difference in PCA (multi-batch), or the actual MNN / batch effect removal step. Alternatively, I could use the corrected expression matrix, and add the UMAP coordinates/clusters from monocle, although I wonder if this is advisable. . If you have any info please let me know, or if I should raise a separate issue etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967:39,integrat,integrating,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967,1,['integrat'],['integrating']
Deployability,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:306,install,installation,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,6,['install'],"['install', 'installation', 'installed']"
Deployability,"Hi all,; any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value.; Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-463299529:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-463299529,1,['update'],['update']
Deployability,"Hi and sorry for the very late response!. 1. Hm, this seems to be related to your matplolib version and I've never seen this before. The code for the plotting function is [here](https://github.com/theislab/scanpy/blob/a17e9f4bac124547fec1c373da8d12b679c84bcc/scanpy/plotting/preprocessing.py#L11-L46). Try installing matplotlib 2.0.0. 2. The warning can be ignored, in my experience. Soon, we'll catch that case explicitly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39#issuecomment-333509613:306,install,installing,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39#issuecomment-333509613,1,['install'],['installing']
Deployability,"Hi guys, I've been following this thread and it's been quiet recently :) wondering if there's any updates on incorporating ScTransform on Scanpy. Thanks!! 🙏🏼",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-853445582:98,update,updates,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-853445582,1,['update'],['updates']
Deployability,"Hi guys,. Sorry to re-open the thread but I am also getting the same error as described by the OP above with the latest release of Scanpy (v1.4.3). . As with #566 I have no trouble running the rank_genes_groups using t-test or logreg, the problem only arises when using method='wilcoxon'. For the example provided below `adata` is using one of our real data sets and I am using custom clusters but I have the same problem when replicating the tutorial workflow. My colleague has had the same experience when trying to work through the tutorial on his system, again using the most recent release of Scanpy. I've made sure 'log_transformed' was being applied to include the #519 fix provided by **a-munoz-rojas** in the hope that this might help but no such luck, I get the same error either way. Any ideas?. Here is what I ran:. ```; sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); ```. And here is the resulting error:. ```; ValueError Traceback (most recent call last); <ipython-input-117-a5ba74ea872c> in <module>; ----> 1 sc.tl.rank_genes_groups(adata, groupby='Custom_clusters_Leiden', groups=['NKT1','NKT2','NKT17','IL2+ aNKT1','aNKT2 & aNKT17','TNF- aNKT1'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, log_transformed, **kwds); 367 ; 368 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(; --> 369 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)); 370 scores[np.isnan(scores)] = 0; 371 pvals = 2 * stats.distribut",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/530#issuecomment-505305611:120,release,release,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530#issuecomment-505305611,2,['release'],['release']
Deployability,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1085#issuecomment-608561597:20,update,updates,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085#issuecomment-608561597,1,['update'],['updates']
Deployability,"Hi thank you so much for you reply. Initially I didn't understand your former statement, but looking at the features table, I understand what you mean now, it's strange that cellranger mixed species alignment adds mm10 or GRCh38 infront of the gene symbols as well which wasn't letting it find the mito-genes on following the scanpy pipeline at the pre-filtering mito genes steps.; ![Screenshot 2023-01-05 at 15 41 09](https://user-images.githubusercontent.com/122033428/210822795-0eb1f9c8-cf9f-4949-a3cf-33dd07a20b03.png); ![Screenshot 2023-01-05 at 15 43 50](https://user-images.githubusercontent.com/122033428/210822805-3b292da8-35d0-4e4a-a600-12eeff38ac7b.png); .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2393#issuecomment-1372394524:333,pipeline,pipeline,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393#issuecomment-1372394524,1,['pipeline'],['pipeline']
Deployability,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:54,install,install,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859,6,['install'],"['install', 'installed', 'installing']"
Deployability,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-48-abf5bf78cb77> in <module>; ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap); 159 if as_heatmap:; 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 161 timeseries_as_heatmap(; 162 adata.X[adata.obs['dpt_order_indices'].values],; 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)); 198 ax.imshow(; --> 199 np.array(X, dtype=np.float_),; 200 aspect='auto',; 201 interpolation='nearest',. ValueError: setting an array element with a sequence.; ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance!. Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140:1627,Update,Update,1627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140,1,['Update'],['Update']
Deployability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:276,release,release,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['release'],['release']
Deployability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so I’ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I don’t agree. It’s super easy. `Union` is “or”, `Optional` is “or `None`”. If there’s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But it’s really not hard…. Honestly I think the `Callable[…]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:624,install,install,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,2,"['install', 'integrat']","['install', 'integrate']"
Deployability,"Hi! Thanks for the addition. In the release notes I guess something like ""Triku, a new feature selection method was added to our ecosystem"" would be fine. . As for committing to the master, sorry for that. I am eager to see it in the 1.8.0 release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669,2,['release'],['release']
Deployability,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file; ; Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:27,Install,Installing,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,1,['Install'],['Installing']
Deployability,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:148,pipeline,pipelines,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,1,['pipeline'],['pipelines']
Deployability,"Hi! You shouldn’t need to try here: Read the documentation at https://scanpy.rtfd.io to figure out if what you want is in `scanpy` or `scanpy.external`. `scanpy.api` is deprecated, you should never use it. The code @ivirshup gave you will work in the next scanpy release, as scanpy 1.4.4 still has the bug #346. Please upgrade to scanpy from git master and try again. You can do that via:. ```bash; pip install git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532633720:263,release,release,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532633720,3,"['install', 'release', 'upgrade']","['install', 'release', 'upgrade']"
Deployability,Hi! so when installing via pip this is not an issue... somehow with conda it doesnt work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166#issuecomment-614743174:12,install,installing,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-614743174,1,['install'],['installing']
Deployability,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1253,release,release,1253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395,2,['release'],['release']
Deployability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:128,integrat,integrating,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,2,['integrat'],"['integrating', 'integration']"
Deployability,Hi!; Could you finally solve the issue of removing colorbar from the figure? I have tried with the legend_loc=None and legend_loc='none' and they don't remove continuous colorbars. I have tried to fix this issue myself but couldn't find the exact place in the code to do that. Any help would be more than welcome!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-895935559:159,continuous,continuous,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-895935559,1,['continuous'],['continuous']
Deployability,"Hi!; Could you finally solve this issue? I am trying to remove a continuous colorbar from a umap and I cannot make it.; Thanks,. Lídia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-939952451:65,continuous,continuous,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-939952451,1,['continuous'],['continuous']
Deployability,"Hi!; Sorry for the late response. Release 0.3 comes today or tomorrow, with many improvements.; Is the following OK for you?; From http://scanpy.readthedocs.io/en/latest/api/scanpy.api.AnnData.html; ```; >>> adata1 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s1', 's2'],; >>> 'anno1': ['c1', 'c2']},; >>> {'var_names': ['a', 'b', 'c']}); >>> adata2 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s3', 's4'],; >>> 'anno1': ['c3', 'c4']},; >>> {'var_names': ['b', 'c', 'd']}); >>> adata3 = AnnData(np.array([[1, 2, 3], [4, 5, 6]]),; >>> {'smp_names': ['s5', 's6'],; >>> 'anno2': ['d3', 'd4']},; >>> {'var_names': ['b', 'c', 'd']}); >>>; >>> adata = adata1.concatenate([adata2, adata3]); >>> adata.X; [[ 2. 3.]; [ 5. 6.]; [ 1. 2.]; [ 4. 5.]; [ 1. 2.]; [ 4. 5.]]; >>> adata.smp; anno1 anno2 batch; s1 c1 NaN 0; s2 c2 NaN 0; s3 c3 NaN 1; s4 c4 NaN 1; s5 NaN d3 2; s6 NaN d4 2; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/44#issuecomment-344124406:34,Release,Release,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/44#issuecomment-344124406,1,['Release'],['Release']
Deployability,"Hi, . I also met this problem. I am using Scanpy 1.7.2, and could you please suggest which version of BBKNN I should use if I don't want to update Scanpy to 1.8.0. Thanks,; Min",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873#issuecomment-872823146:140,update,update,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872823146,1,['update'],['update']
Deployability,"Hi, . I just stumbled upon the same error, maybe due to the installation method.; Anyway, I got it fixed but since my package was outdated and it's been a while since I used python maybe the next point as already been solved:. -typo issue in the notebook examples (phase instead of Phase); at In [8] when you call the pca.scatter function with color 'phase'. > ValueError: ""phase"" is invalid! specify valid sample annotation, one of ['n_genes', 'percent_mito', 'n_counts', 'dropouts', 'complexity', 'S_score', 'G2M_score', 'Phase', 'X_diffmap0', 'louvain_groups']. Very nice package and notebooks BTW,; Raphaël",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/82#issuecomment-368930312:60,install,installation,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/82#issuecomment-368930312,1,['install'],['installation']
Deployability,"Hi, . I updated the pipeline to use [this singularity container](https://github.com/icbi-lab/borst2021/releases/download/containers-0.2.0/vanderburg_edger.sif). The problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481:8,update,updated,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481,3,"['pipeline', 'release', 'update']","['pipeline', 'releases', 'updated']"
Deployability,"Hi, . Thank you for your interest in scanpy and for raising your question here!. It looks like you are interested in getting the results of `sc.tl.rank_genes_groups`.; For this, we recommend using `sc.get.rank_genes_groups` - you can find more about scanpy’s getters [here](https://scanpy.readthedocs.io/en/stable/api.html#module-scanpy.get). This might look for example like this:. ```py; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", pts=True, use_raw=True). dedf = sc.get.rank_genes_groups_df(pbmc, group=""0""); ```. By accessing the `.uns` as you outlined, just using the ordering of the pts column might not match the ordering of the sorted genes.; This behaviour is subject to updates in the future - in any case `sc.get.rank_genes_groups` is the way to go here :). I hope this helps?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2628#issuecomment-1742964494:744,update,updates,744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2628#issuecomment-1742964494,1,['update'],['updates']
Deployability,"Hi, . Thank you!. 1. I've installed scanpy's git version and set ctrl_as_ref=True with the same results, it does not fix https://github.com/scverse/scanpy/issues/3169. 2. I am not entirely sure what you mean here, would you mind elaborating? This bug fix is specific to the way score_genes() ranks the genes into bins & when there's a lot of zero expression in cells, I'm a bit unsure how this connects to ctrl_as_ref and what you would like me to do. . Thanks again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2263791635:26,install,installed,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2263791635,1,['install'],['installed']
Deployability,"Hi, ; I faced the same problem. I solved it by using the development version of scanpy : ; git clone https://github.com/theislab/scanpy; cd scanpy; pip install -e . For bbknn i just pip installed it . Then : ; bbknn.bbknn(adata_bbknn, batch_key=""Batches""). Hope this can help,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/770#issuecomment-521728652:152,install,install,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770#issuecomment-521728652,2,['install'],"['install', 'installed']"
Deployability,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:96,update,updated,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541,1,['update'],['updated']
Deployability,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:366,integrat,integrated,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575,1,['integrat'],['integrated']
Deployability,"Hi, @AlejandraRodelaRo ; It was fixed on github master. You can wait for a new release or install scanpy from github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141:79,release,release,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141,2,"['install', 'release']","['install', 'release']"
Deployability,"Hi, @KabitaBaral1. These happens because of changes in umap 0.4. Please update scanpy to solve this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-610787898:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-610787898,1,['update'],['update']
Deployability,"Hi, @cakirb ; Try installing `loompy` using `pip install -U loompy`, and make sure you are not using version 2.0.2.; see ; https://github.com/theislab/scvelo/issues/20#issuecomment-442186279. **EDITED**: I am encountering the same problem as yours. > Exception: Data must be 1-dimensional",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-493666985:18,install,installing,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-493666985,2,['install'],"['install', 'installing']"
Deployability,"Hi, @plrlhb12 .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856:124,install,install,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856,2,"['install', 'release']","['install', 'release']"
Deployability,"Hi, @sarajimenez ; there is no ingest in scanpy 1.4.4.post1. You need to update scanpy to 1.4.5 and anndata to 0.7 to use ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092#issuecomment-597292098:73,update,update,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-597292098,1,['update'],['update']
Deployability,"Hi, I am trying to use palantir. I've updated by `pip install git+https://github.com/theislab/scanpy.git`. when I try to import scanpy I get the following error: ; ModuleNotFoundError: No module named 'scanpy.external._tools'. I'm fairly new to python packaging. Apologies if this is an issue with my install method. also not an expert with github project managment. Will gladly post this elsewhere if better. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-477674448:38,update,updated,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-477674448,3,"['install', 'update']","['install', 'updated']"
Deployability,"Hi, I cloned this repo, switched to `modern-rng`, and installed it with `pip`. I was able to reproduce the same error.; ```; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\random\_generator.pyx"", line 622, in numpy.random._generator.Generator.integers; File ""numpy\random\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32""; ValueError: high is out of bounds for int32; ```; I am using numpy 1.26, which is the numpy version required by this branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3041#issuecomment-2332066283:54,install,installed,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3041#issuecomment-2332066283,1,['install'],['installed']
Deployability,"Hi, I just tried install a higher version of scipy, and it works now.; ```; pip install scipy==1.2.1; ```; Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/643#issuecomment-492050558:17,install,install,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643#issuecomment-492050558,2,['install'],['install']
Deployability,"Hi, I think the problem is caused by the two different versions of anndata used. We had similar issues when using anndata 0.7 and 0.8 together. Try to upgrade the version to 0.8. Best; Florian",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310#issuecomment-1221390668:151,upgrade,upgrade,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310#issuecomment-1221390668,1,['upgrade'],['upgrade']
Deployability,"Hi, I worked over the docs completely once but still need to do a few small things (release note, final spell check) but need to leave my desk now - will do the final bits either later today or tomorrow!; Best jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065878828:84,release,release,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065878828,1,['release'],['release']
Deployability,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007:224,integrat,integrating,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007,1,['integrat'],['integrating']
Deployability,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913:122,update,updates,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913,1,['update'],['updates']
Deployability,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']; ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048:10,release,release,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048,1,['release'],['release']
Deployability,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058,1,['install'],['install']
Deployability,"Hi, if you use conda, try `conda install pytables`.; If you don't, try installing from the corresponding wheel here https://www.lfd.uci.edu/~gohlke/pythonlibs/#pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261:33,install,install,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261,2,['install'],"['install', 'installing']"
Deployability,"Hi, just wanted to ask again if there was any update on this. Thanks in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892069618:46,update,update,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892069618,1,['update'],['update']
Deployability,"Hi, please give me a reproducible example that uses only public data or better manually typed data. (`np.array([...])`). Also make sure you updated to numpy 0.47 (not 0.46) and llvmlite 0.31 before trying to reproduce the bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974#issuecomment-572765359:140,update,updated,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572765359,1,['update'],['updated']
Deployability,"Hi, sorry for getting back so late, I was alternatingly really busy and sick. This looks nice! One more thing:. > This is also missing release notes in 1.11.0.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2348356839:135,release,release,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2348356839,1,['release'],['release']
Deployability,"Hi, sorry! deep was nonfunctional, I fixed it in 1.4.4.post1 (just released)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/746#issuecomment-515971919:67,release,released,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-515971919,1,['release'],['released']
Deployability,"Hi, thanks for the contribution!. The converting to dense is quite iffy, we should probably add real support for sparse here. [We could use this as a base](https://github.com/scikit-learn/scikit-learn/blob/45cf8ec555a026c4263e8bef12850755a83df10e/sklearn/utils/sparsefuncs.py#L685). Do you think you can do that or should we help?. This is also missing release notes in 1.11.0.md",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449:353,release,release,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2262820449,1,['release'],['release']
Deployability,"Hi, that certainly seems like an improvement! I took the liberty to split out two bugs: #3168 and #3169. We had a similar fix #2875, which is hidden behind a `ctrl_as_ref` flag. I think since that fix is not yet released, we should rename the flag and unify both fixes behind it. Could you please; 1. check if issue #3169 is already fixed by installing scanpy’s git version and setting `ctrl_as_ref=True`; 2. Our backwards compatibility means that changes to the scoring need to be optional. This is why `tests/test_score_genes.py::test_score_with_reference` is failing here, and that’s what `ctrl_as_ref` is for. 	So since that option is not yet released and in order to fix the test, we should probably change that option to incorporate both improvements. We can rename it to reflect the two things it does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2252216712:212,release,released,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2252216712,3,"['install', 'release']","['installing', 'released']"
Deployability,"Hi, try installing python-igraph from the wheel here - https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph; And after that install louvain via pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/786#issuecomment-522249082:8,install,installing,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522249082,2,['install'],"['install', 'installing']"
Deployability,"Hi,. First, the error that you are reporting has to do with series types of the dataframes. Howerver, it's very difficult to provide inputs, because it's unclear what `database` and `groupA` are. Can you report a reproducible example? Also, can you update to scanpy 1.6?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883:249,update,update,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883,1,['update'],['update']
Deployability,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645#issuecomment-492292184:34,update,updated,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645#issuecomment-492292184,1,['update'],['updated']
Deployability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:24,install,install,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,1,['install'],['install']
Deployability,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928:307,integrat,integrated,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928,1,['integrat'],['integrated']
Deployability,"Hi,. thank you for your PR. Could you please:; 1. Update the body of your PR to introduce and explain what, why and if required how you are doing things.; 2. Why do you think that the volcano plot should go into external? It could go into our core plotting functions, no?; 3. Please try to hardcode as few things as possible. Also, please use the scanpy settings object for plots (e.g. the figure size)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2241#issuecomment-1105162468:50,Update,Update,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2241#issuecomment-1105162468,1,['Update'],['Update']
Deployability,"Hi,. which umap package did you install? Which version is it?; Do you know whether the umap package already has support for Apple M1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898340541:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898340541,1,['install'],['install']
Deployability,"Hi,; I converted the obs and values in string and it worked.; Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>; wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error.; > Here's what I ran:; >; > import scanpy as sc; >; > adata = sc.datasets.pbmc3k(); > sc.pp.filter_genes(adata, min_counts=1); > sc.pp.log1p(adata); > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); > sc.pl.highly_variable_genes(adata); > adata = adata[:, adata.var['highly_variable']]; >; > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22); > and try that?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-516115061:557,update,update,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516115061,2,"['release', 'update']","['releases', 'update']"
Deployability,"Hi,; thanks for fixing this task so quickly! When would you estimate that a new release comes (to pypi) which allows for seaborn>=0.13.0?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1812184578:80,release,release,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1812184578,1,['release'],['release']
Deployability,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts=1); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); sc.pl.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-515844864:393,update,update,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-515844864,2,"['release', 'update']","['releases', 'update']"
Deployability,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:74,integrat,integration,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806,1,['integrat'],['integration']
Deployability,"Hi. So @vals `gprofiler` and `gprofiler-official` are different packages, and having them both installed could cause an issue like this. Is this the case here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896#issuecomment-866565353:95,install,installed,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896#issuecomment-866565353,1,['install'],['installed']
Deployability,"Hi. This is unlikely to be a scanpy issue. You probably don’t have enough memory or there’s some problem with your Jupyter configuration. But in any case, we need more information to tell which one it is. Please share the logs that `jupyter lab` created, especially any stack traces around “kernel died, restarting”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889:123,configurat,configuration,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889,1,['configurat'],['configuration']
Deployability,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:290,integrat,integration,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554,2,['integrat'],['integration']
Deployability,"Hm, PyTorch 1.12.1 doesn’t seem to be available, otherwise I was able to install all package versions you specified. I created an attempt at a reproducer here, but it just runs in less than a second for me: https://github.com/flying-sheep/scanpy-2531. No idea how to further debug this. Looks like you’re on macOS. one of the new M1 or M2 ones?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1609286107:73,install,install,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1609286107,1,['install'],['install']
Deployability,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463:102,install,installation,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439746463,1,['install'],['installation']
Deployability,"Hm. the conda package doesn’t list availability for windows: https://anaconda.org/bioconda/scanpy, just “conda install linux-64 v1.3.7, osx-64 v1.3.7”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462259888:111,install,install,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462259888,1,['install'],['install']
Deployability,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>; <summary> My environment </summary>. ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; IPython 7.21.0; PIL 8.1.0; anndata 0.7.5; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.7.0; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.1; scipy 1.6.1; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; storemagic NA; tables 3.6.1; traitlets 5.0.5; wcwidth 0.2.5; -----; Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-03-20 16:27; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hmm, Maybe they have another scanpy version installed or some other weird broken setup. why not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/603#issuecomment-482103939:44,install,installed,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/603#issuecomment-482103939,1,['install'],['installed']
Deployability,"Hmm, it’s in the source package and the wheel, but maybe it doesn’t get installed? Maybe `include_package_data=True` needs to be restored. How did you install from the source package anyway? Pip always uses wheels if possible, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/995#issuecomment-574587706:72,install,installed,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574587706,2,['install'],"['install', 'installed']"
Deployability,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599163401:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599163401,1,['release'],['release']
Deployability,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:168,integrat,integration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638,1,['integrat'],['integration']
Deployability,Hmm. Did your original object already have a pca computed on it? I'm not sure if the values in `obsm` would have been updated when you made a shallow copy with `copy`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631984912:118,update,updated,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631984912,1,['update'],['updated']
Deployability,"Hmm. I'm not able to replicate the exact error, but I get a different one. Could you try running:. ```python; adata = adata.copy(); ```. right before `normalize_total` and see if that works?. ----------------------. Update: tried on a different machine and could replicate your error. The issue is that `normalize_total` doesn't make sure the anndata object isn't a view before assigning to it. As a work-around, you can just run `adata = adata.copy()` before `sc.pp.normalize_total(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991:216,Update,Update,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991,1,['Update'],['Update']
Deployability,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516291062:316,pipeline,pipeline,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516291062,1,['pipeline'],['pipeline']
Deployability,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883:52,install,install,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883,1,['install'],['install']
Deployability,"Hope that's what you mean by a release note @ivirshup , let me know if it needs anything else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2025#issuecomment-987736310:31,release,release,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-987736310,1,['release'],['release']
Deployability,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636,2,"['integrat', 'update']","['integrated', 'update']"
Deployability,"How I save plots is:. ```python; from matplotlib import pyplot as plt. with plt.rc_context(): # Use this to set figure params like size and dpi; sc.pl.plotting_function(..., show=False); plt.savefig(""path/to/file.extension"", bbox_inches=""tight""); ```. I think how this argument works is one of the things we would change in a major API breaking release. @fidelram, would you add anything to this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-734657400:345,release,release,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-734657400,1,['release'],['release']
Deployability,How did you install scanpy? What conda command did you use?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063418303:12,install,install,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063418303,1,['install'],['install']
Deployability,"How did you installed scanpy?. Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial; > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb; >; > the line sc.pp.neighbors(adata) produces the following error:; >; > Inconsistency detected by ld.so: dl-version.c: 205:; > _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > Ubuntu 18.04; > Python 3.6.6; >; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4; > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Can you help me? Thank You; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350:12,install,installed,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-426896350,3,['install'],"['install', 'installed']"
Deployability,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991:37,integrat,integrate,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991,1,['integrat'],['integrate']
Deployability,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this?. Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364:117,release,releases,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364,2,"['continuous', 'release']","['continuously', 'releases']"
Deployability,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:475,install,installing,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['install'],['installing']
Deployability,"Huh, I can't reproduce but can look into this a bit more. Could you share the results of:. ```python; from importlib.metadata import version as v. v(""anndata""); ```. and. ```python; import anndata. anndata.__version__; ```. ?. Also, anything extra you can tell us about your environment? E.g. did you install with pip or conda? `uv`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037230309:301,install,install,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037230309,1,['install'],['install']
Deployability,"Huh. It sounds like `pip` is installing to a different environment than the one you're using for scanpy. How are you starting the relevant python session? Also, are you sure you're restarted that session after updating scipy?. To check to see if you're in the same environment, these commands should tell you where scipy is installed:. ```sh; pip show scipy. python -c ""import scipy; print(scipy.__file__)""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440:29,install,installing,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440,2,['install'],"['installed', 'installing']"
Deployability,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error?; * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166:161,upgrade,upgrade,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166,1,['upgrade'],['upgrade']
Deployability,I added a test and a release node,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2589#issuecomment-1666905237:21,release,release,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2589#issuecomment-1666905237,1,['release'],['release']
Deployability,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:1089,release,release,1089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998,1,['release'],['release']
Deployability,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031:66,integrat,integration,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031,1,['integrat'],['integration']
Deployability,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980:58,install,installation,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980,1,['install'],['installation']
Deployability,"I also encountered this h5py dll error on a Windows 10 machine when trying to install scanpy. Fixed following these instructions, followed by `pip install numpy==1.20` due to a subsequent numpy version conflict with numba. > > In case anyone has this error again, here is what worked for me:; > > ; > > * go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl; > > * with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded).; > > * scanpy should work now; > > ; > > This worked on mine and also on a colleagues windows laptop.; > > I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.; > ; > this helped me out as well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-1018430040:78,install,install,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-1018430040,5,['install'],"['install', 'installing']"
Deployability,"I also have the same problem and I tried to use `pip install louvain`, but I cannot install the package and it says `legacy-install-failure`. The GitHub for the [louvain](https://github.com/vtraag/louvain-igraph/tree/master) says,. > Warning; > ; > This package has been superseded by the [leidenalg](https://github.com/vtraag/leidenalg) package and will no longer be maintained.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756,3,['install'],"['install', 'install-failure']"
Deployability,I also installed DCA and tensorflow in the meantime... Maybe it has to do with different backend functions being used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/246#issuecomment-416565579:7,install,installed,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/246#issuecomment-416565579,1,['install'],['installed']
Deployability,"I also just found this: https://docs.pytest.org/en/stable/pythonpath.html#import-modes. > `importlib`: new in pytest-6.0, this mode uses importlib to import test modules.; > […]; > makes test modules non-importable by each other.; > […]; > ; > **We intend to make importlib the default in future releases.**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332:296,release,releases,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332,1,['release'],['releases']
Deployability,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/643#issuecomment-494058404:93,install,installed,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643#issuecomment-494058404,2,['install'],"['install', 'installed']"
Deployability,"I also recently encountered this issue. I've dug into the problem a little bit and for me the cause seems to be that the sc.pp.scale function introduces the NaN values. This occurs for columns which show very little variance and are almost constant. According to the current documentation this should not be the current expected behaviour though and should only (possibly) occur in future versions: . `Variables (genes) that do not display any variation (are constant across all observations) are retained and (for zero_center==True) set to 0 during this operation. In the future, they might be set to NaNs.`. So I'm not sure if this is a bug or if the documentation has not been updated yet. . I've currently circumvented the issue by scaling in sklearn (which retains 0s instead of NaNs) and manually loading the scaled results into my adata object as this is the behaviour I would like for my dataset. In case my example dataset would be helpful let me know then I can share it with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706:680,update,updated,680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706,1,['update'],['updated']
Deployability,"I am also currently trying to figure out how to get around this. I am getting this error when calling `sc.pl.rank_genes_groups_violin` using a ""gene_symbols"" parameter (this function works when that parameter is omitted but the x-axis labels are not what I desire). In my case, `adata.uns[""rank_genes_groups""][""names""]` content is set to `adata.var_names` (where the index column is separate from what is being passed to the ""gene_symbols"" parameter outlined earlier). I am running anndata==0.7.8 and scanpy==1.8.2 so this issue has been around for a couple release versions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2256#issuecomment-1131777861:558,release,release,558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2256#issuecomment-1131777861,1,['release'],['release']
Deployability,"I am also getting the error `RuntimeWarning: invalid value encountered in log; dispersion = np.log(dispersion)` when running `sc.pp.highly_variable_genes(adata, min_mean=1.7, max_mean=5, min_disp=0.5, flavor='seurat')` on log scale data in the adata.X slot with mean=0 and max=16.336065. Any ideas?. Update: I just noticed that my adata.X contains a numpy array instead of a sparse matrix. Perhaps that's the issue? Will try updating to a sparse matrix and will report back",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-718294561:300,Update,Update,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-718294561,1,['Update'],['Update']
Deployability,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:163,install,install,163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309,2,['install'],['install']
Deployability,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:20,install,install,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112,2,['install'],['install']
Deployability,"I am closing the issue because it was my mistake: I ran the script that I saved several years ago instead of the updated, current one at https://scanpy.readthedocs.io/en/stable/_sources/tutorials/basics/clustering-2017.ipynb; (where the argument rotation is omitted)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140#issuecomment-2206318149:113,update,updated,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140#issuecomment-2206318149,1,['update'],['updated']
Deployability,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:44,update,updated,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992,1,['update'],['updated']
Deployability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:356,pipeline,pipeline,356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['pipeline'],['pipeline']
Deployability,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:537,update,updated,537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994,2,['update'],"['update', 'updated']"
Deployability,"I am getting this same error, namely when I run `sc.pl.umap(adata, color='pid')` I see `UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored` and then `adata.uns['pid_colors']` all gets set to gray, and my whole UMAP plot looks gray instead of being color by 'pid.'. I am using scanpy 1.9.1 and matplotlib 3.6.3. Can someone advise me what I need to upgrade to avoid this error, or how I can work around it with my current package versions? I see that #2212 upped the required matplotlib to 3.4, but since I'm at 3.6.3 I thought I should be okay. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208#issuecomment-1477955919:387,upgrade,upgrade,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208#issuecomment-1477955919,1,['upgrade'],['upgrade']
Deployability,"I am having the same problem,however pip install anndata --upgrade didn't work for me. pip said it is already the latest version: Requirement already satisfied: anndata in d:\python3.10.9\lib\site-packages (0.9.1), then I really don't know what to do. Could you guys help me with that? [crying]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509:41,install,install,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"I am having this same problem. I updated:; anndata 0.7.5; scanpy 1.6.0. It did not fix the problem for me. . I can try with the pbmc data set, what should I use for the groupby?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-767855186:33,update,updated,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-767855186,1,['update'],['updated']
Deployability,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:421,release,releases,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,4,"['install', 'release']","['install', 'releases']"
Deployability,I am not sure what --add does or -c . The following is probably a cleaner way to install. It should not have any unforeseen 'channels' related side effects. . This worked for me on MacOS Catalina; ```; $ conda create --name SCA python=3.8.2; (base) $ conda activate SCA; (SCA) $ conda install scanpy --channel conda-forge --channel bioconda; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-609560683:81,install,install,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-609560683,2,['install'],['install']
Deployability,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:663,update,update,663,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795,1,['update'],['update']
Deployability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:347,update,updated,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,1,['update'],['updated']
Deployability,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much.; scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477:29,upgrade,upgrade,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477,1,['upgrade'],['upgrade']
Deployability,"I am using a Docker container, but from the settings it seems like the first `anndata` install was with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037684581:87,install,install,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037684581,1,['install'],['install']
Deployability,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:164,release,release,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,3,"['install', 'release']","['install', 'release']"
Deployability,"I believe the issues are related. I think pip is trying to get the version of the wheel by parsing it's name instead. For example, I just ran into this with where scanpy's version was being reported as: `1.8.0.dev34-g8d8039d9` – which I don't think is a valid version. I then `pip install -e`'d and got a version of `1.8.0.dev29+gc8309488` (different branches) and it worked fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-784973197:281,install,install,281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-784973197,1,['install'],['install']
Deployability,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:110,install,installing,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011,4,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,"I can recover the previous behavior, i.e., different runs of the notebook give identical UMAP and leiden clusters, by downgrading to scanpy version 1.9.2 (and also pandas to version 1.5.3). I do this in conda and in this environment other relevant installed package versions are numpy 1.23.5, scipy 1.10.1 and scikit-learn 1.2.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555:248,install,installed,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1531696555,1,['install'],['installed']
Deployability,"I can reproduce from the full tutorial. The issue here is that `scanorama` has updated it's API since this tutorial was written. Now `scanorama.correct_scanpy` returns AnnData objects. @giovp, where should this tutorial live?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049184473:79,update,updated,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049184473,1,['update'],['updated']
Deployability,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists.; >; > PS: I accidentally closed the issue for some reason.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/370#issuecomment-440556607:152,upgrade,upgraded,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370#issuecomment-440556607,1,['upgrade'],['upgraded']
Deployability,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801:128,install,install,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801,2,"['install', 'release']","['install', 'release']"
Deployability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,integrat,integration,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,2,['integrat'],['integration']
Deployability,I checked #1468. Tried conda install pytables. But it did not work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063426953:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063426953,1,['install'],['install']
Deployability,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:255,pipeline,pipeline,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,1,['pipeline'],['pipeline']
Deployability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:184,update,updated,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['update'],['updated']
Deployability,"I did, and then I realized that we have the `import scanpy as sc` change and more features, so I called it 1.4. Btw: could you please add me as owner to scanpy and anndata on PyPI? then I can manage releases and delete files on PyPI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460625486:199,release,releases,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460625486,1,['release'],['releases']
Deployability,I didn't add release note yet because I'm not sure if this should already go into 1.10.4,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3295#issuecomment-2421911169:13,release,release,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295#issuecomment-2421911169,1,['release'],['release']
Deployability,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:167,upgrade,upgrade,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500,11,"['install', 'upgrade']","['install', 'installation', 'installations', 'upgrade']"
Deployability,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. update——————————————; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:38,pipeline,pipeline,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918,4,"['install', 'pipeline', 'update']","['installing', 'pipeline', 'update', 'updated']"
Deployability,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140:22,install,installed,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140,1,['install'],['installed']
Deployability,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785722015:420,release,release,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785722015,1,['release'],['release']
Deployability,"I don't know when it's due, but if it's released tomorrow scanpy will stop working and that scares me 😑",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779#issuecomment-524178694:40,release,released,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524178694,1,['release'],['released']
Deployability,I don't think `igraph` or `louvain` are actually being installed on readthedocs. . I think you'll need to modify the `.readthedocs.yml` for this. But not calling `louvain` could also work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1811#issuecomment-827747018:55,install,installed,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811#issuecomment-827747018,1,['install'],['installed']
Deployability,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1363#issuecomment-678257370:44,update,updates,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678257370,1,['update'],['updates']
Deployability,"I don't think this is a segfault, but a `TypeError`. I believe this is due to using an out of date version of `numba`. Could you update that and let me know if the error persists?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852:129,update,update,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852,1,['update'],['update']
Deployability,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-506145082:57,release,release,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-506145082,1,['release'],['release']
Deployability,I downloaded the folder from GitHub and then installed Scanpy and it worked for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544#issuecomment-475911257:45,install,installed,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544#issuecomment-475911257,1,['install'],['installed']
Deployability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:89,patch,patches,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,4,"['Install', 'install', 'patch']","['Installed-Build-Depends', 'installed', 'patches']"
Deployability,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:87,install,installed,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003,3,['install'],"['install', 'installed']"
Deployability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,1,['integrat'],['integrate']
Deployability,"I figured the one-item-thing out: The emitted code is:. ```rst; :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; :type copy: `bool`, optional (default: `False`). :returns: AnnData, None; Depending on `copy` returns or updates `adata` with the corrected data matrix.; ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694:266,update,updates,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694,1,['update'],['updates']
Deployability,"I first understood the warning that the default will be switched so that we have to be explicit (`flavor=""leidenalg""`) if the old default works for us. But the warning persists, maybe suggesting that we should switch to `flavor=""igraph""`. When switching to `igraph`, we get a deprecation warning `resolution_parameter keyword argument is deprecated, use resolution=... instead` which has been deprecated in igraph some years ago ([7848bcb](https://github.com/igraph/python-igraph/commit/7848bcbc8b81fa248362f56d5593d366836deb1f#diff-cba05fe79beed98bcc3a46ca51cc58e92142b971ce1caebb8c23895101fde8dcR467-R469)). Scanpy has not yet updated the parameter since the [initial](https://github.com/scverse/scanpy/commit/9fa4c0f9abf4b050f6e347565dff24f9b317cb32) leiden implementation, or the deprecation was not noticed. Can users ignore the warning, or should users put an upper cap on the igraph version to be safe?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2865#issuecomment-2112993599:629,update,updated,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2865#issuecomment-2112993599,1,['update'],['updated']
Deployability,I fixed the bug: https://github.com/theislab/scanpy/commit/15593d532fbaa696bf1ea328d1991d31b334e175. . And I'll immediately make a new release and put a warning on the webpage... @Koncopd: Thank you for adding the tests!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446373823:135,release,release,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446373823,1,['release'],['release']
Deployability,I fixed this issue!. It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-615359662:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-615359662,1,['update'],['update']
Deployability,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:130,install,install,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116,5,['install'],['install']
Deployability,"I get an error trying to merge multiples slides using the code in the tutorial. Is it possible to install the scanpy version the tutorial is using?. ```python; adata = adata.concatenate(; list(slides.values()),; batch_key=""sample"",; uns_merge=""unique"",; batch_categories=list(sample_data['sample_name'].values), ; index_unique=None; ); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-4-fe8a54a66c17> in <module>; 40 uns_merge=""unique"",; 41 batch_categories=list(sample_data['sample_name'].values),; ---> 42 index_unique=None; 43 ); 44 . TypeError: concatenate() got an unexpected keyword argument 'uns_merge'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014:98,install,install,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014,1,['install'],['install']
Deployability,"I had a false alarm related to this just now (I thought I was on recent anndata+scanpy versions, but a defunct older clone of anndata was sneaking onto my `$PYTHONPATH`). Confirming it's fixed:. **Dockerfile:**; ```Dockerfile; FROM python:3.8.5; ARG anndata; ARG scanpy; RUN pip install anndata==${anndata} scanpy==${scanpy}; ENTRYPOINT [""python"",""-c"",""import scanpy""]; ```. **Works:**; ```bash; docker build --build-arg anndata=0.7.5 --build-arg scanpy=1.7.1 -t scanpy . && docker run scanpy # ✅; ```; **Fails:**; ```bash; docker build --build-arg anndata=0.7.3 --build-arg scanpy=1.6.1 -t scanpy . && docker run scanpy # ❌; # Traceback (most recent call last):; # File ""<string>"", line 1, in <module>; # File ""/usr/local/lib/python3.8/site-packages/scanpy/__init__.py"", line 41, in <module>; # from anndata import AnnData, concat; # ImportError: cannot import name 'concat' from 'anndata' (/usr/local/lib/python3.8/site-packages/anndata/__init__.py); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1439#issuecomment-835827799:279,install,install,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-835827799,1,['install'],['install']
Deployability,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:146,install,install,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876,6,['install'],"['install', 'installed']"
Deployability,"I had a same issue. My environment is; ```; windows10; python3.8.8 (conda env); ```. scanpy installation ; `conda install -c conda-forge -c bioconda scanpy`. It looks work well on command prompt, but it wasn't work on jupyterlab(3.0). To solve this, I just installed all packages using pip, not conda.; here is my install procedure. ```; conda create -n test python=3.8; pip install ipykernel; pip install jupyterlab; pip install scanpy; pip install python-igraph; pip install leidenalg; pip install fa2; ```. I tired a lot of install and environment combination, but always there was a problem with conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-814856541:92,install,installation,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-814856541,11,['install'],"['install', 'installation', 'installed']"
Deployability,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:77,install,installed,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558,2,['install'],"['installed', 'installing']"
Deployability,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536:26,install,install,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536,4,['install'],"['install', 'installation', 'installed']"
Deployability,I had the same issue using ; `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2; but it worked using ; `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914:35,install,install,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914,2,['install'],['install']
Deployability,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:312,install,install,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242,1,['install'],['install']
Deployability,I had this problem importing `import scanpy as sc`.; I'll update you if this problem persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585#issuecomment-479515587:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585#issuecomment-479515587,1,['update'],['update']
Deployability,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-436977469:180,install,installs,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-436977469,1,['install'],['installs']
Deployability,"I have just managed to install successfully (kind of, more details on things going wrong to come in the other repository) with pip without actually doing any of this, so I'm not sure what was actually going on here. One way or the other, this seems to have gone away now somehow on its own.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/49#issuecomment-345195949:23,install,install,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49#issuecomment-345195949,1,['install'],['install']
Deployability,I have matplotlib 2.2.2. . Update!; I just updated my matplotlib to the latest version and it works well there is no misalignment any more! . thanks again,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/480#issuecomment-463308380:27,Update,Update,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480#issuecomment-463308380,2,"['Update', 'update']","['Update', 'updated']"
Deployability,"I have problem installing and importing scrublet on windows please can you help me; Here is my code !pip install scrublet; PackagesNotFoundError: The following packages are not available from current channels:. - annoy. Current channels:. - https://conda.anaconda.org/conda-forge/win-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/win-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/win-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://repo.anaconda.com/pkgs/msys2/win-64; - https://repo.anaconda.com/pkgs/msys2/noarch; - https://conda.anaconda.org/pytorch/win-64; - https://conda.anaconda.org/pytorch/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org/. and use the search bar at the top of the page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194:15,install,installing,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-1755962194,2,['install'],"['install', 'installing']"
Deployability,"I have replicated the error using local installation with 'pip3 install scanpy'; When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/687#issuecomment-502349575:40,install,installation,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687#issuecomment-502349575,2,['install'],"['install', 'installation']"
Deployability,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:148,integrat,integrate,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093,1,['integrat'],['integrate']
Deployability,I have the same issue. I am eager to hear if there are any updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2442#issuecomment-1520632755:59,update,updates,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2442#issuecomment-1520632755,1,['update'],['updates']
Deployability,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:68,install,install,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,1,['install'],['install']
Deployability,"I have yet to install the most latest `scanpy` and I do not have CPUs to test for this specific case, but I had some issue of reproducing `leiden` results from `scanpy` from a published paper, and found that running `leiden` 10 times (per `n_iteration` option) resolved any discrepancy. Wonder whether it adds to the discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831,1,['install'],['install']
Deployability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1401,install,install,1401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,5,"['install', 'update', 'upgrade']","['install', 'updated', 'upgrade']"
Deployability,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1566#issuecomment-753962579:2,install,installed,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753962579,1,['install'],['installed']
Deployability,I installed it now but still have the issue of HTTP Error 403.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978:2,install,installed,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978,1,['install'],['installed']
Deployability,I installed leidenalg through conda today and encountered the same error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341#issuecomment-1265542125:2,install,installed,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341#issuecomment-1265542125,1,['install'],['installed']
Deployability,I installed the latest Scanpy 1.9.1 `conda install -c conda-forge scanpy python-igraph leidenalg`. And the bug was gone!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265#issuecomment-1137289422:2,install,installed,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265#issuecomment-1137289422,2,['install'],"['install', 'installed']"
Deployability,"I installed these packages on PC1, UMAP still not consistent with others.; <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; .xl65; 	{font-size:10.0pt;; 	font-family:""Var\(--jp-code-font-family\)"", sans-serif;; 	mso-font-charset:0;; 	text-align:center;}; .xl66; 	{text-align:center;}; .xl67; 	{color:red;; 	font-size:10.0pt;; 	font-weight:700;; 	font-family:""Var\(--jp-code-font-family\)"", sans-serif;; 	mso-font-charset:0;; 	text-align:center;}; .xl68; 	{color:red;; 	font-weight:700;; 	text-align:center;}; .xl69; 	{color:red;; 	font-size:10.0pt;; 	font-weight:700;; 	font-family:""Var\(--jp-code-font-family\)"", sans-serif;; 	mso-font-charset:0;; 	text-align:center;; 	background:yellow;; 	mso-pattern:black none;}; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016078802:2,install,installed,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016078802,1,['install'],['installed']
Deployability,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); leiden(adata, use_weights=True); sc.tl.louvain(adata, use_weights=True); sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2); ```; ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437046926:636,install,installation,636,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437046926,1,['install'],['installation']
Deployability,"I just encountered the same issue, using scanpy 1.9.1 and matplotlib 3.5.3. I think it's a recent update to matbplotlib which broke something here. When I rolled back matplotlib to 3.5.2, it ran fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1273295202:98,update,update,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1273295202,1,['update'],['update']
Deployability,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:144,update,update,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351,1,['update'],['update']
Deployability,I just had the same issue because I downgraded my matplotlib elsewhere.; So it should work if you upgrade your matplotlib.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2332#issuecomment-1254363045:98,upgrade,upgrade,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2332#issuecomment-1254363045,1,['upgrade'],['upgrade']
Deployability,"I just made an update in the PR #1210 that will solve the issue. Now you can do:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); # make a heatmap with all the 765 genes in the dataset, highlight each 50th gene; ax_dict = sc.pl.heatmap(adata, adata.var_names, groupby='louvain', show=False, show_gene_labels=True, figsize=(7,4)); ax_dict['heatmap_ax'].set_xticks(range(len(adata.var_names))[::50]); ax_dict['heatmap_ax'].set_xticklabels(adata.var_names[::50]) ; ```. ![image](https://user-images.githubusercontent.com/4964309/85733220-4db5df00-b6fc-11ea-9c5c-d657ebb136c8.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1235#issuecomment-649557649:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1235#issuecomment-649557649,1,['update'],['update']
Deployability,"I just noticed that the reply I sent Saturday bounced due to ‘unknown error’. I thought I should provide an update in case other Windows users encounter something similar. After identifying the correct version of the several on the link below, I noticed that it was also necessary to install Pycairo. I then found I needed to install the Louvain algorithm to resolve the issue. The vtraag website indicates this algorithm has been superseded by the Leiden algorithm, which I installed with no problem. Thanks much for your reply within hours, on a weekend no less. From: Koncopd [mailto:notifications@github.com]; Sent: Saturday, May 25, 2019 2:15 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Mention <mention@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). @RicedeKrispy<https://github.com/RicedeKrispy>; Hi, if you are using Windows, you can try to install python-igraph from the wheel here; https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ2OGJSDDXGY4TRK4TPXF62HA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWHWUII#issuecomment-495938081>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEFZFWYPQB7BP5HHCM23PXF62HANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611:108,update,update,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-502908611,5,"['install', 'update']","['install', 'installed', 'update']"
Deployability,"I just updated the notebook linked at the top of the PR. I have a PR at pymde to improve the initialization speed using the GPU (https://github.com/cvxgrp/pymde/pull/55). Using these changes, pymde takes 20 seconds and umap takes around 200 seconds (150k cells). Most of the time of pymde I believe is from the initial pynndescent call. Therefore, if implemented well here and therefore using a precomputed neighbors graph, pymde would take no more than a few seconds for most use cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051311781:7,update,updated,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051311781,1,['update'],['updated']
Deployability,"I just wanted to update that this issue does not depend on scvelo at all, but I can recreate it by just using scanpy. I suspect it is an issue with running umap. I'm using version '0.4.6'. Any help would be much appreciated:. ### Minimal code sample (that we can copy&paste without having any data). ```python; import os; import scanpy as sc; import numpy as np; import pandas as pd; import copy; import anndata; import matplotlib.pyplot as plt; adata_pbmc3k = sc.datasets.pbmc3k_processed(); #del adata_pbmc3k.obsm['X_pca']; #del adata_pbmc3k.obsm['X_umap']; del adata_pbmc3k.obsp['distances']; del adata_pbmc3k.obsp['connectivities']; #sc.pp.pca(adata_pbmc3k, n_comps=50); sc.pp.neighbors(adata_pbmc3k); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863,1,['update'],['update']
Deployability,"I let you know as soon as there is a stable release back on github. 0.2.5 should be stable [as well](https://github.com/theislab/scanpy_usage), but has some other drawbacks. Things are still progressing fast.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/34#issuecomment-324338365:44,release,release,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/34#issuecomment-324338365,1,['release'],['release']
Deployability,"I like the new behaviour. Maybe a parameter like na_colors could be used to specify a different na color if needed. ; Also, should the NaN and its colour be added to the legend (if categorical) or besides it (if continuous)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675307040:212,continuous,continuous,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675307040,1,['continuous'],['continuous']
Deployability,"I looked into that, but I'm not sure it actually makes this any less complicated. The issue is getting the `tqdm` thing to update, and requests would need all of the same logic to do that as far as I can tell. Plus, at that point it's copying from stack overflow vs. copying from python's stdlib. You'd think this would be a convenience function somewhere. Or you'd think that `urlretrieve` could take a `Request` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406:123,update,update,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406,1,['update'],['update']
Deployability,"I managed to get past the error by adding; ```; RUN locale-gen en_US.UTF-8; ENV LC_ALL en_US.UTF-8; ```; to the [Dockerfile](https://gist.github.com/pwl/a26726fda94ac7f4cbfb57e4fe98bf28). Before that the default locale was set to `POSIX`, which caused all of these problems. This is a weird choice of defaults as clearly python code doesn't work as expected. Thanks for helping out @flying-sheep!. EDIT: just to clarify, this dockerfile is not an example of how to install scanpy, it's just a demonstration of how to circumvent the issues with locales. In particular, several libraries are missing and scanpy does not complete the installation. Feel free to update this Dockerfile or add one to the scanpy repository.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559:465,install,install,465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344235559,3,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:179,release,released,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276,1,['release'],['released']
Deployability,"I mean like milestones or projects for upcoming releases. It look like the change that fixes the issue got marked as an improvement, so we'd be waiting on 8.2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2051712645:48,release,releases,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2051712645,1,['release'],['releases']
Deployability,"I mean, @vtraag is is the person I’d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesn’t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package “louvain-igraph“ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:618,install,installed,618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,2,['install'],['installed']
Deployability,"I meet the same problem when I want to install scanpy on my new laptop.; I run the same commands as I used to do on my old computer , but failed to import scanpy.; I've tried re-install of pytables and it still doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047645794:39,install,install,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047645794,2,['install'],['install']
Deployability,I meet the same problem. You could use:. pip install matplotlib==2.2.3. I tried just. And it done.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1227#issuecomment-659856395:45,install,install,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-659856395,1,['install'],['install']
Deployability,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:56,install,install,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274,6,['install'],"['install', 'installation', 'installing']"
Deployability,I met the same problem one day ago. I run the following: ; conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch ; conda install scvi-tools -c conda-forge; conda install -c conda-forge scanpy python-igraph leidenalg ; It works fine now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1259069261:65,install,install,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1259069261,3,['install'],['install']
Deployability,"I might also have a corrupted install of anndata, let me reboot my system and see if the error persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2035502211:30,install,install,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2035502211,1,['install'],['install']
Deployability,"I might have overextended the old environment a bit too much indeed, so I'll just continue with a fresh one. If it's of any help to you, then before the problems started I wanted to try out some of the newer additions to scanpy ecosystem by installing triku, dorothea and progeny. Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-848678562:241,install,installing,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848678562,1,['install'],['installing']
Deployability,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:114,integrat,integrated,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328,1,['integrat'],['integrated']
Deployability,I noticed I'm on scanpy 1.9.3 and upgraded to scanpy 1.9.4 as well and tried again and have the same issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2645#issuecomment-1701530406:34,upgrade,upgraded,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645#issuecomment-1701530406,1,['upgrade'],['upgraded']
Deployability,I noticed now that the library is `python-igraph` and not `igraph` as explained in https://scanpy.readthedocs.io/en/latest/installation.html .; I am closing this issue since I've solved through the steps in the documentation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-385764640:123,install,installation,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-385764640,1,['install'],['installation']
Deployability,I noticed that matplotlib v. 3 is being installed in travis. This may be the reason why some tests not related to the changes are now failing. I am updating my matplotlib version to update de tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033:40,install,installed,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425947033,2,"['install', 'update']","['installed', 'update']"
Deployability,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. ; @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271:773,integrat,integrate,773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271,1,['integrat'],['integrate']
Deployability,"I ran `pip3 install scanpy[doc]`, as instructed by that page, and the doc seems to have built fine locally. ![image](https://user-images.githubusercontent.com/14993986/122047761-cbc53700-cde0-11eb-8fb2-5a180d306554.png). The only fishy thing is the `function` is not clickable, while the other two are, but it built. Also, line 28 is `trim: Optional[int] = None,`, which was there previously already. It's the metric change that's causing this - your argument renaming PR did not hiccup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1868#issuecomment-861433547:12,install,install,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-861433547,1,['install'],['install']
Deployability,"I readded cython to the requirements: https://github.com/theislab/scanpy/commit/2ae826b71c1eefa16b165d4ff85de9f76fc9e62d. I thought it would be unnecessary when directly installing from the .c files, but evidently, it is not. Anyhow, this should fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/22#issuecomment-307050792:170,install,installing,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/22#issuecomment-307050792,1,['install'],['installing']
Deployability,"I recently installed the [miniforge3](https://github.com/conda-forge/miniforge) distribution on my Apple with M1 and both `sc.pp.neighbors` and `sc.pp.calculate_qc_metrics` work nice and quiet. Not sure if that helps with the issue here, but might be worth a try. ; My versions:; ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; anndata2ri 1.2.dev11; appnope 0.1.3; asttokens NA; backcall 0.2.0; backports NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.4; importlib_resources NA; ipykernel 6.22.0; ipython_genutils 0.2.0; ipywidgets 8.0.6; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; markupsafe 2.1.2; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.56.4; numpy 1.22.0; packaging 23.1; pandas 1.2.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.2.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pyparsing 3.0.9; pytz 2023.3; pytz_deprecation_shim NA; rpy2 3.5.11; scipy 1.9.1; scrublet NA; seaborn 0.12.2; session_info 1.0.0; six 1.16.0; sklearn 1.2.2; stack_data 0.6.2; statsmodels 0.13.5; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.3; traitlets 5.9.0; typing_extensions NA; tzlocal NA; wcwidth 0.2.6; yaml 6.0; zipp NA; zmq 25.0.2; -----; IPython 8.12.0; jupyter_client 8.2.0; jupyter_core 5.3.0; notebook 6.5.4; -----; Python 3.8.16 | packaged by conda-forge | (default, Feb 1 2023, 16:01:13) [Clang 14.0.6 ]; macOS-13.2.1-arm64-arm-64bit; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1518690218:11,install,installed,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1518690218,1,['install'],['installed']
Deployability,"I second this initiative. I had used the code from Brent and works quite; well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>; wrote:. > *@Marius1311* commented on this pull request.; > ------------------------------; >; > In scanpy/preprocessing/combat.py; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:; >; > > @@ -0,0 +1,161 @@; > +import numpy as np; > +from scipy.sparse import issparse; > +import pandas as pd; > +import sys; > +from numpy import linalg as la; > +import patsy; > +; > +def design_mat(mod, batch_levels):; > + # require levels to make sure they are in the same order as we use in the; > + # rest of the script.; > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),; >; > thanks, did that!; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676:101,integrat,integrated,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676,1,['integrat'],['integrated']
Deployability,"I see that it can process arrays now, i should check if it better to replace the custom implementation with the updated `stats.mannwhitneyu`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1892#issuecomment-864860024:112,update,updated,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892#issuecomment-864860024,1,['update'],['updated']
Deployability,"I see! File looks like this:. ```csv; barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres; GTCACTTCCTTCTAGA-1,0,0,0,-1567,2629; CACGGTCTCCTTACGA-1,0,0,2,-1569,2811; ATAGCTGCGGATAAGA-1,0,0,4,-1571,2993; GTCAGTATGTCCGGCG-1,0,0,6,-1573,3174; ...; ```. @RaphaelBuzzi you mean `read_visium`, right? There is no `read_spatial` in scanpy. That one seems to assume that; - if the file is called `tissue_positions.csv`, it has a header on the second row (index 1), and; - if it is called `tissue_positions_list.csv`, it has no header line:. https://github.com/scverse/scanpy/blob/692c9e536ab1d3b0a7d16e9c2c6e7d53390f9b5a/scanpy/readwrite.py#L461-L465. The only thing that looks weird about that is the index 1 instead of 0, otherwise it looks identical to the PR by 10x: https://github.com/satijalab/seurat/pull/6208/files. So the question is: why is there a file called `tissue_positions_list.csv` that has a header? That doesn’t seem like what 10x’s pipeline does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607076461:967,pipeline,pipeline,967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607076461,1,['pipeline'],['pipeline']
Deployability,"I see, [densmap](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html). Hmm, I think that `method='densmap'` and `method_kwds={...}` would be a better API for us (which would then be translated into `densmap=True, densmap_kwds=method_kwds`). This also needs tests and a release note. Also we probably should just remove the umap 0.4 compatibility code, what do you think @ivirshup?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449:279,release,release,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1764564449,1,['release'],['release']
Deployability,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537:27,install,installing,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537,2,['install'],"['install', 'installing']"
Deployability,"I somehow missed the documentation section, my bad. I didn't install the dev packages because I only needed a minimal setup to recreate the bug, scanpy actually fails to install without the dev packages, as expected, but that comes later on, after the initial bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344294567:61,install,install,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344294567,2,['install'],['install']
Deployability,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:843,install,installing,843,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299,1,['install'],['installing']
Deployability,"I suspect Elisabetta's concern was more about the general differences in documentation standards between Python packages and R packages (at least within genomics). It can definitely make the transition from R to Python more difficult (I can attest, as I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:564,release,release,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,2,['release'],['release']
Deployability,"I tested myself and obtained exactly the same results. :). You probably don't have the FA2 package installed, that's why your graph look different... :). I'm merging this! Awesome work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487797746:99,install,installed,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487797746,1,['install'],['installed']
Deployability,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:45,pipeline,pipeline,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,"I think I find the reason. When run sc.tl.louvain(adata), louvain_colors will be saved in adata.uns, sc.pl.paga will use louvain_colors. But, when run sc.tl.louvain(adata) again with another resolution and then rerun sc.tl.paga, louvain_colors will not be updated, and the error occurs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/381#issuecomment-456264112:256,update,updated,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/381#issuecomment-456264112,1,['update'],['updated']
Deployability,"I think I found a way around it. The issue here is the error is thrown when the new louvain groups are created by the adata.obs['louvain_colors'] are not updated until the plotting sc.pl function is run. Therefore, when you try to slice anything, it throws out an index out of bounds error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/833#issuecomment-531440165:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531440165,1,['update'],['updated']
Deployability,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473:714,update,updated,714,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473,1,['update'],['updated']
Deployability,I think I have the most recent version of python-graph installed ( python-igraph 0.7.1.post6 ).; pip install python-igraph doesnt solve the issue since it is already updated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324590937:55,install,installed,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324590937,3,"['install', 'update']","['install', 'installed', 'updated']"
Deployability,"I think I see the issue here. The bioconda distribution of scanpy is out of date and unsupported, please install it from condo-forge instead:. https://scanpy.readthedocs.io/en/latest/installation.html#anaconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178#issuecomment-1068920599:105,install,install,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178#issuecomment-1068920599,2,['install'],"['install', 'installation']"
Deployability,"I think `flavor='taynaud'` should [still work](https://github.com/theislab/scanpy/blob/a9ee39f5152d167f1aeb784ffbdd0a6e3dc1409a/scanpy/tools/louvain.py#L142), but is really only meant as a last resort. AFAIK the networkx modules don't even provide a resolution parameter, but most importantly, they don't scale. I think I even was satisfied with convergence of the results and comparisons with, e.g., Seurat. Scanpy can even be installed using bioconda, so there should be no problem with igraph installation these days. Did you checkout https://scanpy.readthedocs.io/en/latest/installation.html?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-440389136:428,install,installed,428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440389136,3,['install'],"['installation', 'installed']"
Deployability,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208:237,release,release,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208,2,['release'],['release']
Deployability,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529:210,upgrade,upgraded,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529,1,['upgrade'],['upgraded']
Deployability,I think it is ok to merge now. . I also updated some of the plotting functions to accept a `gene_symbol` column: . ![image](https://user-images.githubusercontent.com/4964309/52279718-85cb4f00-295a-11e9-99e9-f9b8648609a6.png). What is missing is `sc.pl.rank_genes_groups` and `sc.pl.violin` any volunteers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-460657479:40,update,updated,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-460657479,1,['update'],['updated']
Deployability,"I think it was a bugged `uv` release, but seems to be fixed now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2026135676:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2026135676,1,['release'],['release']
Deployability,"I think it's very likely people will hit this bug, because 1) they typically don't update packages like scipy very often 2) most pipelines use sparse datasets + PCA. I think it deserves a new release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236:83,update,update,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236,3,"['pipeline', 'release', 'update']","['pipelines', 'release', 'update']"
Deployability,"I think maybe i found a solution to solve this problem.; Maybe this problem is caused by the version of scikit—misc，when you use pip install --user scikit-misc or pip install scikit-misc，the system will install scikit-misc==0.1.4.; so,i try to install another verion of scikit-misc,you can use install -i https://test.pypi.org/simple/ ""scikit-misc==0.2.0rc1.; In addition, this line of command needs to be used when python is greater than or equal to 3.8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497:133,install,install,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1738603497,5,['install'],['install']
Deployability,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:416,deploy,deploys,416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,3,"['deploy', 'pipeline']","['deployed', 'deploys', 'pipelines']"
Deployability,"I think that correlation matrix is only in the latest master version. You can install it using:; ```; pip install git+https://github.com/theislab/scanpy.git; ```. Also, be sure to load scanpy as 'import scanpy as sc'. If you use the old method (`import scanpy.api as sc`) it will not work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206:78,install,install,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206,2,['install'],['install']
Deployability,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:188,release,release,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255,2,['release'],['release']
Deployability,"I think that the `all_data.uns['leiden_colors']` list is only updated if the new number of clusters is bigger than the previous cluster number as the goal is to avoid missing colors. . In you use the the `palette` argument, the color list will always be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/420#issuecomment-453067108:62,update,updated,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420#issuecomment-453067108,2,['update'],['updated']
Deployability,I think that the updated docstring is much better.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454026591:17,update,updated,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454026591,1,['update'],['updated']
Deployability,"I think that there is no up to date Conda installation. Only Pip. The current version is 1.3.7. . > On 4 Feb 2019, at 10:26, Bérénice Batut <notifications@github.com> wrote:; > ; > right_margin and left_margin are still listed as parameters for pl.scatter:; > ; > https://github.com/theislab/scanpy/blob/c15a5e8763097082c82cd8ef6fee697954c487dc/scanpy/plotting/_anndata.py#L49; > ; > And ncols, wspace and hspace are not accepted (with the current version on conda) 😟; > ; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-460351011:42,install,installation,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-460351011,1,['install'],['installation']
Deployability,"I think the PR is now good as is. I will be quite busy for some time, therefore I won't have time for further discussions. Please feel free to close, update or merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-557824199:150,update,update,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-557824199,1,['update'],['update']
Deployability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:763,install,installing,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['install'],['installing']
Deployability,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437812546:334,upgrade,upgrade,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437812546,1,['upgrade'],['upgrade']
Deployability,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279:55,integrat,integrated,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279,1,['integrat'],['integrated']
Deployability,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734#issuecomment-509619333:207,upgrade,upgrade,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509619333,1,['upgrade'],['upgrade']
Deployability,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:214,continuous,continuous,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431,1,['continuous'],['continuous']
Deployability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,integrat,integrated,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,1,['integrat'],['integrated']
Deployability,"I think this is a really good idea. Definitely a good hold over until a real GroupBy object is worked out. I'll review in depth later but have a few thoughts:. * How about `split_by`? Rhymes with `group_by` which this is quite like.; * I would like to be able to do this along the `var` axis; * What do you think about including some of the grouping options from [`_prepare_dataframe`](https://github.com/theislab/scanpy/blob/d072abd05bda07f280ea91f5e7e4a84f9782c118/scanpy/plotting/_anndata.py#L1842)? E.g. grouping by multiple keys, binning by continuous values? It would be nice if this could be used in place of `_prepare_dataframe`. The features would not have to be implemented in this PR, but it would be good to think about what the API for them would be and making sure it could be added elegantly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1939#issuecomment-876054434:546,continuous,continuous,546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1939#issuecomment-876054434,1,['continuous'],['continuous']
Deployability,"I think this is an important conversation to have not just for imputation, but also for other analysis methods like visualization and batch effect correction. Every algorithm makes some assumptions and biases, and it is possible to misinterpret for misuse almost any machine learning algorithm. . For example, t-SNE, often used for visualization, is also used as dimensionality reduction for clustering. However, most clustering algorithms assume that global distances in a dataset are relevant. This assumption is broken with t-SNE, as evidenced by the inconsistency of t-SNE embeddings on the same data and inability for t-SNE to capture some global trends in a dataset (especially with continuous data, leading to the popularity of graph-based visualizations). . On top of this, each clustering algorithm makes assumptions that data is in fact distributed in clusters, but this is often not the case in single cell data. I agree that it's important to warn users about the limitations of imputation methods, and make them aware that their decision on which algorithm to run can affect their output. However, it seems to me that this conversation could be much broader in scope. We don't currently have a system for unified benchmarking and standardization of single cell analysis methods, so all approaches should be used with some caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251:689,continuous,continuous,689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-413591251,1,['continuous'],['continuous']
Deployability,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-788791783:371,release,release,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-788791783,1,['release'],['release']
Deployability,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:299,Update,Updated,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865,1,['Update'],['Updated']
Deployability,I think this is not a bug. You have to update to latest version of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3026#issuecomment-2078034967:39,update,update,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3026#issuecomment-2078034967,1,['update'],['update']
Deployability,"I think this is now ready for review. . ## Legends. I've decided to leave showing the null value in continuous legends for another PR, since I don't have an obvious solution now. I have added an argument for specifying whether the na value should show up in the legend, `na_in_legend`. It defaults to `True`. Here's an example:. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""]); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855448-fd3cc400-e3c2-11ea-9e01-6e8266ab6d10.png); ![image](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:100,continuous,continuous,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['continuous'],['continuous']
Deployability,"I think this may be already implemented in https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.ingest.html, however, this function contains extra integration and label transfer steps that are not needed for all applications. It would be great if this could be disentangled to make the umap transform available as a separate function on scanpy umaps. Also, it seems that this function does not use scanpy umap to calculate umap so changes may be needed in how scanpy umap is currently calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704:154,integrat,integration,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704,1,['integrat'],['integration']
Deployability,"I think this was happening because of confusion between igraph and python-igraph, by un-installing both and reinstalling just python-igraph, the issue was solved. Interesting enough installation with conda didnt work, only installation with pip worked.; thanks again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324615579:88,install,installing,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324615579,3,['install'],"['installation', 'installing']"
Deployability,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066:74,update,update,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066,1,['update'],['update']
Deployability,"I think you could get better help on Stackoverflow or https://discuss.scverse.org/. If `pip` reports “Requirement already satisfied” but scanpy can’t import it, then the environment you run `pip` on is not the same you run scanpy in (or the environment is broken and something caused the metadata to be there while the actual package isn’t). If you run `pip --version`, it should tell you its location. That location will probably be different from `/opt/miniconda3/envs/scanpyenvt/`, so you need to make sure you use the correct pip, either by doing. ```py; /opt/miniconda3/envs/scanpyenvt/bin/pip install ...; ```. or by activating it. As said, if you have questions, please go to one of the forums that have a lot of people who can help you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144#issuecomment-2214155826:599,install,install,599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144#issuecomment-2214155826,1,['install'],['install']
Deployability,I think you'll want to update your version of anndata for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254#issuecomment-635750301:23,update,update,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635750301,1,['update'],['update']
Deployability,"I think your issue here is due to having multiple versions of various packages in your path. In general, that will cause problems. I think I can only recommend creating an isolated environment using something like `conda` or `virtualenv` and say that installing with `pip` tends to have the fewest problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-651518306:251,install,installing,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-651518306,1,['install'],['installing']
Deployability,"I thought about adding that functionality. Probably is useful when the; second obs_key has few categories because the second category subdivides; the first category. A quick hack is to add a new observation that is the; combination of the first and second keys. Eg. For 4 clusters, and 2 cell; types, the new observation would be cluster_1_cell_type_1,; cluster_1_cell_type_2, cluster_2_cell_type_1 etc. On Thu, Jun 21, 2018 at 5:11 AM wangjiawen2013 <notifications@github.com>; wrote:. > Great, it is. what if I want to add multiple obs_keys (such as lovain,; > cell_type et al.,), and, when will the new version scanpy be released?; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/178#issuecomment-399032569>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1a4oPw4zOxr4mLTDh7__Q5BsB5dUks5t-2NAgaJpZM4Uq06H>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178#issuecomment-399261667:624,release,released,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399261667,1,['release'],['released']
Deployability,I thought you had mentioned moving this to squidpy? Which is fine to me to do whenever you want. Might be good to do ahead of the next visium releases?. Not sure if another package will happen or a timeline if so.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061645321:142,release,releases,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061645321,1,['release'],['releases']
Deployability,"I too have the exact same issue with scanpy.tl.rank_genes_groups(). Is this something to do with anndata 0.7.5 requires pandas version >=1.0, yet, pandas coming along with installing scanpy with 'pip install scanpy' is in version 0.23.4?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926:172,install,installing,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926,2,['install'],"['install', 'installing']"
Deployability,I uninstalled and then installed scanpy. This resolves the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2438#issuecomment-1474040127:23,install,installed,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2438#issuecomment-1474040127,1,['install'],['installed']
Deployability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:48,install,installed,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,1,['install'],['installed']
Deployability,I updated it to python 3.6.8 and it gets past that error point. ; Thank you very much for all your help.; Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734#issuecomment-509622325:2,update,updated,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509622325,1,['update'],['updated']
Deployability,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:2,update,updated,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072,2,"['release', 'update']","['release', 'updated']"
Deployability,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:2,update,updated,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325,6,"['Install', 'install', 'update']","['Installing', 'install', 'installation', 'installed', 'updated']"
Deployability,"I updated the commits, making bioservices optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-386815219:2,update,updated,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-386815219,1,['update'],['updated']
Deployability,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-435784820:2,update,updated,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435784820,1,['update'],['updated']
Deployability,"I upgrade `scanpy` to `1.9.1`, and now `sc.tl.umap` works as expected:. ![image](https://user-images.githubusercontent.com/33963919/209405851-2798000b-0e90-4b3f-982c-f0de196489c5.png). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364339487:2,upgrade,upgrade,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364339487,1,['upgrade'],['upgrade']
Deployability,I upgraded loompy and scanpy as well but now I am getting an other error. ![screen shot 2018-08-29 at 19 21 34](https://user-images.githubusercontent.com/42487820/44782019-db881800-abc0-11e8-8948-90aa0b0c20a1.png). ![screen shot 2018-08-29 at 19 14 14](https://user-images.githubusercontent.com/42487820/44782040-eb076100-abc0-11e8-961e-75b4ba0c8ec7.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247#issuecomment-416903663:2,upgrade,upgraded,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-416903663,1,['upgrade'],['upgraded']
Deployability,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/370#issuecomment-440522061:2,upgrade,upgraded,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370#issuecomment-440522061,1,['upgrade'],['upgraded']
Deployability,I used `bioconda` as it was the recommended installation method according to the Scanpy documentation site last time I checked (october 2021). My bad then. I only installed `scanpy` through `bioconda` so the version I got (and you see in the Details of the previous post) are from there. `anndata2ri` version in there is from `pip`. If I try with conda-forge channel for both scanpy and anndata2ri the issue is solved.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063103340:44,install,installation,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063103340,2,['install'],"['installation', 'installed']"
Deployability,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```; sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle; sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None); #OR; import mnnpy; mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata); ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. ; ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch); ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756:652,Integrat,Integrate,652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756,1,['Integrat'],['Integrate']
Deployability,"I was able to set up a dev environment with a little work. I think it works? Ran into some other issues though. <details>; <summary> Roughly what I ran </summary>. ```sh; mamba create -yn ""numba-0.55.0rc1-pip"" -c conda-forge python=3.10 pip; conda activate numba-0.55.0rc1-pip; pip install ""numba== 0.55.0rc1-pip""; pip install flit_core setuptools_scm; cd ~/github/anndata; pip install -e "".[dev,doc,test]""; cd ~/github/scanpy; pip install -e --no-build-isolation "".[dev,doc,test]""; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010919911:282,install,install,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010919911,4,['install'],['install']
Deployability,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:83,release,release,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980,1,['release'],['release']
Deployability,"I was following the documentation and kept getting failures when I tried to pass additional args to violin() via kwargs, and found that kwargs was only [added to violin 6 days ago](https://github.com/theislab/scanpy/blame/master/scanpy/plotting/anndata.py#L347). Yay (and thanks) for the updates)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-371026156:288,update,updates,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-371026156,1,['update'],['updates']
Deployability,I was having the same problem. **conda update anndata** solved my problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1434409733:39,update,update,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1434409733,1,['update'],['update']
Deployability,"I was recently directed to the [RStudio tutorial for setting up Python/R with {reticulate}](https://t.co/DjbnfZmjQn?amp=1). After this it worked successfully for me; hope it helps. First, source the `virtualenv` and install the package only to the directory/project your working on ([similar concept to `{packrat}`](https://rstudio.github.io/packrat/)); ![image](https://user-images.githubusercontent.com/5749465/73144449-d0663f80-4073-11ea-85d2-2277fb049342.png). *Voila!*. ![image](https://user-images.githubusercontent.com/5749465/73144482-20450680-4074-11ea-9374-6dac81968f89.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-578560517:216,install,install,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578560517,1,['install'],['install']
Deployability,"I was thinking more like:. ```python; sc.pl.dotplot(; ...,; var_ticklabels_kwargs={; ""fontstyle"": ""italic"",; ... ; },; ); ```. With an internal:. ```python; var_ticklabels_params = {""rotation"": 90, ""ha"": 'center', ""minor"": False}; var_ticklabels_params.update(var_ticklabels_kwargs}. dot_ax.set_xticklabels(var_names, **var_ticklabels_params); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-872712326:253,update,update,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-872712326,1,['update'],['update']
Deployability,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:1540,update,updated,1540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831,1,['update'],['updated']
Deployability,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078:18,integrat,integration,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078,1,['integrat'],['integration']
Deployability,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:414,continuous,continuous,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192:146,integrat,integrate,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192,1,['integrat'],['integrate']
Deployability,I will certainly update my new stuff today at least once (probably more often ) and change the name / add the documentation ; and then let you know as soon as the name has changed,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-361899845:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361899845,1,['update'],['update']
Deployability,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:92,integrat,integrating,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449,1,['integrat'],['integrating']
Deployability,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:43,integrat,integrate,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301,1,['integrat'],['integrate']
Deployability,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:787,pipeline,pipelines,787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823,1,['pipeline'],['pipelines']
Deployability,I would like to do this for the next release series.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1169#issuecomment-833182712:37,release,release,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1169#issuecomment-833182712,1,['release'],['release']
Deployability,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-968903708:104,install,installation,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968903708,1,['install'],['installation']
Deployability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:149,continuous,continuous,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,2,['continuous'],['continuous']
Deployability,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:898,install,install,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514,1,['install'],['install']
Deployability,"I'd be happy with whatever, but it needs to go somewhere!. How about ""Demultiplexing/ Doublet detection"", so it can go with scrublet (#1476) in the next release, and potentially get split out later if more demultiplexing methods are added?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024:153,release,release,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024,1,['release'],['release']
Deployability,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437728767:783,release,release,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437728767,1,['release'],['release']
Deployability,I'd love to have scVI integration! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867:22,integrat,integration,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867,1,['integrat'],['integration']
Deployability,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['integrat'],['integration']
Deployability,"I'm afraid it's the bioconda CI that generates them. . Switching back to bioconda has another caveat I think: The conda-forge channel is supposed to have a higher channel priority than the bioconda one. AFAIK if a more recent version of scanpy was on bioconda, it would still be the older conda-forge version that gets installed (unless the newer version is requested explicitly)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160313163:319,install,installed,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160313163,1,['install'],['installed']
Deployability,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027:839,pipeline,pipeline,839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-542879027,1,['pipeline'],['pipeline']
Deployability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:251,install,installed,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['installed']
Deployability,"I'm getting the same error using the CellBender tutorial output. Attaching the file to make it easier to reproduce. [tiny_10x_pbmc_filtered.h5.zip](https://github.com/scverse/scanpy/files/8766499/tiny_10x_pbmc_filtered.h5.zip). `sc.logging.print_versions()`. ```; -----; anndata 0.7.8; scanpy 1.9.1; -----; PIL 9.0.1; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; doubletdetection 4.2; entrypoints 0.4; executing 0.8.3; google NA; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.9; ipykernel 6.10.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; mudata 0.1.1; muon 0.1.2; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; organize_metadata NA; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.28; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2022.1; scikits NA; scipy 1.8.0; seaborn 0.11.2; session_info 1.0.0; setuptools 62.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.0.2; stack_data 0.2.0; statsmodels 0.13.2; tables 3.7.0; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.63.1; traitlets 5.1.1; typing_extensions NA; umap 0.5.2; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 8.2.0; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.10; -----; Python 3.9.11 (main, Mar 28 2022, 10:10:35) [GCC 7.5.0]; Linux-4.15.0-142-generic-x86_64-with-glibc2.27; -----; Session information updated at 2022-05-24 15:05; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1136479284:1885,update,updated,1885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1136479284,1,['update'],['updated']
Deployability,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:904,Update,Update,904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['Update'],['Update']
Deployability,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:48,update,updated,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,1,['update'],['updated']
Deployability,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1498,pipeline,pipeline,1498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,1,['pipeline'],['pipeline']
Deployability,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1136#issuecomment-614491704:116,update,updates,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136#issuecomment-614491704,1,['update'],['updates']
Deployability,I'm guessing this should also be added to the release notes @Koncopd?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1472#issuecomment-719667764:46,release,release,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-719667764,1,['release'],['release']
Deployability,I'm having trouble installing tables into a conda environment. Now may be the time to pull the trigger on this,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064#issuecomment-987881759:19,install,installing,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064#issuecomment-987881759,1,['install'],['installing']
Deployability,I'm having trouble reproducing this error. Could you share what versions you have installed (ideally also try updating these to the latest releases) and see if you can replicate the issue on one of the datasets in `sc.datasets`?. I think you should probably do differential expression plots using the same values you used to compute the differential expression in most cases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525:82,install,installed,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525,2,"['install', 'release']","['installed', 'releases']"
Deployability,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:181,Install,Installed,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990,2,"['Install', 'install']","['Installed', 'install']"
Deployability,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:261,update,update,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945,2,"['Upgrade', 'update']","['Upgrade', 'update']"
Deployability,"I'm sorry, I wasn't aware I had used so much of my private webspace for Scanpy all these years back. I migrated from some legacy infra a week ago, without trackers to links to external resources. I'll fix this current instance by fixing the URLs and make a PR to have a copy of these images in the static folder in the docs, effective for the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2132#issuecomment-1034760467:348,release,release,348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2132#issuecomment-1034760467,1,['release'],['release']
Deployability,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:777,install,install,777,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650,1,['install'],['install']
Deployability,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:259,install,installed,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246,1,['install'],['installed']
Deployability,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not.; 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101:419,integrat,integrate,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101,1,['integrat'],['integrate']
Deployability,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:171,install,install,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825,2,"['configurat', 'install']","['configuration', 'install']"
Deployability,"I've just pushed 1.6.1 (pinning umap), but you can get the release candidate for 1.7.0 with `pip install ""scanpy==1.7.0rc1""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465,2,"['install', 'release']","['install', 'release']"
Deployability,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516689711:251,release,release,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516689711,1,['release'],['release']
Deployability,I've made a small update to add a `tissue_image_path` parameter to `read_visium` that you can use to specify the tissue image path to put in `adata.uns`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-735881272:18,update,update,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-735881272,1,['update'],['update']
Deployability,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:348,Continuous,Continuous,348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421,1,['Continuous'],['Continuous']
Deployability,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:338,install,installed,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181,1,['install'],['installed']
Deployability,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally.; * What's the appropriate way to set logging level? It seems to keep changing and breaking things; * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932:168,configurat,configuration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932,3,['configurat'],['configuration']
Deployability,"I've ran into this before (`sc.read_10x_mtx()` has the same default of course). One possible issue with defaulting to `gex_only=False` is that someone might accidentally run a 'regular pipeline' with multi-modal data, e.g. log-normalizing RNA+protein+cell hashing counts together without first subsetting the adata based on `.var[""feature_types""]`. By contrast, anyone who know they have multi-modal data would hopefully notice the missing the data with `gex_only=True`. Either way, logging warnings sounds good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652:185,pipeline,pipeline,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652,1,['pipeline'],['pipeline']
Deployability,"I've talked to some other people who had trouble with this. If there's a better way to install it, I think we should mention it in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/786#issuecomment-522896826:87,install,install,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522896826,1,['install'],['install']
Deployability,"I've update the code to ; - test that the file is actually a tiff image; - automatically add the path to the image to `adata.uns['spatial'][library_id]['metadata']['tissue_image_path']`. It's looking for a tiff or jpeg file with the name `""image""` or `library_id""_image""`. This should cover most cases hopefully?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082:5,update,update,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082,1,['update'],['update']
Deployability,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:5,update,updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592,2,['update'],"['update', 'updated']"
Deployability,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:5,update,updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129,1,['update'],['updated']
Deployability,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553392140:140,install,install,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553392140,1,['install'],['install']
Deployability,"If I understand correctly, pandas > 1.2.0 `plot.add_totals(sort='descending')` returns a different sorting compared to previous version. However, most of the categories have the same number of cells and thus a different sorting is equally valid. ![image](https://user-images.githubusercontent.com/4964309/104478840-0a9d5980-55c3-11eb-94b0-b0e2092c55c3.png). My suggestion is to use other dataset in which the number of cells per category is different and update the image.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1562#issuecomment-759557571:455,update,update,455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562#issuecomment-759557571,1,['update'],['update']
Deployability,"If anyone is stuck waiting for the new release, you can edit your `.../lib/python3.7/site-packages/scanpy/tools/_louvain.py` with these changes:. Add: `partition_kwargs[""seed""] = random_state` ; Remove: `louvain.set_rng_seed(random_state)`. From:; https://github.com/theislab/scanpy/pull/1197/commits/b54d67b9d6b41269c1612df0242210d1279ede85",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-630174509:39,release,release,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-630174509,1,['release'],['release']
Deployability,"If it's what I suspect it to be, it should be fixed with anndata 0.5.9. `pip install anndata==0.5.9`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/123#issuecomment-381746375:77,install,install,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123#issuecomment-381746375,1,['install'],['install']
Deployability,"If someone figures out a simple workaround and submits a PR, we'll merge it, but we only support the newest bugfix releases ourselves. You should definitely tell your sysadmin to update to Python 3.5.4, there are security holes in your version as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825:115,release,releases,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477128825,2,"['release', 'update']","['releases', 'update']"
Deployability,"If the issue is continuous color maps, that can be specified with the `cmap` parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/521#issuecomment-766287782:16,continuous,continuous,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521#issuecomment-766287782,1,['continuous'],['continuous']
Deployability,"If the problem is windows, it's possible it will be solved by numpy 2.0. Not sure how easy the upgrade path to numpy 2.0 will be, however. * https://numpy.org/devdocs/numpy_2_0_migration_guide.html#windows-default-integer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034657597:95,upgrade,upgrade,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034657597,1,['upgrade'],['upgrade']
Deployability,"If this is the case, then why does the tutorial work with scanorama=1.7? I'll try again with an even earlier release scanorama when I have a moment though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051048349:109,release,release,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051048349,1,['release'],['release']
Deployability,If we have arpack i can also update the PR with randomized svd approach. Is it needed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-590349532:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590349532,1,['update'],['update']
Deployability,"If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers) as opposed to [BioContainers/mulled](https://github.com/BioContainers/mulled). This is what I meant by adding a mulled container above and that would go some way to solving the problem because we would be able to get a Docker/Singularity Biocontainer. However, in the long term, it's always nicer if these containers come directly via recipe updates from the Bioconda community. The other alternative is that the package is eventually updated and maintained on Bioconda organically like with most other tools there but that's kinda out of our control unless we add it ourselves. Always makes sense to reach out to the tool developers first! 😎 . > I'm pretty against moving back to bioconda. Curious to know why and if it's something that can be overcome? I did see https://github.com/scverse/scanpy/issues/1169",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160442532:519,update,updates,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160442532,2,['update'],"['updated', 'updates']"
Deployability,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:61,install,install,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206,1,['install'],['install']
Deployability,"If you update to more recent releases, you won't be able to access elements of `obsm` or `varm` like an attribute. It should all be through `.__getitem__` (e.g. `adata.obsm[""X_tsne""]`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/778#issuecomment-522895754:7,update,update,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778#issuecomment-522895754,2,"['release', 'update']","['releases', 'update']"
Deployability,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246:269,update,update,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246,1,['update'],['update']
Deployability,"In any case, running: `$ pip install anndata -U --no-deps` solves the problem, as then the problematic part of utils.py is not run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/482#issuecomment-463285227:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482#issuecomment-463285227,1,['install'],['install']
Deployability,"In case anyone has this error again, here is what worked for me:. - go to https://www.lfd.uci.edu/~gohlke/pythonlibs/ and download a .whl file for h5py. For python 3.6 on a 64bit windows OS this is the file h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl; - with your conda environment activated, install that wheel file using `python -m pip install --user --force-reinstall h5py‑2.10.0‑cp36‑cp36m‑win_amd64.whl` (change the file name to the one you downloaded).; - scanpy should work now. This worked on mine and also on a colleagues windows laptop. I guess the problem is that you need a C++ compiler to build the necessary H5DF libraries. This works fine in UNIX based OS (Mac and Linux), but in windows you would need to download the most recent C++ compiler from some microsoft build tools website or alongside Visual Studio. So installing a prebuildt wheel for windows circumwents that problem. I wonder when h5py people will ever fix this for us poor Windows users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-828487442:286,install,install,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-828487442,3,['install'],"['install', 'installing']"
Deployability,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033:116,deploy,deployed,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033,2,"['deploy', 'update']","['deployed', 'updated']"
Deployability,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407:44,integrat,integrate,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407,2,"['integrat', 'pipeline']","['integrate', 'pipeline']"
Deployability,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354:407,integrat,integrated,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354,1,['integrat'],['integrated']
Deployability,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:70,install,installed,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912,3,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832,2,['release'],['release']
Deployability,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:73,install,installing,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644,4,['install'],"['install', 'installing']"
Deployability,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object?. adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:; > ; > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version.; > …; > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread.; > ; > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1.; > Can it be an issue about duplicated gene names/make unique?; > ; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/637#issuecomment-492700214:640,update,update,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637#issuecomment-492700214,1,['update'],['update']
Deployability,"Indeed scanpy 1.9.6 required `seaborn!=0.13.0`, while newer releases have been update to require `seaborn>=0.13.0` (when you look here on GitHub, you will find this updated requirement as you mentioned). If you want to use scanpy 1.9.6 for a specific reason, your environment should use a seaborn version that is not 0.13.x, e.g. the latest 0.12 version. Does upgrading to the latest scanpy version resolve this issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2791#issuecomment-1952249236:60,release,releases,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2791#issuecomment-1952249236,3,"['release', 'update']","['releases', 'update', 'updated']"
Deployability,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1151#issuecomment-616673759:80,update,updated,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-616673759,1,['update'],['updated']
Deployability,"Install it, it’s an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048:0,Install,Install,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048,1,['Install'],['Install']
Deployability,Install old louvain package will solve the problem: pip install louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808:0,Install,Install,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808,2,"['Install', 'install']","['Install', 'install']"
Deployability,"Interestingly, I can't seem to reproduce this even with `pip` on-top of a conda install:. <details>; <summary> me trying </summary>. ```python; isaac@Mimir:~/tmp/genomic-features-docs; $ mamba create -n test-2978 ""anndata==0.9.0"" ipython scanpy; [ ... ]; isaac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:80,install,install,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,2,['install'],['install']
Deployability,"Is because we changed dot edge the defaults shortly before the release. Time to add a test for this. I will make a fix but meanwhile you can trigger the dynamic coloring by setting `dot_edge_color` and `dot_edge_lw` as `None`:. ```PYTHON; sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True)\; .style(color_on='square', dot_edge_color=None, dot_edge_lw=None).show(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282:63,release,release,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282,1,['release'],['release']
Deployability,Is there any update on the progress of this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-943977687:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-943977687,1,['update'],['update']
Deployability,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368:86,integrat,integrates,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368,1,['integrat'],['integrates']
Deployability,Is this issue still relevant? Did you install scikit-misc?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352#issuecomment-1370858567:38,install,install,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352#issuecomment-1370858567,1,['install'],['install']
Deployability,Is this using the development version? The fix hasn't been in a release yet.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1326#issuecomment-661826106:64,release,release,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-661826106,1,['release'],['release']
Deployability,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:74,install,installed,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302,5,"['Install', 'install']","['Installing', 'install', 'installed', 'installing']"
Deployability,"It doesn’t. We could also. 1. wait to merge this until `skmisc` has a new release or; 2. throw a special error when people try to use seurat v3 with numpy 2. Sadly(?) Python doesn’t allow packages to add constraint to other packages’ dependencies, else we could tell the resolver that all currently release skmisc versions are incompatible with numpy 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501:74,release,release,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501,2,['release'],['release']
Deployability,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337:28,install,installing,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337,2,['install'],"['installed', 'installing']"
Deployability,"It looks like the new release breaks most of our usage from (at least) a change in arguments to `simplicial_set_embedding` (ping @Koncopd). <details>; <summary> Example error </summary>. ```pytb; n_epochs = 0 if maxiter is None else maxiter; > X_umap = simplicial_set_embedding(; X,; neighbors['connectivities'].tocoo(),; n_components,; alpha,; a,; b,; gamma,; negative_sample_rate,; n_epochs,; init_coords,; random_state,; neigh_params.get('metric', 'euclidean'),; neigh_params.get('metric_kwds', {}),; verbose=settings.verbosity > 3,; E TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. </details>. It looks like there is also a lot of cool stuff in the new release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308:22,release,release,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308,2,['release'],['release']
Deployability,"It looks like the underlying issue (`np.zeros` was being called with a heterogeneous `shape` tuple) has been marked to be resolved in the next numba release. We could implement a workaround here where we force the dtype, though numba does have a pretty fast release cadence. @fkoegel, if we implemented a fix here would you be able to test it for us on master branches?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/843#issuecomment-532086499:149,release,release,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-532086499,2,['release'],['release']
Deployability,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1714#issuecomment-790279213:100,release,releases,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714#issuecomment-790279213,1,['release'],['releases']
Deployability,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:47,update,update,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158,2,"['release', 'update']","['releases', 'update']"
Deployability,"It makes sense to use AND logic, because the function keeps genes that satisfy all three conditions. ; 1) Fraction of cells inside the cluster expressing the gene must be greater than `min_in_group_fraction`; 2) Fractions of cells outside the cluster expressing the gene must be less than `max_out_group_fraction`; 3) Fold change must be greater than `min_fold_change`. But there are remaining issues (calculation of fold change and using the absolute value of the fold change) in this function that needs to be updated #863",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781:512,update,updated,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781,1,['update'],['updated']
Deployability,"It seems this is of some relevance to users, as shown by it showing up twice independently over the course of the past week. For some reason, my tweaks have killed off ReadTheDocs, and I can't check why as upon pressing the ""details"" button I get the 404 equivalent :) I am now importing `types` so I can correctly define `metric` as also including `types.FunctionType`. This is probably what is causing whatever hiccup is happening. @giovp , some assistance with getting the ball rolling on this? The changes are a result of me expanding BBKNN with pynndescent on your recommendation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1868#issuecomment-861393180:481,rolling,rolling,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-861393180,1,['rolling'],['rolling']
Deployability,"It seems to be a pytables problem. What happens if you install pytables in a fresh python 3.8 environment?; If `import tables` fails, you could also try uninstalling pytables and installing the package from conda-forge.; `conda install -c conda-forge pytables.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012719981:55,install,install,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012719981,3,['install'],"['install', 'installing']"
Deployability,It should work if you install from github.; https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/631#issuecomment-489690557:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631#issuecomment-489690557,1,['install'],['install']
Deployability,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:96,install,install,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,8,['install'],"['install', 'installed']"
Deployability,"It will be included in the next release, don't worry :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693979810:32,release,release,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693979810,1,['release'],['release']
Deployability,"It worked! I guess when I installed pyvdj, I got 3.1.1. Many thx!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599168621:26,install,installed,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599168621,1,['install'],['installed']
Deployability,"It works now by changing umap to 0.3.9. Thanks a lot!. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220:572,install,install,572,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220,2,"['install', 'release']","['install', 'release']"
Deployability,It works when I update the scanpy version from 1.9.8 to 1.10.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3026#issuecomment-2143892864:16,update,update,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3026#issuecomment-2143892864,1,['update'],['update']
Deployability,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python; def iterativley_cluster(; g: igraph.Graph,; *,; n_iterations: int = 10,; random_state: int = 0,; leiden_kwargs: dict = {}; ) -> list:; import random; random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}; _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]; for _ in range(n_iterations-1):; partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs); steps.append(partition). return steps; ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081:564,update,update,564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081,1,['update'],['update']
Deployability,"It's also possible to attach files as assets to github releases. That's what I do for scirpy, e.g. ; https://github.com/icbi-lab/scirpy/releases/tag/d0.1.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2124#issuecomment-1025946640:55,release,releases,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2124#issuecomment-1025946640,2,['release'],['releases']
Deployability,"It's definitely a problem that you are seeing all of these version restrictions at once. This may be related to having too many entries in your PYTHONPATH environment variable. `PYTHONPATH` should probably just be empty, since python already knows to look where pip installs packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532:266,install,installs,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532,1,['install'],['installs']
Deployability,"It's possible this was fixed in pandas 1.4.1, released on Friday https://github.com/pandas-dev/pandas/issues/45640",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2129#issuecomment-1039247646:46,release,released,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2129#issuecomment-1039247646,1,['release'],['released']
Deployability,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:235,Update,Updated,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['Update'],['Updated']
Deployability,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like; > ; > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed!; > ; > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using; > ; > conda install numba.; > ; > or; > ; > pip install numba==0.39.0; > ; > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. ; > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460:128,Update,Update,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427364460,3,"['Update', 'install']","['Update', 'install']"
Deployability,It’s actually still unreleased. We should release 1.9.3 soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607223772:42,release,release,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607223772,1,['release'],['release']
Deployability,"I’ll take a look if you update your issue with a code block that I can copy, that will download the dataset and then reproduce the error without me having to go to any website and download anything manually.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2819#issuecomment-1906406640:24,update,update,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2819#issuecomment-1906406640,1,['update'],['update']
Deployability,"I’m not a fan of duplicating things. We already install optional requirements via the list of extras here:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/.travis.yml#L8. so we should simply add them to the `test` extra:. https://github.com/theislab/scanpy/blob/f428848ece1d7a4794090eb70a34a3b8f1953dee/setup.py#L35. or add more extras (e.g. `dask=['dask[array]'],`) and add them to the list of extras to be installed in .travis.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041:48,install,install,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-457915041,2,['install'],"['install', 'installed']"
Deployability,"I’m quite sure it has a resolution parameter, but at this point I’m also quite sure I’m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess I’m one of the last around who hasn’t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151:166,install,installation,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151,1,['install'],['installation']
Deployability,"I’m really happy that they’re finally adding a resolver. Without officially having a resolver, it technically wasn’t a bug that incompatible stuff got installed, but instead just a missing (if very vital) feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-661004708:151,install,installed,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-661004708,1,['install'],['installed']
Deployability,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:202,install,installation,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026,1,['install'],['installation']
Deployability,"Jumping in on this conversation to ask a related question - I'm using Scanorama to integrate some datasets and generate an aligned low-dimensional embedding. I then subset the data to only look at specific clusters and want to re-make the UMAP/t-SNE plot. Do you usually re-do the integration to generate a new low-dimensional embedding matrix with Scanorama for the subsetted data? I know you can technically subset the original low-dimensional embedding matrix, but I thought it's preferable to re-do the embedding when you have a different subset of cells to capture more of the variance between those cells (re-select HVGs, etc). Any advice would be welcome - thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919:83,integrat,integrate,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919,2,['integrat'],"['integrate', 'integration']"
Deployability,"Just a heads up, there is a remaining issue on anndata master where reading older files with h5py 2.10.0 results in bytestring indexes. > On Sep 12, 2019, at 05:28, Bruce Martin <notifications@github.com> wrote:; > ; > @gokceneraslan - thanks for the fast response. This broke our (cellxgene) travis pipeline as well. Do you have any info on eta for a fix/workaround other than pinning the module version? TY!; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-530659556:300,pipeline,pipeline,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530659556,1,['pipeline'],['pipeline']
Deployability,"Just an extra idea... In case you have docker installed, you could use a dockerized scanpy. The guys running SCENIC have a dockerized version of it in their workflow. Maybe you could ask them nicely for their docker image?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477595971:46,install,installed,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477595971,1,['install'],['installed']
Deployability,"Just an update, I've got the PR mostly done, but I'm having trouble keeping the arguments to `sc.queries.gene_coordinates` simple. Could someone who uses that function show me their use case? Is it that common to want the coordinates for only one gene, but also want to limit the coordinates to particular coordinates? . Would it be reasonable to replace this function with something more simple and open ended? I'm thinking just letting the user specify an organism and the fields they'd like. <details>; <summary>Here's a doc-string for what I'm thinking:</summary>. ```python; def biomart_annotations(org, attrs, host=""www.ensembl.org""):; """"""; Retrieve gene annotations from ensembl biomart. Parameters; ----------; org : `str`; Organism to query. Must be an organism in ensembl biomart. ""hsapiens"",; ""mmusculus"", ""drerio"", etc.; attrs : `List[str]`; Attributes to query biomart for.; host : `str`, optional (default: ""www.ensembl.org""); A valid BioMart host URL. Alternative values include archive urls (like; ""grch37.ensembl.org"") or regional mirrors (like ""useast.ensembl.org""). Returns; -------; A `pd.DataFrame` containing annotations. Examples; --------; Retrieve genes coordinates and chromosomes. >>> annot = sc.query.biomart_annotations(; ""hsapiens"",; [""ensembl_gene_id"", ""start_position"", ""end_position"", ""chromosome_name""],; ).set_index(""ensembl_gene_id""); >>> adata.var[annot.columns] = annot; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434:8,update,update,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460865434,1,['update'],['update']
Deployability,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-761937197:11,update,updated,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-761937197,1,['update'],['updated']
Deployability,"Just came across this - is this still relevant @FionaMoon, or has this been resolved with the updated scanpy versions? :); Glad to hear it worked out in your case @molecularsensei!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1798995383:94,update,updated,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1798995383,1,['update'],['updated']
Deployability,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:95,integrat,integrating,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475,1,['integrat'],['integrating']
Deployability,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:108,install,install,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617,7,"['Install', 'install']","['Install', 'install']"
Deployability,Just checked.. same thing applies for windows. It returns an error until you `conda install pytables`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462140641:84,install,install,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462140641,1,['install'],['install']
Deployability,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-788489841:231,update,update,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-788489841,1,['update'],['update']
Deployability,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:71,install,installed,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994,2,['install'],"['install', 'installed']"
Deployability,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:161,integrat,integration,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154,2,"['integrat', 'release']","['integration', 'release']"
Deployability,"Just pushing the updates now @LuckyMD 😄. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?. Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474:17,update,updates,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474,1,['update'],['updates']
Deployability,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:932,integrat,integration,932,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611,1,['integrat'],['integration']
Deployability,"Just to clarify, are you referring to 3 plots in the middle (PCA loading plots)? In new scanpy release, we render both positive and negative genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992:95,release,release,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992,1,['release'],['release']
Deployability,"Just to follow-up because I also had a similar question to the OP. Here's one way to plot several marker genes in different colors on the same UMAP plot. The trick is to make different colormaps that have an alpha gradient so that cells with NA expression appear transparent. Then just use matplot axes to merge the images. The only issue is that Scanpy doesn't yet allow you to remove colorbars for continuous variables, so the multiple colorbars can throw off the scaling, which you can work around by changing the figure parameters. ```; #make red colormap; colors2 = plt.cm.Reds(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:400,continuous,continuous,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['continuous'],['continuous']
Deployability,Just updated the readme https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050921:5,update,updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050921,1,['update'],['updated']
Deployability,"Just updated to scanpy 1.8.2, but the problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062411153:5,update,updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062411153,1,['update'],['updated']
Deployability,"Just want to leave a comment here that the relevant matplotlib parameter to change is ""patch.edgecolor"". The following should fix the missing grouping lines. ```python; matplotlib.rcParams['patch.edgecolor'] = 'black'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/998#issuecomment-913395778:87,patch,patch,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998#issuecomment-913395778,2,['patch'],['patch']
Deployability,"Kinda related, I was about to open an issue on the memory usage of this function. The current implementation can double the memory usage of a program, I believe due to intermediate arrays in the current code. I'd come up with a slower but fewer allocation method for dense arrays (which could probably be sped up with a little `numba`):. ```python; def lessalloc_dense(X):; mean = X.mean(axis=0); mean_sq = np.apply_along_axis(lambda x: np.square(x).mean(), 0, X); var = (mean_sq - mean**2) * ((X.shape[0]/(X.shape[0]-1))); return mean, var; ```. And looked at memory usage using [`memory_profiler`](https://github.com/pythonprofilers/memory_profiler/releases), including @fidelram 's method:. ![mean_var_memory](https://user-images.githubusercontent.com/8238804/40597918-eacd8764-6287-11e8-98ff-017e697b350d.png). [Full script for benchmark here.](https://gist.github.com/ivirshup/a6facfa1ace5b356ea2d18ff3ffe0cb9)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163#issuecomment-392421567:651,release,releases,651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392421567,1,['release'],['releases']
Deployability,Kinda? This is waiting on https://github.com/theislab/anndata/pull/144 and a following AnnData point release. But I think that PR is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-493013715:101,release,release,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-493013715,1,['release'],['release']
Deployability,"LGTM! . Don't worry about the test failures, those are due to a networkx update changing how plots look, which we'll deal with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018:73,update,update,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018,1,['update'],['update']
Deployability,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553195318:161,install,installation,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553195318,1,['install'],['installation']
Deployability,"Let’s coordinate here and come up with what we think is the best strategy:. - @grst and me thought both are fine solutions, that’s why we closed https://github.com/scverse/scanpy-tutorials/issues/64, let’s update that one with what we come up with; - I think the main discussion should happen in https://github.com/scverse/cookiecutter-scverse/issues/40, then we close this issue when we implemented that for scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2636#issuecomment-1691508076:206,update,update,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2636#issuecomment-1691508076,1,['update'],['update']
Deployability,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:100,integrat,integration,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353,1,['integrat'],['integration']
Deployability,"Likewise, I just ran `pip install leidenalg` on an OSX machine which already had scanpy and louvain on it, and it set up effortlessly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437075453:26,install,install,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437075453,1,['install'],['install']
Deployability,"Look at the virtualenv example - PYTHONPATH was empty. The johnnydep application does not actually do an install, it just downloads all the pieces a package calls for and looks at all the listed requirements - and it gives the same version restriction conflict as an actual installation attemp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734:105,install,install,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734,2,['install'],"['install', 'installation']"
Deployability,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```; > from scipy.misc import factorial; E ImportError: cannot import name 'factorial'; ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError; ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166:215,release,release,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166,4,"['install', 'release', 'update']","['install', 'release', 'updated']"
Deployability,Looking forward to this update or is there any other way to achieve this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1552#issuecomment-1435832592:24,update,update,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1552#issuecomment-1435832592,1,['update'],['update']
Deployability,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/601#issuecomment-482069731:278,release,release,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601#issuecomment-482069731,1,['release'],['release']
Deployability,Looks like there is a release out for the `scikit-misc`: https://github.com/has2k1/scikit-misc/compare/v0.4.0...main. Hopefully this fixes it,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2202419517:22,release,release,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2202419517,1,['release'],['release']
Deployability,Looks like this fix was made in a 2021 release - thanks! Will close the issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1325#issuecomment-1662404711:39,release,release,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1325#issuecomment-1662404711,1,['release'],['release']
Deployability,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:102,pipeline,pipelines,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276,2,"['Pipeline', 'pipeline']","['Pipelines', 'pipelines']"
Deployability,Looks like this was solved on master last month. Any chance we could get a bugfix release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758850929:82,release,release,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758850929,1,['release'],['release']
Deployability,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:695,release,release,695,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105,1,['release'],['release']
Deployability,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157:145,integrat,integrated,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157,1,['integrat'],['integrated']
Deployability,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:76,install,install,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004,2,['install'],['install']
Deployability,"Malte, don't you have `pytest` installed locally? Debugging using all these `added prints` etc. commits doesn't help maintain a clean git history. :wink:. Is it possible that there is any ambiguity regarding floating point precision? It's a bit hard for me to debug this. In case you don't have python 3.5 installed. Simply do `conda create -n py35 python=3.5`. Calling `pytest scanpy/tests/marker_gene_overlap.py` should rapidly reveal what's going on. Or simply debugging this in a notebook. Thank you and sorry that this causes trouble!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950:31,install,installed,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479387950,2,['install'],['installed']
Deployability,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:127,update,updates,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473,1,['update'],['updates']
Deployability,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:247,update,updated,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068,2,"['integrat', 'update']","['integrated', 'updated']"
Deployability,Maybe downgrade numba for the time being? IDK to which version though. @stuartarchibald has more insight here. Please follow numba/numba#5955 for updates!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681:146,update,updates,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681,1,['update'],['updates']
Deployability,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430659918:11,release,release,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430659918,3,['release'],['release']
Deployability,"Maybe it's the upgrade to version 1.5.1 that leads to this bug, I can run this piece of code under 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633443990:15,upgrade,upgrade,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633443990,1,['upgrade'],['upgrade']
Deployability,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:64,release,release,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,2,['release'],['release']
Deployability,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:214,integrat,integration-scanorama,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317,2,['integrat'],"['integration', 'integration-scanorama']"
Deployability,My coworker and I had the same issue as @ttgump. Upgrading setuptools fixed it. Maybe it's worth adding a note to the installation troubleshooting documentation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148#issuecomment-391375544:118,install,installation,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-391375544,1,['install'],['installation']
Deployability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:255,install,installs,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,4,"['install', 'patch']","['installs', 'patch']"
Deployability,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:1546,update,update,1546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['update'],['update']
Deployability,My too. I had loompy version 2.0.17 and now I installed the version 2.0.16 and still I'm getting the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-493887143:46,install,installed,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-493887143,1,['install'],['installed']
Deployability,"Nice! But may I ask why you’re still importing everything from umap instead of from pynndescent?. I’d assume if we’d do that we’d be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583:161,update,updates,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583,1,['update'],['updates']
Deployability,"Nice! Thanks!. On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see; > here; > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>; > for description on how to use the new concat strategy; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513:229,integrat,integration-scanorama,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513,1,['integrat'],['integration-scanorama']
Deployability,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848:48,release,release,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848,2,['release'],['release']
Deployability,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1502,update,update,1502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,1,['update'],['update']
Deployability,"No problem, thanks!. I think conflicts in release note are going to be common. Maybe those commits could be separate from the PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615#issuecomment-489859578:42,release,release,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615#issuecomment-489859578,1,['release'],['release']
Deployability,"No release notes are needed, but milestones are needed for this to get back-ported. Also the check for the release notes is actually checking the box in the PR template.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2972#issuecomment-2031877808:3,release,release,3,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2972#issuecomment-2031877808,2,['release'],['release']
Deployability,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:254,configurat,configuration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466,1,['configurat'],['configuration']
Deployability,"No worries, I would have done this for the next release... . Btw: cool that you made your actual `fit_transform` accept `AnnData` objects... Would not have been necessary, but nice! :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/139#issuecomment-386320788:48,release,release,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139#issuecomment-386320788,1,['release'],['release']
Deployability,"No worries. . Right now I'm just thinking of whether this should be called `n_dims` now, and trying to figure out why I have the sneaking suspicion that this broke something last time I looked at it. One last thing from you, could you add a release note to `docs/release-notes/1.9.0.md`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076261309:241,release,release,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179#issuecomment-1076261309,2,['release'],"['release', 'release-notes']"
Deployability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:156,toggle,toggleswitch,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,1,['toggle'],['toggleswitch']
Deployability,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:121,install,install,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:129,install,install,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008,1,['install'],['install']
Deployability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:96,upgrade,upgraded,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,2,['upgrade'],['upgraded']
Deployability,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706; * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`; * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`; * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658; * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852:86,Update,Updated,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852,2,"['Update', 'update']","['Updated', 'update']"
Deployability,"OK I install umap 0.4 . ```; pip install git+git://github.com/lmcinnes/umap@0.4dev; ```. However, it doesn't seem to run any faster and actually throws an error now. ```; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```; gives; ```; AttributeError Traceback (most recent call last); <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440,2,['install'],['install']
Deployability,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:18,install,install,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862,1,['install'],['install']
Deployability,OK! Please add a release note and we’re good to go I think.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1943677733:17,release,release,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1943677733,1,['release'],['release']
Deployability,"OK! This is a bugfix, so I added this PR to the bugfix milestone. Please add a release note for 1.9.6, then it’s ready!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2698#issuecomment-1770654161:79,release,release,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2698#issuecomment-1770654161,1,['release'],['release']
Deployability,"OK, I backported the fix and will do a hotfix today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830#issuecomment-1911978854:39,hotfix,hotfix,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830#issuecomment-1911978854,1,['hotfix'],['hotfix']
Deployability,"OK, I updated the docs so the `.copy()` is in there!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2368351194:6,update,updated,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2368351194,1,['update'],['updated']
Deployability,"OK, done! https://github.com/theislab/scanpydoc/pull/128 is released and this PR is updated. I’m not touching the `/` shortcut: meta/ctrl+k is advertised in a prominent spot and I don’t want to accidentally swallow a user trying to type `/` somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889299384:60,release,released,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889299384,2,"['release', 'update']","['released', 'updated']"
Deployability,"OK, let’s close this in favor of #1733 and update that one with the newest information if necessary",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2088#issuecomment-1787310117:43,update,update,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2088#issuecomment-1787310117,1,['update'],['update']
Deployability,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:518,release,release,518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['release'],['release']
Deployability,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672:352,upgrade,upgraded,352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672,1,['upgrade'],['upgraded']
Deployability,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:675,Install,Installation,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125,4,"['Install', 'install']","['Installation', 'installation']"
Deployability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:491,install,installed,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,1,['install'],['installed']
Deployability,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619:87,pipeline,pipeline,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619,1,['pipeline'],['pipeline']
Deployability,"OK, this is indeed fixed in scanpy master and the bugfix branch (1.9.x). Please try installing the current bugfix branch:. ```bash; pip install 'scanpy @ git+https://github.com/scverse/scanpy.git@1.9.x'; ```. We will release a new feature version in 3 weeks, so there will probably be no bugfix release before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806#issuecomment-1893367676:84,install,installing,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806#issuecomment-1893367676,4,"['install', 'release']","['install', 'installing', 'release']"
Deployability,"OK, this should work. The only issue is that if users check “No release notes necessary” while not checking another box, the “check-relnotes” job still runs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2569#issuecomment-1759813642:64,release,release,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569#issuecomment-1759813642,1,['release'],['release']
Deployability,"OK, updated this so it follows the decision implemented in #1244",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1898572961:4,update,updated,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1898572961,1,['update'],['updated']
Deployability,"Oh! I did upgrade pip and all the packages needed by scanpy, but didn't have the idea of doing:. pip3 install --upgrade setuptools. This fixed it! Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705:10,upgrade,upgrade,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing.; ; I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528:546,update,update,546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528,2,['update'],['update']
Deployability,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/442#issuecomment-457870095:172,release,release,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442#issuecomment-457870095,2,['release'],['release']
Deployability,"Ok , thanks for letting me know. Please check the pull request. I have; verified my code by keeping weights 1 and it has same values when; observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,; Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, I’ll close this one then 🙂; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492237208:212,update,update,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492237208,1,['update'],['update']
Deployability,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package 😅 ; https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616:161,release,release,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616,1,['release'],['release']
Deployability,"Ok, it's related to pandas 0.23 - runs on pandas 0.22. Don't know what happened but I'll fix this tomorrow. In the meanwhile you could `pip install pandas==0.22.0`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390823067:140,install,install,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390823067,1,['install'],['install']
Deployability,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/150#issuecomment-387215323:4,update,updated,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150#issuecomment-387215323,1,['update'],['updated']
Deployability,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:21,install,install,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479,3,['install'],"['install', 'installs']"
Deployability,"Okay, I solved the issue. In my environment, scanpy==1.7.2 does not work with umap==0.5.2. I pip uninstall scanpy and umap-learn. Next I installed umap-learn through conda which was umap==0.5.1. When installing scanpy again it works as expected. . Not sure if its worth looking further into it but pip install scanpy also installs umap==0.5.2 (which does not work at least for me). . Many thanks! ; ![umap_0 5 1](https://user-images.githubusercontent.com/20926246/148250255-5ac00a46-cbc9-4608-a893-0472e32f5fb5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077:137,install,installed,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077,4,['install'],"['install', 'installed', 'installing', 'installs']"
Deployability,"Okay, it’s merged! @taopeng1100, please install the dev version of scanpy like this, and retry:. ```bash; pip install git+https://github.com/theislab/scanpy.git; # or; pip install --user git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278:40,install,install,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278,3,['install'],['install']
Deployability,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320:55,install,installation,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320,3,['install'],"['install', 'installation']"
Deployability,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:74,update,updated,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138,1,['update'],['updated']
Deployability,One idea: ; 1) Cluster the graph with leiden; 2) Coarsen the graph (collapse cells in a single cluster into super nodes); 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color; 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984:326,patch,patches,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984,1,['patch'],['patches']
Deployability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:62,install,install,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,3,"['install', 'integrat']","['install', 'installation', 'integration']"
Deployability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,1,['integrat'],['integrated']
Deployability,"One question related to #891, is there any plotting order for continuous values like higher expression plotted on top? That's more controversial but sometimes dropout et al might obscure cells expressing a gene. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/893#issuecomment-546341102:62,continuous,continuous,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546341102,1,['continuous'],['continuous']
Deployability,"Oops, forgot to ask for a release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933:26,release,release,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933,1,['release'],['release']
Deployability,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607290916:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607290916,2,['install'],['install']
Deployability,"Out of experience, I don't really feel like conda-forge is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:901,release,release,901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['release'],['release']
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5c0e89e99dc2461c654c549435a73f547f3573ce; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3339: Add PYI lints'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3339-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3339 on branch 1.10.x (Add PYI lints)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5d5d873b1fb0353089569f85580b43437df9c6cd; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3104: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3104-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3104 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 8d046ff37e024ae88eadfb22ea8fd142a6b95aa1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3093: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3093-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3093 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 05dcf68f32ce255447ea804de55babefb3c47c92; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2753: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2753-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2753 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 330a099ffe76286f0f047387701af7e9fd58831a; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2838: Fix pytest 8 compat'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2838-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2838 on branch 1.9.x (Fix pytest 8 compat)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036,6,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 47664d83a7bc47756356b907e5719076ab187361; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2784: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2784-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2784 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 4f4b1c3a655546d981360bcce625d354a4291385; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2811: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2811-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2811 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 585f58c9e4dd82dd7809a831538c4e230b008818; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2841: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2841-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2841 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5ccce795b19a5aa59a6b1f1c3552884ed6fc94d1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2544: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2544-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2544 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 86dc4d5d96eb7547833e7805ea2f7d603bd3ba2d; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2779: Fix anndata warnings'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2779-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2779 on branch 1.9.x (Fix anndata warnings)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 95206dc54c8bb0d9d478f09f47dff9477a5c58c4; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2704: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2704-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2704 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 b23229f9bfc95ff90a5d6393b4d53d062190d5bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2732: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2732-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2732 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 bf5f27aa9e968de6e73fc7abb46a89084ddf6880; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2831: Prepare 1.9.8, stop ignoring citation errors'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2831-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2831 on branch 1.9.x (Prepare 1.9.8, stop ignoring citation errors)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c2f706b35d52a5e21ccf84f1cd299b0dadf49668; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2716: Add missing link targets'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2716-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2716 on branch 1.9.x (Add missing link targets)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c410cd123f5487f25c08b421c8d06da50551ff73; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2799: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2799-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2799 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 e5d41d4aa58a925f0fa5cfcf580cb975167a71c9; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2235: Separate test utils from tests'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2235-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2235 on branch 1.9.x (Separate test utils from tests)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358#issuecomment-438006625:96,update,update,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358#issuecomment-438006625,1,['update'],['update']
Deployability,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737:6,update,updated,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737,2,['update'],"['update', 'updated']"
Deployability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:55,install,installing,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['install'],['installing']
Deployability,PS: `pbmc68k_reduced` and `toggleswitch` are from back in the days; they should remain the only examples that actually have the data in the repository and the PyPI distributions. All other datasets should download their data from some stable URL...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476589204:27,toggle,toggleswitch,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476589204,1,['toggle'],['toggleswitch']
Deployability,"Palantir is not yet in a released version: https://scanpy.readthedocs.io/en/latest/#on-master-march-21-2019. We'll soon have 1.4.1. Until then, you need to install from GitHub as described in the installation section. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-477679626:25,release,released,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-477679626,3,"['install', 'release']","['install', 'installation', 'released']"
Deployability,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:24,update,updated,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788,1,['update'],['updated']
Deployability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:145,pipeline,pipeline,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,1,['pipeline'],['pipeline']
Deployability,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:721,pipeline,pipeline,721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['pipeline'],['pipeline']
Deployability,Please ask usage questions here: https://discourse.scverse.org/. You should not integrate normalized and unnormalized counts. Consider getting the raw counts or integrating on the normalized counts,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652:80,integrat,integrate,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652,2,['integrat'],"['integrate', 'integrating']"
Deployability,Please install the latest scanpy in a clean environment. Feel free to reopen if this issue persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127,1,['install'],['install']
Deployability,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746:7,update,update,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746,2,"['release', 'update']","['release', 'update']"
Deployability,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:74,install,install,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746,2,['install'],['install']
Deployability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:733,release,releases,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['release'],['releases']
Deployability,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies?. https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198:96,install,installed,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198,1,['install'],['installed']
Deployability,"Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4531,Install,Installing,4531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"Quite possibly, I need to look. Just updated from 1.3.x to 1.4.3 this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-493234847:37,update,updated,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-493234847,1,['update'],['updated']
Deployability,"Ran into this today as a coworker wanted to stick a UMAP legend into `lower left` and couldn't. I whipped up a hotfix, which turned out to be exactly the same syntax as you've got going on here, and was on my way to mention it when I found this. Would be nice to see this integrated as the docs imply this is possible, and it seems useful to have on occasion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426:111,hotfix,hotfix,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426,2,"['hotfix', 'integrat']","['hotfix', 'integrated']"
Deployability,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:251,install,installing,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897,4,"['install', 'release']","['installed', 'installing', 'releases']"
Deployability,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682,1,['install'],['install']
Deployability,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:110,continuous,continuous,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932,1,['continuous'],['continuous']
Deployability,"Recently, this tutorial was updated with what you need:; https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from; https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>; wrote:. > Thanks a lot. All of these new features are what we need!; >; > I notice that the tutorial has not been updated yet (such as; > sc.tl.filter_rank_genes_groups( ) and rna velocity function in; > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find; > these features occasionally. Could you add them in scanpy tutorial ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-474006729:28,update,updated,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-474006729,2,['update'],['updated']
Deployability,"Regarding Q3 from my previous comment, I tried few things and I think it is the easiest to keep the coexpression data as continuous and remove the colorbar afterwards. . I have, however, correction to what what was written before. `ax.images.im[-1].colorbar.remove()` doesn't work (in the case of umap) since it is a scatter plot. `ax.collections[-1].colorbar.remove()` needs to be used instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-588981048:121,continuous,continuous,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-588981048,1,['continuous'],['continuous']
Deployability,"Regarding running CI with minimal optional deps, I’d say we could change this line:. https://github.com/scverse/scanpy/blob/86e2a35c1df2b61772e5f898bfcd11abb8d9fb2c/.azure-pipelines.yml#L46. … to be parametric like `pip install .[dev,test$(test_extras))]`, and run things once with `test_extras=''` and once with `test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'`. we’d probably have to make a lot of tests optional with `@skipif(not find_spec('thing'), ...)` though, and of course remove some things from the `test` extra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088710180:172,pipeline,pipelines,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088710180,2,"['install', 'pipeline']","['install', 'pipelines']"
Deployability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:277,install,installed,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,1,['install'],['installed']
Deployability,"Right after booting up my Docker container, this is the result of `conda list | grep anndata`:. `anndata 0.8.0 pyhd8ed1ab_1 conda-forge`. Then, after running `pip install anndata --upgrade`, and then again `conda list | grep anndata`:. `anndata 0.10.6 pypi_0 pypi`. At this point, `pkg_version('anndata')` errors out as described.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037957505:163,install,install,163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037957505,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Same error here...any ideas?. ```; -----; anndata 0.8.0; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 9.0.1; PyQt5 NA; anndata 0.8.0; anndata2ri 0.0.0; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; backports NA; beta_ufunc NA; binom_ufunc NA; bs4 4.10.0; cached_property 1.5.2; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.02.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.10.0; entrypoints 0.4; fsspec 2022.02.0; get_version 3.5.4; h5py 3.6.0; igraph 0.9.9; ipykernel 6.9.1; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; packaging 21.3; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.7; pytz 2021.3; pytz_deprecation_shim NA; rpy2 3.4.2; scanpy 1.8.2; scipy 1.7.3; seaborn 0.11.2; setuptools 59.8.0; sinfo 0.3.1; sip NA; six 1.16.0; sklearn 1.0.2; soupsieve 2.3.1; sphinxcontrib NA; spyder 5.2.2; spyder_kernels 2.2.1; spydercustomize NA; statsmodels 0.13.2; storemagic NA; tables 3.7.0; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.1.1; typing_extensions NA; tzlocal NA; wcwidth 0.2.5; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 7.32.0; jupyter_client 7.1.2; jupyter_core 4.9.2; -----; Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) [GCC 9.4.0]; Linux-5.4.0-109-generic-x86_64-with-debian-bullseye-sid; 16 logical CPU cores, x86_64; -----; Session information updated at 2022-04-20 18:16; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1104127300:1933,update,updated,1933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1104127300,1,['update'],['updated']
Deployability,Same here. I would like to run Scanpy on the latest Python version. Does anyone have an ETA or a way to force installation (at our own risk of course)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1327933295:110,install,installation,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1327933295,1,['install'],['installation']
Deployability,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:57,install,install,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,2,['install'],"['install', 'installing']"
Deployability,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014,1,['install'],['install']
Deployability,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:127,install,installation,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,2,['install'],['installation']
Deployability,"Scanorama's `nn_approx` uses annoy, which is a package that has caused many a headache due to its instability. From my experience, these segfaults started showing up since 1.17.x got released, and tend to begin happening more consistently if anything is installed into the environment after annoy itself somehow. Downgrading to 1.16.3 via pip tends to make them go away. It would be neat if there was some sort of more reliable workaround, keeping annoy stable and not bothering the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866#issuecomment-1976231395:183,release,released,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866#issuecomment-1976231395,2,"['install', 'release']","['installed', 'released']"
Deployability,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:258,continuous,continuous,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721,1,['continuous'],['continuous']
Deployability,"See my last comment. After fixing the colormaps in this PR, I didn’t update the images, but the tests still pass. What’s up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449:69,update,update,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449,1,['update'],['update']
Deployability,Seems like a release is in order. that code was merged in April,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651773726:13,release,release,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651773726,1,['release'],['release']
Deployability,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:144,release,released,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294,1,['release'],['released']
Deployability,"Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598:190,install,installing,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598,1,['install'],['installing']
Deployability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:208,update,update,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,1,['update'],['update']
Deployability,"Should be fixed as of `1.7.2`, which was just released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904:46,release,released,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904,1,['release'],['released']
Deployability,"Should be fixed in scipy 1.11.3: https://github.com/scipy/scipy/issues/18716. You only have 1.10.1 installed. Please try upgrading. If the error persists, feel free to follow up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3141#issuecomment-2210675845:99,install,installed,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3141#issuecomment-2210675845,1,['install'],['installed']
Deployability,Shouldn’t we just depend on `requests` if it’s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:145,install,installed,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642,1,['install'],['installed']
Deployability,Should’ve checked the docs for installation. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-477692840:31,install,installation,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-477692840,1,['install'],['installation']
Deployability,"Since I saw this just linked to another issue (#2540), I'll post my updates and ways I got around this (since I had to re-investigate this last week for a separate script). We have not updated our Scanpy on our platform since I posted this ticket (though will soon), but I ran into another situation where the code worked on an older version of scanpy but not on a later version which was running on my machine. I just wrote a try/except block to handle this. ```python3; """"""; One of the scanpy versions introduced a bug that was recently fixed, ; where pl.rank_genes_groups works but not pl.rank_genes_groups_<plot>. ; I believe it is because adata.var.index is being stored as the ; adata.uns ""gene_symbol"" output for tl.rank_genes_groups, ; and pl.rank_genes_groups correctly looks for the adata.var.index, ; but pl.rank_genes_groups_<plot> is erroneously looking for the adata.var.gene_symbol there instead (per my supplied ""gene_symbols"" arg). Seems to work in scanpy 1.7.2 but is broke in 1.8.2; """""". ax = sc.pl.rank_genes_groups(adata, groups=[query_cluster],; gene_symbols='gene_symbol', n_genes=n_genes, save=""_comp_ranked.png""). try:; # Try 1.7.2 way first; ax = sc.pl.rank_genes_groups_violin(adata, groups=query_cluster, use_raw = False,; gene_symbols=""gene_symbol"", n_genes=n_genes, save=""_comp_violin.png""); except:; # Use gene names if that doesn't work; gene_names = adata.var.loc[adata.uns[""rank_genes_groups""]['names'][query_cluster]][""gene_symbol""][:n_genes].tolist(); ax = sc.pl.rank_genes_groups_violin(adata, groups=query_cluster, use_raw = False,; gene_symbols=""gene_symbol"", gene_names=gene_names, save=""_comp_violin.png""). ```. Earlier, I had mentioned using `use_raw=True` for the previous script I had this issue with, but ultimately removed that parameter due to some other downstream things",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2258#issuecomment-1625573706:68,update,updates,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2258#issuecomment-1625573706,2,['update'],"['updated', 'updates']"
Deployability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:19,update,updated,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['update'],['updated']
Deployability,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865:49,update,updated,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865,1,['update'],['updated']
Deployability,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:27,release,release,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133,3,['release'],['release']
Deployability,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715,8,"['Install', 'install', 'update', 'upgrade']","['Installation', 'install', 'installation', 'update', 'upgrade']"
Deployability,"So actually, I run a test on a fresh docker image (with this [Dockerfile](https://gist.github.com/pwl/005c781cbe19f5e961b59366f738caaf)) and it still fails to install scanpy with the same error. I had some success with changing the default python encoding to utf-8 as shown in the Dockerfile but it only works when calling python3 directly and not for pip3. However, it worked with python2. I guess python3 is not supported by scanpy, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579:159,install,install,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579,1,['install'],['install']
Deployability,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:65,continuous,continuous,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247,2,['continuous'],['continuous']
Deployability,So it seems ComBat outputs np.float64 😐. I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/324#issuecomment-433441386:110,update,update,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433441386,1,['update'],['update']
Deployability,So my X actually contained negative values. I removed my _scanpy.pp.scale_ step and tried this downsampling step earlier in my pipeline and its working. Thanks for taking time to help with this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2131#issuecomment-1033885282:127,pipeline,pipeline,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2131#issuecomment-1033885282,1,['pipeline'],['pipeline']
Deployability,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:21,install,installed,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310,1,['install'],['installed']
Deployability,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768,2,"['continuous', 'update']","['continuous', 'update']"
Deployability,"So the issue is incompatibilities between versions of sphinx and their `objects.inv`? Is there an open issue in sphinx for this?. > > Do you expect this to be compatible with older sphinx versions?; >; > Of course! Why would it not be?. Mostly because the inner workings of Sphinx are a mystery to me, so I have no idea what features your changes rely on 😆. This was mainly me asking if we should bump the minimum version of sphinx allowed. Maybe we should if there are issues with the `objects.inv`s?. > Maybe we can link to the dev docs?. Or we could add the classes to nitpick ignore? Then once the docs are rebuilt it will do the right thing without any intervention, and we don't have to be keeping an eye out for this. My main concern here is that the `scipy.github.io` address may not be permanent, similar to how numpy temporarily used a github.io address while they revamped their docs. Basically, it may just break or go down without notice. I thought we could even just trigger a new build of the anndata stable docs, but there's an issue there, probably to do with sphinx not being pinned on release. I do want to make a new anndata release soon-ish though. -----------. I'm happy for you to pick one of the approaches and merge it. AnnData could also use a fix for this, I've temporarily just pinned sphinx below 4.1 there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295:1104,release,release,1104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295,2,['release'],['release']
Deployability,"So we're really not accepting any packages into scanpy.external anymore and will deprecate external soon. We've now updated our documentation to reflect this. However, we'd be very happy to welcome your package in the scverse ecosystem -> https://scverse.org/packages/#ecosystem. I'm sorry that you put in all this work but then get denied by us like this :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355#issuecomment-1431674978:116,update,updated,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355#issuecomment-1431674978,1,['update'],['updated']
Deployability,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon.; I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261,2,['release'],['release']
Deployability,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,2,['release'],['release']
Deployability,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:60,continuous,continuous,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['continuous'],['continuous']
Deployability,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303,1,['install'],['install']
Deployability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:464,Integrat,Integration,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,"So, this is solved in anndata 0.5 and scanpy 0.4.3. See the release notes (https://scanpy.readthedocs.io) and https://github.com/theislab/anndata/commit/63500075e926f202e856bd04ec673df55bbd2460 and the [example](http://anndata.readthedocs.io/en/latest/anndata.AnnData.concatenate.html). Hope this is a meaningful default. If you pass `index_unique=None`, then it keeps the previous indices including duplicates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-364922530:60,release,release,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-364922530,1,['release'],['release']
Deployability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:306,update,update,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,1,['update'],['update']
Deployability,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:254,update,updated,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517,2,"['release', 'update']","['release', 'updated']"
Deployability,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:744,release,release,744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,1,['release'],['release']
Deployability,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:55,release,release,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['release'],['release']
Deployability,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week.; Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/110#issuecomment-376174066:203,release,release,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110#issuecomment-376174066,1,['release'],['release']
Deployability,"Sorry about this bug in AnnData views, which have only recently been introduced. Is fixed in anndata 0.4.4 `pip install anndata --upgrade` and on the master branch: https://github.com/theislab/anndata/commit/ba9b3eed381ce427920ec67e13331d5423a5d9b3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449:112,install,install,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,integrat,integration,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857,1,['integrat'],['integration']
Deployability,"Sorry for being offline for a few hours and great that you could resolve it. In the next version, this confusion will not appear again. Even though, obviously, any Scanpy release reproduces all examples on https://github.com/theislab/scanpy_usage, we're still at an early stage in the package. Things are progressing very fast and structure and maintenance of the package are becoming more and more professional. Also, soon, many new features and examples where other Python packages are used will be added. Thank you for a bit of patience at this stage.; Alex. PS: We now have an initial version of the documentation: https://scanpy.readthedocs.io.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985:171,release,release,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985,1,['release'],['release']
Deployability,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again?; For me it seems to work; ```python; fig, ax = plt.subplots(1,3, figsize=(20,6)); sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); plt.tight_layout(pad=3.0); plt.show(); ```; ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787:65,update,update,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787,1,['update'],['update']
Deployability,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:75,install,install,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075,4,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:31,upgrade,upgraded,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575,2,['upgrade'],['upgraded']
Deployability,"Sorry for the late reply, thought I responded to this!. > would also make sense to have this as colorbar_loc as this only really applies for continuous coloring. I think that would make sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199:141,continuous,continuous,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199,1,['continuous'],['continuous']
Deployability,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:212,install,installation,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620,1,['install'],['installation']
Deployability,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:133,install,installed,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572,1,['install'],['installed']
Deployability,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:108,update,updated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549,1,['update'],['updated']
Deployability,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:222,install,installation,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243,4,['install'],"['install', 'installation', 'installing']"
Deployability,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,configurat,configuration,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773,2,"['configurat', 'upgrade']","['configuration', 'upgrade']"
Deployability,Sounds good - added an entry to the release note and updated the tests to not use the parameterization.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767,2,"['release', 'update']","['release', 'updated']"
Deployability,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:126,install,installable,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478,2,['install'],['installable']
Deployability,Sounds great to me! Looking forward. Would be interesting to compare these three t-SNEs: ; ```; sc.pp.neighbors(); sc.tl.tsne(binarize=True); sc.tl.tsne(binarize=False); sc.pp.neighbors_tsne(); sc.tl.tsne(binarize=False); ```; on a couple of datasets after the standard scanpy preprocessing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594:291,pipeline,pipeline,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594,1,['pipeline'],['pipeline']
Deployability,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:42,install,install,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560,4,['install'],"['install', 'installing']"
Deployability,"Sure! I wansn't sure if there were other bugs to fix or PRs to merge before we wanted to make a release. I'd also like to get @fidelram to give this a look over. I think I didn't break anything, but he'd be in a better position to tell if that were the case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-509060284:96,release,release,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-509060284,1,['release'],['release']
Deployability,"Sure, by all means, open a PR at https://github.com/ebi-gene-expression-group/scanpy-scripts with the directory reformatting and needed metadata files for pip installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437046217:159,install,installation,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437046217,1,['install'],['installation']
Deployability,"Sure, thank you! Care to do a quick PR? Then we can point @taopeng1100 in the direction of installing scanpy’s dev version and everyone’s happy. @taopeng1100 please reply by GitHub comment and not by email anymore, it spams up this comment section. I always have to remove some junk your email program adds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281:91,install,installing,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281,1,['install'],['installing']
Deployability,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:120,UPDATE,UPDATE,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,2,"['UPDATE', 'integrat']","['UPDATE', 'integration-scanorama']"
Deployability,"Temporary fix for anyone looking in case an upgrade is not yet possible:; ```python; import scipy.spatial.distance as ssd; from contextlib import contextmanager. def squareform_force_zero_diagonal(X, *args, **kwargs):; if len(X.shape) == 2:; if isinstance(X, pd.DataFrame):; X.iloc[(np.arange(X.shape[0]), np.arange(X.shape[0]))] = 0; else:; X[(np.arange(X.shape[0]), np.arange(X.shape[0]))] = 0; return _squareform(X, *args, **kwargs). @contextmanager; def patch_squareform():; _squareform = ssd.squareform; ssd.squareform = squareform_force_zero_diagonal; try:; yield; finally:; ssd.squareform = _squareform. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); with patch_squareform():; sc.tl.dendrogram(adata, groupby='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2125#issuecomment-1042300588:44,upgrade,upgrade,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2125#issuecomment-1042300588,1,['upgrade'],['upgrade']
Deployability,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284,1,['update'],['update']
Deployability,Thank you again! I'm merging this. Release prior to this is 1.3.4.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-443067468:35,Release,Release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-443067468,1,['Release'],['Release']
Deployability,"Thank you and sorry about the confusion, I remembered this was an option present in earlier releases, but I was wrong!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/560#issuecomment-476837808:92,release,releases,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560#issuecomment-476837808,1,['release'],['releases']
Deployability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:1487,pipeline,pipelines,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,1,['pipeline'],['pipelines']
Deployability,"Thank you for checking the code. I have also been thinking about possible test(s), the only thing that comes to my mind is to check the output against a fixed reference result, which I could verify on a few different machines. But I'm not sure whether this wouldn't cause problems in the future. This could be troublesome to maintain as the matrix will need be stored somewhere and may need re-checking if e.g. numpy has some relevant updates. I am open to suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349:435,update,updates,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349,1,['update'],['updates']
Deployability,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275#issuecomment-427036065:78,update,updated,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275#issuecomment-427036065,1,['update'],['updated']
Deployability,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:162,update,update,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324,1,['update'],['update']
Deployability,"Thank you for the comment. Do you mean these packages will be installed when we install Scanpy? Sorry that I don't understand. When I try to use `scanpy.tl.louvain`, it says `ModuleNotFoundError: No module named 'louvain'`, and I don't know how to solve it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938:62,install,installed,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938,2,['install'],"['install', 'installed']"
Deployability,"Thank you for the detailed explanation @takluyver, this helps a lot!. @ivirshup A dev environment should successfully execute the `try` block. I designed that `except` clause to be hit when people import the installed production version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490:208,install,installed,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490,1,['install'],['installed']
Deployability,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:45,install,install,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292,4,['install'],"['install', 'installation']"
Deployability,Thank you for the release update! I just had to; `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475:18,release,release,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475,3,"['install', 'release', 'update']","['install', 'release', 'update']"
Deployability,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:139,toggle,toggleswitch,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055,1,['toggle'],['toggleswitch']
Deployability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:99,update,update,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,1,['update'],['update']
Deployability,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/428#issuecomment-456015073:355,update,updated,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428#issuecomment-456015073,1,['update'],['updated']
Deployability,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/143#issuecomment-386718816:42,update,update,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143#issuecomment-386718816,1,['update'],['update']
Deployability,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:82,update,update,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480,1,['update'],['update']
Deployability,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:371,install,install,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244,1,['install'],['install']
Deployability,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:73,update,updates,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980,1,['update'],['updates']
Deployability,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good!. The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531,1,['update'],['updated']
Deployability,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/227#issuecomment-411656281:64,update,update,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227#issuecomment-411656281,1,['update'],['update']
Deployability,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:388,release,release,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969,1,['release'],['release']
Deployability,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,integrat,integrating,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335,1,['integrat'],['integrating']
Deployability,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```; pip install dask[array] zappy zarr; pip install pytest; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-456825801:75,install,installed,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-456825801,4,['install'],"['install', 'installed']"
Deployability,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,integrat,integrated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439,1,['integrat'],['integrated']
Deployability,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:38,upgrade,upgraded,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097,1,['upgrade'],['upgraded']
Deployability,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545:708,update,updates,708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545,1,['update'],['updates']
Deployability,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Thanks Philipp, I have updated and push the code. I hope you will accept; this pull request now. To support PCA and scanpy for weighted sampling, you; can just set a parameter , observations/samples weights at the time user; input matrix and then we can modify PCA and remaining this code is fine. I; am asking for weights because user may extracted those weights either with; sampling technique or may be sometime user can give weights of his own; desired e.g. he want to focus one cell type etc. So we should support; weights generally rather specifically. Thanks,; Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,; >; > But i will suggest to just support weights instead of coreset, may be user; > want to sample data with some other weighting technique. So we should ask; > them to just put the weights for observations, then we need to modify PCA; > as well and i think my code will support most of plots and marker genes,; > but not PCA, because my input is PCA matrix with weights for each; > observations.; >; > Thanks,; > Khalid; >; > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>; > wrote:; >; >> Long-term, we should think about the design here: Specifying weights all; >> the time is possible, but not very nice for users. So a few questions come; >> to mind:; >>; >> Should we add scanpy.pp.coreset, which would create a sampling and add; >> adata.obs['coreset_weights'] or simply adata.obs['weights']?; >>; >> If we do that or plan to in the future, how should the added weights; >> parameter to all these functions work?; >>; >> I think it might default to 'coreset_weights', and the functions would; >> automatically use that .obs column if it exists. Users should also still; >> be able to specify weights manually as in this PR.; >>; >> So the type of the parameter would be Union[str, pd.DataFrame,; >> Sequence[Union[float, int]]].; >> ------------------------------; >>; >> All of that doesn’t really affect th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004:23,update,updated,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004,1,['update'],['updated']
Deployability,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:297,install,install,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596,2,['install'],"['install', 'installed']"
Deployability,Thanks a lot. All of these new features are what we need!. I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-473309434:99,update,updated,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-473309434,1,['update'],['updated']
Deployability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:883,update,update,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,1,['update'],['update']
Deployability,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:47,pipeline,pipeline-for-everything,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593,2,['pipeline'],"['pipeline-for-everything', 'pipelines']"
Deployability,"Thanks for catching this! Could you add a test so this doesn't happen again in the future?. Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802,1,['release'],['release']
Deployability,"Thanks for clarification. I had thought it is for filtering out genes. On Mon, May 18, 2020 at 2:21 AM Rachel Ng <notifications@github.com> wrote:. > It makes sense to use AND logic, because the function keeps genes that; > satisfy all three conditions.; >; > 1. Fraction of cells inside the cluster expressing the gene must be; > greater than min_in_group_fraction; > 2. Fractions of cells outside the cluster expressing the gene must be; > less than max_out_group_fraction; > 3. Fold change must be greater than min_fold_change; >; > But there are remaining issues (calculation of fold change and using the; > absolute value of the fold change) in this function that needs to be; > updated #863 <https://github.com/theislab/scanpy/issues/863>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1213#issuecomment-629970781>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADG2PVZVFSYU3ST4ESJFS33RSDHVXANCNFSM4NAA5V2A>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750:684,update,updated,684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750,1,['update'],['updated']
Deployability,"Thanks for clarifying this again, @ivirshup! We should have changed the default value already for 1.4. I'll add a note to the release notes and it's fine... ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-475422571:126,release,release,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-475422571,1,['release'],['release']
Deployability,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:397,update,updated,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151,1,['update'],['updated']
Deployability,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; CoreFoundation NA; Foundation NA; PIL 9.5.0; PyObjCTools NA; anyio NA; appnope 0.1.3; argcomplete NA; asttokens NA; attr 23.1.0; babel 2.12.1; backcall 0.2.0; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; fastjsonschema NA; google NA; h5py 3.9.0; idna 3.4; igraph 0.10.4; importlib_resources NA; ipykernel 6.23.3; ipython_genutils 0.2.0; ipywidgets 8.0.6; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonschema 4.17.3; jupyter_events 0.6.3; jupyter_server 2.6.0; jupyterlab_server 2.23.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; louvain 0.8.0; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.0; numba 0.57.0; numexpr 2.8.4; numpy 1.24.3; objc 9.2; overrides NA; packaging 23.1; pandas 2.0.2; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.0; prometheus_client NA; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pydev_jupyter_utils NA; pydev_jupyter_vars NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pygments 2.15.1; pyparsing 3.1.0; pyrsistent NA; pythonjsonlogger NA; pytz 2023.3; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; scipy 1.11.0; seaborn 0.12.2; send2trash NA; session_info 1.0.0; setuptools 68.0.0; six 1.16.0; sklearn 1.2.2; sniffio 1.3.0; socks 1.7.1; stack_data 0.6.2; statsmodels 0.14.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 1.12.1; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; urllib3 2.0.3; wcwidth 0.2.6; w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1608050519:144,update,updated,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1608050519,1,['update'],['updated']
Deployability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:208,install,install,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,2,['install'],"['install', 'installation']"
Deployability,"Thanks for letting me know!. If it's working in the newest version, there's not much for us to fix. Please let us know if you run into the error on the latest release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126:159,release,release,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126,1,['release'],['release']
Deployability,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:105,install,installed,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860,1,['install'],['installed']
Deployability,Thanks for opening an issue!. I believe scipy's `mmread` (which is being used under the hood) recently switched to using `fast_matrix_market` as of scipy `1.12`. * https://github.com/alugowski/fast_matrix_market/issues/22; * https://github.com/scipy/scipy/pull/18631. So I'm sure any action here is needed. Please let me know if this isn't behaving as expected with the newest scipy release.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846#issuecomment-1938362632:383,release,release,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846#issuecomment-1938362632,1,['release'],['release']
Deployability,"Thanks for opening the issue. It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this. Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484:171,update,update,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484,2,"['install', 'update']","['installation', 'update']"
Deployability,"Thanks for opening this PR!. Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476:485,release,release,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476,1,['release'],['release']
Deployability,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:493,install,install-,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],['install-']
Deployability,"Thanks for the PR!. On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes?. I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187:144,release,release,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187,3,"['release', 'update']","['release', 'updated']"
Deployability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:499,update,updated,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,1,['update'],['updated']
Deployability,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs?. Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033:162,release,release,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033,1,['release'],['release']
Deployability,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488,1,['integrat'],['integration']
Deployability,Thanks for the fix! . I've just added a release note.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651:40,release,release,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651,1,['release'],['release']
Deployability,Thanks for the fix! Can we include this in the next scanpy release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693958817:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693958817,1,['release'],['release']
Deployability,"Thanks for the info!. I was about to rant that this is weird and broken, but locales are literally the first section in the Ubuntu image docs, so I can’t blame them (too much): https://hub.docker.com/_/ubuntu. They probably use `POSIX` as `C.UTF-8` isn’t standard. ([blame the C standard consortium](https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe)). They recommend `ENV LANG C.UTF-8` though, maybe that works for you. (otherwise installing `locales` and your instructions work too)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344265736:464,install,installing,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344265736,1,['install'],['installing']
Deployability,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-410620495:128,update,updated,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-410620495,1,['update'],['updated']
Deployability,Thanks for the investigation! @metoru can you try with #2928?. ```shell; pip install git+https://github.com/scverse/scanpy.git@fix-dendro-corr; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2804#issuecomment-2006642107:77,install,install,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2804#issuecomment-2006642107,1,['install'],['install']
Deployability,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:331,update,update,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056,1,['update'],['update']
Deployability,"Thanks for the report. I'm having trouble reproducing this behaviour locally. Two thoughts:. 1. It looks like there's a newer version of leidenalg available, could you upgrade that?; 2. Maybe there is something about the neighborhood graph. Could you either: reproduce this with some dummy data (e.g. `sc.datasets.blobs`) or share the `test` object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906#issuecomment-1997818178:168,upgrade,upgrade,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906#issuecomment-1997818178,1,['upgrade'],['upgrade']
Deployability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:782,release,releases,782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['release'],['releases']
Deployability,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:74,install,install,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986,3,['install'],"['install', 'installing']"
Deployability,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:220,update,update,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133,2,"['integrat', 'update']","['integrated', 'update']"
Deployability,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/855#issuecomment-537134057:60,install,installing,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855#issuecomment-537134057,1,['install'],['installing']
Deployability,"Thanks for the suggestions and sorry for the late response. Yes, I was using google colab, and a newer version of matplotlib did the trick. I upgraded to the 3.5.3 version of matplotlib, and restarted the runtime, and that did the trick, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2332#issuecomment-1328259095:142,upgrade,upgraded,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2332#issuecomment-1328259095,1,['upgrade'],['upgraded']
Deployability,"Thanks for the thorough issue report! I recall running into this before with some other igraph algorithm, so there might be a similar patch somewhere else in the codebase. Would you be interested in opening a PR to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568:134,patch,patch,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568,1,['patch'],['patch']
Deployability,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924#issuecomment-554854817:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924#issuecomment-554854817,2,['update'],['update']
Deployability,Thanks for the update! Glad to hear it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271,1,['update'],['update']
Deployability,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992,1,['update'],['update']
Deployability,Thanks for the update! Merged via #1595,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049,1,['update'],['update']
Deployability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['update'],['update']
Deployability,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814:15,update,updated,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814,2,['update'],['updated']
Deployability,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:394,pipeline,pipelines,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745,1,['pipeline'],['pipelines']
Deployability,Thanks for this PR @sjfleming . If you don't mind I will integrate this functionality into a new PR that also covers #512 and #525,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944:57,integrat,integrate,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944,1,['integrat'],['integrate']
Deployability,"Thanks for this PR, this looks interesting! Sorry for taking a while to get back to you, we've been quite busy getting a release out. We'll try and get back to you with more in the next couple weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191:121,release,release,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191,1,['release'],['release']
Deployability,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:239,install,installed,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412,2,['install'],"['installation', 'installed']"
Deployability,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:25,update,updated,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,3,['update'],['updated']
Deployability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1365,release,releases,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['release'],['releases']
Deployability,"Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-718667650:16,update,update,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-718667650,1,['update'],['update']
Deployability,"Thanks! And sorry, this was something I had already fixed locally, but not yet pushed. Now it's up (https://github.com/theislab/scanpy/commit/320fa421aa2e1dcb15d047c629416b8640eb1635). The first stable release is not far away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/24#issuecomment-308729587:202,release,release,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/24#issuecomment-308729587,1,['release'],['release']
Deployability,"Thanks! I've moved the release note to 1.10, since this is more of a feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1948544838:23,release,release,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1948544838,1,['release'],['release']
Deployability,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-460613715:55,update,update,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-460613715,1,['update'],['update']
Deployability,"Thanks! This will, however, only work for `anndata>=0.7.2a1`. So you'd also have to update `scanpy/requirements.txt`. I left some more comments in #1439.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351:84,update,update,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351,1,['update'],['update']
Deployability,Thanks! you can just specify `n_top_genes=2000` or so until the fix is released,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427#issuecomment-1860902089:71,release,released,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427#issuecomment-1860902089,1,['release'],['released']
Deployability,"Thanks!. I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487264046:14,update,updated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487264046,1,['update'],['updated']
Deployability,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:39,release,released,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435,3,"['patch', 'release']","['patch', 'release', 'released']"
Deployability,"Thanks, good catch!. This is already fixed in #3194, we just need to cut a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3219#issuecomment-2348487691:75,release,release,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3219#issuecomment-2348487691,1,['release'],['release']
Deployability,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,integrat,integrated,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['integrat'],['integrated']
Deployability,"Thanks, what is the default backend on macOS? I only install matplotlib via the scanpy/anndata depenancies in a conda environment (and `MPLBACKEND` is unset). Upon installing wxpython, I can confirm the used backend. ```; >>> matplotlib.__version__; '3.2.2'; >>> matplotlib.rcParams['backend']; 'WXAgg'; >>> matplotlib.get_backend(); 'WXAgg'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483,2,['install'],"['install', 'installing']"
Deployability,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:360,install,install,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023,1,['install'],['install']
Deployability,Thanks. I merged your PR and released a new package. Closing this issue now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/793#issuecomment-526320133:29,release,released,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793#issuecomment-526320133,1,['release'],['released']
Deployability,"Thanks. On Mon, 6 May 2019 at 18:49, Koncopd <notifications@github.com> wrote:. > It should work if you install from github.; > fe2580c; > <https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/631#issuecomment-489690557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4SWBVPGTIQHOG365L3PUBOS3ANCNFSM4HLAPBTA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/631#issuecomment-489750461:104,install,install,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631#issuecomment-489750461,1,['install'],['install']
Deployability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:121,update,updated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,4,"['install', 'update']","['install', 'installing', 'update', 'updated']"
Deployability,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:112,install,installed,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092,1,['install'],['installed']
Deployability,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:27,install,install,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015,4,['install'],"['install', 'installed', 'installing']"
Deployability,"That'd be great, thanks!. I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270:177,pipeline,pipelines,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270,1,['pipeline'],['pipelines']
Deployability,"That's a great point, I'll definitely implement that to speed things up. I also read that same paper, which made me realize that we (and most other pipelines as far as I know) use mean log. I agree that the original interpretation is log mean. Any thoughts on switching to log mean (for everything except t-tests)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471054932:148,pipeline,pipelines,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471054932,1,['pipeline'],['pipelines']
Deployability,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,integrat,integrate,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228,1,['integrat'],['integrate']
Deployability,"That's really cool, thank you!. I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913:160,release,release,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913,1,['release'],['release']
Deployability,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:140,install,installation,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630,1,['install'],['installation']
Deployability,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/687#issuecomment-503484684:97,install,installing,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687#issuecomment-503484684,1,['install'],['installing']
Deployability,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,integrat,integrate,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209,2,"['integrat', 'upgrade']","['integrate', 'upgraded']"
Deployability,"The API for setting the random seed changed in the recent release (`v0.7`) of `louvain`, this is fixed on master, which should see a release soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212:58,release,release,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212,2,['release'],['release']
Deployability,"The UMAP (actually neighbours in the current implementation) is already now derived from any embedding the user wants, including integration embedding, so this is not an issue in itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985:129,integrat,integration,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985,1,['integrat'],['integration']
Deployability,The above issue was fixed by installing python-igraph and not igraph. Just run; `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728:29,install,installing,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728,2,['install'],"['install', 'installing']"
Deployability,"The bio conda builds look like their broken at the moment, and we haven't had the bandwidth to fix them yet (we are not the direct maintainers of the bio-conda builds). You can find up to date installation instructions which avoid this on the [latest docs](https://scanpy.readthedocs.io/en/latest/installation.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806:193,install,installation,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806,2,['install'],['installation']
Deployability,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381973984:37,install,installed,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381973984,1,['install'],['installed']
Deployability,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem...; >; > import matplotlib.colors; >; > #if user defined, then use the vmax, vmin keywords, else use data to generate them...; > if ('vmax' in kwds) and ('vmin' in kwds):; > _vmax = kwds['vmax']; > _vmin = kwds['vmin']; > else:; > _vmax = max(mean_flat); > _vmin = min(mean_flat); >; > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); >; > I'll submit a pull request.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444388428:145,patch,patch,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444388428,1,['patch'],['patch']
Deployability,The color is continuous means that the color is gradient according to the default setting in package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048:13,continuous,continuous,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048,1,['continuous'],['continuous']
Deployability,"The data also allows to detect a next issue: When multiple genes have the same value of `disp_cut_off`. Can be found if here e.g. dont do `sc.pp.normalize_total`:. ```py; import scanpy as as; adata = sc.datasets.pbmc3k(); # sc.pp.normalize_total(adata, target_sum=10000); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); adata.var[""highly_variable""].sum(); ```; ```; 10367; ```; Which is due to many genes having the value selected for the `disp_cut_off` here, having . ...`x[n-2]` = `x[n-1 ]` = `x[n]` = `x[n+1] `= `x[n+2]`... https://github.com/scverse/scanpy/blob/b918a23eb77462837df90d7b3a30a573989d4d48/src/scanpy/preprocessing/_highly_variable_genes.py#L408-L418. I tried to check how Seurat is proceeding in such a case, expecting to see how it breaks the ties. (data downloaded from [here](https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz)); Here I'm actually not sure how to turn off the `scale.factor` argument? Its set to 10'000 by default. ```R; library(dplyr); library(Seurat); library(patchwork). pbmc.data <- Read10X(data.dir = ""filtered_gene_bc_matrices/hg19/""). pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k""). pbmc <- NormalizeData(pbmc, normalization.method = ""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot"", nfeatures = 10000). length(VariableFeatures(pbmc)); ```; ```; 2292; ```; However, it turns out Seurat seems to restrict to the genes which are variable in the sense of passing the set mean threshold and normalized dispersion thresholds. These thresholds are ignored in scanpy if the number of genes is given. So not really an insight of how to break ties in this case. Would suggest to make a new issue, which the potential project on comparing the frameworks could address.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888:1078,patch,patchwork,1078,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888,1,['patch'],['patchwork']
Deployability,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921,3,"['install', 'update']","['installed', 'update']"
Deployability,"The docs look great! I just wonder about the above: In the release notes, we refer to everything as `scanpy.*`, not `sc.*`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-504889159:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-504889159,1,['release'],['release']
Deployability,"The fix was done in #2999, but we didn’t release 1.10.2 yet. Does installing scanpy from git work for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102#issuecomment-2164806504:41,release,release,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102#issuecomment-2164806504,2,"['install', 'release']","['installing', 'release']"
Deployability,The following fix worked for me (informed by this issue: https://github.com/HumanCellAtlas/data-consumer-vignettes/issues/78). ```bash; # uninstall packages (most important one is igraph); pip uninstall igraph python-igraph leiden scanpy. # reinstall scanpy; pip install 'scanpy[leiden]'. # check to see that you can import scanpy and print the version; python -c 'import scanpy as sc; print(sc.__version__)'; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556:263,install,install,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556,1,['install'],['install']
Deployability,"The idea of having ""smart subsample"" functionality available in scanpy has been a topic of discussion for a while. I would like to see a benchmark of these methods on single cell data before choosing one to include here. Are you aware of anything in this space?. Update:. It looks like the lab it's from have put out some writing on this: https://dl.acm.org/doi/pdf/10.1145/3388440.3412409",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-1948573055:263,Update,Update,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-1948573055,1,['Update'],['Update']
Deployability,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I don’t think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After; --- | ---; ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441704873:623,continuous,continuous,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441704873,1,['continuous'],['continuous']
Deployability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:469,update,updated,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['update'],['updated']
Deployability,"The issue persists with anndata 0.7.6. I've also been trying to update h5py, but it has conflicts with other packages. I'll post an update if I get it updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754:64,update,update,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754,3,['update'],"['update', 'updated']"
Deployability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:113,update,updated,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,3,['update'],"['update', 'updated']"
Deployability,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665:223,install,installed,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665,3,['install'],"['install', 'installed']"
Deployability,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955,6,"['install', 'release', 'update']","['install', 'installing', 'release', 'released', 'update']"
Deployability,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:67,install,installed,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,4,"['install', 'update']","['install', 'installed', 'update']"
Deployability,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:539,upgrade,upgraded,539,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441,1,['upgrade'],['upgraded']
Deployability,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927:156,release,releases,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927,1,['release'],['releases']
Deployability,"The original line was; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True; ```; which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446372971:382,release,release,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446372971,1,['release'],['release']
Deployability,The problem is that `np.concatenate` is being called on an `AnnData`. You may have to go back further in the scanorama releases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051044283:119,release,releases,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051044283,1,['release'],['releases']
Deployability,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,5,['install'],"['install', 'installed']"
Deployability,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,configurat,configuration,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['configurat'],['configuration']
Deployability,The release with PCA bug fix is now on pypi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151:4,release,release,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151,1,['release'],['release']
Deployability,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015:237,upgrade,upgrade,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015,1,['upgrade'],['upgrade']
Deployability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:290,install,install,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['install'],['install']
Deployability,"The scanpy install directory is super wrong, as it’s not writable for many people. There’s exactly one correct way of determining a global place for cache* files like this: [`appdirs.user_cache_dir(...)`](https://pypi.org/project/appdirs/). Alex and me talked in the past and decided for a visible directory in the working directory. I’d be up for changing it to `user_cache_dir(…)` for the data. *the data are cache files since reexccuting their function after deleting the files will redownload them without loss of information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564:11,install,install,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564,1,['install'],['install']
Deployability,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:141,install,install,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324,1,['install'],['install']
Deployability,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```; import scanpy as sc; import pandas as pd; import numpy as np. from anndata import AnnData. def harmony_integrate(; adata: AnnData,; key: str,; basis: str = ""X_pca"",; adjusted_basis: str = ""X_pca_harmony"",; **kwargs,; ):; try:; import harmonypy; except ImportError:; raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(); adata_merge.X = adata_merge.layers['counts']; sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(); sc.experimental.pp.normalize_pearson_residuals(adata_merge); adata_merge.layers['apr'] = adata_merge.X.copy(); sc.tl.pca(adata_merge, svd_solver=""arpack""); adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(); adata2 = adata_merge.copy(); ```. The frist test:. ```; # scanpy 1.9.6 that changes of this PR won't have taken effect yet.; # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py; harmony_integrate(adata1, key='batch', basis='X_pca_30'); harmony_integrate(adata2, key='batch', basis='X_pca_30'); np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%); Max absolute difference: 1.20792265e-12; Max relative difference: 4.37537551e-09; x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,; 0.564897],; [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:409,install,install,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,2,['install'],['install']
Deployability,"The upper bound for python versions we support is set by `numba` and `llvm-lite`. These rely on python internals, so can take a bit to be compatible with new releases. There should be python 3.10 support in the next numba release (https://github.com/numba/numba/issues/7562), which has a candidate out now ([see details here](https://numba.discourse.group/t/numba-0-55-0-rc1/1075)). I was able to get scanpy to import by doing:. ```sh; mamba create -yn ""numba-0.55.0rc1"" ""python=3.10""; conda activate numba-0.55.0rc1; mamba install -c numba -c numba/label/numpy numba=0.55.0rc1 numpy=1.21; pip install scanpy; python -c ""import scanpy""; ```. I couldn't install `scanpy` through conda since `pytables` doesn't have a 3.10 conda build afaict. Please let me know if try this out and run into any issues!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010871653:158,release,releases,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010871653,5,"['install', 'release']","['install', 'release', 'releases']"
Deployability,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:71,update,update,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759,1,['update'],['update']
Deployability,"The whole PR is pretty awesome already! I wrote some comments... Can you also add the function to the docs, cross reference the deprecated and the new function in the Notes section and add it to the [release notes](https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/330#issuecomment-434090862:200,release,release,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330#issuecomment-434090862,1,['release'],['release']
Deployability,Then I guess light grays would need to be removed from the default colour maps? And the question remains for how to deal with this in a continuous covariate.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163:136,continuous,continuous,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163,1,['continuous'],['continuous']
Deployability,"There hasn't actually been a release of UMAP since the pull request that should fix this (https://github.com/lmcinnes/umap/pull/261). I think I see what happened here, so I've opened a PR to fix it here. For now, this can be worked around by running:. ```python; sc.tl.umap(adata, init_pos=sc.tl._utils.get_init_pos_from_paga(adata)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-519055058:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519055058,1,['release'],['release']
Deployability,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/660#issuecomment-495141236:102,upgrade,upgrade,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660#issuecomment-495141236,1,['upgrade'],['upgrade']
Deployability,"There is no new release with these changes, yet. You'd need to install from github if you want them. However, these updates aren't critical and you can continue using the old functions without any bad conscience. A new release is obviously pushed to GitHub, PyPI and BioConda, and announced on twitter (https://twitter.com/falexwolf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/537#issuecomment-474286898:16,release,release,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537#issuecomment-474286898,4,"['install', 'release', 'update']","['install', 'release', 'updates']"
Deployability,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,integrat,integrates,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['integrat'],['integrates']
Deployability,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:; > ; > I ran this:; > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'); > ; > But the image has something weird. Here are the snapshot:; > ; > The lines don't align well with the heatmap.; > ; > Additionally, the lines don't align well with the group ID colors, either; > ; > ; > And the ID colors seem to be not aligned well with the heatmap either.; > ; > Any thoughts?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/637#issuecomment-492510374:169,update,update,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637#issuecomment-492510374,1,['update'],['update']
Deployability,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147:61,install,installed,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147,1,['install'],['installed']
Deployability,There was an update in how `pandas` treats categoricals a couple of weeks ago (I think 0.24); we had to update `anndata` in order to incorporate the changes; that's why a newer version of `anndata` was necessary. I'm sure you unconsciously updated `pandas` before. :). Thanks for fielding this one @shayanhoss!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515#issuecomment-469639489:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515#issuecomment-469639489,3,['update'],"['update', 'updated']"
Deployability,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,integrat,integrated,488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['integrat'],['integrated']
Deployability,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477937882:90,update,updated,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477937882,2,['update'],"['update', 'updated']"
Deployability,There’s an 8.2 release planning issue now: https://github.com/pytest-dev/pytest/issues/12213,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2056338635:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2056338635,1,['release'],['release']
Deployability,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/560#issuecomment-476770853:109,release,released,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560#issuecomment-476770853,6,"['install', 'release']","['install', 'installation', 'installed', 'released']"
Deployability,"They should if you call; ```; pip install scanpy[louvain] -U; ```; anndata will also update if you call; ```; pip install scanpy -U; ```. The fact that we have `[igraph]` and `[louvain]` not in the default install is that they take quite a while. If the docs are unclear about it, happy to have you point us to it: https://scanpy.readthedocs.io/en/latest/installation.html :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518#issuecomment-470310775:34,install,install,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518#issuecomment-470310775,5,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915:189,configurat,configuration,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915,1,['configurat'],['configuration']
Deployability,"This allows `pip install -e` to do a standardized thing! Unclear how much this helps/ hurts overall though, particularly for asking ""what version is this installation"". I think we would be able to tell that an install was ""editable"" through `importlib.metadata.files`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954:17,install,install,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954,3,['install'],"['install', 'installation']"
Deployability,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that?. ```py; >>> import scanpy, anndata; >>> scanpy.external.tl.palantir(anndata.AnnData()); ImportError: ; please install palantir: . git clone git://github.com/dpeerlab/Palantir.git; cd Palantir; sudo -H pip3 install .; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585#issuecomment-479508185:320,install,install,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585#issuecomment-479508185,2,['install'],['install']
Deployability,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload???. But the following worked for me. ; ```; ipython; In [1]: %load_ext autoreload; In [2]: %autoreload 2; ln [3]: import scanpy as sc; ln [4]: sc.plotting._tools.scatterplots.tr_test(); [does nothing as expected, then change source to print something out]; ln [5]: sc.plotting._tools.scatterplots.tr_test(); hellya!!!. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468#issuecomment-461985115:85,update,updated,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468#issuecomment-461985115,1,['update'],['updated']
Deployability,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364279207:99,update,update,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364279207,1,['update'],['update']
Deployability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:195,install,install,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,1,['install'],['install']
Deployability,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/565#issuecomment-477833445:91,release,released,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565#issuecomment-477833445,1,['release'],['released']
Deployability,This has been implemented in anndata 0.5 and scanpy 0.4.3. See release notes (https://scanpy.readthedocs.io). See https://github.com/theislab/anndata/commit/8cabf9c86a38d6db88c664e2ea28e3fb29bdf99e and a few fixes after that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/69#issuecomment-364919294:63,release,release,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69#issuecomment-364919294,1,['release'],['release']
Deployability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,configurat,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,5,"['configurat', 'install', 'update']","['configuration', 'install', 'installation', 'installed', 'updated']"
Deployability,This is also fixed by newer releases of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462142035:28,release,releases,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462142035,1,['release'],['releases']
Deployability,This is being fixed in #1210. for the time being you can try to set `dendrogram=False` or you can install the branch with the fix.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718:98,install,install,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718,1,['install'],['install']
Deployability,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:176,pipeline,pipelines,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619,1,['pipeline'],['pipelines']
Deployability,"This is fixed in pytest-dev/pytest#12169. Let’s wait for the pytest release, and then bump the min pytest version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2049963856:68,release,release,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2049963856,1,['release'],['release']
Deployability,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1843407929:43,update,updated,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1843407929,1,['update'],['updated']
Deployability,This is great! 👍 . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924:32,update,update,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924,1,['update'],['update']
Deployability,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047:129,install,installation,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047,1,['install'],['installation']
Deployability,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:174,release,release,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000,1,['release'],['release']
Deployability,"This is no longer needed since UMAP 0.4 (not yet released) will use pynndescent if it is installed, see https://github.com/lmcinnes/umap/pull/278.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-534951024:49,release,released,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-534951024,2,"['install', 'release']","['installed', 'released']"
Deployability,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,integrat,integration,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730,1,['integrat'],['integration']
Deployability,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:447,pipeline,pipeline,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,1,['pipeline'],['pipeline']
Deployability,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like ; `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488#issuecomment-1650697912:8,update,updated,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488#issuecomment-1650697912,2,"['install', 'update']","['install', 'updated']"
Deployability,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811:463,update,update,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811,1,['update'],['update']
Deployability,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:606,install,install,606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is?. Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737:107,update,updates,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737,1,['update'],['updates']
Deployability,"This looks good!. I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:; - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. ; - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931:302,release,release,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931,1,['release'],['release']
Deployability,This looks like an bug in the most recent release of `louvain`. Try downgrading?. I would also recommend using `leiden` clustering instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246:42,release,release,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246,1,['release'],['release']
Deployability,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight?. ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages?; * How does this impact users vs. developers?; * Is IO special, or should more parts go into sub-packages?; * What gets re-exported from ""main"" modules?; * Who manages the sub-packages?. A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419:559,install,installation,559,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419,1,['install'],['installation']
Deployability,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:31,install,install,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129,2,['install'],['install']
Deployability,"This might be due to updates in pandas >1.3.0. The command ; `pbmc.rename_categories('phase', new_cluster_names)`; seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be ; `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified. . Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925:21,update,updates,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925,2,['update'],"['updated', 'updates']"
Deployability,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164:155,release,released,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164,2,"['release', 'update']","['released', 'updated']"
Deployability,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837:129,release,release,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837,1,['release'],['release']
Deployability,"This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805:59,patch,patched,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805,2,['patch'],['patched']
Deployability,This seems to be a problem related to h5py build.; You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900#issuecomment-549045842:59,install,install,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900#issuecomment-549045842,1,['install'],['install']
Deployability,"This seems to be working now. Given that matplotlib updates are known to introduce bugs, I'm not sure we want to pin matplotlib to `3.3.3`. I'm concerned that's too strong of a restriction, since it's likely there's someone out there who needs to use an older version. Right now this is causing CI to fail, since there's a canary test that fails when the function works 😆.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803:52,update,updates,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803,2,"['canary', 'update']","['canary', 'updates']"
Deployability,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:184,install,installations,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069,4,['install'],"['install', 'installations', 'installed']"
Deployability,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318:321,pipeline,pipeline,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318,1,['pipeline'],['pipeline']
Deployability,This was fixed in https://github.com/scverse/scanpy/pull/2424:. @ivirshup when should we release 1.10?. ![image](https://github.com/scverse/scanpy/assets/291575/f1d7f2e3-1943-492c-a1f6-2b0499affe94),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1846864918:89,release,release,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1846864918,1,['release'],['release']
Deployability,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757:140,update,updates,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757,2,"['release', 'update']","['release', 'updates']"
Deployability,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:383,install,install,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267,1,['install'],['install']
Deployability,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:58,install,installed,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267,2,['install'],"['install', 'installed']"
Deployability,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway?. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542:66,install,installed,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542,3,"['Install', 'install']","['Installed', 'installed']"
Deployability,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```; $ pip install 'matplotlib<3.7'; ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2411#issuecomment-1429887964:59,install,installing,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411#issuecomment-1429887964,2,['install'],"['install', 'installing']"
Deployability,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-440740958:122,update,updated,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-440740958,1,['update'],['updated']
Deployability,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390636706:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636706,1,['install'],['install']
Deployability,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/319#issuecomment-432357859:173,update,updated,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432357859,1,['update'],['updated']
Deployability,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,integrat,integrates,697,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691,1,['integrat'],['integrates']
Deployability,"UPDATE: ; So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:; `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498837162:0,UPDATE,UPDATE,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498837162,1,['UPDATE'],['UPDATE']
Deployability,"Umap currently (0.5.4) suppresses that warning, so I guess it’s ok: https://github.com/lmcinnes/umap/blame/master/umap/spectral.py#L532-L536. that was fixed in https://github.com/lmcinnes/umap/pull/1031. You probably just need to upgrade umap to 0.5.4 to no longer see that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3139#issuecomment-2210680950:230,upgrade,upgrade,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3139#issuecomment-2210680950,1,['upgrade'],['upgrade']
Deployability,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …); - leave original AnnData alone, return; - new AnnData; - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done.; but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664351687:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664351687,1,['update'],['update']
Deployability,"Unfortunately, the team did not get around to implementing the desired behavior yet and therefore, we do not have any updates yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276#issuecomment-2446254307:118,update,updates,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276#issuecomment-2446254307,1,['update'],['updates']
Deployability,"Update here: I met @ivirshup at the SCG conference in Utrecht and we briefly chatted about this issue on the way to a pub. Isaac seemed supportive of the whole setup in this PR, which would allow to do this:. ``` Python; sc.pp.neighbors(); sc.tl.tsne() # default binarize='auto' resolves to binarize=True here. UMAP kNN graph but binary weights; sc.tl.tsne(binarize=False) # this would use UMAP weights but normalize to sum to 1; sc.pp.neighbors_tsne(); sc.tl.tsne() # default binarize='auto' resolves to binarize=False here; ```. As @pavlin-policar showed above, these three t-SNE calls yield very similar embeddings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427,1,['Update'],['Update']
Deployability,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262,1,['Update'],['Update']
Deployability,Update: I was able to get rid of the error by pip installing `dask` manually in my conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2210#issuecomment-1088508941:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2210#issuecomment-1088508941,2,"['Update', 'install']","['Update', 'installing']"
Deployability,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103,2,"['Update', 'update']","['Update', 'updated']"
Deployability,"Update: The initial issue has been fixed, we don't have a more generic embedding plotting function yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-522915275:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-522915275,1,['Update'],['Update']
Deployability,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315,4,"['Update', 'install', 'upgrade']","['Update', 'install', 'installs', 'upgrade']"
Deployability,Updated plots. <details>; <summary> ranked_genes </summary>. Orig:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785867-1b087600-d1d5-11ea-85dc-e8ea31303ae7.png). Current:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785705-e0064280-d1d4-11ea-966e-f856ee2468f6.png). </details>. <details>; <summary> ranked_genes_sharey </summary>. Orig:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785853-13e16800-d1d5-11ea-997a-d72955c5b3ce.png). Current:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785819-06c47900-d1d5-11ea-8a74-22ef2b4803d0.png). </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920,1,['Update'],['Updated']
Deployability,Updated the PR:; - renamed argument in `visium_sge` to `include_hires_tiff`; - renamed argument in `read_visium` to `source_image_path` and removed guessing of image file if `source_image_path` is None,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299,1,['Update'],['Updated']
Deployability,Updated the notebook above with `to_adata_joint` example - this function returns the concatenation of `adata_ref` and `adata_new` with projected representations and mapped labels.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-517078460:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-517078460,1,['Update'],['Updated']
Deployability,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697:0,Upgrade,Upgrade,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697,1,['Upgrade'],['Upgrade']
Deployability,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269:191,release,release,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269,1,['release'],['release']
Deployability,"Using a dict is an interesting idea. Right now I'd prefer that it matches with other ""vectorized"" arguments (like `vmin`, `vmax`) which take a list. I think for categorical values the continuous arguments are ignored (right @fidelram?). Using a `dict` would get rid of the ""sometimes arguments are ignored"" part of this, but I think consistency is more important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471:184,continuous,continuous,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471,1,['continuous'],['continuous']
Deployability,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,integrat,integration,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791,1,['integrat'],['integration']
Deployability,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370:146,update,updated,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370,1,['update'],['updated']
Deployability,Was the first install also with `pip`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037626429:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037626429,1,['install'],['install']
Deployability,We call out to `loompy` to read in loom files. I also am having trouble replicating using `loompy 3.0.6`. Could you try updating your loompy install and seeing if the problem persists?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056:141,install,install,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056,1,['install'],['install']
Deployability,"We currently distribute `scanpy` through `conda-forge`, so I would recommend using that for up to date versions. Will this still error if you've done that?. Also, it's a little unclear if your session info came from the conda environment you're running into issues with. Is this definitely the case? There shouldn't be that new of releases of scanpy or anndata available through bioconda as far as I know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063056234:331,release,releases,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063056234,1,['release'],['releases']
Deployability,We discussed outside @flying-sheep - we will release with the bound then and just wait here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2186436517:45,release,release,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2186436517,1,['release'],['release']
Deployability,We have SCVI working on anndata objects for data integration in our benchmarking study.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617:49,integrat,integration,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617,1,['integrat'],['integration']
Deployability,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,integrat,integration,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122,1,['integrat'],['integration']
Deployability,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-460560962:45,install,install,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-460560962,2,['install'],"['install', 'installation']"
Deployability,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475782293:18,update,update,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475782293,1,['update'],['update']
Deployability,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:259,hotfix,hotfix,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['hotfix'],['hotfix']
Deployability,We recently made a release (1.8.2) which has a bug fix for an incompatibility with umap 0.5.2. The fix was actually contributed by the author of UMAP!. Upgrading should fix your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069:19,release,release,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069,1,['release'],['release']
Deployability,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:258,release,release,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:126,install,installing,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565,1,['install'],['installing']
Deployability,"We should probably let users provide a set of clusters for the reference. . Until then, could you do:. ```python; groups_to_test = (; adata.obs[""clusters""]; .value_counts(); .loc[lambda x: x > 1]; .index; ); subset_adata = adata[adata.obs[""clusters""].isin(groups_to_test)].copy(); sc.tl.rank_genes_groups(subset_adata, ...); adata.uns[""rank_genes_groups""] = subset_adata.uns[""rank_genes_groups""]; ```. Or, if you want the singlets in the reference:. ```python; sc.tl.rank_genes_groups(adata, groups=list(groups_to_test), ...); ```. BTW, about the h5py bytes thing, things written as strings are read as strings with h5py 3 as of anndata 0.7.5, which just got released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542:659,release,released,659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542,1,['release'],['released']
Deployability,"We sped up our release process, which means it’s not a huge deal to make one. But it’s still more than just clicking a button, so since we just made a release, maybe in 1-2 weeks?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1814013706:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1814013706,2,['release'],['release']
Deployability,"We'll have 1.5.0 in a couple of days, and so it's fine to make a few behavior changes. We should put a warning in the release notes, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/957#issuecomment-627228911:118,release,release,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-627228911,1,['release'],['release']
Deployability,"We're also hitting this. I didnt test this yet, but from our CI history it seems like this worked with seaborn-0.12.2, but broke when that updated to seaborn-0.13.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761944327:139,update,updated,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761944327,1,['update'],['updated']
Deployability,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:476,update,update,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487,1,['update'],['update']
Deployability,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:212,Install,Installing,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984,4,"['Install', 'install']","['Installing', 'installation', 'installations', 'installs']"
Deployability,"Well, I think it is better to be consistent with the paper [Diffusion pseudotime robustly reconstructs lineage branching](https://www.nature.com/nmeth/journal/v13/n10/full/nmeth.3971.html) since this paper first introduces the diffusion pseudotime concept. In Figure 1 (c) of this paper, there is a _DPT order_. It seems the dpt order in this paper is just a global rank for each individual cell according to their pseudotime. Therefore, I suggest that the adata.smp['dpt_order'] and the one in the figure should have the same meaning, though IMHO dpt_order only matters for cells on the same branch. If we extract cells by their dpt_group, then the dpt_order is still applicable even though it is not continuous now. . In short, I think a dpt_order defined as the global rank by pseudotime like the one in the original paper is more understandable. By the way, if there are multiple branches in the diffusion map, is there some way to assign the cells to a certain branch? That is to say, can we provide an _adata.smp['branch']_ field?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836:702,continuous,continuous,702,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836,1,['continuous'],['continuous']
Deployability,"Well, it makes working with all this easier, specifically if you can just see your figures inline instead of popup windows. You can try out jupyterlab here to get a small tutorial: https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/try.jupyter.org?urlpath=lab. If you think you’d like it, just `pip install jupyterlab` and start it with `jupyter lab`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/850#issuecomment-532657402:302,install,install,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532657402,1,['install'],['install']
Deployability,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439747800:203,install,install,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439747800,1,['install'],['install']
Deployability,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now!. Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/888#issuecomment-547828011:35,release,release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888#issuecomment-547828011,1,['release'],['release']
Deployability,We’ll update this issue when we merge the PR. You can subscribe to scanpy releases on GitHub to be notified when we release something!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892350467:6,update,update,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892350467,3,"['release', 'update']","['release', 'releases', 'update']"
Deployability,"What command are you running?. You might be running into the fact that we no longer put scanpy on bioconda, but instead use conda-forge. So `conda install -c conda-forge scanpy` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580,1,['install'],['install']
Deployability,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:45,install,installed,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616,2,"['install', 'update']","['installed', 'update']"
Deployability,"What do you mean? Scanpy’s metadata only specifies a lower Python version bound:. https://github.com/scverse/scanpy/blob/d7e13025b931ad4afd03b4344ef5ff4a46f78b2b/pyproject.toml#L13. Which of the following is it?. - Does it refuse to install because some dependency is not Python 3.11 compatible?; - Does it crash when run there?; - Something else?. I assume it’s just that numba is incompatible still (which will always be a recurring problem as long as we depend on it), but please let me know. With installation issues, you can always run `pip -vv install scanpy` to get much more output that could be helpful. Please [use code blocks](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) when including it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462:233,install,install,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462,3,['install'],"['install', 'installation']"
Deployability,"What happens is . 1. `pip install -e` adds the repo directory to `sys.path`, since we don‘t use a `src` directory; 2. something imports the `data` directory instead of `scanpy.testing._helpers.data` and stores it in the wrong place in `sys.modules`:. ```; >>> sys.modules[""scanpy.testing._helpers.data""]; <module 'scanpy.testing._helpers.data' (namespace) from ['/home/phil/Dev/Python/Single Cell/scanpy/data']>; ```. I assume that some of the attempts to fix our doctest woes in Pytest 8.1 led to this bug being introduced in Pytest 8.1. But I’d like to figure out what exactly causes that bug before we do anything. PS: Of course things like this are exactly why we switched anndata to a `src` layout: that way `src` can be in `sys.path` instead of a kitchen sink directory, and `import <something>` will only see directories that are meant to contain exclusively python packages. Pytest shouldn’t `import data` and then store the resulting module as `sys.modules['scanpy.testing._helpers.data']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2045317672:26,install,install,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2045317672,1,['install'],['install']
Deployability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:28,update,update,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['update'],['update']
Deployability,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:31,install,installation,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825,12,"['Install', 'install']","['Installing', 'install', 'installation', 'installations', 'installing']"
Deployability,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['update'],['update']
Deployability,What version of scanpy are you using? I don't see a function called `_get_color_values` in `scatterplots.py` in the HEAD version. Can you try installing the development version and seeing if you get the same error? The issue might be that the version you are currently using does not include the fix I made above. ```; git clone https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451:142,install,installing,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451,2,['install'],"['install', 'installing']"
Deployability,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:246,install,install,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375,2,['install'],['install']
Deployability,"What would be a better name?. Also, believe this has been discussed before, but not sure where. I think we wanted to normalize on `frac` vs `pct` vs `percent` or something?. This was also based on `scater`'s qc metrics function, which now lives in `scuttle`. The argument there is named `percent.top`: https://bioconductor.org/packages/release/bioc/manuals/scuttle/man/scuttle.pdf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2842#issuecomment-1934570365:336,release,release,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2842#issuecomment-1934570365,1,['release'],['release']
Deployability,What you can do is. 1. go into the folder from the extracted zip; 2. `git init`; 3. `git tag v1.4.5.dev0`; 4. `pip install -e .`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-533015846:115,install,install,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533015846,1,['install'],['install']
Deployability,"What you describe doesn‘t need to happen, and you can fix this!. 1. go to https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/; 2. make a PR that patches conda’s dependency data to include this constraint; 3. the problem is gone",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158:126,patch,patches-feedstock,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158,2,['patch'],"['patches', 'patches-feedstock']"
Deployability,Whats the timeline here? When will there be a release that includes this fix? Or should I downgrade pandas in the meantime?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015:46,release,release,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015,1,['release'],['release']
Deployability,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340,7,['install'],['install']
Deployability,When I do this it tells me I am using version 1.4.1 but when I run sc.logging.print_versions() it comes up as version 1.01. I assumed this is somehow related to the version installed with scanpy that it uses by default maybe? But I feel I am not proficient enough in python to determine that,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683:173,install,installed,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683,1,['install'],['installed']
Deployability,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :); I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440#issuecomment-456702805:156,update,update,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440#issuecomment-456702805,1,['update'],['update']
Deployability,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:589,install,installed,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316,1,['install'],['installed']
Deployability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:529,install,install,529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,When data is continuous and legend is a continuous color bar a second legend could be added below it showing the categorical NaN (like the usual categorical legend).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540:13,continuous,continuous,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540,2,['continuous'],['continuous']
Deployability,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:41,release,released,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652,3,"['release', 'update']","['release', 'released', 'update']"
Deployability,"When would you want `na_as_category` off? Is it important enough to add a new parameter for?. Also, how should the continuous color bar show that there is a null value? (Suggestions with code very welcome)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327:115,continuous,continuous,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327,1,['continuous'],['continuous']
Deployability,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,integrat,integration,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['integrat'],['integration']
Deployability,Will there be an update on this issue?; It would be really helpful!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/984#issuecomment-627067415:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-627067415,1,['update'],['update']
Deployability,"With #3056 merged, this now says. > before | after | ratio | benchmark; > --- | --- | --- | ---; > 448±100ms | 381±100ms | ~0.85 | `preprocessing_counts.time_scrublet('pbmc68k_reduced')`. but ASV seems to think that’s not enough to report. Also unclear why it’s reported as taking 26 minutes by github:. > <img width=""252"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/291575/6117cece-a145-4b46-85a4-dd86a61819ef"">. I see in the server logs. > - May 14 10:15:57 scvbench benchmark[1462905]: 2024-05-14T10:15:57.547945Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting; > - […running benchmarks]; > - May 14 10:27:18 scvbench benchmark[1462905]: 2024-05-14T10:27:18.793352Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting. which means that between setting the check run to “running” and to “done”, 11m21s passed. Maybe GitHub counts the queue time?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339:567,PATCH,PATCH,567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339,2,['PATCH'],['PATCH']
Deployability,"With pandas 1.3.4 and 1.3.3. * I can't replicate the initial issue; * I can replicate @michalk8's example. This looks very upstream in pandas. I will try and submit an issue/ check that this hasn't been reported to pandas already tomorrow. This may be a kinda easy fix (e.g. check value shape better during column assignment in pandas), but it can take a bit to figure out how to fix things there. AFAIK, we removed calls in scanpy which assigned (n x 1) matrices to pandas because of related, non-formatting error. Is the current scanpy release assigning these matrices anywhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046:538,release,release,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046,1,['release'],['release']
Deployability,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,integrat,integrated,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988,1,['integrat'],['integrated']
Deployability,"Would be good to know if there's any nice tooling around this, as this could make fixtures and parametrized tests a bit difficult. For example, not installing `dask` makes test collection fail in the `test_normalization.py` file. <details>; <summary> Error </summary>. ```pytb; _______________________ ERROR collecting scanpy/tests/test_normalization.py ________________________; ImportError while importing test module '/Users/isaac/github/scanpy/scanpy/tests/test_normalization.py'.; Hint: make sure your test modules/packages have valid Python names.; Traceback:; ../../miniconda3/envs/scanpy-minimal/lib/python3.9/importlib/__init__.py:127: in import_module; return _bootstrap._gcd_import(name[level:], package, level); scanpy/tests/test_normalization.py:5: in <module>; import dask.array as da; E ModuleNotFoundError: No module named 'dask'; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088611630:148,install,installing,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088611630,1,['install'],['installing']
Deployability,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707,2,"['install', 'update']","['installation', 'update']"
Deployability,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434087328:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434087328,1,['release'],['release']
Deployability,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released; 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-473615430:321,release,released,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-473615430,1,['release'],['released']
Deployability,Yeah noted there is an issue with scipy... Not related to this the single-line update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2739#issuecomment-1803816341:79,update,update,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2739#issuecomment-1803816341,1,['update'],['update']
Deployability,"Yeah, I decided just to go for it 🤠. I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/385#issuecomment-456026742:53,update,update,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385#issuecomment-456026742,1,['update'],['update']
Deployability,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:224,Update,Updates,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522,1,['Update'],['Updates']
Deployability,"Yeah, please file your issue with [scanpy-scripts](https://github.com/ebi-gene-expression-group/scanpy-scripts) then, and ask them why they want an old scipy version and if they can upgrade their code to work with scipy 1.3+",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559:182,upgrade,upgrade,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559,1,['upgrade'],['upgrade']
Deployability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,"['release', 'update']","['release', 'updates']"
Deployability,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,configurat,configuration,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328,1,['configurat'],['configuration']
Deployability,"Yep, but still you need data passed to not only have the same dimensionality, you need dimensions to have the same meaning any time you want to project new data. If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. Using genes to fit a initial UMAP will ensure that you can transform new data, provided you have the same genes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205:174,integrat,integrated,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205,1,['integrat'],['integrated']
Deployability,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553415534:69,install,installed,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553415534,1,['install'],['installed']
Deployability,Yes! Please wait for the next release. Please also checkout squidpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2462#issuecomment-1503220199:30,release,release,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2462#issuecomment-1503220199,1,['release'],['release']
Deployability,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,integrat,integrate,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514,1,['integrat'],['integrate']
Deployability,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:463,update,updated,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291,1,['update'],['updated']
Deployability,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/906#issuecomment-551413589:7,install,installed,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906#issuecomment-551413589,1,['install'],['installed']
Deployability,"Yes, I know about an issue that is probably related to that: At some point in `add_or_update_graph_in_adata`, numpy takes more cores than it's supposed to - that's the only instance in the whole of Scanpy. Otherwise it's well-behaved. When Scanpy 1.0 will be released in the next few days, this will be resolved. Do you think this will do the job for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/101#issuecomment-371489944:259,release,released,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101#issuecomment-371489944,1,['release'],['released']
Deployability,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699:222,update,updated,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699,1,['update'],['updated']
Deployability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:187,install,installation,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['install'],"['installation', 'installed']"
Deployability,"Yes, It helped. I updated the magic version and it seems to be working fine now. ; Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/208#issuecomment-405897974:18,update,updated,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208#issuecomment-405897974,1,['update'],['updated']
Deployability,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169:116,release,release,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169,2,['release'],['release']
Deployability,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032:91,install,installation,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032,2,"['install', 'release']","['installation', 'release']"
Deployability,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035:317,pipeline,pipeline,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035,1,['pipeline'],['pipeline']
Deployability,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:61,install,installation,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231,1,['install'],['installation']
Deployability,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019,1,['update'],['update']
Deployability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,integrat,integrated,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,4,"['continuous', 'integrat']","['continuous', 'integrated', 'integration']"
Deployability,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615#issuecomment-489554643:62,release,release,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615#issuecomment-489554643,1,['release'],['release']
Deployability,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357:22,update,update,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357,1,['update'],['update']
Deployability,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/420#issuecomment-453700295:54,update,updated,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420#issuecomment-453700295,1,['update'],['updated']
Deployability,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124:209,install,installs,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124,1,['install'],['installs']
Deployability,"Yes, this is a duplicate of #1260. Please follow #1319 to track the next release",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267:73,release,release,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267,1,['release'],['release']
Deployability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,integrat,integration,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['integrat'],['integration']
Deployability,"Yes, we should as soon as many people report seemless installation of the leiden package. I'm still using louvain as I never had any problems with it, but I agree that we should migrate when leiden is stable, mature and easily-installable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-482500682:54,install,installation,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-482500682,2,['install'],"['installable', 'installation']"
Deployability,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178#issuecomment-399889641:158,release,release,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399889641,1,['release'],['release']
Deployability,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460560176:74,release,releases,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460560176,2,['release'],"['release', 'releases']"
Deployability,You can also try creating a new conda environment and installing pytables there at first and try to import it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173:54,install,installing,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173,1,['install'],['installing']
Deployability,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-424700191:72,release,release,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424700191,1,['release'],['release']
Deployability,You can just use scanpy master if you need this fix ASAP. Otherwise I think we should release a new version pretty soon after merging #1038.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986:86,release,release,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986,1,['release'],['release']
Deployability,"You can use the `categories_order` argument, though would have to check which version that was added in. Are you having issues with newer releases of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461:138,release,releases,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461,1,['release'],['releases']
Deployability,"You could also not make this breaking by checking if there are any `/` in the string and appending `umap` or `violin` after the last `/`? Would be a bit messy, but quicker to release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343:175,release,release,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343,1,['release'],['release']
Deployability,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396:77,install,installation,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396,2,['install'],['installation']
Deployability,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564#issuecomment-1645368662:76,release,release,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564#issuecomment-1645368662,1,['release'],['release']
Deployability,You have to first uninstall the version you have already installed of numba.; pip uninstall numba; pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914:57,install,installed,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914,2,['install'],"['install', 'installed']"
Deployability,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/344#issuecomment-436069713:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344#issuecomment-436069713,1,['install'],['install']
Deployability,You should be able to install an experimental m1 native numba here:. https://numba.discourse.group/t/wheels-for-apple-silicon-m1/1282,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2237#issuecomment-1101038489:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2237#issuecomment-1101038489,1,['install'],['install']
Deployability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['integrat'],['integrate']
Deployability,"You’re right: They would be failing on master if master had been tested since anndata 0.7 was released earlier today. We have to merge #989 to get tests to pass again before we can merge this. /edit: done, should work now",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388:94,release,released,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388,1,['release'],['released']
Deployability,"Yup, not a bug with scanpy, but a problem with installing louvain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/786#issuecomment-522570296:47,install,installing,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522570296,1,['install'],['installing']
Deployability,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2413,install,install,2413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],['install']
Deployability,] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33213,pipeline,pipeline,33213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopyt,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31765,pipeline,pipeline,31765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32507,pipeline,pipeline,32507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34144,pipeline,pipeline,34144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplemente,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34378,pipeline,pipeline,34378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILE,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20376,pipeline,pipeline,20376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25807,pipeline,pipeline,25807,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.fi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25594,pipeline,pipeline,25594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29619,pipeline,pipeline,29619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38973,pipeline,pipeline,38973,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21529,pipeline,pipeline,21529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19442,pipeline,pipeline,19442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19209,pipeline,pipeline,19209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17358,pipeline,pipeline,17358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18748,pipeline,pipeline,18748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,`FutureWarning` for the next release to begin the process.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2865#issuecomment-2012549256:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2865#issuecomment-2012549256,1,['release'],['release']
Deployability,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332466200:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332466200,1,['install'],['install']
Deployability,"```; from importlib.metadata import version as v. v(""anndata""); ```; returns Python `None`. ```anndata.__version__```. returns `'0.10.6'`. I had `anndata==0.8.0` in my conda environment, and then I did `pip install anndata -U` to get `0.10.6`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037490510:207,install,install,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037490510,1,['install'],['install']
Deployability,```; sc.__version__; '1.8.0.dev78+gc488909a'; ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. ; In other words if adata.raw was missing the notation it failed for me (different error though). ; I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537:458,update,update,458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537,1,['update'],['update']
Deployability,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:112,install,install,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050,2,['install'],['install']
Deployability,"```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4992,install,installed,4992,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['installed']
Deployability,"```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=0.5). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""FCGR3A""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. before:; ![image](https://user-images.githubusercontent.com/25887487/100453873-0e7d2700-30bc-11eb-9451-0240f6170e2f.png). after:; ![image](https://user-images.githubusercontent.com/25887487/100453885-15a43500-30bc-11eb-9b95-66d0881aad31.png). as you can see, heatmap colors for rows (genes) is consistent with columns (clusters) ; @wkopp is this helpful? notebook still to be updated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743:753,update,updated,753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743,1,['update'],['updated']
Deployability,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:1020,update,update,1020,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788,1,['update'],['update']
Deployability,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438,1,['install'],['install']
Deployability,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457,1,['install'],['install']
Deployability,"`flit install --deps none -s` breaks `conda list` for me.; using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847,1,['install'],['install']
Deployability,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947,3,['install'],"['install', 'installed']"
Deployability,`pip install -U pynndescent`; This fix my problems; I hope it's able to help others!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944,1,['install'],['install']
Deployability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:103,install,install,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,5,['install'],"['install', 'installation', 'installing']"
Deployability,"`tbb` was installed via conda i guess, try to uninstall `tbb` with conda and then install `scanpy` again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706#issuecomment-1777066040:10,install,installed,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706#issuecomment-1777066040,2,['install'],"['install', 'installed']"
Deployability,a-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:37316,pipeline,pipeline,37316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,a2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; r,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:4080,Install,Installing,4080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,3,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4872,install,installed,4872,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['install'],['installed']
Deployability,"aac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Version('0.10.6')>; ```. </details>. Interesting to see that this seems to work now!. I do think the recommended solution here is ""don't do this"", but I'm considering just using `anndata.__version__` here since that will work when the package metadata is broken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:1247,Install,Installing,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,3,"['Install', 'install']","['Installing', 'installation', 'installed']"
Deployability,able_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_vari,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23797,pipeline,pipeline,23797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"able_pyobject:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); [669](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:18948,pipeline,pipeline,18948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"ack `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1536,install,install,1536,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['install'],['install']
Deployability,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.59.0; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.3; wcwidth 0.2.5; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.10; notebook 6.2.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-44-generic-x86_64-with-glibc2.10; 28 logical CPU cores; -----; Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:2491,update,updated,2491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,1,['update'],['updated']
Deployability,"add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1631,release,release,1631,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['release'],['release']
Deployability,adding release note in #1740,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441:7,release,release,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441,1,['release'],['release']
Deployability,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5555,install,install,5555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,"ah, and I forgot to add release notes. I need to get https://github.com/scverse/scanpy/pull/2569 done …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2672#issuecomment-1764686065:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2672#issuecomment-1764686065,1,['release'],['release']
Deployability,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513183516:92,update,update,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513183516,1,['update'],['update']
Deployability,"aha, sorry, I didn't install the 'fa2' package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411:21,install,install,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411,1,['install'],['install']
Deployability,"al branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata.; > ; > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change.; > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:1129,Update,Update,1129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['Update'],['Update']
Deployability,als_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21300,pipeline,pipeline,21300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplement,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34611,pipeline,pipeline,34611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED sca,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18287,pipeline,pipeline,18287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22230,pipeline,pipeline,22230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21069,pipeline,pipeline,21069,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22460,pipeline,pipeline,22460,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897,1,['integrat'],['integration']
Deployability,"an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1259,update,update,1259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['update'],['update']
Deployability,"anges from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:1949,update,update,1949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061,1,['update'],['update']
Deployability,"anpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2258,update,update,2258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['update'],['update']
Deployability,anpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[pca] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[spatial] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_dimen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:44872,pipeline,pipeline,44872,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1871205457:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1871205457,1,['update'],['update']
Deployability,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package?. Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763250072:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763250072,1,['update'],['updates']
Deployability,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1573,pipeline,pipeline,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,ariable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23564,pipeline,pipeline,23564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ariable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24245,pipeline,pipeline,24245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,arson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29384,pipeline,pipeline,29384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:40,release,released,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896,2,['release'],"['release', 'released']"
Deployability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['integrat'],['integrate']
Deployability,bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 pypi_0 pypi; pandas 1.2.4 py38h2531618_0 ; pandoc 2.12 h06a4308_0 ; pandocfilters 1.4.3 py38h06a4308_1 ; pango 1.42.4 h049681c_0 ; parso 0.7.0 py_0 ; partd 1.2.0 pyhd3eb1b0_0 ; patchelf 0.12 h2531618_1 ; path 15.1.2 py38h06a4308_0 ; path.py 12.5.0 0 ; pathlib2 2.3.5 py38h06a4308_2 ; pathspec 0.7.0 py_0 ; pathtools 0.1.2 py_1 ; patsy 0.5.1 py38_0 ; pcre 8.44 he6710b0_0 ; pep8 1.7.1 py38_0 ; pexpect 4.8.0 pyhd3eb1b0_3 ; phenograph 1.5.7 pypi_0 pypi; pickleshare 0.7.5 pyhd3eb1b0_1003 ; pillow 8.2.0 py38he98fc37_0 ; pip 21.1.1 py38h06a4308_0 ; pixman 0.40.0 h7b6447c_0 ; pkginfo 1.7.0 py38h06a4308_0 ; pluggy 0.13.1 py38h06a4308_0 ; ply 3.11 py38_0 ; progeny-py 1.0.3 pypi_0 pypi; progressbar2 3.37.1 py38h06a4308_0 ; prometheus_client 0.10.1 pyhd3eb1b0_0 ; prompt-toolkit 3.0.17 pyh06a4308_0 ; prompt_toolkit 3.0.17 hd3eb1b0_0 ; protobuf 3.17.0 pypi_0 pypi; psutil 5.8.0 py38h27cfd23_1 ; ptyprocess 0.7.0 pyhd3eb1b0_2 ; py 1.10.0 pyhd3eb1b0_0 ; py-lief 0.10.1 py38h403a769_0 ; py-spy 0.3.7 pypi_0 pypi; pyasn1 0.4.8 pypi_0 pypi; pyasn1-modules 0.2.8 pypi_0 pypi; pycairo 1.19.1 py38h708ec4a_0 ; pycodestyle 2.6.0 pyhd3eb1b0_0 ; pycosat 0.6.3 py38h7b6447c_1 ; pycparser 2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:12434,patch,patchelf,12434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['patch'],['patchelf']
Deployability,"bject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17186,install,install,17186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,ble_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24456,pipeline,pipeline,24456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['integrat'],['integrating']
Deployability,bset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32978,pipeline,pipeline,32978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1411,install,install-,1411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918:8,install,install,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918,2,['install'],['install']
Deployability,"can you link a doc build failure, please? Then I can check out why it fails. I assume they changed the documented location of the class. We have it in `qualname_overrides` and should probably just update the location there (or remove it that line if the new documented location now matches the qualified name). https://github.com/theislab/scanpy/blob/e5d246aacc71fb9ed71d49e2e7d5e26743fd4acb/docs/conf.py#L136-L140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781:197,update,update,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781,1,['update'],['update']
Deployability,can you please update your code sample so we can just copy and paste it? There’s an `import scanpy as ac` and a `adata = ???` missing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2540#issuecomment-1612612204:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540#issuecomment-1612612204,1,['update'],['update']
Deployability,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965,1,['update'],['update']
Deployability,ching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_spatial_external_img - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching typ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:8978,pipeline,pipeline,8978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"cksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3618,update,updated,3618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1362,patch,patch,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,4,['patch'],['patch']
Deployability,"columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__; filename=filename, filemode=filemode); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual; X = X.astype(dtype, copy=False); ValueError: setting an array element with a sequence.; ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:2411,release,release,2411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['release'],['release']
Deployability,"com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(None); E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); E non-precise type pyobject; E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); E ; E File ""scanpy/preprocessing/_simple.py"", line 763:; E def do_scale(X, maxv, nthr):; E <source elided>; E # t0= time.time(); E s = np.zeros((nthr, X.shape[1])); E ^ ; E ; E This error may have been caused by the following argumen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:1445,pipeline,pipeline,1445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,1,['pipeline'],['pipeline']
Deployability,conda install -c conda-forge pynndescent. This also fixed my problem. Thanks @FlyPythons for the hint.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075,1,['install'],['install']
Deployability,conda install pytables. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. But there is still ImportError.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063703998:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063703998,2,['install'],"['install', 'installed']"
Deployability,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5876,pipeline,pipelines,5876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipelines']
Deployability,cov.io/gh/scverse/scanpy/pull/3191?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `66.66667%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.61%. Comparing base [(`243a46e`)](https://app.codecov.io/gh/scverse/scanpy/commit/243a46e674f97f04c835893320dfd21543f89827?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`deb9f38`)](https://app.codecov.io/gh/scverse/scanpy/commit/deb9f3876ec6848562b1f5f3e5ce9f9a8551eead?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 49 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3191?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3191?src=pr&el=tree&filepath=src%2Fscanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9nZXQvZ2V0LnB5) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3191?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3191 +/- ##; ==========================================; - Coverage 76.61% 76.61% -0.01% ; ==========================================; Files 109 109 ; Lines 12529 12532 +3 ; ==========================================; + Hits 9599 9601 +2 ; - Misses 2930 2931 +1 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3191#issuecomment-2263391738:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3191#issuecomment-2263391738,1,['Patch'],['Patch']
Deployability,cov.io/gh/scverse/scanpy/pull/3264?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `92.00000%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.95%. Comparing base [(`d998742`)](https://app.codecov.io/gh/scverse/scanpy/commit/d9987426be03f9ef1bdab065f50959d046734ea4?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`9d0ffa5`)](https://app.codecov.io/gh/scverse/scanpy/commit/9d0ffa5b52b7311999bd52f7c856096a2b3d7653?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3264?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3264?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 81.81% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3264?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3264 +/- ##; ==========================================; - Coverage 76.96% 76.95% -0.02% ; ==========================================; Files 109 109 ; Lines 12469 12466 -3 ; ==========================================; - Hits 9597 9593 -4 ; - Misses 2872 2873 +1 ; ```. | [Flag](https://app.codecov.io/gh/scverse/scanpy/pull/3264/flags?src,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3264#issuecomment-2376822003:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264#issuecomment-2376822003,1,['Patch'],['Patch']
Deployability,cov.io/gh/scverse/scanpy/pull/3335?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `90.54054%` with `7 lines` in your changes missing coverage. Please review.; > Project coverage is 76.56%. Comparing base [(`6440515`)](https://app.codecov.io/gh/scverse/scanpy/commit/6440515ebce6e38b62bac5bce6d656f71fbeaa5b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`b426035`)](https://app.codecov.io/gh/scverse/scanpy/commit/b4260358866324a1097cdce17315ceebfe0cef0b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3335?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 87.23% | [6 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 87.50% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&utm_medium,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2450235904:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2450235904,1,['Patch'],['Patch']
Deployability,"cvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:981,install,install,981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,2,['install'],['install']
Deployability,cverse/scanpy/pull/3115?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `57.14286%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.50%. Comparing base [(`8d9a5f0`)](https://app.codecov.io/gh/scverse/scanpy/commit/8d9a5f0d2b303abeb42f7e4c9252d505000fd05c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`8fb2a0e`)](https://app.codecov.io/gh/scverse/scanpy/commit/8fb2a0eac8778931a22d70c16e76b9f516f9ef78?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 48 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3115?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3115?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3115?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3115 +/- ##; ==========================================; + Coverage 76.31% 76.50% +0.18% ; ==========================================; Files 109 109 ; Lines 12515 12474 -41 ; ==========================================; - Hits 9551 9543 -8 ; + Misses 2964 2931 -33 ; ```. | [Fil,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2181074546:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2181074546,1,['Patch'],['Patch']
Deployability,cverse/scanpy/pull/3246?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `25.00000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.71%. Comparing base [(`2553c67`)](https://app.codecov.io/gh/scverse/scanpy/commit/2553c67af6e47992abde5cb13e4c9deb82a3adbc?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`2e3ca25`)](https://app.codecov.io/gh/scverse/scanpy/commit/2e3ca25422b317736d49b2b14a61683cb9bfa98b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3246?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_stacked\_violin.py](https://app.codecov.io/gh/scverse/scanpy/pull/3246?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_stacked_violin.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fc3RhY2tlZF92aW9saW4ucHk=) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3246?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3246 +/- ##; ==========================================; - Coverage 76.76% 76.71% -0.05% ; ==========================================; Files 109 109 ; Lines 12529 12533 +4 ; ==========================================; - Hits 9618 9615 -3 ; - Misses 2911 2918 +7 ; ```. | [Files with m,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3246#issuecomment-2363411554:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3246#issuecomment-2363411554,1,['Patch'],['Patch']
Deployability,cverse/scanpy/pull/3258?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.24528%` with `22 lines` in your changes missing coverage. Please review.; > Project coverage is 76.96%. Comparing base [(`8b2088d`)](https://app.codecov.io/gh/scverse/scanpy/commit/8b2088de18452ff11e555bac0c147eaf15cf27f4?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`1b1fbc9`)](https://app.codecov.io/gh/scverse/scanpy/commit/1b1fbc93200bdbf7b69d0dd87f01a5c0ede2bdc1?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3258?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_tools/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_tools%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdG9vbHMvX19pbml0X18ucHk=) | 44.44% | [5 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_sim.py](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_sim.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fc2ltLnB5) | 58.33% | [5 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3258#issuecomment-2371725350:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3258#issuecomment-2371725350,1,['Patch'],['Patch']
Deployability,"d, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1732,install,install,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['install'],['install']
Deployability,"d.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3390,install,install,3390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:37546,pipeline,pipeline,37546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"de counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1158,install,installing,1158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['install'],['installing']
Deployability,duals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18516,pipeline,pipeline,18516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18056,pipeline,pipeline,18056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17594,pipeline,pipeline,17594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19676,pipeline,pipeline,19676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17823,pipeline,pipeline,17823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"duplicate of #2345, #2565, and so on. we’ll do a release soon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488#issuecomment-1653487974:49,release,release,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488#issuecomment-1653487974,1,['release'],['release']
Deployability,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2032,update,update,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['update']
Deployability,e matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImple,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:13745,pipeline,pipeline,13745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,integrat,integration,2121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1491,Install,Installing,1491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2450,pipeline,pipeline,2450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['pipeline'],['pipeline']
Deployability,"e169-11ea-86f9-597bb1d5da01.png). ### Master. ![output_6_0](https://user-images.githubusercontent.com/8238804/90475475-da17d780-e16a-11ea-9ae9-6fb61fd6357b.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""]); ```. ### Current. ![output_8_0](https://user-images.githubusercontent.com/8238804/90474984-ec454600-e169-11ea-935c-cb094f520a1a.png). ### Master. ![output_8_0](https://user-images.githubusercontent.com/8238804/90475481-de43f500-e16a-11ea-80da-2636b1b6dddf.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""], groups=[""0"", ""1""]); ```. ### Current. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475011-fb2bf880-e169-11ea-90a6-17e13388672e.png). ### Master. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475498-e69c3000-e16a-11ea-876c-747ec02fae6d.png). </details>. ## Continuous values. <details>; <summary> Spatial </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475149-4219ee00-e16a-11ea-9b42-aea86a8883a4.png). ### Master. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475522-eef46b00-e16a-11ea-8d88-4ede97e16252.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""], vmin=""p10"", vmax=""p90""); ```. ### Current. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475174-5100a080-e16a-11ea-91ae-d3c883829b9a.png). ### Master. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475536-f3b91f00-e16a-11ea-8ceb-fc954a9601e6.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![outpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322:2800,Continuous,Continuous,2800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322,1,['Continuous'],['Continuous']
Deployability,"e; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\_typing.py in <module>; 82 # array-like; 83 ; ---> 84 ArrayLike = Union[""ExtensionArray"", np.ndarray]; 85 AnyArrayLike = Union[ArrayLike, ""Index"", ""Series""]; 86 . AttributeError: module 'numpy' has no attribute 'ndarray'; ```; It looks like an endless error. What's wrong with the `!pip install scanpy[leiden]`? It installed so many incompatible packages which never happened before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:8526,install,install,8526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],"['install', 'installed']"
Deployability,"e\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=349) msg = ""Failed in %s mode pipeline (step: %s)"" % \; [351](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=350) (self.pipeline_name, pass_desc); [352](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=351) patched_exception = self._patch_error(msg, e); --> [353](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=352) raise patched_exception. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:341, in PassManager.run(self, state); [339](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=338) pass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:22478,pipeline,pipeline,22478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,e_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:30527,pipeline,pipeline,30527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"e_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:10719,install,installed,10719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,earson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29153,pipeline,pipeline,29153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ecause I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1051,update,update,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458,1,['update'],['update']
Deployability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4727,Install,Installing,4727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,3,"['Install', 'install']","['Installing', 'installation', 'installed']"
Deployability,ementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16433,pipeline,pipeline,16433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16204,pipeline,pipeline,16204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,end.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31266,pipeline,pipeline,31266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,eneral[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20609,pipeline,pipeline,20609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5910,install,install,5910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install']
Deployability,ents&utm_term=scverse). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3044 +/- ##; =======================================; Coverage 76.27% 76.28% ; =======================================; Files 117 117 ; Lines 12803 12802 -1 ; =======================================; Hits 9766 9766 ; + Misses 3037 3036 -1 ; ```. | [Files](https://app.codecov.io/gh/scverse/scanpy/pull/3044?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Coverage Δ | |; |---|---|---|; | [scanpy/preprocessing/\_scrublet/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L19faW5pdF9fLnB5) | `96.80% <100.00%> (+0.10%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/pipeline.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fpipeline.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3BpcGVsaW5lLnB5) | `94.59% <100.00%> (+0.30%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/sparse\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fsparse_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3NwYXJzZV91dGlscy5weQ==) | `89.28% <71.42%> (-1.90%)` | :arrow_down: |. ... and [1 file with indirect coverage changes](https://app.codecov.io/gh/scverse/scanpy/pull/3044/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scv,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2096282888:1797,pipeline,pipeline,1797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2096282888,1,['pipeline'],['pipeline']
Deployability,"equirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); tsundoku@tsundoku-OptiPlex-7070:~/networkanalyst/src/main/webapp/resources/data$ pip install scanpy; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:6079,install,install,6079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,eral[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29851,pipeline,pipeline,29851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,erse/scanpy/pull/3017?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 75.86%. Comparing base [(`3ba3f46`)](https://app.codecov.io/gh/scverse/scanpy/commit/3ba3f46b4e6e77e8c6f0551db9663822097b486a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`277c1bf`)](https://app.codecov.io/gh/scverse/scanpy/commit/277c1bfb0885234aa757d0fdaeaa9103eb8568e2?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 47 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3017?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/preprocessing/\_highly\_variable\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3017?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_highly_variable_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2hpZ2hseV92YXJpYWJsZV9nZW5lcy5weQ==) | 85.71% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3017?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3017 +/- ##; ==========================================; - Coverage 75.87% 75.86% -0.01% ; ==========================================; Files 110 110 ; Lines 12533 12533 ; ==========================================; - Hits 9509 9508 -1 ; - Misses 3024 3025 +1 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3017#issuecomment-2069245430:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3017#issuecomment-2069245430,1,['Patch'],['Patch']
Deployability,erse/scanpy/pull/3134?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `57.14286%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.50%. Comparing base [(`a5eadd5`)](https://app.codecov.io/gh/scverse/scanpy/commit/a5eadd5b723799105d724b5e9f80b711e0be87ca?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`25f0c97`)](https://app.codecov.io/gh/scverse/scanpy/commit/25f0c97e81e9143bece1ded7c4838964ed7d3866?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 43 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3134?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3134?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3134?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3134 +/- ##; ==========================================; + Coverage 76.31% 76.50% +0.18% ; ==========================================; Files 109 109 ; Lines 12515 12474 -41 ; ==========================================; - Hits 9551 9543 -8 ; + Misses 2964 2931 -33 ; ```. | [F,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3134#issuecomment-2202709708:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3134#issuecomment-2202709708,1,['Patch'],['Patch']
Deployability,ertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:43786,pipeline,pipeline,43786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6771,update,updates,6771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['update'],['updates']
Deployability,es.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25144,pipeline,pipeline,25144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,esiduals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18979,pipeline,pipeline,18979,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,est_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:27322,pipeline,pipeline,27322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,eta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython m,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:36346,pipeline,pipeline,36346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:415,pipeline,pipeline,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5006,pipeline,pipeline,5006,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['pipeline'],['pipeline']
Deployability,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1602758898:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1602758898,1,['install'],['install']
Deployability,"fixed in #2424, reported many times. please use the search function. we’ll do a release soon!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2598#issuecomment-1667789925:80,release,release,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2598#issuecomment-1667789925,1,['release'],['release']
Deployability,fixed in https://github.com/theislab/anndata/commit/555c15c8a170944b762ba7ce1d8c0b41f4e4dfbe. the fix should be in the next anndata release (0.6.2 or 0.7),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-391984289:132,release,release,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391984289,1,['release'],['release']
Deployability,"for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:1469,release,release,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['release'],['release']
Deployability,for the record: @odorea had the distribution “igraph” installed which contains the “jgraph” package. Scanpy needs the distribution “python-igraph” containing hte package “igraph”. … which is all confusing and annoying so I appreciate that “jgraph” has changed its name away from “igraph”.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-527179520:54,install,installed,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-527179520,1,['install'],['installed']
Deployability,"for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:976,install,install-,976,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],['install-']
Deployability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2297,pipeline,pipelines,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipelines']
Deployability,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1971,update,updated,1971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['updated']
Deployability,gend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding.py::test_umap_init_paga[fr] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._e,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:11666,pipeline,pipeline,11666,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,gend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38737,pipeline,pipeline,38737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signa,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33909,pipeline,pipeline,33909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_hi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23329,pipeline,pipeline,23329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17357,Install,Installing,17357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,4,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,gh/scverse/scanpy/pull/3098?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.16667%` with `5 lines` in your changes missing coverage. Please review.; > Project coverage is 76.35%. Comparing base [(`d34e575`)](https://app.codecov.io/gh/scverse/scanpy/commit/d34e5756aa6a6f763e06d48c060efdd0a94fa468?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`a01cf79`)](https://app.codecov.io/gh/scverse/scanpy/commit/a01cf79bf6dc5809357d037d17bece02d92616f5?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 35 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3098?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_score\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&filepath=scanpy%2Ftools%2F_score_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL19zY29yZV9nZW5lcy5weQ==) | 82.35% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&filepath=scanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2dldC9nZXQucHk=) | 66.66% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3098#issuecomment-2147840847:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3098#issuecomment-2147840847,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3230?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `33.33333%` with `30 lines` in your changes missing coverage. Please review.; > Project coverage is 76.72%. Comparing base [(`0f6acdf`)](https://app.codecov.io/gh/scverse/scanpy/commit/0f6acdf5dda52b698fbf3e675d018ef75806115c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`a98157b`)](https://app.codecov.io/gh/scverse/scanpy/commit/a98157b52c1b8fa5108a348a5dcf33bd123cc5e6?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3230?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 25.00% | [15 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_draw\_graph.py](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_draw_graph.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fZHJhd19ncmFwaC5weQ==) | 31.57% | [13 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3230#issuecomment-2348590448:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3230#issuecomment-2348590448,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3248?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `87.87879%` with `4 lines` in your changes missing coverage. Please review.; > Project coverage is 76.75%. Comparing base [(`b0597a9`)](https://app.codecov.io/gh/scverse/scanpy/commit/b0597a9f6f114a1aee6737e0acae6b1ca403e1b8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`3cce3f2`)](https://app.codecov.io/gh/scverse/scanpy/commit/3cce3f28e94d29dc907f94056bd6995f197d9f93?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3248?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/external/tl/\_wishbone.py](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&filepath=src%2Fscanpy%2Fexternal%2Ftl%2F_wishbone.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9leHRlcm5hbC90bC9fd2lzaGJvbmUucHk=) | 50.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 75.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tre,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3248#issuecomment-2363527588:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3248#issuecomment-2363527588,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3251?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `94.59459%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.71%. Comparing base [(`b325b50`)](https://app.codecov.io/gh/scverse/scanpy/commit/b325b50f942ba75d77e1a4caa181d67f83d0a057?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`408a7b5`)](https://app.codecov.io/gh/scverse/scanpy/commit/408a7b58758a609147276eea01e9f86ae16855ee?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3251?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3251?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 93.10% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3251?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3251 +/- ##; ==========================================; - Coverage 76.72% 76.71% -0.01% ; ==========================================; Files 109 109 ; Lines 12536 12541 +5 ; ==========================================; + Hits 9618 9621 +3 ; - Misses 2918 2920 +2 ; ```. | [Flag](https://app.codecov.io/gh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3251#issuecomment-2363756462:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3251#issuecomment-2363756462,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3316?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 77.21%. Comparing base [(`3d220a9`)](https://app.codecov.io/gh/scverse/scanpy/commit/3d220a93c83fdd60ee3220c94db3dd8d5533c60d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`f5f2775`)](https://app.codecov.io/gh/scverse/scanpy/commit/f5f27756930da430c3f6d803800076e8501952e6?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3316?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3316?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 85.71% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3316?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3316 +/- ##; ==========================================; - Coverage 77.23% 77.21% -0.02% ; ==========================================; Files 111 111 ; Lines 12605 12597 -8 ; ==========================================; - Hits 9735 9727 -8 ; Misses 2870 2870 ; ```. | [Files with missing lines](ht,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316#issuecomment-2435332939:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316#issuecomment-2435332939,1,['Patch'],['Patch']
Deployability,"ghts the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this param",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1195,install,install,1195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['install']
Deployability,h/scverse/scanpy/pull/3314?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `88.88889%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 76.95%. Comparing base [(`947afa1`)](https://app.codecov.io/gh/scverse/scanpy/commit/947afa157474130bd94b5130dd2de433692e06ff?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`ca18a68`)](https://app.codecov.io/gh/scverse/scanpy/commit/ca18a68e60d72a0b16263ea43d5b950fcf9b0c44?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3314?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3314?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 88.88% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3314?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3314 +/- ##; =======================================; Coverage 76.94% 76.95% ; =======================================; Files 109 109 ; Lines 12462 12467 +5 ; =======================================; + Hits 9589 9594 +5 ; Misses 2873 2873 ; ```. | [Files with missing lines](https://app.codeco,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3314#issuecomment-2434892685:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3314#issuecomment-2434892685,1,['Patch'],['Patch']
Deployability,"h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17005,Install,Installing,17005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:223,continuous,continuous,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544,1,['continuous'],['continuous']
Deployability,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513181427:62,update,update,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513181427,1,['update'],['update']
Deployability,"handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3171,install,install,3171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,['install'],['install']
Deployability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Integrat,Integration,966,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,3,"['Integrat', 'integrat']","['Integration', 'integrate', 'integration']"
Deployability,"heck_bool_indexer(self.index, key); 910 ; --> 911 return self._get_with(key); 912 ; 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 return self.loc[key]; 952 ; --> 953 return self.reindex(key); 954 except Exception:; 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs); 3732 @Appender(generic.NDFrame.reindex.__doc__); 3733 def reindex(self, index=None, **kwargs):; -> 3734 return super(Series, self).reindex(index=index, **kwargs); 3735 ; 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4354 # perform the reindex on the axes; 4355 return self._reindex_axes(axes, level, limit, tolerance, method,; -> 4356 fill_value, copy).__finalize__(self); 4357 ; 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4367 ax = self._get_axis(a); 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,; -> 4369 tolerance=tolerance, method=method); 4370 ; 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 else:; 502 if not target.is_unique:; --> 503 raise ValueError(""cannot reindex with a non-unique indexer""); 504 ; 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer; ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264:2852,install,installed,2852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264,1,['install'],['installed']
Deployability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['integrat'],['integration']
Deployability,hi @dm8000 did you install it with pip or conda? I would try to re installl with either of the two methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210554228:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210554228,2,['install'],"['install', 'installl']"
Deployability,"hi @ktpolanski ,thank you for the heads up. > A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument spaceranger_image_path to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. > The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location. @LucaMarconato do we have any datasets that test for spaceranger 3.0.1 ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2332223204:48,hotfix,hotfix,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2332223204,1,['hotfix'],['hotfix']
Deployability,"hi @pacificoceanmist , which scanpy version are you using, and could you update to latest version ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305#issuecomment-1210560162:73,update,update,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305#issuecomment-1210560162,1,['update'],['update']
Deployability,"hi @yotamcons ,. thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561,1,['update'],['updated']
Deployability,"hi, sorry for the confusion. i removed the comments not relevant to the issue. for the record:. - continuum inc. is responsible for providing packages installable with `conda`, we can’t influence that IIRC. they will probably eventually include scanpy.; - python comes with `pip` by default, and `pip` can be used to install all packages on [the python package index](https://pypi.python.org); - the only advantage of anaconda is that it comes with preinstalled packages, but installing scanpy via `pip` will always be just as fast as it will eventually be with `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798:151,install,installable,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798,3,['install'],"['install', 'installable', 'installing']"
Deployability,highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24030,pipeline,pipeline,24030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,hly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:26231,pipeline,pipeline,26231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,integrat,integrating-data-using-ingest,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561,1,['integrat'],['integrating-data-using-ingest']
Deployability,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-966679910:69,install,install,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-966679910,2,['install'],['install']
Deployability,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:417,update,update,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015,1,['update'],['update']
Deployability,"ib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1805,update,updated,1805,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['update'],['updated']
Deployability,iduals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILE,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28683,pipeline,pipeline,28683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,iduals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21998,pipeline,pipeline,21998,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ighly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_hi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24692,pipeline,pipeline,24692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4227,install,install,4227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['install']
Deployability,iled in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28003,pipeline,pipeline,28003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1214,install,install,1214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"iniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py (53); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:33208,pipeline,pipeline,33208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:0,install,install,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692,7,['install'],"['install', 'installing']"
Deployability,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1122,install,installation,1122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,1,['install'],['installation']
Deployability,io/gh/scverse/scanpy/pull/3084?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `62.50000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 75.85%. Comparing base [(`5dc489d`)](https://app.codecov.io/gh/scverse/scanpy/commit/5dc489d5c71fa91fd0cabe6adb363172119ce5eb?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`6ba3676`)](https://app.codecov.io/gh/scverse/scanpy/commit/6ba36767cbb6fe59524c1ac54347b3a36de41a28?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 46 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3084?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/external/exporting.py](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&filepath=scanpy%2Fexternal%2Fexporting.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2V4dGVybmFsL2V4cG9ydGluZy5weQ==) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/preprocessing/\_combat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_combat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2NvbWJhdC5weQ==) | 0.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&utm_medium=referral&utm_s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3084#issuecomment-2141838152:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3084#issuecomment-2141838152,1,['Patch'],['Patch']
Deployability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9850,pipeline,pipeline,9850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,ionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_pl,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:44347,pipeline,pipeline,44347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding.py::test_umap_init_paga[fr] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' d,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:11435,pipeline,pipeline,11435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing byteco,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32744,pipeline,pipeline,32744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3258,install,install,3258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,['install'],['install']
Deployability,"ith this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1391,Install,Installing,1391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['Install'],['Installing']
Deployability,"just mention this PR with brief description and your name here: https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst. nice! only one PR to go, thank you @hspitzer @ivirshup !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015:116,release,release-latest,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015,1,['release'],['release-latest']
Deployability,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898,5,['install'],['install']
Deployability,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420:35,release,release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420,1,['release'],['release']
Deployability,l[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33678,pipeline,pipeline,33678,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"le__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1547,install,install-,1547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,leiden install via conda code is wrong in the current page. it should be:. ```; conda install -c conda-forge leidenalg ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015,2,['install'],['install']
Deployability,"ler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self); 134 if self.generator_info is None:; 135 self.genlower = None; --> 136 self.lower_normal_function(self.fndesc); 137 else:; 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7735,pipeline,pipeline,7735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,"lib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.3.2; matplotlib_inline NA; ml_collections NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.1; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2021.3; requests 2.27.1; rich NA; scipy 1.8.0; scrublet NA; scvi 0.19.0; seaborn 0.11.2; session_info 1.0.0; setuptools 60.9.3; setuptools_scm NA; six 1.16.0; sklearn 0.23.2; socks 1.7.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.13.2; tensorboard 2.10.1; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 1.11.0; torchmetrics 0.10.2; torchvision 0.12.0; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; umap 0.5.2; urllib3 1.26.8; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.9; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.4.0-139-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-02-26 19:13; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:6085,update,updated,6085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,"lled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway?. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-03 21:49. </p>; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542:2026,update,updated,2026,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542,1,['update'],['updated']
Deployability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integration']
Deployability,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,integrat,integrating,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182,1,['integrat'],['integrating']
Deployability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:5293,Install,Installing,5293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,4,"['Install', 'install', 'update']","['Installing', 'installation', 'installed', 'updated']"
Deployability,ls_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_hig,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22691,pipeline,pipeline,22691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1735,install,installs,1735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,2,['install'],['installs']
Deployability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3312,install,installed,3312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,mentedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: u,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:35609,pipeline,pipeline,35609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,mmh can I help you with how to go wrt to update?. @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129:41,update,update,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129,1,['update'],['update']
Deployability,"mmmm #2...; The previous email regarding decorator issue, may have been a problem from; namespace shadowing in a jupyter notebook. I think the previous pull; request works--but suggest testing with and without vmin vmax arguments.; I'll update you as I do more testing myself.; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444661453:237,update,update,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444661453,2,"['patch', 'update']","['patch', 'update']"
Deployability,"mmmm...; looks like there are some difficulties here.; The decorator sitting ontop of dotplot() causes a weird error for kwds; dictionary lookups. If I leave the decorator in place, then I get a; keywords error when, vmin is left out as a parameter. If I take the; decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to; ensure that the __doc__ string starts with a '\' character. And in the case; of dotplot() it already does. When I comment out the decorator, the code; works. This error is too strange for me to understand. I don't often use; decorators, and it seems to be the problem here.; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444632665:936,patch,patch,936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444632665,1,['patch'],['patch']
Deployability,mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40573,continuous,continuous-,40573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['continuous'],['continuous-']
Deployability,mplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:27769,pipeline,pipeline,27769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"n them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_k",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3372,install,installed,3372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['installed']
Deployability,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3879,pipeline,pipeline,3879,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,3,['pipeline'],['pipeline']
Deployability,n_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_gen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39422,pipeline,pipeline,39422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,n_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21763,pipeline,pipeline,21763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"naconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10787,install,install,10787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,ncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signa,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:36852,pipeline,pipeline,36852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"nd we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1247,pipeline,pipeline,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,ng bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40486,pipeline,pipeline,40486,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:216,continuous,continuous,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['continuous'],['continuous']
Deployability,npy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:26462,pipeline,pipeline,26462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"nstall the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1029,install,install,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027,1,['install'],['install']
Deployability,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1599,update,updates,1599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['update'],['updates']
Deployability,ntedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:41894,pipeline,pipeline,41894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,o/gh/scverse/scanpy/pull/3097?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.16667%` with `5 lines` in your changes missing coverage. Please review.; > Project coverage is 76.35%. Comparing base [(`4f40d68`)](https://app.codecov.io/gh/scverse/scanpy/commit/4f40d68c8958ef74fd8abe5f97601c40ffee9337?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`e89ebac`)](https://app.codecov.io/gh/scverse/scanpy/commit/e89ebaceddc37589fe22bfe32b1e8a9f1b5746f9?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 41 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3097?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_score\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&filepath=scanpy%2Ftools%2F_score_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL19zY29yZV9nZW5lcy5weQ==) | 82.35% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&filepath=scanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2dldC9nZXQucHk=) | 66.66% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3097#issuecomment-2147774048:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3097#issuecomment-2147774048,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3220?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `33.33333%` with `30 lines` in your changes missing coverage. Please review.; > Project coverage is 76.73%. Comparing base [(`d4e1fb4`)](https://app.codecov.io/gh/scverse/scanpy/commit/d4e1fb4cb290d9835710fba2b5b9594d97176601?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`403dd30`)](https://app.codecov.io/gh/scverse/scanpy/commit/403dd30f9e523ae84eba4ac239c6fb72fb439585?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3220?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 25.00% | [15 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_draw\_graph.py](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_draw_graph.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fZHJhd19ncmFwaC5weQ==) | 31.57% | [13 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3220#issuecomment-2323386009:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220#issuecomment-2323386009,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3249?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `94.59459%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.75%. Comparing base [(`1650aed`)](https://app.codecov.io/gh/scverse/scanpy/commit/1650aed30fd0141a97c01a6a6b19c2735e058c77?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`b3fa09b`)](https://app.codecov.io/gh/scverse/scanpy/commit/b3fa09ba950806f9e2a2c5060b32d3d768f0f14e?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3249?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3249?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 93.10% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3249?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3249 +/- ##; ==========================================; - Coverage 76.75% 76.75% -0.01% ; ==========================================; Files 109 109 ; Lines 12551 12556 +5 ; ==========================================; + Hits 9634 9637 +3 ; - Misses 2917 2919 +2 ; ```. | [Files with missing lines](https:/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3249#issuecomment-2363632936:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3249#issuecomment-2363632936,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3307?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `95.45455%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 77.23%. Comparing base [(`2f0afac`)](https://app.codecov.io/gh/scverse/scanpy/commit/2f0afac72be3644624cf996323197239580f14f9?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`65c74cf`)](https://app.codecov.io/gh/scverse/scanpy/commit/65c74cfa67b2f9d9493b9cd2384685245ecacc2c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 2 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3307?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_qc.py](https://app.codecov.io/gh/scverse/scanpy/pull/3307?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_qc.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19xYy5weQ==) | 94.28% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3307?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3307 +/- ##; ==========================================; + Coverage 77.21% 77.23% +0.02% ; ==========================================; Files 111 111 ; Lines 12597 12618 +21 ; ==========================================; + Hits 9727 9746 +19 ; - Misses 2870 2872 +2 ; ```. | [Files with missing lines](https,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3307#issuecomment-2429279414:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307#issuecomment-2429279414,1,['Patch'],['Patch']
Deployability,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:2180,update,update,2180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,1,['update'],['update']
Deployability,"oftware/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session information updated at 2022-05-17 14:56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:2694,update,updated,2694,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['update'],['updated']
Deployability,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:188,release,release,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112,2,['release'],['release']
Deployability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:48,install,installing,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,1,['install'],['installing']
Deployability,on_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedErro,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:15440,pipeline,pipeline,15440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,onda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge; h5py 2.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1573,patch,patch,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['patch'],['patch']
Deployability,"ong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=492) """"""; [494](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=493) Populate and run pipeline for bytecode input; [495](file:///",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:19268,pipeline,pipeline,19268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17271,install,install,17271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,"orceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1630,install,install,1630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['install', 'release']","['install', 'release']"
Deployability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:1715,update,update,1715,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['update'],['update']
Deployability,"orm; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1275,release,release,1275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['release'],['release']
Deployability,ov.io/gh/scverse/scanpy/pull/2684?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `83.33333%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 71.83%. Comparing base [(`42e3c2a`)](https://app.codecov.io/gh/scverse/scanpy/commit/42e3c2a04e2bee8431da4e831abd16d198bc8323?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`7b6a875`)](https://app.codecov.io/gh/scverse/scanpy/commit/7b6a875a75529e41e4a53993a0e04a4ad04a9d78?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 223 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/2684?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_umap.py](https://app.codecov.io/gh/scverse/scanpy/pull/2684?src=pr&el=tree&filepath=scanpy%2Ftools%2F_umap.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL191bWFwLnB5) | 83.33% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/2684?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #2684 +/- ##; ==========================================; - Coverage 71.98% 71.83% -0.16% ; ==========================================; Files 108 108 ; Lines 11920 11921 +1 ; ==========================================; - Hits 8581 8563 -18 ; - Misses 3339 3358 +19 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1763195554:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1763195554,1,['Patch'],['Patch']
Deployability,ov.io/gh/scverse/scanpy/pull/3283?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `87.50000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.94%. Comparing base [(`be99b23`)](https://app.codecov.io/gh/scverse/scanpy/commit/be99b230fa84e077f5167979bc9f6dacc4ad0d41?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`8268e54`)](https://app.codecov.io/gh/scverse/scanpy/commit/8268e543090721ae9b056355c0cbb8d2ac742d13?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3283?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/external/pl.py](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&filepath=src%2Fscanpy%2Fexternal%2Fpl.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9leHRlcm5hbC9wbC5weQ==) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 83.33% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&utm_medium=referral&utm_source=github,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3283#issuecomment-2411417242:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283#issuecomment-2411417242,1,['Patch'],['Patch']
Deployability,"ows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1254,patch,patches,1254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['patch'],['patches']
Deployability,"p.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1291,install,install-,1291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,"packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-04 01:16. </p>; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:2108,update,updated,2108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,1,['update'],['updated']
Deployability,"pbmc.uns[""rank_genes_groups""][""names""][0]). In [4]: sc.pl.stacked_violin(pbmc, var_names=genes, groupby='louvain', swap_axes=True); ```. <details>; <summary> Versions </summary>. ```; -----; scanpy 1.9.0.dev63+gb69015e9; session_info 1.0.0; -----; PIL 9.0.0; anndata 0.8.0rc1; appnope 0.1.2; asciitree NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.0.0; cycler 0.10.0; cython_runtime NA; dask 2021.12.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; executing 0.8.2; fasteners NA; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 6.7.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.13.1; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; msgpack 1.0.3; natsort 8.0.2; nbinom_ufunc NA; numba 0.55.0; numcodecs 0.9.1; numexpr 2.7.3; numpy 1.21.5; packaging 21.3; pandas 1.4.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.22; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.1; pyarrow 6.0.1; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyexpat NA; pygments 2.10.0; pyparsing 3.0.6; pytz 2021.3; ruamel NA; scipy 1.7.3; setuptools 60.5.0; setuptools_scm NA; sinfo 0.3.4; sitecustomize NA; six 1.16.0; sklearn 1.0.2; sparse 0.13.0; sphinxcontrib NA; stack_data 0.1.4; tables 3.7.0; tblib 1.7.0; texttable 1.6.4; threadpoolctl 3.0.0; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zappy NA; zarr 2.10.3; zmq 22.3.0; -----; IPython 8.0.0; jupyter_client 7.1.0; jupyter_core 4.9.1; jupyterlab 3.2.6; notebook 6.4.7; -----; Python 3.9.9 (main, Nov 21 2021, 03:23:42) [Clang 13.0.0 (clang-1300.0.29.3)]; macOS-11.6.2-x86_64-i386-64bit; -----; Session information updated at 2022-01-24 16:14; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020206162:2136,update,updated,2136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020206162,1,['update'],['updated']
Deployability,pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28471,pipeline,pipeline,28471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28235,pipeline,pipeline,28235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecod,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28917,pipeline,pipeline,28917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:4,install,install,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"please check if my edits made some part worse in your opinion. if you want, you can add some release notes for this :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3122#issuecomment-2188843289:93,release,release,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3122#issuecomment-2188843289,1,['release'],['release']
Deployability,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/727#issuecomment-508692089:21,update,update,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508692089,1,['update'],['update']
Deployability,prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.10 3_cp310 conda-forge; pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch; pytorch-cuda 11.8 h7e8668a_5 pytorch; pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge; pytorch-mutex 1.0 cuda pytorch; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyyaml 6.0 py310h5764c6d_5 conda-forge; pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge; rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge; re2 2023.03.02 h8c504da_0 conda-forge; readchar 4.0.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; referencing 0.30.0 pyhd8ed1ab_0 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge; rich 13.4.2 pyhd8ed1ab_0 conda-forge; rpds-py 0.9.2 py310hcb5633a_0 conda-forge; scanpy 1.9.3 pyhd8ed1ab_0 conda-forge; scikit-learn 1.3.0 py310hf7d194e_0 conda-forge; scipy 1.11.1 py310ha4c1d20_0 conda-forge; scvi-tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:18493,install,installer,18493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['install'],['installer']
Deployability,"protip: when wanting to link to lines, press the <kbd>y</kbd> key to change the url from `…/master/…` to `…/<sha1sum>/…`. that way the copied URL will stay valid and not break once the file is modified on the master branch. no objections to the release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298897726:245,release,release,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298897726,1,['release'],['release']
Deployability,ptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16665,pipeline,pipeline,16665,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"pytables 3.7.0 is incompatible with Windows. Installing pytables 3.6.1 solves the problem.; For the UMAP, it's quite weird. Whether I restart anaconda between installing individual packages plays the trick.; It should be done in this way.; ```python; !pip install tables==3.6.1. !pip install scanpy[leiden]; !pip install -U scvelo; !pip install tqdm; !pip install ipywidgets; !pip install cellrank. close Edge. run anaconda powershell prompt (anaconda3) as admin; conda activate HYJ_py38; conda install -y -c anaconda cytoolz; pip install pyscenic. restart anaconda3. !pip install adjustText openpyxl; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391:45,Install,Installing,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391,11,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplemented,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40894,pipeline,pipeline,40894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2150,install,installation,2150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],"['install', 'installation']"
Deployability,r_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33446,pipeline,pipeline,33446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"release done, we can wait here …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2188807240:0,release,release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2188807240,1,['release'],['release']
Deployability,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1269,install,installation,1269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['install'],['installation']
Deployability,resolved via . pip install 'matplotlib<3.7',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462121858:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462121858,1,['install'],['install']
Deployability,rge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:42128,pipeline,pipeline,42128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,riable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39638,pipeline,pipeline,39638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:15209,pipeline,pipeline,15209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"rom you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1723,install,install,1723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['install'],['install']
Deployability,ror: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matchi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:14749,pipeline,pipeline,14749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,rson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22904,pipeline,pipeline,22904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ry familiar with Scanpy (which seems like a fantastic library!), so please bear with me if some of the things I mention are not relevant to Scanpy. PyMDE (documentation here: https://pymde.org/) has a few benefits:; - as @adamgayoso mentioned, PyMDE supports computing embeddings on GPU. This makes it possible to compute very large embeddings quickly (often 4-10x faster than CPU).; - PyMDE is a very general embedding library. It is based on a general framework for embedding, and this framework includes many well-known methods --- such as UMAP, PCA, Laplacian embedding, multi-dimensional scaling, and more --- as special cases. This makes it easy to compare different methods using a single framework.; - PyMDE also supports creating entirely new types of embeddings, as custom instances of our framework.; - PyMDE provides ways to reason about how much an embedding distorts the original neighborhood graph. There are some comparisons to UMAP & openTSNE in the third part of our manuscript, which has been published in Foundations & Trends in Machine Learning and is available here: https://web.stanford.edu/~boyd/papers/pdf/min_dist_emb.pdf; - on CPU, UMAP and PyMDE are comparable in speed, with UMAP often having a slight edge; on GPU PyMDE can be much faster; - unlike UMAP/openTSNE, PyMDE allows users to fit constrained embeddings. Right now the supported constraints are standardization (zero mean, unit covariance; this forces embeddings to spread out, but not too much, and as a result standardized embeddings are typically similarly scaled), centering, and anchoring (pre-specifying the coordinates of a subset of the items); - PyMDE allows for more types of embeddings, in addition to UMAP-style embeddings. On the other hand, PyMDE is young software. If you do depend on it, I would recommend including it as an optional dependency, not a required one. Happy to chat more, to answer any questions, and to help with integration, if that is something you are ultimately interested in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627:2057,integrat,integration,2057,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627,1,['integrat'],['integration']
Deployability,s._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not co,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:14980,pipeline,pipeline,14980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40070,pipeline,pipeline,40070,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:30083,pipeline,pipeline,30083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20838,pipeline,pipeline,20838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39187,pipeline,pipeline,39187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"same here, wish I tried to install earlier....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955:27,install,install,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955,1,['install'],['install']
Deployability,"sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", n_top_genes=3000); error:Please install skmisc package via `pip install --user scikit-misc; I have the same problem as the blogger, so how should I solve it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1518455042:87,install,install,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1518455042,4,['install'],['install']
Deployability,"sc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): #",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5102,install,installed,5102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['installed']
Deployability,"scanpy could just check on import if one of them is installed instead of the package that should be installed, and raise a nicely phrased ImportError.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181:52,install,installed,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181,2,['install'],['installed']
Deployability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2771,install,installed,2771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['install'],['installed']
Deployability,scverse/scanpy/pull/3330?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `22.44898%` with `38 lines` in your changes missing coverage. Please review.; > Project coverage is 72.25%. Comparing base [(`a70582e`)](https://app.codecov.io/gh/scverse/scanpy/commit/a70582ee03556cf6821eb45148560cb259a5fb34?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`25e7cd4`)](https://app.codecov.io/gh/scverse/scanpy/commit/25e7cd418ac258329944ced2b4ba443d5d06865b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 103 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3330?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3330?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 22.44% | [38 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3330?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3330 +/- ##; ==========================================; - Coverage 76.27% 72.25% -4.03% ; ==========================================; Files 117 111 -6 ; Lines 12795 12639 -156 ; ==========================================; - Hits 9760 9132 -628 ; - Misses 3035 3507 +472 ; ```. | [Files with mi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3330#issuecomment-2443557729:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330#issuecomment-2443557729,1,['Patch'],['Patch']
Deployability,se/scanpy/pull/3082?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 75.80%. Comparing base [(`b3b9d05`)](https://app.codecov.io/gh/scverse/scanpy/commit/b3b9d0576897a8da5a4ae765b4b0b5609cebc890?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`2709e08`)](https://app.codecov.io/gh/scverse/scanpy/commit/2709e08fe39d3440c904b3cfcb1913611d9bc672?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 39 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3082?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/preprocessing/\_highly\_variable\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3082?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_highly_variable_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2hpZ2hseV92YXJpYWJsZV9nZW5lcy5weQ==) | 85.71% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3082?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3082 +/- ##; =======================================; Coverage 75.80% 75.80% ; =======================================; Files 110 110 ; Lines 12500 12501 +1 ; =======================================; + Hits 9475 9476 +1 ; Misses 3025 3025 ; ```. | [Files with mi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3082#issuecomment-2141612509:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3082#issuecomment-2141612509,1,['Patch'],['Patch']
Deployability,"sers\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra; return self._compile_bytecode(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode; return self._compile_core(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core; raise e; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core; pm.run(self.state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run; raise patched_exception; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run; self._runPass(idx, pass_inst, state); File ""C:\Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:5002,pipeline,pipeline,5002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['pipeline'],['pipeline']
Deployability,siduals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17126,pipeline,pipeline,17126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"sk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:2948,pipeline,pipeline,2948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['pipeline'],['pipeline']
Deployability,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1346,install,install,1346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027,3,['install'],['install']
Deployability,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:24,install,installations,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416,3,['install'],"['installation', 'installations', 'installing']"
Deployability,"sort_values(""dispersions_norm"", ascending=False).iloc[:10, :]; print(top10_scanpy[[""means"", ""dispersions"", ""dispersions_norm""]]); ```. ```; mvp.mean mvp.dispersion mvp.dispersion.scaled; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242922 5.806298 7.895355; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598463; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; means dispersions dispersions_norm; index ; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242923 5.806298 7.895356; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598464; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; ```. To generate seurat_hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:3415,patch,patchwork,3415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,1,['patch'],['patchwork']
Deployability,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6011,release,release,6011,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,3,"['patch', 'release']","['patch', 'release']"
Deployability,"t force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1099,install,install,1099,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['install']
Deployability,t import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-leg,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:12168,pipeline,pipeline,12168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"t); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ; ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]); 30 ; 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'; ```. I used Jupyter Notebook. Should I update any package? Thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:2197,update,update,2197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622,1,['update'],['update']
Deployability,"ta object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_cells_by_counts"". Number of cells this expression is ; measu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1485,update,updates,1485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['update'],['updates']
Deployability,ta-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:42507,pipeline,pipeline,42507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ta.raw.X; > v = adata[:, 0 : adata.shape[1] // 2]; > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; > assert v.is_view; > with pytest.warns(Warning, match=""view""):; > > sc.pp.scale(v, flavor=flavor); > ; > scanpy/tests/test_preprocessing.py:127: ; > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1511,pipeline,pipeline,1511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['pipeline'],['pipeline']
Deployability,tails><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3058 +/- ##; ==========================================; - Coverage 76.27% 76.08% -0.20% ; ==========================================; Files 117 117 ; Lines 12803 12802 -1 ; ==========================================; - Hits 9766 9740 -26 ; - Misses 3037 3062 +25 ; ```. | [Files](https://app.codecov.io/gh/scverse/scanpy/pull/3058?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Coverage Δ | |; |---|---|---|; | [scanpy/preprocessing/\_scrublet/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L19faW5pdF9fLnB5) | `96.80% <100.00%> (+0.10%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/pipeline.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fpipeline.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3BpcGVsaW5lLnB5) | `94.59% <100.00%> (+0.30%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/sparse\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fsparse_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3NwYXJzZV91dGlscy5weQ==) | `89.28% <71.42%> (-1.90%)` | :arrow_down: |. ... and [12 files with indirect coverage changes](https://app.codecov.io/gh/scverse/scanpy/pull/3058/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3058#issuecomment-2110286934:1824,pipeline,pipeline,1824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3058#issuecomment-2110286934,1,['pipeline'],['pipeline']
Deployability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:2207,configurat,configuration,2207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,2,['configurat'],['configuration']
Deployability,"te-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:7673,pipeline,pipeline,7673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['pipeline'],['pipeline']
Deployability,tedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mod,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:35377,pipeline,pipeline,35377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,tep: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files di,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:43552,pipeline,pipeline,43552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24910,pipeline,pipeline,24910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,thank you! the bug was fixed in https://github.com/theislab/scanpy/commit/a4baaaf6c29b8da4f3d9026552719039d2600ca9 and release 0.4.1: `pip install scanpy --upgrade`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104:119,release,release,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104,3,"['install', 'release', 'upgrade']","['install', 'release', 'upgrade']"
Deployability,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163,1,['release'],['release']
Deployability,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671,2,['install'],"['install', 'installed']"
Deployability,"the newest versions of leidenalg and igraph should be compatible with each other. update both, e.g. `pip install -U leidenalg igraph`. `python-igraph` is no longer necessary, you can remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1646862855:82,update,update,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1646862855,2,"['install', 'update']","['install', 'update']"
Deployability,theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38051,pipeline,pipeline,38051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"thor', xlabel='leiden_0.6', condition=None); print(relative_frequencies); sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None); ```. I am getting this output; ```. 0 1 2 3 4 5 \; Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 ; Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 ; Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 ; Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author ; Benitez 0.111111 0.133191 0.021368 0.000000 Benitez ; Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari ; Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari ; Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-44-a5b07a2bfb6d> in <module>; 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None); 7 print(relative_frequencies); ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save); 835 for i, typ in enumerate(reversed(cell_types)):; 836 fig = sb.barplot(; --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize; 838 ); 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643:2047,patch,patches,2047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643,2,"['Patch', 'patch']","['Patch', 'patches']"
Deployability,"throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1013,release,release,1013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['release'],['release']
Deployability,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,integrat,integration,1980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['integrat'],['integration']
Deployability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,2,"['install', 'release']","['install', 'release']"
Deployability,"ts that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4258,install,install-,4258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability,uals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); F,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19908,pipeline,pipeline,19908,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,uals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20139,pipeline,pipeline,20139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:39,install,install,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609,1,['install'],['install']
Deployability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Integrat,Integration,996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,"['Integrat', 'integrat']","['Integration', 'integrate']"
Deployability,"update on this, seems like this is an issue when the package is installed through conda,. reinstalled the package using pip and everything works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280,2,"['install', 'update']","['installed', 'update']"
Deployability,"update:. managed to get a confidence thresholding with this type of logic:. ```py; def _knn_classify(self, labels):; # ensure it's categorical; cat_array: pd.Series = self._adata_ref.obs[labels].astype(""category""); values = []; confidences = []. for inds in self._indices:; mode_value = cat_array.iloc[inds].mode()[0]; mode_count = (cat_array.iloc[inds] == mode_value).sum(); confidence = mode_count / len(inds); values.append(mode_value); confidences.append(confidence); ; # Create a DataFrame for better readability; classification_df = pd.DataFrame({; ""Mode Values"": values,; ""Confidences"": confidences; }); print(classification_df). return pd.Categorical(values=values, categories=cat_array.cat.categories), np.array(confidences). def map_labels(self, labels, method, confidence_threshold: float = 0.5):; """"""\; Map labels of `adata` to `adata_new`. This function infers `labels` for `adata_new.obs`; from existing labels in `adata.obs`.; `method` can be only 'knn'.; """"""; if method == ""knn"":; classified_labels, confidences = self._knn_classify(labels); mask = confidences >= confidence_threshold; ; filtered_labels = [; label if mask[idx] else np.nan ; for idx, label in enumerate(classified_labels); ]; ; classified_labels = pd.Categorical(; filtered_labels,; categories=classified_labels.categories; ); ; self._adata_new.obs[labels] = classified_labels; self._adata_new.obs[labels + '_confidence'] = confidences; else:; raise NotImplementedError(""Ingest supports knn labeling for now.""); ```; . would love to get input on whether or not this makes sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3160#issuecomment-2270570908:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3160#issuecomment-2270570908,1,['update'],['update']
Deployability,"updated it to the latest version, I'm still encountering the same error. -----; anndata 0.9.2; scanpy 1.10.2; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 2.1.0; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-09-04 17:28",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344:2061,update,updated,2061,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344,1,['update'],['updated']
Deployability,upgrade your kali linux with the command ; sudo apt upgrade; pip3 install scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460:0,upgrade,upgrade,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"ustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2789,release,release,2789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['release'],['release']
Deployability,"utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:7215,install,install,7215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,v.io/gh/scverse/scanpy/pull/3192?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `66.66667%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.61%. Comparing base [(`a60a96f`)](https://app.codecov.io/gh/scverse/scanpy/commit/a60a96fb7790e35abe8d007abd6f5a17b1573d7d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`fca69e6`)](https://app.codecov.io/gh/scverse/scanpy/commit/fca69e6d08e90f86c01a033fa4c8749cc2cb4ea3?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 43 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3192?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3192?src=pr&el=tree&filepath=src%2Fscanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9nZXQvZ2V0LnB5) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3192?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3192 +/- ##; ==========================================; - Coverage 76.61% 76.61% -0.01% ; ==========================================; Files 109 109 ; Lines 12529 12532 +3 ; ==========================================; + Hits 9599 9601 +2 ; - Misses 2930 2931 +1 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3192#issuecomment-2263491216:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3192#issuecomment-2263491216,1,['Patch'],['Patch']
Deployability,v.io/gh/scverse/scanpy/pull/3265?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `92.00000%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.91%. Comparing base [(`7ccf96d`)](https://app.codecov.io/gh/scverse/scanpy/commit/7ccf96d4b6ac10f0a718010578f725e3de2902f7?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`bd99e6d`)](https://app.codecov.io/gh/scverse/scanpy/commit/bd99e6de4235131acaf426f62649a929db32c699?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3265?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3265?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 81.81% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3265?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3265 +/- ##; ==========================================; - Coverage 76.93% 76.91% -0.02% ; ==========================================; Files 109 109 ; Lines 12454 12451 -3 ; ==========================================; - Hits 9581 9577 -4 ; - Misses 2873 2874 +1 ; ```. | [Flag](https://app.codecov.io/gh/scverse/scanpy/pull/3265/flags?s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3265#issuecomment-2377150033:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3265#issuecomment-2377150033,1,['Patch'],['Patch']
Deployability,variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25378,pipeline,pipeline,25378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,verse/scanpy/pull/3267?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `95.16129%` with `6 lines` in your changes missing coverage. Please review.; > Project coverage is 77.03%. Comparing base [(`bbcd4b1`)](https://app.codecov.io/gh/scverse/scanpy/commit/bbcd4b173aabebb8b4793cf2cdd6ea8b31e31005?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`7516acc`)](https://app.codecov.io/gh/scverse/scanpy/commit/7516acc4474f54b86cc7a880e3508a20b8a40169?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3267?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_pca/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3267?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_pca%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19wY2EvX19pbml0X18ucHk=) | 93.18% | [6 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3267?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3267 +/- ##; ==========================================; + Coverage 76.95% 77.03% +0.08% ; ==========================================; Files 109 110 +1 ; Lines 12465 12492 +27 ; ==========================================; + Hits 9592 9623 +31 ; + Misses 2873 2869 -4 ; ```. ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267#issuecomment-2378904712:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267#issuecomment-2378904712,1,['Patch'],['Patch']
Deployability,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928:200,update,update,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928,1,['update'],['update']
Deployability,we should update the tutorials and notebooks to use ` leiden` instead of `louvain`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-482127774:10,update,update,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-482127774,1,['update'],['update']
Deployability,"working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1313,install,install,1313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"would also make sense to have this as `colorbar_loc` as this only really applies for continuous coloring, right? I can definetly see the benefit of removing colorbar from figure if this is wanted by the user",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825:85,continuous,continuous,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825,1,['continuous'],['continuous']
Deployability,y-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31031,pipeline,pipeline,31031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"y48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=476) else:; [478](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=477) raise CompilerError(""All available pipelines exhausted""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:21428,pipeline,pipelines,21428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipelines']
Deployability,y::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[pca] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[spatial] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_dimension_broadcasting - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_marker_broadcasting - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_plotting.py::test_rank_genes_groups[ranked_genes_dotplot_gene_names-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin-fn9] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_correlation - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_paga.py::test_paga_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:45595,pipeline,pipeline,45595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,y_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_high,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23120,pipeline,pipeline,23120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433:55,update,update,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433,2,"['install', 'update']","['install', 'update']"
Deployability,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,integrat,integrate,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573,1,['integrat'],['integrate']
Deployability,"yes, apparently the problem is associated with these 'nan' values. I assumed this has to do with a package update, because it was working before. I rolled back to scanpy 1.7.1 and this block is working fine. The 'nan's are just omitted in the plots. I appreciate you offered to give a code example as my python skills are limited. But I figured a workaround so you don't have to bother with it. . I won't close the issue yet because I assume this behavior was not intended with the update.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437:107,update,update,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437,2,['update'],['update']
Deployability,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1592,install,installed,1592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,['install'],['installed']
Deployability,"ypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; triton 2.0.0 pypi_0 pypi; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023.3 pypi_0 pypi; umap-learn 0.5.3 pypi_0 pypi; urllib3 1.26.13 pypi_0 pypi; uvicorn 0.23.1 pypi_0 pypi; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; websocket-client 1.6.1 pypi_0 pypi; websockets 11.0.3 pypi_0 pypi; wheel 0.41.0 pyhd8ed1ab_0 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pypi_0 pypi; xz 5.2.6 h166bdaf_0 conda-forge; yamlordereddictloader 0.4.0 pypi_0 pypi; yarl 1.9.2 pypi_0 pypi; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; bro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:8418,install,install,8418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,3,['install'],['install']
Deployability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1930,release,release,1930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,"['release', 'update']","['release', 'update']"
Deployability,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4886,install,install,4886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['install'],['install']
Deployability,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-517982303:215,update,updated,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-517982303,1,['update'],['updated']
Deployability,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1679,install,installing,1679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['install'],['installing']
Deployability,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:13,install,installs,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242,5,['install'],"['install', 'installs']"
Deployability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3796,pipeline,pipeline,3796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['pipeline'],['pipeline']
Energy Efficiency," :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution?. > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1180,efficient,efficient,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaptive']
Energy Efficiency," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaption']
Energy Efficiency," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1203,power,power,1203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['power'],['power']
Energy Efficiency," get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3639,efficient,efficient,3639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1159,efficient,efficient,1159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['efficient'],['efficient']
Energy Efficiency,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:640,Efficient,Efficient,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,1,['Efficient'],['Efficient']
Energy Efficiency,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,adapt,adapted,937,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['adapt'],['adapted']
Energy Efficiency,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit; 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2413#issuecomment-1801466509:155,schedul,schedule,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413#issuecomment-1801466509,1,['schedul'],['schedule']
Energy Efficiency,"); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086688-66a3af40-ecc1-4b2e-a3e4-8038f7f207b0.png). ```; #make purple colormap; colors2 = plt.cm.Purples(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap4 = mymap(np.arange(mymap.N)); my_cmap4[:,-1] = np.linspace(0, 1, mymap.N); my_cmap4 = colors.ListedColormap(my_cmap4). sc.pl.umap(adata, color=['VWF'], use_raw=True, color_map=my_cmap4, show=False, frameon=False, vmax=5); ```; ![image](https://user-images.githubusercontent.com/56206488/126086713-d8b24873-f3ea-4067-8573-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:1635,Green,Greens,1635,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['Green'],['Greens']
Energy Efficiency,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446368749:587,efficient,efficiently,587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446368749,1,['efficient'],['efficiently']
Energy Efficiency,"= False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var = (1 / ((N - 1) * np.square(reg_std))) * (; (N * np.square(mean)); + squared_batch_counts_sum; - 2 * batch_counts_sum * mean; ); norm_gene_vars.append(norm_gene_var.reshape(1, -1)). norm_gene_vars = np.concatenate(norm_gene_vars, axis=0); # argsort twice gives ranks; ranked_norm_gene_vars = np.argsort(np.argsort(norm_gene_vars, axis=1), axis=1); median_norm_gene_vars = np.median(norm_gene_vars, axis=0); median_ranked = np.median(ranked_norm_gene_vars, axis=0). num_batches_high_var = np.sum(; ranked_norm_gene_vars >= (adata.X.shape[1] - n_top_genes), axis=0; ); df = pd.DataFrame(index=np.array(adata.var_names)); df[""highly_variable_nbatches""] = num_batches_high_var; df[""highly_variable_median_rank""] = median_ranked. df[""highly_variable_median_variance""] = median_norm_gene_vars; df.sort_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:1794,power,power,1794,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,1,['power'],['power']
Energy Efficiency,"=========; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1205,reduce,reducer,1205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"========================= test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1107,reduce,reducer,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"=================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1168,reduce,reducer,1168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:452,allocate,allocated,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['allocate'],['allocated']
Energy Efficiency,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here?; > ; > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ?; > ; > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?!. I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464:497,efficient,efficient,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464,1,['efficient'],['efficient']
Energy Efficiency,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:73,efficient,efficient,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531,2,['efficient'],['efficient']
Energy Efficiency,"> @gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?. Honestly, I don't know. But @ivirshup also brought it up here https://github.com/theislab/scanpy/pull/1180#discussion_r412871280 and here https://github.com/theislab/scanpy/pull/1180#discussion_r413445806, I just didn't have time to address it. I am fine with making them not None.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822:186,reduce,reduce,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822,1,['reduce'],['reduce']
Energy Efficiency,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-457914722:142,efficient,efficient,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-457914722,1,['efficient'],['efficient']
Energy Efficiency,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:1185,reduce,reducedDims,1185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,1,['reduce'],['reducedDims']
Energy Efficiency,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {‘umap’, ‘gauss’, ‘rapids’}, None (default: 'umap'). Use ‘umap’ [McInnes18] or ‘gauss’ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use ‘rapids’ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,adapt,adaptive,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742,1,['adapt'],['adaptive']
Energy Efficiency,"> I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different.; > ; > If we only switch the green or red color with another, most people won’t even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310:204,green,green,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310,2,['green'],['green']
Energy Efficiency,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-514006827:50,monitor,monitor,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-514006827,1,['monitor'],['monitor']
Energy Efficiency,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['adapt'],['adapted']
Energy Efficiency,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:34,efficient,efficiently,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265,1,['efficient'],['efficiently']
Energy Efficiency,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:147,efficient,efficiently,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['efficient'],['efficiently']
Energy Efficiency,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455:75,efficient,efficient,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455,2,['efficient'],['efficient']
Energy Efficiency,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:; > ; > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes?; > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code.; > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?. Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551#issuecomment-1640426447:398,allocate,allocated,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551#issuecomment-1640426447,3,['allocate'],"['allocate', 'allocated', 'allocates']"
Energy Efficiency,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:184,schedul,scheduling,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240,1,['schedul'],['scheduling']
Energy Efficiency,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:796,reduce,reduce,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,1,['reduce'],['reduce']
Energy Efficiency,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:507,reduce,reduced,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599,1,['reduce'],['reduced']
Energy Efficiency,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:102,reduce,reduce,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['reduce'],['reduce']
Energy Efficiency,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589:322,reduce,reduced,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589,2,"['efficient', 'reduce']","['efficient', 'reduced']"
Energy Efficiency,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:247,power,power-method,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937,2,['power'],"['power-method', 'powering']"
Energy Efficiency,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['adapt'],['adapt']
Energy Efficiency,"> `batch1.X.mean(1)` should give you the desired result.; > Or maybe i don't get what you actually need. Thanks for the reply. Sometimes I got index error when I used the subset data view for further analysis, saying the dimension was not matched. ; It could be solved by assigning .copy() when subsetting the adata. So I was just wondering if any memory-efficient way to do this also?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520:355,efficient,efficient,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520,1,['efficient'],['efficient']
Energy Efficiency,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:19,green,green,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868,3,['green'],['green']
Energy Efficiency,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:227,reduce,reduce,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,1,['reduce'],['reduce']
Energy Efficiency,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:305,power,powerful,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,1,['power'],['powerful']
Energy Efficiency,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3067,Power,Powered,3067,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['Power'],['Powered']
Energy Efficiency,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124:152,power,powers,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124,1,['power'],['powers']
Energy Efficiency,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261:132,adapt,adaption,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261,1,['adapt'],['adaption']
Energy Efficiency,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327:45,efficient,efficient,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327,2,['efficient'],['efficient']
Energy Efficiency,"@LuckyMD Thanks for sharing your code, I will try it. As for a dataset with very similar cells, I think the pbmc68k has T-cells that are very similar to each other. You can take a quick look at a reduced datase by doing:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/290#issuecomment-428935816:196,reduce,reduced,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428935816,1,['reduce'],['reduced']
Energy Efficiency,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147:71,efficient,efficient,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147,1,['efficient'],['efficient']
Energy Efficiency,"@VolkerBergen ; Also appropriately implemented power method (last section of [this](http://www.cs.yale.edu/homes/el327/datamining2013aFiles/07_singular_value_decomposition.pdf), for example) for svd should be fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446612573:47,power,power,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446612573,1,['power'],['power']
Energy Efficiency,"@falexwolf , I used '0.4.2' version, I will update to the latest.; Thanks, scanpy is powerful tool for single cell RNASeq data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754:85,power,powerful,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754,1,['power'],['powerful']
Energy Efficiency,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```; adata[:, 'gene_name'].X; ```. This is easy but not optimal. Any idea?. I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458:30,efficient,efficient,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458,1,['efficient'],['efficient']
Energy Efficiency,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651:174,power,powerful,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651,2,"['power', 'reduce']","['powerful', 'reduced']"
Energy Efficiency,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:495,reduce,reduce,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062,1,['reduce'],['reduce']
Energy Efficiency,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:164,sustainab,sustainable,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,2,"['adapt', 'sustainab']","['adapting', 'sustainable']"
Energy Efficiency,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:160,reduce,reduce,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['reduce'],['reduce']
Energy Efficiency,"@flying-sheep `normalize_total` is newer and more efficient. And we should use it instead of `normalize_per_cell`, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/816#issuecomment-531900088:50,efficient,efficient,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531900088,1,['efficient'],['efficient']
Energy Efficiency,@flying-sheep do you want to close this in favor of your future much more powerful plotting revamp?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681:74,power,powerful,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681,1,['power'],['powerful']
Energy Efficiency,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,adapt,adapt,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951,1,['adapt'],['adapt']
Energy Efficiency,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,adapt,adapted,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250,1,['adapt'],['adapted']
Energy Efficiency,"@gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141:184,reduce,reduce,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141,1,['reduce'],['reduce']
Energy Efficiency,"@ivirshup Looks great! I like the new spatial test image ;) well done!. I just give it a try and didn't find any problem. One little change: can you add to the legend of `na_color` that this is also the color used when the parameter for `color` is not given. . I noticed two parameters in the embedding that I think belong only to the spatial.. Those are `bw` and `alpha_img`. In embeddings they do nothing. . Other issue, that I don't expect to address at the moment, is the increase in parameters because is becoming difficult to go through the list of parameters when browsing through the documentation. To help on this we can start using alphabetical order for all optional parameters. Other suggestion is to add to the documentation in which version a parameter was added. Thus, power users can easily track changes and try new options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918:784,power,power,784,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918,1,['power'],['power']
Energy Efficiency,"@jlause I think I caught the remaining bug for the docs, now they should pass. ; EDIT: they do!. I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:; - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042:563,green,green,563,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042,1,['green'],['green']
Energy Efficiency,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:427,allocate,allocated,427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727,2,"['allocate', 'schedul']","['allocated', 'scheduled']"
Energy Efficiency,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:17,power,powers,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872,2,['power'],"['power', 'powers']"
Energy Efficiency,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495:0,Adapt,Adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495,1,['Adapt'],['Adapted']
Energy Efficiency,"Ah I see. I did produce the results on the same machine with the same package version and number of CPUs. . The clustering seems to be hanged which becomes visible from these plots:; ![image](https://github.com/scverse/scanpy/assets/127406679/5f0513c8-a3b0-48b2-ac5e-03ed3642dfd5); This is the unclustered map, where you can see that the bottom group in cluster 1 (orange) actually is pulled toward the group in cluster 2 (green) when I clustered them initially: . ![clustered_UMAP](https://github.com/scverse/scanpy/assets/127406679/9eca16a4-c8ca-4a66-b567-2beb4bc87ae2); So here you see that part of cluster 1 is actually added to cluster 2 (which also make sense when looking at the expression profiles of those groups).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2020653583:423,green,green,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2020653583,1,['green'],['green']
Energy Efficiency,"Ah sorry, I should have told you that. All of the preprocessing functions can be migrated to only work with AnnData; there is no important setting in which you want to pass an array or a sparse matrix. That's also remniscient from the early days when I thought people might not like to use AnnData. But that's of course stupid, they wouldn't use Scanpy in that case, anyway. I'm merging this for now so that we have something working for 1.4.1, but if you want to simplify and reduce this to AnnData-only, happy to merge a PR on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016:477,reduce,reduce,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016,1,['reduce'],['reduce']
Energy Efficiency,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857:683,adapt,adapt,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857,1,['adapt'],['adapt']
Energy Efficiency,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546:180,reduce,reduce,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546,1,['reduce'],['reduce']
Energy Efficiency,"As a possible alternate/ supplement, what about a discourse forum? I think threaded conversations are useful, especially when you can't expect everyone who has input to be online at the same time. It also looks like google indexes discourse, which could reduce repeated questions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476053717:254,reduce,reduce,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476053717,1,['reduce'],['reduce']
Energy Efficiency,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,adapt,adapting,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001,1,['adapt'],['adapting']
Energy Efficiency,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/658#issuecomment-495248091:28,efficient,efficient,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658#issuecomment-495248091,3,"['efficient', 'reduce']","['efficient', 'reduce']"
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2901""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2962""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2974""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2984""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3120""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3216""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3222""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303,1,['Power'],['Powered']
Energy Efficiency,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639:207,reduce,reduced,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639,1,['reduce'],['reduced']
Energy Efficiency,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529:96,reduce,reduce,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529,1,['reduce'],['reduce']
Energy Efficiency,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:90,efficient,efficient,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,2,"['adapt', 'efficient']","['adapted', 'efficient']"
Energy Efficiency,Green light on my side to merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441549419:0,Green,Green,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441549419,1,['Green'],['Green']
Energy Efficiency,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:110,Power,Powershell,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['Power'],['Powershell']
Energy Efficiency,"Hello @davidhbrann,; Thanks for the response.; I did pip install --user scikit-misc --force in the anaconda powershell, but this bug kept the same. Not solved.; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473:108,power,powershell,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473,1,['power'],['powershell']
Energy Efficiency,"Hey @buesra-oezmen! I don't think this is a UMAP issue, but instead an issue of the MultiVI parameterization or the data. But as I know the data quite well in this case, I'm pretty sure it's maybe just a MultiVI model that is not sufficiently trained. Maybe try for a few more epochs? Or otherwise maybe the network architecture needs to be adapted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471:341,adapt,adapted,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471,1,['adapt'],['adapted']
Energy Efficiency,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467,1,['adapt'],['adapted']
Energy Efficiency,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:245,efficient,efficient,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395,1,['efficient'],['efficient']
Energy Efficiency,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:150,efficient,efficient,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,1,['efficient'],['efficient']
Energy Efficiency,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,adapt,adapt,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,1,['adapt'],['adapt']
Energy Efficiency,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:138,reduce,reduce,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943,1,['reduce'],['reduce']
Energy Efficiency,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:113,efficient,efficient,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764,4,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:411,reduce,reduce,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661,1,['reduce'],['reduce']
Energy Efficiency,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:213,reduce,reduced,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817,3,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:159,reduce,reduce,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221,1,['reduce'],['reduce']
Energy Efficiency,"Hi Dan, about 1.: I’m asking you what the semantic meaning is :smile: . about 2.: there are two functions called `dendrogram`, and they have compatible signatures. Each computed dendrogram can be plotted. So what I’m saying is that the plotting version hasn’t been adapted. Also an important question: in `tl.dendrogram`, we call `_choose_representation`, which will compute a PCA for the .obs axis. When specifying `axis='var'`, should it compute a PCA for the `var` axis instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869:265,adapt,adapted,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869,1,['adapt'],['adapted']
Energy Efficiency,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693:510,reduce,reduce,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693,1,['reduce'],['reduce']
Energy Efficiency,"Hi Joshua,. ok, first of all, you should store the group id as a field in the annotation of observations/cells (`.obs`). Taking your AnnData, you'd do the following:; ```; adata.obs['mygroups'] = [name.split('--')[0] for name in adata.obs_names]; ```; You can then do ; ```; sc.pl.violin(adata, 'mygene', group_by='mygroups'); ```; as in the [standard example](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), box [32]. Instead of 'mygene', you can use any annotation key; for instance gene scores, as produced by `sc.pp.score_genes`. Or averages over genes `adata.obs['my_gene_set_avg'] = adata[:, gene_set].X.mean(axis=1)`. Does this help or did I misunderstand something?. PS: As a principle rule for writing efficient python code: never loop over more than a 100 items... absolutely never use nested loops. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370141359:762,efficient,efficient,762,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370141359,1,['efficient'],['efficient']
Energy Efficiency,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,adapt,adapt,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798,1,['adapt'],['adapt']
Energy Efficiency,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:489,reduce,reduce,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,3,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,adapt,adapt,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,1,['adapt'],['adapt']
Energy Efficiency,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1594388914:52,reduce,reduce,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1594388914,1,['reduce'],['reduce']
Energy Efficiency,"Hm, I adapted your reproducer to use scanpy 1.10.3’s code and it doesn’t seem to be an issue: https://gist.github.com/flying-sheep/b2ae449ab70a9358e07a82f284de5dca#file-score_genes_diagnostics_tests2-ipynb. I’m going to assume that this is fixed in 1.10.3. If you can reproduce it with 1.10.3, we can reopen it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983:6,adapt,adapted,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983,1,['adapt'],['adapted']
Energy Efficiency,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Energy Efficiency,"How can one get a DEG table with a pts column for each cluster? So that for each group there would be 4 columns: 'names', 'logfoldchanges', 'pvals_adj' and 'pts'?. Manual sorting from 2 files is not quite optimal:; ```; sc.tl.rank_genes_groups(adata, 'cell_types', method='wilcoxon', pts=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; degs_by_cluster = pd.DataFrame({group + '_' + key[:14]: result[key][group]; for group in groups for key in ['names', 'logfoldchanges', 'pvals_adj']}); degs_by_cluster.to_csv(""DEG_adata_cell_types_pct_to_sort.csv""); pts=pd.DataFrame(adata.uns['rank_genes_groups']['pts']); pts.to_csv(""pts_adata.csv""); ```. Could you help with a more efficient way to do that? ; @fidelram @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407:765,efficient,efficient,765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407,1,['efficient'],['efficient']
Energy Efficiency,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457056054:23,charge,charge,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457056054,1,['charge'],['charge']
Energy Efficiency,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358:158,power,power,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358,2,['power'],['power']
Energy Efficiency,"I agree, I've been trying to reduce the dataset and save it as 10x_h5 format again but it seems to be more complicated than I thought. I think it would be best to have an 10x_h5 format so to also use it for the read function test. Also, yes will only keep the lowres image",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692:29,reduce,reduce,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692,1,['reduce'],['reduce']
Energy Efficiency,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt); [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt); [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt); [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048:15,reduce,reduced,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048,1,['reduce'],['reduced']
Energy Efficiency,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,adapt,adapted,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688,1,['adapt'],['adapted']
Energy Efficiency,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:557,charge,charge,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['charge'],['charge']
Energy Efficiency,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:483,green,green,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795,2,['green'],['green']
Energy Efficiency,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:241,efficient,efficient,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,1,['efficient'],['efficient']
Energy Efficiency,"I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. If we only switch the green or red color with another, most people won’t even notice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444418923:195,green,green,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444418923,1,['green'],['green']
Energy Efficiency,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-513822396:48,monitor,monitor,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513822396,1,['monitor'],['monitor']
Energy Efficiency,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,adapt,adapted,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069,1,['adapt'],['adapted']
Energy Efficiency,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444805820:62,green,green,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805820,1,['green'],['green']
Energy Efficiency,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:143,efficient,efficient,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['efficient'],['efficient']
Energy Efficiency,I realized that this is an issue originated from coercing the sparse matrix to be of np.int8 type to reduce the size the ann object and the mtx file to be written for another application. Making it np.int32 fixes the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547#issuecomment-1624533422:101,reduce,reduce,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547#issuecomment-1624533422,1,['reduce'],['reduce']
Energy Efficiency,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Energy Efficiency,"I suppose to do this properly one ought to scan the code base for uses of igraph, check which among them require the RNG and then add the seeding to those modules?. Since I'm still very new to scanpy and rather swamped at the moment, I won't have the time to do this anytime soon. Maybe add a help wanted tag? If nobody takes it up by the time my schedule lightens I'll see what I can do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544:347,schedul,schedule,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544,1,['schedul'],['schedule']
Energy Efficiency,"I think that if you store a categorical value in a pandas dataframe (like; .obs) the storage of this redundant information is quite efficient. In a; quick search I found this:; https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>; wrote:. > To clarify a bit... I think it would be good to enable something like:; > sc.pl.umap(adata, color=(uns_dict_key, obs_column)); >; > Where the sc.pl.umap() function then does:; >; > if isinstance(color, tuple):; > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]; > sc.pl.plot_scatter(adata, color=color_vector, ...); >; > It might need to be a pandas dataframe rather than a dictionary with the; > above setup.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/658#issuecomment-495177880:132,efficient,efficient,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658#issuecomment-495177880,1,['efficient'],['efficient']
Energy Efficiency,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:371,efficient,efficient,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255,1,['efficient'],['efficient']
Energy Efficiency,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975:34,efficient,efficient,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975,1,['efficient'],['efficient']
Energy Efficiency,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Energy Efficiency,"I think there's definitely room for more plotting libraries in the ecosystem, but have some doubts about whether all needs can be met by one library. I personally use `seaborn`/ `matplotlib`, `bokeh`, `datashader`, and `altair` for different cases. I also think making a good plotting API is exceedingly difficult, especially if you target both high and low level use cases. I would note that the plotting code in scanpy feels like some of the most maintenance intensive code in the library. > provides helper functions for handling colors, saving figures, etc. We can do a bit more of this here. But of course, much of it would end up being `matplotlib` specific. > encourages a consistent plotting API (e.g. by defining abstract base classes). I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on `seaborn` plotting classes more heavily here, potentially contributing features upstream. Here's one example https://github.com/mwaskom/seaborn/issues/2487 of a feature which could fit the `AnnData` data model nicely. > there is quite some duplicated code in the plotting section. We'd definitely like to reduce the amount of duplicated code, which is what drove the addition of `sc.get`. This seems to be working out internally, if slowly. > All the scanpy helper functions for plotting (e.g. savefig_or_show, _set_color_for_categorical_obs etc.) are private scanpy functions. I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is `_set_color_for_categorical_obs`, which I'd also like to make accessible through `sc.get`. Adding `groupby` support to `anndata` would help a lot here too (https://github.com/theislab/anndata/issues/556). `save_fig_or_show` is something that I don't think we should export, and may need a rework (#1508).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749:1165,reduce,reduce,1165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749,1,['reduce'],['reduce']
Energy Efficiency,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:287,reduce,reduces,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760,1,['reduce'],['reduces']
Energy Efficiency,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r; prettier_colors <- c(; setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),; setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')); ); ```. In python that would be:. ```py; import numpy as np; from matplotlib import cm. prettier_colors = dict(zip(; [; 'turquoise', 'magenta', 'yellow', 'black',; 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',; ], np.concatenate([; np.array(cm.Dark2.colors)[[0,3,5,7]],; np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],; ]); )); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480730857:771,green,green,771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480730857,2,['green'],['green']
Energy Efficiency,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,adapt,adaptation,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214,1,['adapt'],['adaptation']
Energy Efficiency,"I'm confused by the figure above. In Cluster 2 (shown in green below the heatmap) the marker gene that is coming up most prominently is CD3D, which was defined as a marker for Cluster 0, though.; The color code on the right seems correct, but why is CD3D most prominently expressed in Cluster 2 and not in Cluster 0?; The same seems to be the case for the other clusters.; Or were the marker genes perhaps just mislabelled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006:57,green,green,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006,1,['green'],['green']
Energy Efficiency,I'm fine with such rather small changes. This should indeed not bother people. Tell me when you decided on a green that satisfies @ftheis. We don't want to make this change multiple times... :wink:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444729710:109,green,green,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444729710,1,['green'],['green']
Energy Efficiency,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1156,power,powershell,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['power'],['powershell']
Energy Efficiency,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,1,['adapt'],['adapted']
Energy Efficiency,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,adapt,adapting,1718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,1,['adapt'],['adapting']
Energy Efficiency,I'm having the same issue using pp.neighbors for a slightly different clustering purpose. It seems to handle ~200k data points just fine but above ~300k gives `Segmentation fault (core dumped)` no matter how much memory I allocate to the job. ; The quick fix with threads above did not work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1426965264:222,allocate,allocate,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1426965264,1,['allocate'],['allocate']
Energy Efficiency,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Energy Efficiency,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:349,adapt,adapt,349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358,1,['adapt'],['adapt']
Energy Efficiency,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Energy Efficiency,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:145,efficient,efficiently,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['efficient'],['efficiently']
Energy Efficiency,"Indeed, ~3-7x faster for me & of course quite a bit more memory efficient (quickly checked with scalene). Tests keep working (for `seurat_v3`/`seurat_v3_paper` somewhat tight numeric comparison with Seurat results), nice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3017#issuecomment-2122484190:64,efficient,efficient,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3017#issuecomment-2122484190,1,['efficient'],['efficient']
Energy Efficiency,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171:199,schedul,scheduling,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171,1,['schedul'],['scheduling']
Energy Efficiency,"Isaac says that lobpcg seems much less precise and is probably not worth it, so we should probably just adapt the warning to mention the imprecision and call it a day",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465:104,adapt,adapt,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465,1,['adapt'],['adapt']
Energy Efficiency,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:; * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image.; * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425866204:453,reduce,reduce,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425866204,1,['reduce'],['reduce']
Energy Efficiency,"It would also be nice to add some stats for vars. Let's say we have aggregated cells from control and stim samples. ; For example, the percentage of cells expressing gene A in clusterX for every group sample would allow us to filter genes expressed in so few cells but with relatively high counts. It will reduce the false positives in pseudobulk differential gene expression analysis caused by these genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3063#issuecomment-2234253911:306,reduce,reduce,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3063#issuecomment-2234253911,1,['reduce'],['reduce']
Energy Efficiency,"It'll take a little doing, but it's certainly do-able. Something like this should do it:. ```python; import numpy as np; from functools import reduce. def concat(arrays: ""list[np.recarray]""):; names = arrays[0].dtype.names; dtypes = [dict(a.dtype.descr) for a in arrays]; assert all(arrays[0].dtype.names == a.dtype.names for a in arrays[1:]), ""All arrays should have same names""; ; offset = 0; out_dtypes = {}; for k in names:; out_dtype = reduce(np.result_type, (dtype[k] for dtype in dtypes)); out_dtypes[k] = (out_dtype, offset); offset += out_dtype.alignment. out_recarray = np.recarray(sum(map(len, arrays)), dtype=out_dtypes) ; np.concatenate(arrays, out=out_recarray); ; return out_recarray; ```. Maybe the solution should happen upstream though. . Do we concatenate recarrays often?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652:143,reduce,reduce,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652,2,['reduce'],['reduce']
Energy Efficiency,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:626,efficient,efficient,626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['efficient'],['efficient']
Energy Efficiency,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes?; - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code.; - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551#issuecomment-1630946384:390,allocate,allocated,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551#issuecomment-1630946384,3,['allocate'],"['allocate', 'allocated', 'allocates']"
Energy Efficiency,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:117,power,powers,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140,2,['power'],['powers']
Energy Efficiency,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>; <summary>Script </summary>. ```python; import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:272,reduce,reduce,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['reduce'],['reduce']
Energy Efficiency,"Just wanted to mention that this would add much convenience. Since Fidel rewrote the plotting API, I saw that it was the right thing to do to reduce all the code in the almost-identical function headers, but I was missing the autocomplete, which had obviously disappeared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474279197:142,reduce,reduce,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474279197,1,['reduce'],['reduce']
Energy Efficiency,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494:104,efficient,efficient,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494,1,['efficient'],['efficient']
Energy Efficiency,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:474,schedul,scheduled,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,1,['schedul'],['scheduled']
Energy Efficiency,"Most of the time? There is an issue with fairly old CPUs (no AVX2, so like >5 years), but that was the last I saw. My guess is that there are more reproducibility issues on windows than linux, likely because it is tested less. I would like to confirm that it's UMAP and not the PCA though. After that could be worth checking the threading (e.g. reduce to one thread, though I thought UMAP should be as reproducible as possible w.r.t. threading by default).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016712689:345,reduce,reduce,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016712689,1,['reduce'],['reduce']
Energy Efficiency,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:1054,power,powerful,1054,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,1,['power'],['powerful']
Energy Efficiency,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:130,reduce,reduced,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,1,['reduce'],['reduced']
Energy Efficiency,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:106,efficient,efficiently,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466,1,['efficient'],['efficiently']
Energy Efficiency,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:867,reduce,reduce,867,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879,1,['reduce'],['reduce']
Energy Efficiency,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,1,['adapt'],['adapt']
Energy Efficiency,"Not currently, but since the scope of that PR got reduced, it shouldn’t be too much work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2578#issuecomment-1945679038:50,reduce,reduced,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578#issuecomment-1945679038,1,['reduce'],['reduced']
Energy Efficiency,"Not the developer of scanpy. ; The reason for this is that scanpy will try to initialize the palette automatically if you do not provide it. ; Then this palette will be saved in `adata.uns` . When the number of clusters is large enough, scanpy cannot find a suitable palette, so all colors are initialized to grey.; After the number of clusters is reduced, you can just delete `adata.uns['leiden_colors']` to force the scanpy to reinitialize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620:348,reduce,reduced,348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620,1,['reduce'],['reduced']
Energy Efficiency,"OK, done, please check out https://github.com/scverse/scanpy/issues/2828. The sister API `sc.pp.louvain` uses `flavor` for this exact use case. It also says about the `'vtraag'` implementation: “Much more powerful than 'igraph', and the default”. I’m not sure why we consider it that much more “powerful” …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1910351580:205,power,powerful,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1910351580,2,['power'],['powerful']
Energy Efficiency,"Ok, so the first problem is normalization for some reason.; @falexwolf , it seems we should just use new normalize_total when the pull request is accepted, it is more memory efficient.; ![image](https://user-images.githubusercontent.com/3065736/54003760-9afeed80-4153-11e9-9187-8e474a2fa03e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470782494:174,efficient,efficient,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470782494,1,['efficient'],['efficient']
Energy Efficiency,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:7,adapt,adapt,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['adapt'],['adapt']
Energy Efficiency,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:397,monitor,monitor,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,1,['monitor'],['monitor']
Energy Efficiency,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/724#issuecomment-510394228:59,reduce,reduced,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-510394228,1,['reduce'],['reduced']
Energy Efficiency,"So I've been sitting offline with @VolkerBergen trying to get to the bottom of this. It seems that the precision of `adata.X` is reduced after subsetting. This is the case when using either of:. ```; adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; adata_hvg._inplace_subset_var(disp_filter['gene_subset']); ```. Either way `adata[:,disp_filter['gene_subset']].X` gives a higher precision than `adata_hvg.X`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/324#issuecomment-433010405:129,reduce,reduced,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433010405,1,['reduce'],['reduced']
Energy Efficiency,"So what’s the difference between normalize_per_cell and normalize_total?. If they’re replacable, why don’t we just use the more efficient one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/816#issuecomment-531486992:128,efficient,efficient,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531486992,1,['efficient'],['efficient']
Energy Efficiency,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:592,power,power,592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['power'],['power']
Energy Efficiency,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green; - blue and purple; - orange and kakhi. Playing around a bit, it’s easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444803441:478,green,green,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444803441,1,['green'],['green']
Energy Efficiency,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:107,reduce,reduce,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,1,['reduce'],['reduce']
Energy Efficiency,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only; `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`; or one dict for all attributes of each categorical; `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-487178978:981,power,powerful,981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-487178978,1,['power'],['powerful']
Energy Efficiency,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,adapt,adapted,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289,1,['adapt'],['adapted']
Energy Efficiency,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Energy Efficiency,Thank you both. @Zethson I've rebased off master. Please merge when this goes green.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637:78,green,green,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637,1,['green'],['green']
Energy Efficiency,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:259,allocate,allocate,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055,2,"['allocate', 'efficient']","['allocate', 'efficient']"
Energy Efficiency,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:653,power,powerful,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['power'],['powerful']
Energy Efficiency,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:; ```; preprocessing/_combat.py:150: in combat; s_data, design, var_pooled, stand_mean = stand_data(model, data); preprocessing/_combat.py:78: in stand_data; design = design_mat(model, batch_levels); preprocessing/_combat.py:32: in design_mat; model, return_type=""dataframe""); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix; NA_action, return_type); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design; NA_action); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders; formula_like = ModelDesc.from_formula(formula_like); ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula; tree = parse_formula(tree_or_string); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula; _atomic_token_types); ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse; for token in token_source:; ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula; yield _read_python_expr(it, end_tokens); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr; for pytype, token_string, origin in it:; ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next; return six.advance_iterator(self._it); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):; # Since formulas can only contain Python expressions, and Python; # expressions cannot meaningfully c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530:62,adapt,adapting,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530,1,['adapt'],['adapting']
Energy Efficiency,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,adapt,adapt,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444,1,['adapt'],['adapt']
Energy Efficiency,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following?. > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211:358,reduce,reduce,358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211,1,['reduce'],['reduce']
Energy Efficiency,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:51,green,green,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382,1,['green'],['green']
Energy Efficiency,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:261,reduce,reduce,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079:134,efficient,efficient,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Energy Efficiency,"Thanks for the example! Could we maybe expand the documentation, asking the users to think about this step and select an appropriate strategy? Also, what is the most efficient way to aggregate the features with the same names? Would be great to have a function for that in scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363151960:166,efficient,efficient,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363151960,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:127,green,green,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,1,['green'],['green']
Energy Efficiency,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:35,reduce,reduce,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,3,['reduce'],['reduce']
Energy Efficiency,"Thanks for your information. I am surprised that this step is taking too; long as is was supposed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:101,reduce,reduce,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for your thorough response! And I apologize for the late reply, I've been away at a conference. I agree with what you mentioned - in my experience I also don't see a 1,000-10,000 fold change in size factors, even when you're comparing drastically different cells with dramatic differences in RNA content (e.g. regulatory T-cells with large cells like macrophages). In this scenario, however, any sensitive differential testing should be done within similar cell types to account for these massive differences. For differential testing, I agree with you that their suggestion of increasing the pseudo-count reduces power and is not ideal. Your suggestion of checking size-factor distributions is definitely an interesting idea, but beyond the scope of the package I think. However, I do like the idea for further downstream verification of differentially regulated genes, particularly those with non-obvious distribution changes. In general, it seems that the authors of the paper are particularly worried about spurious architecture in the embeddings, but some of these can be tested and discarded with diff expression testing. Of note, I usually also see a count depth effect in embedding as well, even with normalization methods other than CPM normalization. I've never been satisfied with attributing all count-number variation as technical noise, which like you said is definitely incorrect, particularly when comparing conditions that could include drastic changes in transcription. All of this is to say, it seems that effect of log-mean vs mean-log should be manageable for embeddings, and like you mentioned, so much of the downstream processing depends on approximation of normal distributions that it seems impossible to get rid of the log-transformation. Overall, I haven't been able to find a satisfying solution either. In general, I tend to say that any obvious expression differences should be minimally affected by this problem, however, when you're trying to distinguish more c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823:613,reduce,reduces,613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823,2,"['power', 'reduce']","['power', 'reduces']"
Energy Efficiency,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:131,efficient,efficient,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['efficient'],['efficient']
Energy Efficiency,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:109,power,power,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702,3,['power'],['power']
Energy Efficiency,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:48,reduce,reducer,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365,3,['reduce'],"['reduced', 'reducer']"
Energy Efficiency,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:167,efficient,efficiently,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975,4,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,adapt,adapting,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891,1,['adapt'],['adapting']
Energy Efficiency,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:404,sustainab,sustainable,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['sustainab'],['sustainable']
Energy Efficiency,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:25,power,powerful,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['power'],['powerful']
Energy Efficiency,"These t-SNE optimizations are mentioned in the following paper. Adding it here for reference.; https://arxiv.org/abs/2212.11506; Accelerating Barnes-Hut t-SNE Algorithm by Efficient Parallelization on Multi-Core CPUs; N Chaudhary, A Pivovar, P Yakovlev, A Gorshkov… - arXiv preprint arXiv:2212.11506, 2022",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798:172,Efficient,Efficient,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798,1,['Efficient'],['Efficient']
Energy Efficiency,"This field is developing very fast, more and more advanced DE test methos are emerging, it's better to adopt these powerful methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-551411475:115,power,powerful,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-551411475,1,['power'],['powerful']
Energy Efficiency,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:161,power,powerful,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['power'],['powerful']
Energy Efficiency,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780:259,reduce,reduce,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780,1,['reduce'],['reduce']
Energy Efficiency,"We had a journal club about this recently and couldn't really come up with a good solution tbh. Mean-log is so much easier for a lot of applications. Log-transforming data also has a variance-stabilizing effect and it reduces skewness so that the data at least better approximates a normal distribution than before, which many downstream methods assume (although data is often still far from normal). So I don't see how we can forgo log transformation without modeling count data for everything directly. The effect outlined in the paper is the most pronounced for differential expression tests between groups for which size factor distributions differ... So you could check size factor distributions before performing the test to estimate whether the log-mean vs mean-log difference will affect the test. If yes, try without log transforming the data and see if the test can deal with the outliers. Especially for the t-test, the poorer approximation of normality may not have as strong an effect as the log-mean vs mean-log difference. However, in our experience size factors tend to have a range of ~100-fold difference, and not 1,000-10,000 fold as was shown in the paper. We weren't so taken with the suggestion in the paper of increasing the pseudocount as that essentially removes fold-change effects... and also removes zeros (making all matrices dense). As for embeddings... you could remove size factor outliers for the PCA calculation and do it without log-transformation. Although in practice we found it gives very similar results. Thus, our solution was to visualize size factor distributions on embeddings to see whether there is an effect. Usually you do see a count depth effect in the embedding though... and that's not that surprising for CPM normalization, as you assume that all cells are of similar molecule count, which is incorrect. With other normalization methods it shouldn't be as bad. But yeah... overall, it's a complicated problem without a good solution. I imagine it i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918:218,reduce,reduces,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918,1,['reduce'],['reduces']
Energy Efficiency,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:88,reduce,reduced,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['reduce'],['reduced']
Energy Efficiency,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:305,green,green,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487,2,['green'],['green']
Energy Efficiency,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and there’s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095:207,adapt,adapted,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095,1,['adapt'],['adapted']
Energy Efficiency,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306:158,power,power,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306,2,['power'],['power']
Energy Efficiency,"Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1423,reduce,reduce,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['reduce'],['reduce']
Energy Efficiency,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:292,drain,draining,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,1,['drain'],['draining']
Energy Efficiency,Yes I think traversing a minimum spanning tree is good enough since you can never perfectly order cell states in one dimension while capturing all the key features.; ArchR's code to do clustering (addClusters) is here:; https://github.com/GreenleafLab/ArchR/blob/master/R/Clustering.R. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275:239,Green,GreenleafLab,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275,1,['Green'],['GreenleafLab']
Energy Efficiency,Yes I would also be keen to know the most efficient way to aggregate the features,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363174209:42,efficient,efficient,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363174209,1,['efficient'],['efficient']
Energy Efficiency,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,adapt,adapting,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139,1,['adapt'],['adapting']
Energy Efficiency,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:701,efficient,efficient,701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,1,['efficient'],['efficient']
Energy Efficiency,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you 😄). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,adapt,adapt,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468,1,['adapt'],['adapt']
Energy Efficiency,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-433267386:30,reduce,reduce,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-433267386,1,['reduce'],['reduce']
Energy Efficiency,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```; def sparse_pca(X,npcs,mu = None):; # X -- scipy sparse data matrix; # npcs -- number of principal components; # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features; if mu is None: ; mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means; mmat = mdot = mu.dot ; # dot product operator for the transposed means; mhmat = mhdot = mu.T.dot ; # dot product operator for the data; Xmat = Xdot = X.dot ; # dot product operator for the transposed data; XHmat = XHdot = X.T.conj().dot ; # dot product operator for a vector of ones; ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means; def matvec(x): ; return Xdot(x) - mdot(x); def matmat(x): ; return Xmat(x) - mmat(x); def rmatvec(x): ; return XHdot(x) - mhdot(ones(x)); def rmatmat(x): ; return XHmat(x) - mhmat(ones(x)); ; # construct the LinearOperator; XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,; matmat = matmat,; shape = X.shape,; rmatvec = rmatvec, rmatmat = rmatmat); ; u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs); ; # i like my eigenvalues sorted in decreasing order; idx = np.argsort(-s); S = np.diag(s[idx]); # principal components; pcs = u[:,idx].dot(S) ; # equivalent to PCA.components_ in sklearn ; components_ = v[idx,:] ; return pcs,components_; ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727:1634,efficient,efficient,1634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727,1,['efficient'],['efficient']
Energy Efficiency,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:76,Green,GreenleafLab,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348,2,"['Green', 'reduce']","['GreenleafLab', 'reduced']"
Energy Efficiency,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854:242,reduce,reduce,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854,1,['reduce'],['reduce']
Energy Efficiency,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:138,reduce,reduced,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296,1,['reduce'],['reduced']
Energy Efficiency,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754:219,adapt,adapted,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754,1,['adapt'],['adapted']
Energy Efficiency,"`n_genes` was calculated when you filtered cells. I agree that this implicit behavior can be confusing. btw, if your expression is in a sparse matrix, this would be a more efficient way to count the number of expressed genes:. ```python; adata.X.eliminate_zeros() # Removes explicit zeros; n_genes = adata.X.getnnz(axis=1) # Counts explicit values. # or, slightly less efficient but still more efficient than making a dense array; np.ravel((adata.X != 0).sum(axis=1)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/488#issuecomment-464412167:172,efficient,efficient,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/488#issuecomment-464412167,3,['efficient'],['efficient']
Energy Efficiency,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:295,power,powerful,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['power'],['powerful']
Energy Efficiency,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:2009,efficient,efficient,2009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178,1,['efficient'],['efficient']
Energy Efficiency,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3172,consumption,consumption,3172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['consumption'],['consumption']
Energy Efficiency,"check:. <details>; <summary> Code </summary>. ```python; import scanpy as sc; from sparse_wrapper._core.coordinate_ops import difference_indices; import numpy as np; from scipy import sparse. def find_steps_val(adj, n_steps):; """"""Finding positions with value of 1.""""""; cur = adj.astype(int); diffs = [adj.copy()]; for i in range(n_steps):; cur.data[:] = 1; cur = adj @ cur; diffs.append(cur == 1); return diffs. def find_steps(adj, n_steps):; """"""Finding differences in sparsity patterns.""""""; diffs = [adj.copy()]; prev = adj + sparse.eye(adj.shape[0]); for i in range(n_steps):; cur = prev @ adj; diffs.append(difference_indices(cur, prev)); prev = cur; return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""); sc.pp.visium_connectivity(adata); adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1); val_res = find_steps_val(adj, N-1). for i in range(N):; adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int); adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(; adata,; color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],; ncols=N; ); ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>; <summary> </summary>. ```python; from operator import add; from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False); adata.obs[""fireworks""] = (; reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))); .max(axis=0); .toarray(); .ravel(); ); sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614:2001,reduce,reduce,2001,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614,2,['reduce'],['reduce']
Energy Efficiency,"da3/envs/py48/lib/site-packages/umap/layouts.py?line=31) cache=True,; [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=32) locals={; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=33) ""result"": numba.types.float32,; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=34) ""diff"": numba.types.float32,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=35) ""dim"": numba.types.int32,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=36) },; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=37) ); ---> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=40) ; [42](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=41) Parameters; (...); [49](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=48) The squared euclidean distance between x and y; [50](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=49) """"""; [51](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=50) result = 0.0. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\decorators.py:219, in _jit.<locals>.wrapper(func); [217](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=216) with typeinfer.register_dispatcher(disp):; [218](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=217) for s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:12555,Reduce,Reduced,12555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['Reduce'],['Reduced']
Energy Efficiency,"don’t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,adapt,adapting,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361,1,['adapt'],['adapting']
Energy Efficiency,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2827,schedul,scheduler,2827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['schedul'],['scheduler']
Energy Efficiency,"e); ```; I get the following traceback; ```; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>; adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function; ```. When i run the `python-igraph` leiden implementation like so...; ```; obs_name = f""leiden {leiden_suffix}""; g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(; objective_function=""modularity"", ; resolution_parameter=0.1, ; weights = 'weight',; n_iterations=2; ); adata.obs[obs_name] = (; pd.Series(; clustering.membership, ; dtype='category', ; index=adata.obs.index; ); ); ```; I get the following, similar traceback; ```; UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473; running Leiden clustering; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>; obs_name = f""leiden {leiden_suffix}""; File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden; g = _utils.get_igraph_from_adjacency(adjacency, directed=directed); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285:2332,power,power,2332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285,1,['power'],['power']
Energy Efficiency,fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7841,green,greenlet,7841,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['green'],['greenlet']
Energy Efficiency,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:167,green,green,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544,2,"['green', 'reduce']","['green', 'reduces']"
Energy Efficiency,"https://github.com/theislab/anndata/pull/85; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/memory_issue_huge.ipynb. I wasn't right about recursion, almost no effect. `read_direct` is efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600:202,efficient,efficient,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600,1,['efficient'],['efficient']
Energy Efficiency,"import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__; filename=filename, filemode=filemode); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual; X = X.as",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:1150,reduce,reduce,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['reduce'],['reduce']
Energy Efficiency,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:245,Power,Powershell,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692,1,['Power'],['Powershell']
Energy Efficiency,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1365,efficient,efficiently,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['efficient'],['efficiently']
Energy Efficiency,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,adapt,adapt,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['adapt'],['adapt']
Energy Efficiency,"orsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086688-66a3af40-ecc1-4b2e-a3e4-8038f7f207b0.png). ```; #make purple colormap; colors2 = plt.cm.Purples(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap4 = mymap(np.arange(mymap.N)); my_cmap4[:,-1] = np.linspace(0, 1, mymap.N); my_cmap4 = colors.ListedColormap(my_cmap4). sc.pl.umap(adata, color=['VWF'], use_raw=True, color_map=my_cmap4, show=False, frameon=False, vmax=5); ```; ![image](https://user-images.githubusercontent.com/56206488/12608",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:1602,green,green,1602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['green'],['green']
Energy Efficiency,"ot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wondering if there's a good way to show ""colors cannot be trusted in this region"". This could be done like how camera's do zebra stripes – where a texture is overlaid on the viewfinder for the sensor pixels which are saturated with light.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:2190,sensor,sensor,2190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155,1,['sensor'],['sensor']
Energy Efficiency,"p(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3475,Reduce,Reduced,3475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['Reduce'],['Reduced']
Energy Efficiency,"pytables 3.7.0 is incompatible with Windows. Installing pytables 3.6.1 solves the problem.; For the UMAP, it's quite weird. Whether I restart anaconda between installing individual packages plays the trick.; It should be done in this way.; ```python; !pip install tables==3.6.1. !pip install scanpy[leiden]; !pip install -U scvelo; !pip install tqdm; !pip install ipywidgets; !pip install cellrank. close Edge. run anaconda powershell prompt (anaconda3) as admin; conda activate HYJ_py38; conda install -y -c anaconda cytoolz; pip install pyscenic. restart anaconda3. !pip install adjustText openpyxl; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391:424,power,powershell,424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391,1,['power'],['powershell']
Energy Efficiency,"quick practical comment on this very interesting discussion.; @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well.; Sorry to put pressure but we are on a tight schedule 😅 . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491:314,schedul,schedule,314,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491,1,['schedul'],['schedule']
Energy Efficiency,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,adapt,adapted,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['adapt'],['adapted']
Energy Efficiency,"s!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warning to notify the user if the weights were computed in a non-standard way; * There should be a function for computing a perplexity weighted nearest neighbor graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:1324,efficient,efficient,1324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['efficient'],['efficient']
Energy Efficiency,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:34,reduce,reduced,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158,1,['reduce'],['reduced']
Energy Efficiency,"sure! in short: alex said he didn’t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,adapt,adapt,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977,1,['adapt'],['adapt']
Energy Efficiency,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested?. > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:; > ; > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):; > ; > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf; > ; > Normal	Deuteranomaly; > 	; > 	• red and green; > 	• blue and purple; > 	• orange and kakhi; > Playing around a bit, it’s easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf; > ; > Normal	Deuteranomaly; > 	; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210:66,green,green,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210,2,['green'],['green']
Energy Efficiency,"tivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5567,Reduce,Reduced,5567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['Reduce'],['Reduced']
Energy Efficiency,"u keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently bei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1333,power,power,1333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,1,['power'],['power']
Energy Efficiency,"when you use the firefox extension I quoted above, you’ll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444852474:82,green,green,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444852474,1,['green'],['green']
Integrability," Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10416,wrap,wrap,10416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:2219,message,messages,2219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['message'],['messages']
Integrability," a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>; <summary> 🍝 </summary>. ```python; def log1p(data, copy=False, chunked=False, chunk_size=None):; """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters; ----------; data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; copy : `bool`, optional (default: `False`); If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; if copy:; if not isinstance(data, AnnData):; data = data.astype(np.floating); data = data.copy(); elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):; raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):; if issparse(X):; np.log1p(X.data, out=X.data); else:; np.log1p(X, out=X). return X. if isinstance(data, AnnData):; if not np.issubdtype(data.X.dtype, np.floating):; data.X = data.X.astype(np.floating, copy=False); if chunked:; for chunk, start, end in data.chunked_X(chunk_size):; data.X[start:end] = _log1p(chunk); else:; _log1p(data.X); else:; _log1p(data). return data if copy else None; ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239:1225,depend,depending,1225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239,1,['depend'],['depending']
Integrability," about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_visium(path, schema=""v1""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:1076,depend,depend,1076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['depend'],['depend']
Integrability," even when you're comparing drastically different cells with dramatic differences in RNA content (e.g. regulatory T-cells with large cells like macrophages). In this scenario, however, any sensitive differential testing should be done within similar cell types to account for these massive differences. For differential testing, I agree with you that their suggestion of increasing the pseudo-count reduces power and is not ideal. Your suggestion of checking size-factor distributions is definitely an interesting idea, but beyond the scope of the package I think. However, I do like the idea for further downstream verification of differentially regulated genes, particularly those with non-obvious distribution changes. In general, it seems that the authors of the paper are particularly worried about spurious architecture in the embeddings, but some of these can be tested and discarded with diff expression testing. Of note, I usually also see a count depth effect in embedding as well, even with normalization methods other than CPM normalization. I've never been satisfied with attributing all count-number variation as technical noise, which like you said is definitely incorrect, particularly when comparing conditions that could include drastic changes in transcription. All of this is to say, it seems that effect of log-mean vs mean-log should be manageable for embeddings, and like you mentioned, so much of the downstream processing depends on approximation of normal distributions that it seems impossible to get rid of the log-transformation. Overall, I haven't been able to find a satisfying solution either. In general, I tend to say that any obvious expression differences should be minimally affected by this problem, however, when you're trying to distinguish more closely-related subpopulations, mean-log might have an undesirable effect. However, at this stage, no test or transformation is ideal, and we should be validating these subpopulations using several methods anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823:1661,depend,depends,1661,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823,1,['depend'],['depends']
Integrability," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1991,wrap,wrap,1991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['wrap'],['wrap']
Integrability," is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1024,depend,depend,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['depend']
Integrability," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1425,depend,depends,1425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852,1,['depend'],['depends']
Integrability," methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1438,integrat,integration,1438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643,1,['integrat'],['integration']
Integrability," past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspective",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1142,depend,dependents,1142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['dependents']
Integrability," the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3553,Depend,Depending,3553,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['Depend'],['Depending']
Integrability," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1091,wrap,wrap,1091,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021,1,['wrap'],['wrap']
Integrability,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,inject,injecting,2178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['inject'],['injecting']
Integrability,"## Test failures on 141eb6a315542317ddab2f7a413a24559c84492f. 252 failed, 650 passed, 59 skipped, 5 xfailed, 1038 warnings, 128 errors in 451.20s (0:07:31). Noting some of the big causes:. ### Expected warnings not thrown. A few times. ### Older versions of pandas do not support `na_action`. Likely caused during [Fix more pandas warnings by flying-sheep · Pull Request #2789 · scverse/scanpy](https://github.com/scverse/scanpy/pull/2789). Which did also bump up the required pandas version to 2.1.3. However, I think we'll want to revert that bump because:. * According to the scientific python versioning specification we're still meant to be on 1.4 ; * More than half of pandas users are still on 1.*; * Bumping pandas up to 2.1.3 actually requires bumping the versions on a number of other dependencies whose current minimums do not work with pandas 2.1.3. ### ufunc equal. Something is happening in a lot of plotting functions with the `equal` ufunc. ### Numba NotImplementedError. During `test_highly_variable_genes_pearson_residuals_general`. ### AnnData private methods used in tests. A lot of private anndata methods are used at test time. But these didn't exist at the time. Not totally sure what the best solution here is. * Vendoring anndata test helpers over here.; * Literally pulling in the file is probably not so bad; * I will investigate to see how many functions are really needed, possible it's just a few one liners (`as_dense_dask_array` is getting hit often); * Make a new package with just the test helpers? Probably too much of a pain. ### ImportError: cannot import name 'check_is_fitted' from 'sklearn.base'. <details>; <summary> Raw test output </summary>. ```python; FAILED scanpy/tests/test_datasets.py::test_krumsiek11 - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.; FAILED scanpy/tests/test_datasets.py::test_toggleswitch - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.; FAILED scanpy/get/get.py::",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:795,depend,dependencies,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['depend'],['dependencies']
Integrability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:840,interface,interface,840,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,2,"['integrat', 'interface']","['integrating', 'interface']"
Integrability,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:354,wrap,wrapping,354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['wrap'],['wrapping']
Integrability,"*kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1367,wrap,wrapper,1367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['wrap'],['wrapper']
Integrability,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1339,message,message,1339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['message'],['message']
Integrability,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7900,depend,depending,7900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['depend'],['depending']
Integrability,".); 768 **kwds,; 769 ):; 770 """"""\; 771 Plot ranking of genes using dotplot plot (see :func:`~scanpy.pl.dotplot`); 772 ; (...); 872 tl.rank_genes_groups; 873 """"""; --> 874 return _rank_genes_groups_plot(; 875 adata,; 876 plot_type='dotplot',; 877 groups=groups,; 878 n_genes=n_genes,; 879 groupby=groupby,; 880 values_to_plot=values_to_plot,; 881 var_names=var_names,; 882 gene_symbols=gene_symbols,; 883 key=key,; 884 min_logfoldchange=min_logfoldchange,; 885 show=show,; 886 save=save,; 887 return_fig=return_fig,; 888 **kwds,; 889 ). File [/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py:531](/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py#line=530), in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, var_names, min_logfoldchange, key, show, save, return_fig, gene_symbols, **kwds); 529 values_df = None; 530 if values_to_plot is not None:; --> 531 values_df = _get_values_to_plot(; 532 adata,; 533 values_to_plot,; 534 var_names_list,; 535 key=key,; 536 gene_symbols=gene_symbols,; 537 ); 538 title = values_to_plot; 539 if values_to_plot == 'logfoldchanges':. File [/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py:1636](/mnt/data1/liz/software/miniconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_tools/__init__.py#line=1635), in _get_values_to_plot(adata, values_to_plot, gene_names, groups, key, gene_symbols); 1629 message = (; 1630 ""Please run `sc.tl.rank_genes_groups` with ""; 1631 ""'n_genes=adata.shape[1]' to save all gene ""; 1632 f""scores. Currently, only {df.shape[0]} ""; 1633 ""are found""; 1634 ); 1635 logg.error(message); -> 1636 raise ValueError(message); 1637 df['group'] = group; 1638 df_list.append(df). ValueError: Please run `sc.tl.rank_genes_groups` with 'n_genes=adata.shape[1]' to save all gene scores. Currently, only 2238 are found; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3049#issuecomment-2107618181:2484,message,message,2484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3049#issuecomment-2107618181,3,['message'],['message']
Integrability,.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.17.0 py_0 conda-forge; jpeg 9d h0b31af3_0 conda-forge; jsonschema 3.2.0 py38h32f6830_1 conda-forge; jupyter_client 6.1.7 py_0 conda-forge; jupyter_core 4.6.3 py38h32f6830_2 conda-forge; jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge; keyring 21.4.0 py38h32f6830_2 conda-forge; kiwisolver 1.3.0 py38h02bb52f_0 conda-forge; krb5 1.17.1 h75d18d8_3 conda-forge; lazy-object-proxy 1.4.3 py38h64e0658_2 conda-forge; lcms2 2.11 h174193d_0 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; libblas 3.9.0 2_openblas conda-forge; libcblas 3.9.0 2_openblas conda-forge; libclang 10.0.1 default_hf57f61e_1 conda-forge; libcurl 7.71.1 h9bf37e3_8 conda-forge; libcxx 11.0.0 h439d374_0 conda-forge; libedit 3.1.20191231 hed1e85f_2 conda-forge; libev 4.33 haf1e3a3_1 conda-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:3095,wrap,wrap,3095,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['wrap'],['wrap']
Integrability,".read(self.source, self.parser,; File ""/usr/local/lib/python3.8/site-packages/sphinx/io.py"", line 108, in read; self.parse(); File ""/usr/local/lib/python3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:6897,message,messages,6897,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"0.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; easydev 0.10.1; get_version 2.1; gseapy 0.10.1; h5py 2.10.0; idna 2.10; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.15.2; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; lxml 4.5.2; markupsafe 1.1.1; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 169",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1464,wrap,wrapt,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['wrap'],['wrapt']
Integrability,1 ; jeepney 0.6.0 pyhd3eb1b0_0 ; jinja2 3.0.0 pyhd3eb1b0_0 ; joblib 1.0.1 pyhd3eb1b0_0 ; jpeg 9b h024ee3a_2 ; json5 0.9.5 py_0 ; jsonschema 3.2.0 py_2 ; jupyter 1.0.0 py38_7 ; jupyter-packaging 0.7.12 pyhd3eb1b0_0 ; jupyter_client 6.1.12 pyhd3eb1b0_0 ; jupyter_console 6.4.0 pyhd3eb1b0_0 ; jupyter_contrib_core 0.3.3 py_2 conda-forge; jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge; jupyter_core 4.7.1 py38h06a4308_0 ; jupyter_highlight_selected_word 0.2.0 py38h578d9bd_1002 conda-forge; jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge; jupyter_nbextensions_configurator 0.4.1 py38h578d9bd_2 conda-forge; jupyter_server 1.4.1 py38h06a4308_0 ; jupyterlab 3.0.14 pyhd3eb1b0_1 ; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.4.0 pyhd3eb1b0_0 ; jupyterlab_widgets 1.0.0 pyhd3eb1b0_1 ; keyring 23.0.1 py38h06a4308_0 ; kiwisolver 1.3.1 py38h2531618_0 ; krb5 1.17.1 h173b8e3_0 ; lazy-object-proxy 1.6.0 py38h27cfd23_0 ; lcms2 2.12 h3be6417_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 py_0 conda-forge; leidenalg 0.8.2 py38habedc41_0 conda-forge; libarchive 3.4.2 h62408e4_0 ; libcurl 7.69.1 h20c2e04_0 ; libedit 3.1.20210216 h27cfd23_1 ; libev 4.33 h7b6447c_0 ; libffi 3.2.1 hf484d3e_1007 ; libgcc-ng 9.3.0 h2828fa1_19 conda-forge; libgfortran-ng 7.3.0 hdf63c60_0 ; libgomp 9.3.0 h2828fa1_19 conda-forge; libiconv 1.15 h63c8f33_5 ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-i,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:9905,wrap,wrap,9905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['wrap'],['wrap']
Integrability,"1. You could create a heuristic grid size depending on cell numbers, or it's probably easier to just put grid dimensions as a user parameter with some (low) default value.; 2. I've been approaching this from the perspective that you care about where the densities occur on the visualization. That's why you can change the `basis` for the calculations and plotting. From my perspective, calculating densities over clusters and comparing these is actually just a sub-optimal replacement for testing for differential compositions. This is a separate problem, where the data should be modeled statistically, accounting for the compositional nature of the data. So sticking to the visualization is probably the right way forward for this function. On that note... we could use a seaborn heatmap function to plot the differential grid points. Overall I reckon we are moving toward a new plotting function here which does some calculations on the backend. Something like `sc.pl.embedding_density_diff()` where you take the output from `sc.tl.embedding_density()` and interpolate to a grid layout, rescale to sum to 1 across each grid separately, take the diff of two conditions, and then plot everything in a heatmap. Doesn't seem as difficult as I thought. I will get onto this when (read: if) I (ever) have some spare time 😉.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478914589:42,depend,depending,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478914589,1,['depend'],['depending']
Integrability,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508:33,depend,dependencies,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508,1,['depend'],['dependencies']
Integrability,"3 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2345 self.sca(current_ax); 2346 self.stale = True. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1731, in colorbar_factory(cax, mappable, **kwargs); 1729 cb = ColorbarPatch(cax, mappable, **kwargs); 1730 else:; -> 1731 cb = Colorbar(cax, mappable, **kwargs); 1733 cid = mappable.callbacksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defuse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3121,wrap,wrapper,3121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,2,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"39 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2889,wrap,wrapper,2889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4721,depend,dependency,4721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['depend'],['dependency']
Integrability,"8 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:2464,wrap,wrap,2464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967,2,['wrap'],['wrap']
Integrability,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:168,depend,dependencies,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191,1,['depend'],['dependencies']
Integrability,:smile: Now this raises a proper error message: https://github.com/theislab/scanpy/commit/2490bec27c1c37e1388cb1da44369c81e176df6c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782:39,message,message,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366287782,1,['message'],['message']
Integrability,"; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:1107,wrap,wraps,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['wrap'],['wraps']
Integrability,; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:1747,wrap,wrap,1747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['wrap'],['wrap']
Integrability,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8765,wrap,wrapt,8765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['wrap'],['wrapt']
Integrability,<details>; <summary>Installed scanpy on jupyter notebook/ anaconda: </summary>. ```; pip install scanpy. Requirement already satisfied: scanpy in c:\users\charles\anaconda3\lib\site-packages (1.7.2); Requirement already satisfied: numba>=0.41.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.44.1); Requirement already satisfied: tables in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.7.0); Requirement already satisfied: anndata>=0.7.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.7.6); Requirement already satisfied: legacy-api-wrap in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.2); Requirement already satisfied: packaging in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (21.3); Requirement already satisfied: pandas>=0.21 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.3.4); Requirement already satisfied: scipy>=1.4 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.7.3); Requirement already satisfied: umap-learn>=0.3.10 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.1); Requirement already satisfied: h5py>=2.10.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.10.0); Requirement already satisfied: scikit-learn>=0.21.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.0.2); Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.13.0); Requirement already satisfied: matplotlib>=3.1.2 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (3.5.1); Requirement already satisfied: numpy>=1.17.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\an,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:585,wrap,wrap,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['wrap'],['wrap']
Integrability,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:570,wrap,wrap,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,1,['wrap'],['wrap']
Integrability,"<summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/seri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1055,message,messages,1055,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"> 2. Have the info in notebook. I think I'd be happy to recommend calling `session_info` directly for this. IIRC, we have these functions at all because a package which did a good job of displaying the imported packages and dependencies didn't really exist. . > and leave print_versions unchanged?. With the update to use `session_info`?. I'd even be fine to deprecate the `file` argument, since it's not super useful. Plus `session_info` provides this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2089#issuecomment-998063447:224,depend,dependencies,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2089#issuecomment-998063447,1,['depend'],['dependencies']
Integrability,"> > > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > > ; > > > > ; > > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > > ; > > > ; > > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > > ; > > ; > > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it; > ; > do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it. collaborators that generated visium data sent me jpg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032:710,depend,depending,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032,1,['depend'],['depending']
Integrability,"> > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > ; > > > ; > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > ; > > ; > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > ; > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it. do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503:686,depend,depending,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503,1,['depend'],['depending']
Integrability,"> > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > ; > > ; > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > ; > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?. good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814:662,depend,depending,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814,1,['depend'],['depending']
Integrability,"> > @VladimirShitov can you give an example of how you use the gen_mpl_labels function? I tried it and got somewhat different results. For example it lacked the lines pointing to the cluster centers. Thanks!; > ; > Have you solved this problem? I still can't show the lines pointing to the cluster centers. Hi @nnnanchen. I apologize, I forgot to add the `adjust_text` call in the very last line of the `gen_mpl_labels` above. I edited the previous message. Can you try again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372:449,message,message,449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372,1,['message'],['message']
Integrability,"> > If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers); >; > We do make heavy use of optional dependencies, so this might be the way to go regardless. Just saw there's already a pr for this! . * BioContainers/multi-package-containers#2209",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160565536:220,depend,dependencies,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160565536,1,['depend'],['dependencies']
Integrability,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:17,wrap,wrapper,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682,3,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"> > ```python; > > scipy.io.mmwrite; > > ```; > ; > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083:225,wrap,wrapper,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1520696083,1,['wrap'],['wrapper']
Integrability,"> @Brycealong could you also try this in a new isolated environment, please? There might be some dependency that's interfering. Would be glad to know which one, but it's tricky... Ofc. I can run the code on google colab and i'm stick to that. I think there's something interferring the process in my own computer...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1298073650:97,depend,dependency,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1298073650,1,['depend'],['dependency']
Integrability,"> @Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`.; > ; > I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.); > ; > I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly. Thank you for taking the time to dig into this! Much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549:345,depend,dependency,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549,1,['depend'],['dependency']
Integrability,"> @Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet. Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?. No, I'm afraid that these are hard criteria. However, I can highly recommend that you support AnnData first class :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656332039:123,depend,depend,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656332039,1,['depend'],['depend']
Integrability,"> A downside of this is it takes a really long time to compile on first run, which might be off-putting. Right, numba only compiles stuff when first run (because otherwise it can’t know the types) so this doesn’t slow down importing scanpy. > So... probably worth it?. We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. If we do this in a generic way we could even defer importing numpy, saving on import duration (although there probably isn’t much functionality without running numpy-driven functions)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279:278,wrap,wrap,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279,1,['wrap'],['wrap']
Integrability,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well?. I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286:658,depend,dependent,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286,1,['depend'],['dependent']
Integrability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:403,interface,interface,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,2,['interface'],['interface']
