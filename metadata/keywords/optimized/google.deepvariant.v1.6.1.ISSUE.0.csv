quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability," ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Sun Jan 30 20:56:13 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Case1_father	Case1_mother	Case1_proband; X	48684399	X_48684399_C_A	C	A	45	.	AF=0.333333;AQ=45	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:45:45,0,54:..	0/1:18:0,18:4:33,0,1:..; ```. Here the male proband, fully hemizygous for the variant, is represented as 0/1: 0/1:18:0,18:4:33,0,1:.. # Why it matters; Now, this is a **BIG** problem, because downstream tools like Exomiser will treat the output here for variant prioritization in rare disease cases. So take a look at this. Exomiser HTML for DeepVariant, where variant is represented as 1/1. This gene is the top ranked hit for this simulated rare disease case. Good:. ![image](https://user-images.githubusercontent.com/16579982/154753891-458869c2-741b-4728-a74f-f2d58ca7f816.png). Exomiser HTML output for DeepTrio, where variant is represented as 0/1. Notice that now the gene is not even in the top 10 candidates, as Exomiser has interpreted this variant incorrectly (likely assuming it was a PAR region on X). The variant score dropped to 0.00 because the assumption on the GT string was violated:. ![image](https://user-images.githubusercontent.com/16579982/154754163-4833835e-905e-450e-b081-a447340d034c.png). I can send the HTML outputs, VCFs, or even CRAMs if you are interested by email or other file transfer. Anyways, going to try with 1.3 while I wait for an answer here. Cheers,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:11830,down,downstream,11830,,https://github.com/google/deepvariant/issues/518,1,['down'],['downstream']
Availability," ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; W",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:2783,Error,Error,2783,,https://github.com/google/deepvariant/issues/60,1,['Error'],['Error']
Availability," # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(; I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started.; I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True; I0327 22",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:4633,checkpoint,checkpoints,4633,,https://github.com/google/deepvariant/issues/797,1,['checkpoint'],['checkpoints']
Availability," (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:6349,error,error,6349,,https://github.com/google/deepvariant/issues/171,2,['error'],['error']
Availability," ++ PYTHON_BIN_PATH=/home/huangl/publib/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:2837,error,error,2837,,https://github.com/google/deepvariant/issues/6,1,['error'],['error']
Availability," ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:2929,error,error,2929,,https://github.com/google/deepvariant/issues/6,1,['error'],['error']
Availability," --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1847,checkpoint,checkpoint,1847,,https://github.com/google/deepvariant/issues/857,2,['checkpoint'],['checkpoint']
Availability," / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isn’t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7017,error,error,7017,,https://github.com/google/deepvariant/issues/171,2,['error'],['error']
Availability," /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.4; - DeepVariant version: pepper_deepvariant:r0.8-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command: ; ```; 	docker run --ipc=host \; 	--gpus all \; 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; 	-v ""${BASE}"":""${BASE}"" \; 	-v ""${REF}"":""${REF}"" \; 	-v ""${BAMPATH}"":""${BAMPATH}"" \; 	kishwars/pepper_deepvariant:r0.8-gpu \; 	run_pepper_margin_deepvariant call_variant \; 	-o ""${OUTPUT_DIR}"" \; 	-b ""${BAM}"" \; 	-f ""${REF}"" \; 	-p ""${OUTPUT_PREFIX}"" \; 	-t ${THREADS} \; 	-g \; 	--ont_r9_guppy5_sup; ```. - Error trace: (if applicable); ; [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:2663,Error,Error,2663,,https://github.com/google/deepvariant/issues/539,1,['Error'],['Error']
Availability," 1.6.0; - Installation method (Docker, built from source, etc.): apptainer pull docker://google/deepvariant:""1.6.1""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) bam files, ONT. **Steps to reproduce:**; - Command: ; apptainer exec --bind /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint /work/XXXX/ls6/deepvariant/deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant --model_type ONT_R104 --ref /work/XXXX/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 152, in <m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:1245,Error,Error,1245,,https://github.com/google/deepvariant/issues/854,1,['Error'],['Error']
Availability," 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |; | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:6841,down,download,6841,,https://github.com/google/deepvariant/issues/689,1,['down'],['download']
Availability," 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; range_tracker.sub_range_tracker(source_ix)):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 175, in read_records; record = _TFRecordUtil.read_record(file_handle); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 131, in read_record; buf = file_handle.read(buf_length_expected); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 240, in read; self._fetch_to_internal_buffer(num_bytes); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 197, in _fetch_to_internal_buffer; decompressed = self._decompressor.decompress(buf); MemoryError. ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:3297,ERROR,ERROR,3297,,https://github.com/google/deepvariant/issues/91,2,['ERROR'],['ERROR']
Availability," 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; range_tracker.sub_range_tracker(source_ix)):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 175, in read_records; record = _TFRecordUtil.read_record(file_handle); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 131, in read_record; buf = file_handle.read(buf_length_expected); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 240, in read; self._fetch_to_internal_buffer(num_bytes); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 199, in _fetch_to_internal_buffer; self._read_buffer.write(decompressed); MemoryError: out of memory. ERROR:root:Giving up after 4 attempts.; WARNING:root:A task failed with exception: out of memory; Traceback (most recent call last):; File ""./shuffle_tfrecords_beam.py"", line 229, in <module>; main(); File ""./shuffle_tfrecords_beam.py"", line 224, in main; known_args.output_dataset_config_pbtxt); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/pipeline.py"", line 410, in __exit__; self.run().wait_until_finish(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py"", line 421, in wait_until_finish; self._executor.await_completion(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 398, in await_completion; self._executor.await_completion(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 444, in await_completion; six.reraise(t, v, tb); File ""/home/suanfa/virtualenv_beam/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:8643,ERROR,ERROR,8643,,https://github.com/google/deepvariant/issues/91,1,['ERROR'],['ERROR']
Availability," 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:7645,Error,Error,7645,,https://github.com/google/deepvariant/issues/689,1,['Error'],['Error']
Availability," 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz; gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json; gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz; gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json; gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz; gvcf.tfrecord-00005-of-00016.gz make_examples.tfrecord-00002-of-00016.gz.example_info.json make_examples.tfrecord-00010-of-00016.gz.example_info.json; gvcf.tfrecord-00006-of-00016.gz make_examples.tfrecord-00003-of-00016.gz make_examples.tfrecord-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:3654,error,error,3654,,https://github.com/google/deepvariant/issues/733,1,['error'],['error']
Availability," ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PH",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:5650,echo,echo,5650,,https://github.com/google/deepvariant/issues/491,1,['echo'],['echo']
Availability," File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; root@a8d04f73bc21:/opt/deepvariant/bin# exit. i also test version 1.0.0, it also return same exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:9270,error,error,9270,,https://github.com/google/deepvariant/issues/455,2,['error'],['error']
Availability," INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); INFO:tensorflow:Calling model_fn.; I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was finalized.; I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt; INFO:tensorflow:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:9890,Avail,Available,9890,,https://github.com/google/deepvariant/issues/537,1,['Avail'],['Available']
Availability," LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; __________________________________________________________________________________________________; Layer (type) Output Shape Param # Connected to ; ==================================================================================================; input_1 (InputLayer) [(None, 100, 221, 7 0 [] ; )] ; ...; classification (Dense) (None, 3) 6147 ['dropout[0][0]'] ; ; ============================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14481,error,error,14481,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability," TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:7305,checkpoint,checkpoint,7305,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoint']
Availability," This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:31:59.620996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:31:59.623967 140288433825600 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpd74of138; I0217 23:31:59.629002 140288433825600 run_deepvariant.py:551] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load input/weights-51-0.995354.ckpt* instead. ***** Intermediate results will be written to /tmp/tmpd74of138 in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_ADDRESS = ""en_US.UTF-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:4033,error,error,4033,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability," \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2168,down,downstream,2168,,https://github.com/google/deepvariant/issues/833,1,['down'],['downstream']
Availability," ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1943,checkpoint,checkpoint,1943,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability," anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1730,mainten,maintenance,1730,,https://github.com/google/deepvariant/issues/733,1,['mainten'],['maintenance']
Availability," behavior, where the program hangs on a sharded BAM that holds only alt-contig mapping reads. **Setup**; - Operating system: on GCE via Google Life Sciences API (through Cromwell); - DeepVariant version: v1.6; - Installation method (Docker, built from source, etc.): official v1.6 docker; - Type of data: Both PacBio HiFi and ONT (10.4), on GRCh38. . **Steps to reproduce:**. - Command. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --haploid_contigs chrX,chrY \; --par_regions_bed GRCh38.PAR.bed \; --reads=/cromwell_root/<sample_id>.alts.bam \; --output_vcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.vcf.gz \; --output_gvcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.g.vcf.gz \; --num_shards=16; ```. - Relevant log ; (note it says ""0 examples"", so I suspect it is when no examples are available, not just when there's only alt-mapping reads, e.g. if one simulates reads error-free from the reference itself, it probably will have the same issue). ```; /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00011-of-00016.gz.example_info.json; I0203 17:23:03.253894 135328978921280 make_examples_core.py:2958] example_shape = None; I0203 17:23:03.254237 135328978921280 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:03.255900 135328978921280 make_examples_core.py:301] Task 11/16: Found 0 candidate variants; I0203 17:23:03.256017 135328978921280 make_examples_core.py:301] Task 11/16: Created 0 examples; I0203 17:23:04.930985 137565708298048 make_examples_core.py:301] Task 7/16: Writing example info to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00007-of-00016.gz.example_info.json; I0203 17:23:04.931358 137565708298048 make_examples_core.py:2958] example_shape = None; I0203 17:23:04.931699 137565708298048 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:04.933463 13756570829804",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:1047,error,error-free,1047,,https://github.com/google/deepvariant/issues/769,1,['error'],['error-free']
Availability," being called on the T2T reference fasta. **Steps to reproduce:**; /opt/deepvariant/bin/run_deepvariant \; --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \; --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \; --customized_model=model.ckpt-364300 \; --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \; --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \; --model_type WGS \; --make_examples_extra_args phase_reads=true,channels=blank \; --regions CHM13v2.chrY \; --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca""; I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/563:1395,checkpoint,checkpoint,1395,,https://github.com/google/deepvariant/issues/563,1,['checkpoint'],['checkpoint']
Availability," container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 6: bazel-out/host/bin/external/clif/proto: from: not found; bazel-out/host/bin/external/clif/proto: 9: bazel-out/host/bin/external/clif/proto: Syntax error: ""("" unexpected (expecting ""then""). It would be great i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:1275,error,error,1275,,https://github.com/google/deepvariant/issues/122,1,['error'],['error']
Availability," deepvariant RNA model on HG005 data locally (not restricted to chr20 like the Github example) but the model keeps generating this error while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1060,checkpoint,checkpoint,1060,,https://github.com/google/deepvariant/issues/845,2,['checkpoint'],['checkpoint']
Availability," different type of issue. I also want to train DeepVariant model from the WGS data in our laboratory using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-000",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1093,error,errors,1093,,https://github.com/google/deepvariant/issues/133,1,['error'],['errors']
Availability," have very low or zero DP:; e.g. ; `; 1	1319056	1_1319056_A_G	A	G	51	.	AF=0.32848;AQ=51	GT:DP:AD:GQ:PL:RNC	0/0:0:0,0:1:0,0,0:..	1/1:2:0,2:3:29,6,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/1:2:0,2:1:12,2,0:..	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vcf header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GLnexusVersion=v1.4.1-0-g68e25e5; ##GLnexusConfigName=DeepVariant; ##GLnexusConfigCRC32C=2932316105; ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, snv_prior_calibration: 0.600000",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:1527,down,downstream,1527,,https://github.com/google/deepvariant/issues/633,1,['down'],['downstream']
Availability," hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:8076,Error,Error,8076,,https://github.com/google/deepvariant/issues/859,1,['Error'],['Error']
Availability," in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel; return hook(metadata_directory, config_settings); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel; self.run_setup(); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 248, in run_setup; super(_BuildMetaLegacyBackend,; File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 142, in run_setup; exec(compile(code, __file__, 'exec'), locals()); File ""setup.py"", line 499, in <module>; setup_package(); File ""setup.py"", line 479, in setup_package; generate_cython(); File ""setup.py"", line 274, in generate_cython; raise RuntimeError(""Running cythonize failed!""); RuntimeError: Running cythonize failed!; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; /*********************************************/; My conda environment contains the following libraries:; conda list; # packages in environment at /opt/miniconda3/envs/deepvariant:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:11232,error,error,11232,,https://github.com/google/deepvariant/issues/859,1,['error'],['error']
Availability," in prepare_metadata_for_build_wheel; return hook(metadata_directory, config_settings); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel; self.run_setup(); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 248, in run_setup; super(_BuildMetaLegacyBackend,; File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 142, in run_setup; exec(compile(code, __file__, 'exec'), locals()); File ""setup.py"", line 499, in <module>; setup_package(); File ""setup.py"", line 479, in setup_package; generate_cython(); File ""setup.py"", line 274, in generate_cython; raise RuntimeError(""Running cythonize failed!""); RuntimeError: Running cythonize failed!; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; /*********************************************/; My conda environment contains the following libraries:; conda list; # packages in environment at /opt/miniconda3/envs/deepvariant:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:11356,error,error,11356,,https://github.com/google/deepvariant/issues/859,1,['error'],['error']
Availability," libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[=============================================================================================================================================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2333,error,error,2333,,https://github.com/google/deepvariant/issues/489,2,['error'],['error']
Availability," model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1566,down,downsampling,1566,,https://github.com/google/deepvariant/issues/876,1,['down'],['downsampling']
Availability," of DeepVariant from docker I get. ```; I0618 16:51:26.002005 140449503209216 make_examples.py:1116] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of-00048.gz; I0618 16:51:26.059495 140449503209216 make_examples.py:1120] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00000-of-00048.gz; 2019-06-18 16:51:26.064354: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-06-18 16:51:26.183271: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0618 16:51:26.184211 140449503209216 genomics_reader.py:218] Reading /input/out.bam with NativeSamReader; I0618 16:51:26.728389 140449503209216 make_examples.py:1149] Task 0: 0 candidates (0 examples) [0.67s elapsed]; I0618 16:51:53.064081 140449503209216 make_examples.py:1149] Task 0: 119 candidates (119 examples) [26.34s elapsed]; I0618 16:52:15.696561 140449503209216 make_examples.py:1149] Task 0: 230 candidates (230 examples) [22.63s elapsed]; I0618 16:52:28.003529 140449503209216 make_examples.py:1149] Task 0: 346 candidates (346 examples) [12.31s elapsed]; I0618 16:52:33.347760 140449503209216 make_examples.py:1149] Task 0: 438 candidates (438 examples) [5.34s elapsed]; I0618 16:52:39.804543 140449503209216 make_examples.py:1149] Task 0: 501 candidates (501 examples) [6.46s elapsed]; I0618 16:53:01.098323 140449503209216 make_examples.py:1149] Task 0: 606 candidates (606 examples) [21.29s elapsed]. ```; It seems to be running anyway, is it a benign error? ; Here is the command I used. ```; sudo docker run \ ; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/Avaga.Masurca.Graal.l5_n100_c1_N5_unpolished.fasta.sorted.min1000.renamed \; --reads=/input/out.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=48; ```; Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/190:1570,error,error,1570,,https://github.com/google/deepvariant/issues/190,1,['error'],['error']
Availability," paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2834,Error,Error,2834,,https://github.com/google/deepvariant/issues/666,1,['Error'],['Error']
Availability," single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/intel/openvino/`, while there is nothing like that in the deeptrio image. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416:1868,error,error,1868,,https://github.com/google/deepvariant/issues/416,1,['error'],['error']
Availability," the docker image of `deepvariant` but running into problems. I'm on `r1.7` branch and my branch is up to date. I am also facing the same issues on `r1.6.1` branch as well. The command I am using is the following -. ```; docker build --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 \; --build-arg=DV_GPU_BUILD=1 \; -t deepvariant_gpu .; ```. I am building this docker image on my laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invali",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1021,error,error,1021,,https://github.com/google/deepvariant/issues/902,2,['error'],['error']
Availability," very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>; from apache_beam import coders; File ""/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:2022,error,error,2022,,https://github.com/google/deepvariant/issues/793,1,['error'],['error']
Availability," was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8442,ERROR,ERROR,8442,,https://github.com/google/deepvariant/issues/608,1,['ERROR'],['ERROR']
Availability," without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isn’t absol",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:6842,Error,Error,6842,,https://github.com/google/deepvariant/issues/171,2,"['Error', 'error']","['Error', 'error']"
Availability,"""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.vcf.gz"",; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; input:; reference_fasta = ""/project/databases/bacteroides_genome/reference_genomic.fna"",; reads = rules.sam2bam.output.sorted_bam; params:; inter_dir = ""../../results/deepVariant/{dataset}/{sample}/intermediate"",; log_dir = ""../../results/deepVariant/{dataset}/{sample}/log"",; work_dir = ""/project/"",; deepvariant = ""/project/software/deepVariant.sif""; shell:; """"""; module load singularity/3.7.0; singularity exec -B {params.work_dir} {params.deepvariant} /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref={input.reference_fasta} \; --reads={input.reads} \; --output_vcf={output.vcf} \; --output_gvcf={output.vcf} \; --make_examples_extra_args --channels=insert_size \; --intermediate_results_dir {params.inter_dir} \; --num_shards=6 \; --logging_dir={params.log_dir}; """"""; - Error trace: ; ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --outfile_base ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant"". I0626 19:01:30.369722 139699125458752 genomics_reader.py:222] Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_report.py"", line 103, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:1391,Error,Error,1391,,https://github.com/google/deepvariant/issues/839,1,['Error'],['Error']
Availability,"""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:2622,error,error,2622,,https://github.com/google/deepvariant/issues/610,4,"['avail', 'error']","['available', 'error']"
Availability,"""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb; ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?. Thanks!; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:2465,error,error,2465,,https://github.com/google/deepvariant/issues/514,2,['error'],['error']
Availability,"# OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:1199,error,error,1199,,https://github.com/google/deepvariant/issues/345,1,['error'],['error']
Availability,"### Issue; When running the build-prereq shell script, I'm getting an error when the Tensorflow install begins. ### Error message; ```; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; - [1 files][ 41.1 MiB/ 41.1 MiB] 1.0 MiB/s ; Operation completed over 1 objects/41.1 MiB. ; tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.; ```. ### Debugging efforts; After browsing around a bit, I discovered that this issue was solved for some through installing the .whl separately. So, I download the whl from the [GCloud bucket](https://console.cloud.google.com/storage/browser/deepvariant/packages/tensorflow/) and executed `sudo python2.7 pip install <name of .whl file>` through the terminal. It ran, just to tell me “.dist-info directory not found”. I think this might be due to some inconsistency in the packages installed through the build-prereq.sh script, because I can see that all the packages that it installed (e.g. numpy) are for Python 3.5, but the Tensorflow version it's trying to get is for cp27 (Python 2.7). Not sure about where to go from here, would love some assistance :). ### System details; OS: Ubuntu 16.04 LTS; Python interpreters: Default with Ubuntu (2.7 and 3.5.2); Deep Variant version: Installed it today from the main repo, so probably r0.4.1. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:70,error,error,70,,https://github.com/google/deepvariant/issues/30,3,"['Error', 'down', 'error']","['Error', 'download', 'error']"
Availability,"#16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6185,down,downloader,6185,,https://github.com/google/deepvariant/issues/608,1,['down'],['downloader']
Availability,#include <optional> error after update from v0.8.0 to v0.9.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:20,error,error,20,,https://github.com/google/deepvariant/issues/236,1,['error'],['error']
Availability,"${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:2962,error,errors,2962,,https://github.com/google/deepvariant/issues/60,2,['error'],"['error', 'errors']"
Availability,"${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3994,echo,echo,3994,,https://github.com/google/deepvariant/issues/151,3,"['echo', 'error']","['echo', 'error']"
Availability,'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619:21,error,error,21,,https://github.com/google/deepvariant/issues/619,1,['error'],['error']
Availability,(TRAINING) model-ckpt-0 shows low accuracy even when loaded from a previous checkpoint,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383:76,checkpoint,checkpoint,76,,https://github.com/google/deepvariant/issues/383,1,['checkpoint'],['checkpoint']
Availability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2940,error,error,2940,,https://github.com/google/deepvariant/issues/845,1,['error'],['error']
Availability,")); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/MADDOG/c_elegans.PRJEB28388.WS274.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/MADDOG/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmpl3fvinw4/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_eleg",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:7540,error,error,7540,,https://github.com/google/deepvariant/issues/292,1,['error'],['error']
Availability,"); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5034,ERROR,ERROR,5034,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3434,error,errors,3434,,https://github.com/google/deepvariant/issues/429,1,['error'],['errors']
Availability,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:268,error,errors,268,,https://github.com/google/deepvariant/issues/342,2,['error'],"['error', 'errors']"
Availability,"**Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55; `; Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588; `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0; BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**; - Operating system:; - DeepVariant version: 1.3.0, latest ; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**; - Command: Call variants with run_deepvariant wrapper script. ; - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520:1428,Error,Error,1428,,https://github.com/google/deepvariant/issues/520,1,['Error'],['Error']
Availability,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:🆑:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:370,error,error,370,,https://github.com/google/deepvariant/issues/380,3,['error'],['error']
Availability,"**Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**; - Operating system: Ubuntu 22.04; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```; time docker run --rm --gpus 1 \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt""; ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```; docker run --rm --gpus '""device=1""' \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_eval \; --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --checkpoint_dir=""${TRAINING",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:78,checkpoint,checkpoints,78,,https://github.com/google/deepvariant/issues/611,5,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:879,Error,Error,879,,https://github.com/google/deepvariant/issues/367,1,['Error'],['Error']
Availability,"**Describe the issue:**; After running, no VCF is found, the logs however are available. **Setup**; - Operating system: ubuntu 22.04 (WSL2); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): docker; - Type of data: (I find variant only in chr17 for easier reading and faster speed); - input read: aligned HG004 to CHM13 (https://downloads.pacbcloud.com/public/revio/2022Q4/HG004-rep1/); - reference genome: CHM13 (https://github.com/marbl/CHM13). **Steps to reproduce:**; - Command:; `docker run --volume ""/root/deepvariant/input"":""/input"" --volume ""/root/deepvariant/output"":""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/chm13v2.0.fa --reads=/input/aligntest.bam --regions ""chr17"" --output_vcf=/output.vcf.gz --output_gvcf=/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results --logging_dir=/output`; ; **Any additional context:**; [make_examples.log](https://github.com/user-attachments/files/16189177/make_examples.log); [call_variants.log](https://github.com/user-attachments/files/16189180/call_variants.log); [postprocess_variants.log](https://github.com/user-attachments/files/16189182/postprocess_variants.log); [vcf_stats_report.log](https://github.com/user-attachments/files/16189186/vcf_stats_report.log)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/850:78,avail,available,78,,https://github.com/google/deepvariant/issues/850,2,"['avail', 'down']","['available', 'downloads']"
Availability,"**Describe the issue:**; After upgrading to v1.6, we noticed this strange behavior, where the program hangs on a sharded BAM that holds only alt-contig mapping reads. **Setup**; - Operating system: on GCE via Google Life Sciences API (through Cromwell); - DeepVariant version: v1.6; - Installation method (Docker, built from source, etc.): official v1.6 docker; - Type of data: Both PacBio HiFi and ONT (10.4), on GRCh38. . **Steps to reproduce:**. - Command. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --haploid_contigs chrX,chrY \; --par_regions_bed GRCh38.PAR.bed \; --reads=/cromwell_root/<sample_id>.alts.bam \; --output_vcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.vcf.gz \; --output_gvcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.g.vcf.gz \; --num_shards=16; ```. - Relevant log ; (note it says ""0 examples"", so I suspect it is when no examples are available, not just when there's only alt-mapping reads, e.g. if one simulates reads error-free from the reference itself, it probably will have the same issue). ```; /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00011-of-00016.gz.example_info.json; I0203 17:23:03.253894 135328978921280 make_examples_core.py:2958] example_shape = None; I0203 17:23:03.254237 135328978921280 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:03.255900 135328978921280 make_examples_core.py:301] Task 11/16: Found 0 candidate variants; I0203 17:23:03.256017 135328978921280 make_examples_core.py:301] Task 11/16: Created 0 examples; I0203 17:23:04.930985 137565708298048 make_examples_core.py:301] Task 7/16: Writing example info to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00007-of-00016.gz.example_info.json; I0203 17:23:04.931358 137565708298048 make_examples_core.py:2958] example_shape = None; I0203 17:23:04.931699 137565708298048 make_examples_core.py:2959] example_chann",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:962,avail,available,962,,https://github.com/google/deepvariant/issues/769,1,['avail'],['available']
Availability,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/333:446,reliab,reliable,446,,https://github.com/google/deepvariant/issues/333,1,['reliab'],['reliable']
Availability,"**Describe the issue:**; Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**; - Operating system: Amazon Linux 2023; - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda; - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**; - Command: `mamba install deepvariant -c bioconda`; - Error trace: ; ```; Pinned packages:; - python 3.10.*. Could not solve for environment specs; The following packages are incompatible; └─ deepvariant is installable with the potential options; ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.10.0|1.0.0] would require; │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; ├─ deepvariant [0.7.1|0.7.2] would require; │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);; └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel).; ```. **Does the quick start test work on your system?**; N/A. **Any additional context:**; My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736:265,down,down,265,,https://github.com/google/deepvariant/issues/736,5,"['Error', 'avail', 'down', 'error']","['Error', 'available', 'down', 'error']"
Availability,"**Describe the issue:**; Cannot install latest DeepVariant via Conda in my new environment. Error prompts, similar to that of #736. If having everything in default, I can have python=2.7 and deepvariant=0.7.0; but cannot update to 1.15 or latest. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (ssh to university, docker unavailable); - DeepVariant version: 0.7.0 installable, but cannot get 1.15 or latest; - Installation method (Docker, built from source, etc.): conda; - Type of data: N/A. **Steps to reproduce:**; - Command:; $ create -n deepvariant python=3.8 (current version 3.8.19); $ conda install deepvariant. - Error trace: (if applicable). > #warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE; > warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > failed; > ; > LibMambaUnsatisfiableError: Encountered problems while solving:; > - package deepvariant-0.4.1-np113py27_0 requires python 2.7*, but none of the providers can be installed; > ; > Could not solve for environment specs; > The following packages are incompatible; > ├─ deepvariant is installable with the potential options; > │ ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; > │ │ └─ tensorflow 1.12.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.10.0|1.0.0] would require; > │ │ └─ tensorflow 2.0.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; > │ │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; > │ ├─ deepvariant [0.7.1|0.7.2] would require; > │ │ └─ tensorflow 1.11.* , which conflicts with any installable versions previously reported;; > │ └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; > │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835:92,Error,Error,92,,https://github.com/google/deepvariant/issues/835,2,['Error'],['Error']
Availability,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334:839,down,downstream,839,,https://github.com/google/deepvariant/issues/334,1,['down'],['downstream']
Availability,"**Describe the issue:**; Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:114,error,error,114,,https://github.com/google/deepvariant/issues/677,3,['error'],['error']
Availability,"**Describe the issue:**; Hello, I am trying to run the deepvariant RNA model on HG005 data locally (not restricted to chr20 like the Github example) but the model keeps generating this error while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:185,error,error,185,,https://github.com/google/deepvariant/issues/845,8,"['checkpoint', 'error']","['checkpoint', 'checkpoints', 'error']"
Availability,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/340:445,Error,Error,445,,https://github.com/google/deepvariant/issues/340,1,['Error'],['Error']
Availability,"**Describe the issue:**; Hello, i am trying to recreate the steps done in this post: https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/; I have a bam file with phasing information which a got by running whatshap on the data and now i want use deepvariant for a second time just like the steps in the post. ; I have a illumina bam file which i phased with nanopore data. I run this command: ; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads whatshap/HG003.GRCh38.chr20.haplotagged.bam \; --use_hp_information \; --output_vcf deepvariant2/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20. And get this error : ; NotImplementedError: The --use_hp_information flag has been deprecated. DeepVariant now phases internally for PacBio mode. I get what deprecated means but what is the thing i have to do to specify to deepvariant that my bam file is phased because i dont think it is using the haplotagged info in the bam file now?. **Setup**; - DeepVariant version: latest; - Installation method : source; - Type of data: bam file with phasing information by whatshap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/822:800,error,error,800,,https://github.com/google/deepvariant/issues/822,1,['error'],['error']
Availability,"**Describe the issue:**; Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**; - Operating system: CentOS Linux 7 (Core); - Singularity version: 3.5-8.el7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: WGS. **Steps to reproduce:**; - Command:; > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=${reference_genome} \; > --reads=${bam} \; > --regions=""chr1"" \; > --output_vcf=${vcf_dir}/${sample}.vcf.gz \; > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \; > --intermediate_results_dir ${tmp_dir} \; > --num_shards=${ncpu}. - Error trace: (if applicable); > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**; I also tried with `--no-home` flag which did not work at all. ; I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/601:263,error,error,263,,https://github.com/google/deepvariant/issues/601,4,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:89,checkpoint,checkpoint,89,,https://github.com/google/deepvariant/issues/339,3,"['Error', 'checkpoint', 'error']","['Error', 'checkpoint', 'error']"
Availability,"**Describe the issue:**; I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:; ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**; - Operating system: Linux Ubuntu 20.04; - DeepVariant version: 1.4; - Installation method: Docker; Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**; /opt/deepvariant/bin/run_deepvariant \; --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \; --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \; --customized_model=model.ckpt-364300 \; --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \; --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \; --model_type WGS \; --make_examples_extra_args phase_reads=true,channels=blank \; --regions CHM13v2.chrY \; --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca""; I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/563:121,error,error,121,,https://github.com/google/deepvariant/issues/563,3,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"**Describe the issue:**; I am following the tutorial for [PacBio HiFi data](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md). When I reach the step for calling singularity to `run_deepvariant` ([this step](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments)), I receive an error which appears to be associated with the tempfile/TMPDIR path:. Command: ; ```; BIN_VERSION=""1.3.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. **Error 1**; ```; INFO: Using cached SIF image; I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it canno",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:402,error,error,402,,https://github.com/google/deepvariant/issues/533,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:76,error,error,76,,https://github.com/google/deepvariant/issues/471,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam ✅; 2. non-parallel + cram ✅ ; 3. parallel + bam ✅ ; 4. non-parallel + cram 🔴 . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:355,error,error,355,,https://github.com/google/deepvariant/issues/786,3,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; I want to debug source code，and execute “python deepvariant/call_variants.py”. **Setup**; i have execute build-prereq.sh and build_and_test.sh. In order to get the compilation result .; i execute ""bazel build ..."",get the file like this :; <img width=""529"" alt=""image"" src=""https://github.com/google/deepvariant/assets/15654389/50fbd52c-afed-4ade-a3fa-f2eaf0859b3d"">; ; The same name comes from different directories.so,the error happy:; “from third_party.nucleus.io import sharded_file_utils ” from root workspace; “from third_party.nucleus.protos import variants_pb2” from bazel-bin. <img width=""647"" alt=""image"" src=""https://github.com/google/deepvariant/assets/15654389/76e1373b-dbfb-48b6-95d1-59bd07badbcc"">. `root@7065ad26b62a:/deepvariant# python deepvariant/call_variants.py; 2023-12-19 06:28:51.254398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Traceback (most recent call last):; File ""deepvariant/call_variants.py"", line 53, in <module>; from deepvariant import dv_utils; File ""/deepvariant/./deepvariant/dv_utils.py"", line 47, in <module>; from deepvariant.protos import deepvariant_pb2; File ""/deepvariant/bazel-bin/deepvariant/protos/deepvariant_pb2.py"", line 17, in <module>; from deepvariant.protos import realigner_pb2 as deepvariant_dot_protos_dot_realigner__pb2; File ""/deepvariant/bazel-bin/deepvariant/protos/realigner_pb2.py"", line 21, in <module>; from third_party.nucleus.protos import range_pb2 as third__party_dot_nucleus_dot_protos_dot_range__pb2; ImportError: cannot import name 'range_pb2' from 'third_party.nucleus.protos' (/deepvariant/./third_party/nucleus/protos/__init__.py); `; How can i debug the source code in the right way?; How to correctly co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756:449,error,error,449,,https://github.com/google/deepvariant/issues/756,1,['error'],['error']
Availability,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411:368,Error,Error,368,,https://github.com/google/deepvariant/issues/411,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; Make_Example fail because of bed.file. **Setup**; - Operating system:ubuntu18.04; - DeepVariant version:v1.0.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: same as case study. **Steps to reproduce:**; - Command:; /usr/local/seqslab/deepvariant/bazel-bin/deepvariant/make_examples \; --mode calling \; --ref /opt/command/test_dir/ref.fa \; --reads /opt/command/test_dir/0-0.bam \; --regions /opt/command/test_dir/part_0.bed \; --examples /opt/command/test_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:584,Error,Error,584,,https://github.com/google/deepvariant/issues/374,1,['Error'],['Error']
Availability,"**Describe the issue:**; Running DeepVariant v1.1.0 on viral amplicon PacBio HiFi data, aligned with pbmm2, using the run_deepvariant script. Core dump during the make_examples step with exit status 252. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), singularity version 3.5.3-1.el7; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): singularity image pulled from docker://google/deepvariant:1.1.0; - Type of data: PacBio HiFi amplicons. **Steps to reproduce:**; - Command:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ref.fa \; --reads reads.bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:811,Error,Error,811,,https://github.com/google/deepvariant/issues/419,1,['Error'],['Error']
Availability,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434:245,error,error,245,,https://github.com/google/deepvariant/issues/434,1,['error'],['error']
Availability,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402:1380,Error,Error,1380,,https://github.com/google/deepvariant/issues/402,1,['Error'],['Error']
Availability,"**Describe the issue:**; When doing the variant calling with a VCF file of proposed variants (`variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}`) and specifying GVCF output, ref call sites end up with only 2 entries in the FORMAT field AD while the ALT field contain also 2 entries which should result in 3 AD entries. ; Example: `chr1 11391 . T A,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:24:0:0,0:0:0,26,27,990,990,990`. Here the AD field has 2 entries but should have 3. Thereafter, the merging with GLNexus fails with the following error `Failed to genotype: Invalid: genotyper: VCF allele depth FORMAT field is malformed (sample <0>:11391-11391 (AD))`. . **Setup**; - Operating system: Linux; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Singularity. **Steps to reproduce:**; - Command: ` singularity exec -B /usr/lib/locale/:/usr/lib/locale/ ${SINGULARITY_IMG} bash /opt/deepvariant/bin/run_deepvariant --model_type WGS --customized_model ${MODEL} --ref ${REF} --reads ${IN_BAM} --output_vcf ${OUT_VCF}.2.vcf.gz --output_gvcf ${OUT_VCF}.2.g.vcf.gz --num_shards 24 --make_examples_extra_args=\""realign_reads=false,min_mapping_quality=${MIN_MAPQ},min_base_quality=${MIN_QS},variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}\"" --sample_name ${IN_PN} ${BED_ARG};""; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/341:552,error,error,552,,https://github.com/google/deepvariant/issues/341,1,['error'],['error']
Availability,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354:294,down,downsampling,294,,https://github.com/google/deepvariant/issues/354,3,"['Down', 'down', 'error']","['Downsample', 'downsampling', 'error']"
Availability,"**Describe the issue:**; i have use docker pull from google/deepvariant:1.6.0-gpu ,but the python version is 3.11 in this contain，i don't know why. <img width=""1165"" alt=""iShot_2023-12-14_15 23 49"" src=""https://github.com/google/deepvariant/assets/15654389/d61f07f0-b540-4bda-8a9e-ccb633cfe3e7"">. I want to debug with source code, but there will be this error, does this deepvariant.proto need to be compiled manually？. `ironment variable `TF_ENABLE_ONEDNN_OPTS=0`.; Traceback (most recent call last):; File ""/code/deepvariant/deepvariant/call_variants.py"", line 48, in <module>; from deepvariant import dv_utils; File ""/code/deepvariant/deepvariant/dv_utils.py"", line 44, in <module>; from deepvariant.protos import deepvariant_pb2; ImportError: cannot import name 'deepvariant_pb2' from 'deepvariant.protos' (unknown location); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753:354,error,error,354,,https://github.com/google/deepvariant/issues/753,1,['error'],['error']
Availability,**Describe the issue:**; nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**; - Operating system: linux; - DeepVariant version: 1.5. **Steps to reproduce:**; - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04; - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown; ; Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support?; Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/676:95,avail,available,95,,https://github.com/google/deepvariant/issues/676,3,"['Error', 'avail']","['Error', 'available']"
Availability,"**Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:716,Error,Error,716,,https://github.com/google/deepvariant/issues/455,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Describe the issue:**; while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**; - OS: CentOS Linux release 7.4.1708 (Core); - DeepVariant version:conda install bioconda/label/cf201901::deepvariant; - Installation method (Docker, built from source, etc.): Conda; - Type of data: NA. **Steps to reproduce:**; - Command: conda install bioconda/label/cf201901::deepvariant; - Error trace: ; '''conda install bioconda/label/cf201901::deepvariant -y; Collecting package metadata: done; Solving environment: '''. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**; NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806:441,Error,Error,441,,https://github.com/google/deepvariant/issues/806,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? [](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-wgs-case-study.md). **Describe the issue:**; Merging vcf files error.; **Setup**; - Operating system: working on cluster ; - DeepVariant version:latest; - Installation method (Docker):; - Type of data: (GIAB AshkenazimTrio [HG002,HG003,HG004] analysis.). **Steps to reproduce:**; - Command: ; ```; udocker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```; - Error trace: (if applicable); ; > Num BCF records read 118736378 query hits 14552613; > [E::bgzf_read_block] Invalid BGZF header at offset 265038798; > [E::bgzf_read] Read block operation failed with error 2 after 0 of 32 bytes; > [E::bgzf_read] Read block operation failed with error 3 after 0 of 32 bytes; > Error: BCF read err. ![Screenshot from 2024-05-06 15-00-29](https://github.com/google/deepvariant/assets/45700858/2c6e5565-78aa-4e55-9488-82b2f5f04514)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815:158,error,error,158,,https://github.com/google/deepvariant/issues/815,5,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**. We found a back-to-back call of two SNPs that we cannot explain as the BAM file suggests a deletion. IGV screenshot: https://www.dropbox.com/s/c0wfelxc1cca14b/igv_snapshot.png?dl=0. Happy to provide a BAM file etc. But maybe this is easy enough to explain; I just cannot figure out why this comes out as:; TC and GC and not as TC and G/-. chr19 15174241 rs1044006 T C 66 . AC=1;AF=0.5;AN=2;AQ=66;DP=78 GT:AD:DP:GQ:PL:RNC 0/1:0,78:78:10:20,0,9:.; chr19 15174242 chr19_15174242_G_C G C 53 . AC=1;AF=0.5;AN=2;AQ=53;DP=177 GT:AD:DP:GQ:PL:RNC 0/1:77,100:177:50:53,0,52:. **Setup**; - Operating system: Centos 7; - DeepVariant version: 1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Novaseq 6000, exomes, GRCh38. **Steps to reproduce:**; - Command: Does not apply.; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/453:1012,Error,Error,1012,,https://github.com/google/deepvariant/issues/453,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459:437,Error,Error,437,,https://github.com/google/deepvariant/issues/459,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470:1547,avail,available,1547,,https://github.com/google/deepvariant/issues/470,2,"['Error', 'avail']","['Error', 'available']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:205,down,downloaded,205,,https://github.com/google/deepvariant/issues/462,1,['down'],['downloaded']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476:283,error,error,283,,https://github.com/google/deepvariant/issues/476,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I have tried to run in my personal computer the WES deepvariant case. However I get the following error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@16.gz --gvcf /output/inter_res/gvcf.tfrecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:219,error,error,219,,https://github.com/google/deepvariant/issues/483,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:437,Error,Error,437,,https://github.com/google/deepvariant/issues/480,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:178,error,error,178,,https://github.com/google/deepvariant/issues/491,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:218,error,error,218,,https://github.com/google/deepvariant/issues/522,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity image built from docker image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**; - Command: ; ```; # Modified script; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=hs37d5_PhiX.fa \; --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \; --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \; --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \; --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \; --num_shards=15; ```; I have also tried postprocessing with `group_variants`, which also produces a similar error.; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/postprocess_variants \; --group_variants=false \; --ref=hs37d5_PhiX.fa \; --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \; --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz; ```; - Error t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:125,error,error,125,,https://github.com/google/deepvariant/issues/517,2,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**; I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```; 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores; 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0); Fatal Python error: Aborted; ```; and the job eventually fails:. ```; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:863,error,error,863,,https://github.com/google/deepvariant/issues/539,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/525:437,Error,Error,437,,https://github.com/google/deepvariant/issues/525,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**; - Operating system: Centos7; - DeepVariant version: 1.2.0, 1.3.0, latest; - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16; - ; - Error trace: (if applicable). Command output:; sys.exit(main(argv)); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main; proto_utils.uses_fast_cpp_protos_or_die(); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die; raise ValueError('Expected to be using C++ protobuf implementation '; ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python; Traceback (most recent call last):; File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:148,error,error,148,,https://github.com/google/deepvariant/issues/499,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**; - Operating system: CentOS 7 ; - DeepVariant version: 1.4.0 (google/deepvariant:latest); - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`; - Type of data: N/A. **Steps to reproduce:**; - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`; - Error trace:; ```; Traceback (most recent call last):; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportE",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:571,Error,Error,571,,https://github.com/google/deepvariant/issues/610,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: yes. **Describe the issue:**; I am running DeepVariant on a custom genome assembly using a hybrid of pacbio hifi and illumina short reads and it's been running for 17days. I wonder if something is wrong and is there a way to speed thing up? I am already using 30 shards. **Setup**; - Operating system: centOS 7; - DeepVariant version: 1.4.0 and 1.1.0 (tried version 1.1.0 been running for 17 days then I'm trying the 1.4.0 and it's been running for 3 days now); - Installation method (Docker, built from source, etc.): converted docker image to singularity image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) I am using custom genome. **Steps to reproduce:**; - Command: ; ```; singularity exec ~/virtual_server/deepvariant.sif \; bash -c ""; /opt/deepvariant/bin/run_deepvariant \; --model_type=""HYBRID_PACBIO_ILLUMINA"" \; --ref=""${REF_DIR}""/scaffolds_FINAL.fasta \; --reads=""${INPUT_DIR}""/hybrid_hifi_Kapa_combined.bam \; --output_vcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.g.vcf.gz \; --num_shards=$SLURM_CPUS_PER_TASK \; --logging_dir=""${OUTPUT_DIR}""/logs \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp""; ```; - Error trace: (if applicable) attached r_deepvariant_hybrid_2.txt. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? I was able to run quick start. I was also able to run DeepVariant on the same singularity system with pacbio HiFi reads only, using human reference genome hg19. **Any additional context:**; [r_deepvariant_hybrid_2_662510.txt](https://github.com/google/deepvariant/files/9853335/r_deepvariant_hybrid_2_662510.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578:1378,Error,Error,1378,,https://github.com/google/deepvariant/issues/578,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated.; (A clear and concise description of what the issue is.). **Setup**; - Operating system:Mac OS ; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true; - Error trace: No error.(if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561:995,Error,Error,995,,https://github.com/google/deepvariant/issues/561,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**; - Operating system: HPC; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker --> Singularity; - Type of data: WGS data. **Steps to reproduce:**; - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`; - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41; /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz""; ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/599:630,Error,Error,630,,https://github.com/google/deepvariant/issues/599,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:379,avail,available,379,,https://github.com/google/deepvariant/issues/545,2,"['Error', 'avail']","['Error', 'available']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; According to the running log: the setlocale failed when trying to change LC_ALL to 'en_US.UTF-8'. **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker pull ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) DNA seq. **Steps to reproduce:**; - Command: ; - singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale"" \; --env LANG=""en_US.UTF-8"" \; --env LC_ALL=""C"" \; --env LANGUAGE=""en_US.UTF-8"" \; --env LC_CTYPE=""UTF-8"" \; ...... - Error trace: (if applicable); ![image](https://user-images.githubusercontent.com/40780228/190950415-84faaa5d-7371-42a7-9e13-f6caf53a3dea.png). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566:696,Error,Error,696,,https://github.com/google/deepvariant/issues/566,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; Had an issue with the quickstart tutorial where I received this error:; ```; /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format.; See '/usr/bin/docker-current run --help'.; deepvariant_test.sh: line 11: make_examples: command not found; deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**; ```; docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; -v ""${BIN_VERSION}"":; docker.io/google/deepvariant \; /opt/deepvariant/bin/run_deepvariant \; ...; ```; Should be ; ```; BIN_VERSION=""1.4.0""; docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; -v ""${BIN_VERSION}"":; docker.io/google/deepvariant \; /opt/deepvariant/bin/run_deepvariant \; ...; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/604:181,error,error,181,,https://github.com/google/deepvariant/issues/604,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; Running singularity on the test data I get the following:; ```. OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ...; ...; Try --helpfull to get a list of all flags.; deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; deepvariant.sing.sh: line 18: make_examples: command not found; deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```; I have checked and these paths and files exist and can be opened used the above links. . **Setup**; - Operating system: linux; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/558:963,error,error,963,,https://github.com/google/deepvariant/issues/558,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**; - Operating system: Debian GNU/Linux 9; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.):; - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**; - Command: singularity run --bind ${PWD} \; /software/deepvariant/deepvariant.img \; /opt/deepvariant/bin/run_deepvariant \; --model_type HYBRID_PACBIO_ILLUMINA \; --ref ragtag.fasta \; --reads hifi_illu.bam \; --intermediate_results_dir ./tmp \; --output_vcf rep1.hifi-illu.Chr1.vcf.gz \; --num_shards 4 \; --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code; [[{{node IteratorGetNext}}]]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/548:797,Error,Error,797,,https://github.com/google/deepvariant/issues/548,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:246,error,error,246,,https://github.com/google/deepvariant/issues/580,3,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants.; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - 150bp paired-end Illumina data. **Steps to reproduce:**; - Command: ; `/opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; --make_examples_extra_args=""normalize_reads=true"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \; --num_shards=${threads}`; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):; ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/612:1150,Error,Error,1150,,https://github.com/google/deepvariant/issues/612,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; At the call_variants.py step, running into error that tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; (A clear and concise description of what the issue is.). **Setup**; - Operating system:CentOS7 ; - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.):singularity run with SIF image pulled from docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:165,error,error,165,,https://github.com/google/deepvariant/issues/564,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:245,ERROR,ERROR,245,,https://github.com/google/deepvariant/issues/640,1,['ERROR'],['ERROR']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity image build from dockerhub; - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --version; - Error trace: (if applicable); ```; tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.4.0; ```; The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** ; The GPU is NVIDIA GeForce 3090; The GPU Driver Version: 520.61.05; The CUDA version in the host is V11.8.89 as followings:; ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png); It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. ; ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619:549,Error,Error,549,,https://github.com/google/deepvariant/issues/619,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Downstream association analysis has yielded a high number of false positive findings, essentially a product of low quality data. It is crucial that these sites are filtered out. . **Setup**; - Operating system: Linux; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Illumina WES. **Steps to reproduce:**; - Command: . DeepVariant:; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \. GLnexus:; glnexus_cli --config DeepVariantWES --bed ${regions} \; 2_gvcf/*.g.vcf.gz > 3_bcf/Exomes.bcf. **Any additional context:**. Hi there!; Apologies for bringing up another similar issue, but I would like some help with the correct filtering of my merged vcf file.; Essentially, I have identified a significant number of false positive sites in a downstream assoc. analysis, where MAF for these variants is widely different than the population average. This strongly suggests that these sites are of low quality and need to be filtered out. Here are some examples from the merged vcf file. For each variant I have only shown a handful of samples (total is over 5000):. False positive / bad site that needs filtering:. `1	1722625	1_1722625_A_T	A	T	48	.	AF=0.222894;AQ=48	GT:DP:AD:GQ:PL:RNC	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:7:0,7:42:44,47,0:..	0/0:0:0,0:1:0,0,0:..	1/1:6:0,6:36:38,38,0:..	0/1:12:3,9:0:19,2,0:..	./.:3:3,0:0:20,0,50:II	1/1:2:0,2:23:29,25,0:..	0/0:2:2,0:6:0,6,59:..	1/1:2:0,2:22:31,24,0:..	1/1:2:0,2:26:28,29,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	1/1:7:0,7:40:43,42,0:..	./.:3:3,0:0:20,0,50:II	1/1:4:1,3:1:28,3,0:..	0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:121,Down,Downstream,121,,https://github.com/google/deepvariant/issues/645,1,['Down'],['Downstream']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Having problem running deeptrio examples [https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md) . **Setup**; - Operating system: Ubuntu 22.04, Docker 23+; - DeepVariant version: deeptrio-1.5.0-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). It always give ```Error: The directory ""/reference/GRCh38_no_alt_analysis_set.sdf"" already exists. Please remove it first or choose a different directory.``` even after I ensure that there are no GRCh38_no_alt_analysis_set.sdf exist in said directory; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And that being said, this command also raises another error showing ```Error: An IO problem occurred: ""Not in GZIP format""```; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ``` . **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? **Quick start on single variant analysis is optimal**. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632:641,Error,Error,641,,https://github.com/google/deepvariant/issues/632,4,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Hello,. Using WES model, deepvariant calls the following variant in the vcf file:; ```; NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44; ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. ; What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample?. Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/618:1236,Error,Error,1236,,https://github.com/google/deepvariant/issues/618,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651:437,Error,Error,437,,https://github.com/google/deepvariant/issues/651,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**; - Operating system: linux（Centos）; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**; - Error trace: (if applicable); <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:257,error,error,257,,https://github.com/google/deepvariant/issues/668,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Hi developers,; I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:529,error,error,529,,https://github.com/google/deepvariant/issues/653,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. **Describe the issue:**; Run into Fatal python Bus error repeatedly. **Setup**; - Operating system: CentOS 7 ; - DeepVariant version: 1.4 (DeepTrio); - Installation method (Docker, built from source, etc.): singularity ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Illumina NovaSeq data, reference genome hg19. ; **Steps to reproduce:**; - Command:; - `/opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`; - Error trace: (if applicable); `Fatal Python error: Bus error. Current thread 0x00002af3a27fa740 (most recent call first):; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 58 in <dictcomp>; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 57 in get_candidates; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:148,error,error,148,,https://github.com/google/deepvariant/issues/646,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. ; **Describe the issue:**; I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . ; This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :; ; ``` ; (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] ; 22:08 $ ls; GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp; ```; I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/).; ; **Setup**; - Operating system: Linux Mint 21.1 x86_64 ; - Kernel: 5.15.0-69-generic ; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Docker image run through Singularity; - Singularity Verion : singularity-ce version 3.11.3; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```; (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] ; 20:46 $ samtools flagstat GFX.bam ; ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads); 881297 + 0 primary; 0 + 0 secondary; 59254 + 0 supplementary; 0 + 0 duplicates; 0 + 0 primary duplicates; 940551 + 0 mapped (100.00% : N/A); 881297 + 0 primary mapped (100.00% : N/A); 0 + 0 paired in sequencing; 0 + 0 read1; 0 + 0 read2; 0 + 0 properly paired (N/A : N/A); 0 + 0 with itself and mate mapped; 0 + 0 singletons (N/A : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a diffe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:352,error,error,352,,https://github.com/google/deepvariant/issues/666,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:102,error,error,102,,https://github.com/google/deepvariant/issues/678,3,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; yes，i have checked this FAQ document. ; **Describe the issue:**; I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:; Data comparison to reference genome:; ```; echo HG002.merged.fastq.gz > HG002.fofn ; pbmm2 align \; --preset HIFI \; genome/hg38.fa.mmi \; HG002.fofn \; --sample HG002 \; -j 10 \; HG002.aligned.tmp.bam ; samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam ; samtools index -@ 10 HG002.aligned.tmp.sort.bam ; chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) ; for chromosome in ""${chromosomes[@]}""; \; do \; samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & ; done ; wait ; samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam ; samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam ; samtools index -@ 10 HG002.sort.bam ; ```; Family analysis code:; ```; rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 ; samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam ; samtools view --write-index --threads 10 -h -b -S HG003.sort.b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:964,echo,echo,964,,https://github.com/google/deepvariant/issues/689,1,['echo'],['echo']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**; - Operating system: Cent; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**; - Command:; ```bash; #!/bin/bash; sample=$1; threads=$2. chr=$3; indir=""01.mapping""; outdir=""02.snps""; sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}""; rm -rf ${outdir}/${sample}_chr${chr}; ```; - Error trace: (if applicable); ```bash; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]; I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]; I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]; I0325 17:32:27.143459 47041007318848 mak",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:193,error,error,193,,https://github.com/google/deepvariant/issues/794,3,"['error', 'fault']","['error', 'fault']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:1463,Error,Error,1463,,https://github.com/google/deepvariant/issues/856,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**; - Operating system: Ubuntu 22.04 LTS; - DeepVariant version:; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**; - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc); - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810:1044,Error,Error,1044,,https://github.com/google/deepvariant/issues/810,2,"['Error', 'error']","['Error', 'errors']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**; - Operating system: Ubuntu 22.04.2 LTS ; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**; - Command: sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/RILWLs1.fasta \; --reads=/input/Out.fastq \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Yes. Test data works fine. ; ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807:134,error,error,134,,https://github.com/google/deepvariant/issues/807,4,"['Error', 'error', 'fault']","['Error', 'error', 'fault']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am having some errors while fetching variants from chrX, Could you please have a look I added a error.txt file for reference. ; Thank you. > INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chrX"" start: 14000 end: 15000. **Setup**; - Operating system:linux; - DeepVariant version:latest; - Installation method (Docker, built from source, etc.):udocker; - Type of data: test file for checking Calling variants in non-autosomal contigs [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-xy-calling-case-study.md). **Steps to reproduce:**; - Command:; BIN_VERSION=""1.6.1"". ```; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". udocker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"" ; ```. - Error trace: ; ; [error.txt](https://github.com/user-attachments/files/16281125/error.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853:136,error,errors,136,,https://github.com/google/deepvariant/issues/853,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am running deepvariant 1.6.1 through singularity (apptainer) on both WGS and RNAseq bams. The WGS bam was much larger in file size, but was processed much more quickly than the RNAseq bams produced by STAR. ; Because deepvariant 1.6.1 does not support an rnaseq model, so I just ran the WES model on it, providing a BED file containing all regions with at least 3X read depth. Here is the script I used:. `sID=$1 #sample ID; sBAM=$2 #full path to BAM; REF=$3 #full path to fasta ref; CPU=$4 #number of CPUs to use. module load apptainer/1.2.5; module load clusterbasics; module load samtools; module load bedtools. OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:861,echo,echo,861,,https://github.com/google/deepvariant/issues/867,1,['echo'],['echo']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:292,error,error,292,,https://github.com/google/deepvariant/issues/879,2,"['error', 'fault']","['error', 'fault']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; Python encountered a MemoryError, no matter how much we increase the memory allocation the error exists. **Setup**; - Operating system:; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): apptainer pull docker://google/deepvariant:""1.6.1""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) bam files, ONT. **Steps to reproduce:**; - Command: ; apptainer exec --bind /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint /work/XXXX/ls6/deepvariant/deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant --model_type ONT_R104 --ref /work/XXXX/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:210,error,error,210,,https://github.com/google/deepvariant/issues/854,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; The postprocess_variants step fails with following error message:; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Setup**; - Operating system: CentOS Linux 7 (Core); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio Sequencing. **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:170,error,error,170,,https://github.com/google/deepvariant/issues/818,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); `run_deepvariant` is erroring out in the `postprocess_variants` step. **Setup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:202,error,erroring,202,,https://github.com/google/deepvariant/issues/901,1,['error'],['erroring']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I can't pull the container using singularity. **Setup**; - Operating system: Ubuntu; - Installation method (Docker, built from source, etc.): tried with singularity; My system has singularity installed, I tried getting the container but it failed:. I used this code; ```; BIN_VERSION=""1.6.1""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; ```. and It got several warning messages:. WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; /usr/bin/env: ‘python’: No such file or directory; Cleaning up...; ERROR: pulling container failed!. I was thinking of installing DeepVariant with conda, but the version in conda is 1.5.0, and I was told to always run the most up-to-date one.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/886:772,ERROR,ERROR,772,,https://github.com/google/deepvariant/issues/886,1,['ERROR'],['ERROR']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I have processed around 30 samples albeit having some issues with GPU, possibly due to nvidia driver / cuda version. However, recently postprocess has started stalling with the same error. Any help troubleshooting this would be greatly appreciated!. **Setup**; - Operating system: ; NAME=Red Hat Enterprise Linux; VERSION=9.4 (Plow); - DeepVariant version: deepvariant:1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker (via podman); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:306,error,error,306,,https://github.com/google/deepvariant/issues/849,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; google/deepvariant:1.5.0-gpu google/deepvariant:1.6.1-gpu docker images run as CPU-only because they are using ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:843,Error,Error,843,,https://github.com/google/deepvariant/issues/844,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:1057,Error,Error,1057,,https://github.com/google/deepvariant/issues/865,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:YES. **Describe the issue:**; When Running deep variant wes mode, there arised an assetion error when loading the weights of the model. **Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:183,error,error,183,,https://github.com/google/deepvariant/issues/857,2,"['Error', 'error']","['Error', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** ; (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:350,error,error,350,,https://github.com/google/deepvariant/issues/733,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:437,Error,Error,437,,https://github.com/google/deepvariant/issues/743,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; I am struggling to get DeepTrio to run to completion on a small dataset. It completes at the end of call_variants.py but my system just collapses when at postprocess_variants.; Through using --dry_run=true, I'm able to keep going only after being sufficiently confident the last step has completed without error.; So in short, is it possible to re-run the wrapper command and have the analysis pipeline pick up where it left off? . **Setup**; - Operating system: Rocky Linux 8; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): through Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) illumina, 151bp, same reference as case studies; - RAM 64 GB; - CPUs 32 (c6i.8xlarge). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes they do. they complete because they are small. ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Unfortunately, i cant run it on g4dn.8xlarge available to me since that EC2 running Amazon Linux 2, and GPU DeepVariant seems to need Ubuntu.; In short, a ""step_x_completed"" sentinel file at end of each step would be great IMO. . Thanks,; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749:423,error,error,423,,https://github.com/google/deepvariant/issues/749,3,"['Error', 'avail', 'error']","['Error', 'available', 'error']"
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; The step of postprocess_variants cannot find the VCF file. - Operating system:Centos; - DeepVariant version:1.6.0; - Installation method :singularity; - Type of data: (NGS sequence). **Steps to reproduce:**; - Command:; mkdir /public1/home/yinhang/data/tmp/${SAMPLE}/; singularity exec /public/software/apps/deepvariant-1.6.0-cpu_singularity/deepvariant-1.6.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${ref_genome} \; --reads=""${SORTED_BAM_DIR}/${SAMPLE}/${SAMPLE}.sorted.bam"" \; --output_vcf=""${VCF_DIR}/${SAMPLE}/${SAMPLE}.vcf.gz"" \; --output_gvcf=""${VCF_DIR}/${SAMPLE}/${SAMPLE}.g.vcf.gz"" \; --intermediate_results_dir=/public1/home/yinhang/data/tmp/${SAMPLE}/ \; --num_shards=60 \; --sample_name ${SAMPLE}. - Error trace:. I0217 17:31:41.680527 139931052529472 call_variants.py:623] Complete: call_variants. real	339m11.258s; user	13508m55.048s; sys	183m10.091s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public1/home/yinhang/projects/two_genomes/04_T2T/04_ragtag/03_syri/01_data/SH_N0.fasta"" --infile ""/public1/home/yinhang/data/tmp/SRR1572254/call_variants_output.tfrecord.gz"" --outfile ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz"" --cpus ""60"" --gvcf_outfile ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public1/home/yinhang/data/tmp/SRR1572254/gvcf.tfrecord@60.gz"" --sample_name ""SRR1572254"". /bin/bash: warning: setlocale: LC_ALL: cannot change locale (zh_CN.UTF-8); /bin/bash: warning: setlocale: LC_ALL: cannot change locale (zh_CN.UTF-8); I0217 17:31:47.293663 139808123168576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: SRR1572254; I0217 17:31:47.294324 139808123168576 postprocess_variants.py:1216] --sample_name ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/773:862,Error,Error,862,,https://github.com/google/deepvariant/issues/773,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; When variant is not detected, the program will freeze in the last step；. **Setup**; - Operating system:Centos7.6; - DeepVariant version: 1.6 ; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO-SMART；A reference sequence for a normal person；. **Steps to reproduce:**; - Command: /bin/singularity run -B /work/:/work/ /work/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=human_geneA_reference.fa --reads=reference.align.bam --output_vcf=out.vcf --output_gvcf=out.gvcf --num_shards=32; - Error trace: Last line： I0119 11:43:53.450599 47012502976320 call_variants.py:623] Complete: call_variants（Stuck at this step）. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. [deepvariant_1.6.pdf](https://github.com/google/deepvariant/files/13986125/deepvariant_1.6.pdf)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/764:770,Error,Error,770,,https://github.com/google/deepvariant/issues/764,1,['Error'],['Error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; YES, RNA and STAR are not covered not covered. **Describe the issue:**; 2023-12-14 03:00:18.822708: F deepvariant/allelecounter.cc:204] Check failed: offset + len <= read.aligned_quality_size() (8 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f351d854740 (most recent call first):; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 72 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN029",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/752:310,error,error,310,,https://github.com/google/deepvariant/issues/752,1,['error'],['error']
Availability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; Yes. **Describe the issue:**; DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**; - Operating system:; Linux 3.10.0-1160.81.1.el7.x86_64; - DeepVariant version:; 1.6; - Installation method (Docker, built from source, etc.):; Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**; - Command:; ```; local args=(); args+=(""--model_type"" ""ONT""); args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""); args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""); args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""); args+=(""--sample_name_child"" ""HG002""); args+=(""--sample_name_parent1"" ""HG002_copy""); args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""); args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""); args+=(""--num_shards"" ""6""); args+=(""--regions"" ""regions_chunk_8.bed""); args+=(""--intermediate_results_dir"" ""intermediate_results""); args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""); args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}""; ```; content of .bed file:; ```; $ cat regions_chunk_8.bed; chr9 0 138394717; ```. stats of .bam file:; ```; chr1 248956422 1319 0; chr2 242193529 929 0; chr3 198295559 749 0; chr4 190214555 1042 0; chr5 181538259 649 0; chr6 170805979 667 0; chr7 159345973 613 0; chr8 145138636 622 0; chr9 138394717 586 0; chr10 133797422 622 0; chr11 135086622 538 0; chr12 133275309 4",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724:177,fault,fault,177,,https://github.com/google/deepvariant/issues/724,4,"['down', 'fault']","['downloaded', 'downloads', 'fault']"
Availability,"**Hello, I ran deep variant for mutation calling of single cell iso seq bam file (Pacbio long read rna-seq), and I got this error messages:**; `9-03 12:26:09.055921: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/44871400: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055938: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/22677071: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055954: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31298741: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055967: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31321506: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055980: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/31927218: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.055993: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25969828: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056006: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/6236354: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056021: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/32560810: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056034: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/12686626: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056046: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/12686630: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056060: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/50928829: NOT_FOUN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:124,error,error,124,,https://github.com/google/deepvariant/issues/877,1,['error'],['error']
Availability,"**ISSUE**; First of all, I found DeepVariant to be a very good and innovative tool. I'm considering including it in my exome analysis pipeline. I followed the tutorial (DeepVariant worked correctly with the Complete Genomics model), and I created my own model using Genome in a Bottle samples. To do this, I sequenced the same reference sample three times to use each BAM file for training, validation, and testing. I didn't encounter any errors during the model creation process, but when I tried to test it, the process got stuck at the call_variants step. **Setup**; - Operating system: Ubuntu 22.04.4 LTS; - DeepVariant version:1.6.1; - Installation method:docker; - Type of data: MGI DNBSEQ 400, exome sequencing. **Steps to reproduce:**; - Command:; _Create examples for trainning set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/26_r_groups.bam"" --examples ""/output/training_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:439,error,errors,439,,https://github.com/google/deepvariant/issues/869,1,['error'],['errors']
Availability,"*Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1196,checkpoint,checkpoint,1196,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability,"++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3969,ERROR,ERROR,3969,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3086,echo,echo,3086,,https://github.com/google/deepvariant/issues/151,1,['echo'],['echo']
Availability,"- Command:#docker; BIN_VERSION=""1.5.0""; reference=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/reference; INPUT_DIR=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/input; OUTPUT_DIR=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/outdir; singularity run --nv -B /home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${reference}""/GRCh38_no_alt_analysis_set.fasta \; --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ; - Error trace: INFO: Using cached SIF image; WARNING: Could not find any nv files on this host!; 2023-04-22 17:10:50.025707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 51, in <module>; from ._api.v2 import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/__init__.py"", line 37, in <module>; from . import v1; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>; from . import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 38, in <module>; from . import v2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634:842,Error,Error,842,,https://github.com/google/deepvariant/issues/634,1,['Error'],['Error']
Availability,"- Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1815,down,downstream,1815,,https://github.com/google/deepvariant/issues/733,1,['down'],['downstream']
Availability,--regions results in 'parallel: Error: Command line too long (<num> >= 65524)',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/556:32,Error,Error,32,,https://github.com/google/deepvariant/issues/556,1,['Error'],['Error']
Availability,--track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 39; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/09505/s223885/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/09505/s223885/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --examples /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/gvcf.tfrecord@64.gz --max_reads_per_partition 600 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 52; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/09505/s223885/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/09505/s223885/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --examples /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/gvcf.tfrecord@64.gz --max_reads_per_partition 600 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 57. real	0m4.925s; user	0m4.781s; sys	0m19.092s. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Did not face this error in DeepVariant version: 1.5.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:15628,error,error,15628,,https://github.com/google/deepvariant/issues/854,1,['error'],['error']
Availability,"-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:2168,error,error,2168,,https://github.com/google/deepvariant/issues/223,1,['error'],['error']
Availability,"-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb; ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?. Than",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:1606,error,error,1606,,https://github.com/google/deepvariant/issues/514,1,['error'],['error']
Availability,"-zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-tesla-k80', '--accelerator-count', '1']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180503-232937-37 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]].; (exit status 1); ```. It seems like google api can't insert instance in the make example steps. ; Any helps would be appreciated. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:2187,checkpoint,checkpoint,2187,,https://github.com/google/deepvariant/issues/70,4,"['Error', 'checkpoint', 'error']","['Error', 'checkpoint', 'error']"
Availability,". I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:942,checkpoint,checkpoints,942,,https://github.com/google/deepvariant/issues/636,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"..........; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/50",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4267,ERROR,ERROR,4267,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:373,error,error,373,,https://github.com/google/deepvariant/issues/231,2,"['avail', 'error']","['available', 'error']"
Availability,".15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 6: bazel-out/host/bin/external/clif/proto: from: not found; bazel-out/host/bin/external",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:1091,error,error,1091,,https://github.com/google/deepvariant/issues/122,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,".2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5995,Down,Download,5995,,https://github.com/google/deepvariant/issues/608,1,['Down'],['Download']
Availability,".3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build_release_binaries.sh; [sudo] password for solokopi: ; build_release_binaries.sh: line 39: bazel: command not found; build_release_binaries.sh: line 43: bazel: command not found; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:19010,error,error,19010,,https://github.com/google/deepvariant/issues/89,2,"['Error', 'error']","['Error', 'error']"
Availability,".468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0; I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None; W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:7201,checkpoint,checkpoint,7201,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoint']
Availability,".8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:21445,checkpoint,checkpoint,21445,,https://github.com/google/deepvariant/issues/537,1,['checkpoint'],['checkpoint']
Availability,".gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2662,echo,echo,2662,,https://github.com/google/deepvariant/issues/151,1,['echo'],['echo']
Availability,.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8795,down,download,8795,,https://github.com/google/deepvariant/issues/292,1,['down'],['download']
Availability,".pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 693, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 572, in create_all_commands_and_logfiles; check_flags(); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 544, in check_flags; raise RuntimeError(; > RuntimeError: The model files /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/2fullindividualmodel/checkpoints/ckpt-14902* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open. Can someone please help me figure out what's going on? The link provided showed a different set of files than the ones I have. Am I missing files? Is something upstream not functioning properly? I have trained models before, with the same kinds out output, and have been able to test them before. What am I missing? . Thank you for your help!. Best,; Haley Arnold",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:3398,checkpoint,checkpoints,3398,,https://github.com/google/deepvariant/issues/866,1,['checkpoint'],['checkpoints']
Availability,"//10.73.74.226:8470) for TPU system metadata.; I0524 21:18:26.625535 140032543119168 tpu_system_metadata.py:90] Querying Tensorflow master (grpc://10.73.74.226:8470) for TPU system metadata.; 2022-05-24 21:18:26.626490: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.; INFO:tensorflow:Found TPU system:; I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:; INFO:tensorflow:*** Num TPU Cores: 8; I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8; INFO:tensorflow:*** Num TPU Workers: 1; I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1; INFO:tensorflow:*** Num TPU Cores Per Worker: 8; I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8; INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:6307,Avail,Available,6307,,https://github.com/google/deepvariant/issues/537,1,['Avail'],['Available']
Availability,"/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6282,Down,Download,6282,,https://github.com/google/deepvariant/issues/608,1,['Down'],['Download']
Availability,/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20116,error,error,20116,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${PWD}/${params.reference} \; --reads ${PWD}/output/4.markDuplicate/${sample_id}_sorted_md.bam \; --regions ${PWD}/${params.bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepvariant_log.txt 2>&1. Command exit status:; 127. Command output:; (empty). Command error:; docker: Error response from daemon: open /var/lib/docker/overlay2/fe3663cd03e849890d83be14603f217249f3f43f9585b554df599d0318909f21/.tmp-committed2046174062: no such file or directory.; See 'docker run --help'. Work dir:; /home/ubuntu/dd/nextflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:2044,error,error,2044,,https://github.com/google/deepvariant/issues/883,1,['error'],['error']
Availability,"/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2631,down,down,2631,,https://github.com/google/deepvariant/issues/602,1,['down'],['down']
Availability,/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5610,ERROR,ERROR,5610,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5802,ERROR,ERROR,5802,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5994,ERROR,ERROR,5994,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6186,ERROR,ERROR,6186,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6378,ERROR,ERROR,6378,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6570,ERROR,ERROR,6570,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6762,ERROR,ERROR,6762,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:6954,ERROR,ERROR,6954,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7146,ERROR,ERROR,7146,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7338,ERROR,ERROR,7338,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenc,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7530,ERROR,ERROR,7530,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googl,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7722,ERROR,ERROR,7722,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7914,ERROR,ERROR,7914,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:801::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:801::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:10084,down,download,10084,,https://github.com/google/deepvariant/issues/89,1,['down'],['download']
Availability,"/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:6507,down,download,6507,,https://github.com/google/deepvariant/issues/89,1,['down'],['download']
Availability,"/google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-08-26 20:44:28.903729: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-08-26 20:44:28.905866: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:1508,checkpoint,checkpoint,1508,,https://github.com/google/deepvariant/issues/564,1,['checkpoint'],['checkpoint']
Availability,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:2027,error,error,2027,,https://github.com/google/deepvariant/issues/483,1,['error'],['error']
Availability,"/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt.; 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory.; 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1; I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt.; I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. real	3m6.726s; user	2m58.710s; sys	0m25.780s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:124310,checkpoint,checkpoints,124310,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoints']
Availability,"/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(; I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started.; I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True; I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6].; I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-0000",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:5042,Error,Error,5042,,https://github.com/google/deepvariant/issues/797,1,['Error'],['Error']
Availability,"0 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1662,error,error,1662,,https://github.com/google/deepvariant/issues/902,1,['error'],['error']
Availability,"0 candidate variants; I0921 06:50:42.154923 139913398449920 make_examples.py:535] Task 1/4: Created 0 examples; I0921 06:50:42.137424 140575591024384 make_examples.py:535] Task 3/4: 0 candidates (0 examples) [0.11s elapsed]; I0921 06:50:42.147503 140575591024384 make_examples.py:535] Task 3/4: Found 0 candidate variants; I0921 06:50:42.147607 140575591024384 make_examples.py:535] Task 3/4: Created 0 examples; I0921 06:50:42.173398 140201192457984 make_examples.py:535] Task 0/4: 0 candidates (0 examples) [0.14s elapsed]; I0921 06:50:42.195395 140201192457984 make_examples.py:535] Task 0/4: Found 0 candidate variants; I0921 06:50:42.195529 140201192457984 make_examples.py:535] Task 0/4: Created 0 examples. real	0m2.637s; user	0m8.272s; sys	0m1.419s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". I0921 06:50:44.216155 140117089298176 call_variants.py:316] Set KMP_BLOCKTIME to 0; W0921 06:50:44.234806 140117089298176 call_variants.py:323] Unable to read any records from /output/intermediate_results_dir/make_examples.tfrecord@4.gz. Output will contain zero records. real	0m1.979s; user	0m1.868s; sys	0m0.327s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@4.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". I0921 06:50:46.249888 139970848761600 postprocess_variants.py:966] call_variants_output is empty. Writing out empty VCF.; I0921 06:50:46.250269 139970848761600 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF.; I0921 06:50:46.250618 139970848761600 genomics_writer.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354:9644,checkpoint,checkpoint,9644,,https://github.com/google/deepvariant/issues/354,1,['checkpoint'],['checkpoint']
Availability,"0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1031,down,downstream,1031,,https://github.com/google/deepvariant/issues/722,1,['down'],['downstream']
Availability,"0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" dire",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:1662,error,error,1662,,https://github.com/google/deepvariant/issues/292,1,['error'],['error']
Availability,"0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13799,checkpoint,checkpoint,13799,,https://github.com/google/deepvariant/issues/166,1,['checkpoint'],['checkpoint']
Availability,"0.meta output/models/model.ckpt-34008.meta; output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evalu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:3546,checkpoint,checkpoints,3546,,https://github.com/google/deepvariant/issues/611,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5418,ERROR,ERROR,5418,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"01 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>> && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel; 52 |; --------------------; ERROR: failed to solve: process ""/bin/sh -c ./build-prereq.sh && PATH=\""${HOME}/bin:${PATH}\"" ./build_release_binaries.sh # PATH for bazel"" did not complete successfully: exit code: 100; ```. Looks like the repositories are either old or the sign has expired. How to fix this error?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:2392,error,error,2392,,https://github.com/google/deepvariant/issues/902,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"0828 10:40:42.658734 140318776715072 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; I0828 10:40:47.952382 140318776715072 keras_modeling.py:325] Number of l2 regularizers: 95.; I0828 10:40:48.007193 140318776715072 keras_modeling.py:362] inceptionv3: load_weights from checkpoint: /home/training_outs/epoch1//checkpoints/ckpt-5997; I0828 10:40:49.193293 140318776715072 train.py:191] Exponential Decay: initial_learning_rate=0.0001; decay_steps=45448; learning_rate_decay_rate=0.947; I0828 10:40:49.193522 140318776715072 train.py:203] Use LinearWarmup:; warmup_steps=10000; warmup_learning_rate=1e-05; I0828 10:40:49.401860 140318776715072 keras_modeling.py:472] Restored checkpoint ckpt-5997 at step=0. tune/f1_weighted=tf.Tensor(0.0, shape=(), dtype=float32); WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be rem>; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:4914,checkpoint,checkpoint,4914,,https://github.com/google/deepvariant/issues/876,1,['checkpoint'],['checkpoint']
Availability,0908051506_s1_p0/18204/0_11826: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083564: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m141207_021027_42177R_c100762112550000001823161707071506_s1_p0/128181/16776_21715: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083624: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150304_023026_42163R_c100791492550000001823175409091556_s1_p0/94463/13434_17465: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083666: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150205_114659_42163R_c100780092550000001823165208251532_s1_p0/41607/2555_6463: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083710: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150209_042724_42177R_c100779832550000001823165208251590_s1_p0/129762/7102_9303: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083973: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150309_153203_42156_c100797772550000001823175109091511_s1_p0/155759/0_8477: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.084070: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150304_151228_42163R_c100797832550000001823175109091520_s1_p0/45204/2004_7491: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.111139: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (1261 vs. 0); Fatal Python error: Aborted. cmd:; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref hs37d5.fasta \; --reads HG003_PacBio_GRCh37.bam \; --output_vcf HG003_PacBio.depv.vcf.gz \; --output_gvcf HG003_PacBio.depv.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir intermediate_results_dir; ; What can i do to fix it?; Looking forward to your reply. Thanks.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:39579,error,error,39579,,https://github.com/google/deepvariant/issues/631,1,['error'],['error']
Availability,"0:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restoring parameters from /tmp/deepvariant/model.ckpt-0; I0502 10:58:57.455770 139632719935232 tf_logging.py:82] Restoring parameters from /tmp/deepvariant/model.ckpt-0; INFO:tensorflow:Starting Session.; I0502 10:59:09.842276 139632719935232 tf_logging.py:82] Starting Session.; INFO:tensorflow:Saving checkpoint to path /tmp/deepvariant/model.ckpt; I0502 10:59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = Ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:2484,checkpoint,checkpoint,2484,,https://github.com/google/deepvariant/issues/69,1,['checkpoint'],['checkpoint']
Availability,1 6504 0 0; chrUn_KI270588v1 6158 0 0; chrUn_KI270593v1 3041 0 0; chrUn_KI270591v1 5796 0 0; chrUn_KI270330v1 1652 0 0; chrUn_KI270329v1 1040 0 0; chrUn_KI270334v1 1368 0 0; chrUn_KI270333v1 2699 4 0; chrUn_KI270335v1 1048 0 0; chrUn_KI270338v1 1428 0 0; chrUn_KI270340v1 1428 0 0; chrUn_KI270336v1 1026 1 0; chrUn_KI270337v1 1121 0 0; chrUn_KI270363v1 1803 0 0; chrUn_KI270364v1 2855 0 0; chrUn_KI270362v1 3530 0 0; chrUn_KI270366v1 8320 0 0; chrUn_KI270378v1 1048 0 0; chrUn_KI270379v1 1045 0 0; chrUn_KI270389v1 1298 0 0; chrUn_KI270390v1 2387 0 0; chrUn_KI270387v1 1537 0 0; chrUn_KI270395v1 1143 0 0; chrUn_KI270396v1 1880 0 0; chrUn_KI270388v1 1216 0 0; chrUn_KI270394v1 970 0 0; chrUn_KI270386v1 1788 0 0; chrUn_KI270391v1 1484 0 0; chrUn_KI270383v1 1750 0 0; chrUn_KI270393v1 1308 0 0; chrUn_KI270384v1 1658 0 0; chrUn_KI270392v1 971 0 0; chrUn_KI270381v1 1930 0 0; chrUn_KI270385v1 990 0 0; chrUn_KI270382v1 4215 0 0; chrUn_KI270376v1 1136 0 0; chrUn_KI270374v1 2656 0 0; chrUn_KI270372v1 1650 0 0; chrUn_KI270373v1 1451 0 0; chrUn_KI270375v1 2378 0 0; chrUn_KI270371v1 2805 0 0; chrUn_KI270448v1 7992 0 0; chrUn_KI270521v1 7642 0 0; chrUn_GL000195v1 182896 4 0; chrUn_GL000219v1 179198 2 0; chrUn_GL000220v1 161802 37 0; chrUn_GL000224v1 179693 6 0; chrUn_KI270741v1 157432 1 0; chrUn_GL000226v1 15008 48 0; chrUn_GL000213v1 164239 1 0; chrUn_KI270743v1 210658 3 0; chrUn_KI270744v1 168472 13 0; chrUn_KI270745v1 41891 0 0; chrUn_KI270746v1 66486 2 0; chrUn_KI270747v1 198735 2 0; chrUn_KI270748v1 93321 2 0; chrUn_KI270749v1 158759 0 0; chrUn_KI270750v1 148850 0 0; chrUn_KI270751v1 150742 4 0; chrUn_KI270752v1 27745 0 0; chrUn_KI270753v1 62944 1 0; chrUn_KI270754v1 40191 0 0; chrUn_KI270755v1 36723 0 0; chrUn_KI270756v1 79590 3 0; chrUn_KI270757v1 71251 2 0; chrUn_GL000214v1 137718 9 0; chrUn_KI270742v1 186739 9 0; chrUn_GL000216v2 176608 17 0; chrUn_GL000218v1 161147 3 0; chrEBV 171823 10 0; * 0 0 0; ```; - Error trace: (if applicable); ```; Fatal Python error: Segmentation fault.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724:7291,Error,Error,7291,,https://github.com/google/deepvariant/issues/724,3,"['Error', 'error', 'fault']","['Error', 'error', 'fault']"
Availability,"10, in conv2d; name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 1071, in conv2d; data_format=data_format, dilations=dilations, name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3360, in create_op; attrs, op_def, compute_device); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3429, in _create_op_internal; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 1751, in __init__; self._traceback = tf_stack.extract_stack(). real	0m10.613s; user	0m11.112s; sys	0m4.718s; I0924 03:47:46.482943 140410383501056 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 1. falllowing my ndvida-smi it consumes all the memmory, htere is a way to limit memmory?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:22602,checkpoint,checkpoint,22602,,https://github.com/google/deepvariant/issues/358,1,['checkpoint'],['checkpoint']
Availability,"1162] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was finalized.; I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt; INFO:tensorflow:prediction_loop marked as finished; I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished; WARNING:tensorflow:Reraising captured error; W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[{{node save_1/RestoreV2}}]]. During handling",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:11137,error,error,11137,,https://github.com/google/deepvariant/issues/537,1,['error'],['error']
Availability,"173517489984 make_examples_core.py:301] Task 0/2: Created 3569 examples; I0105 15:55:21.254777 140329169033024 make_examples_core.py:301] Task 1/2: Writing example info to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00001-of-00002.gz.example_info.json; I0105 15:55:21.255679 140329169033024 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0105 15:55:21.255904 140329169033024 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0105 15:55:21.262568 140329169033024 make_examples_core.py:301] Task 1/2: Found 3672 candidate variants; I0105 15:55:21.263317 140329169033024 make_examples_core.py:301] Task 1/2: Created 3944 examples. real 1m56.796s; user 3m3.813s; sys 0m4.710s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow communit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:12516,checkpoint,checkpoint,12516,,https://github.com/google/deepvariant/issues/761,1,['checkpoint'],['checkpoint']
Availability,"199 --norealign_reads --regions chr17 --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 2. real 0m4.188s; user 0m3.295s; sys 0m0.885s`. **this is the form of bam files I have used:**; `molecule/22487242 0 chr1 11823 22 1S22=165N39=1X71=1X27=1X9=1X9=1X16=1D3=1X10=1X12=1X13=385N109=761N9=1I16=1X104=1X70=1X91=1I37=1X24=1X54=1D18=1X67=1X1=1X132=1X132=1X49=1X55=1X58= * 0 2590 GTAAACGAGATTGCCAGCACCGGGTGTCTGACTTCCAGCAACTGCTGGCCTGTGCCAGGGTGGAAGCTGAGCACTGGAGTGGAGTTTTCCTGTGGAGAGGAGCCATGCCTAGAGTGGGATGGGCCATTGTTCATATTCTGGCCCCTGTTGTCTGCATGTAACCTAATACCACGACCAGGCATGGGGGAAAGATTGGAGGAAGTTGAGTGAGAGGATCAACTTCTCTGACAACCTAGGCCAGTGTGTGGTGATGCCAGGCATGCCCTTCCCCAGCATCAGGTCTCCAGAGCTGCAGAAGACGACGGCCGACTTGGATCACACTCTTGTGAGTGTCCCCAGTGTTGCAGAGGCAGCTGCACCCACTGCCTGGCGCTGCGCCCTTCCTTTGCTCTGCCCGCTGGAGACGGTGTTTGTCATGGGCCTGGTCTGCAGGGATCCTGCTACAAAGGTGAAACCCAGGAGAGTGTGGAGTCCAGAGTGATGCCAGGACCCAGGCACAGGCATTAGTGCCCGTTGGAGAAAACAGGGGAATCCCGAAGAAATGGTGGGTCTTGGCCATCCGTGAGATCTTCCCAGGGCAGCTCCCCTCTGTGGAATCCAATCTGTCTTCCATCCTGCGTGGCCGAGGGCCAGGCTTCTCACTGGGGCCTCTGCAGGAGGCTGCCATTTGTCCTGCCCACCGTCTTAGAAGCGAGACGGAGCAGACTCATCTGCTACTGCCCTTTCTATAATAACTAAAGTTAGCTGCCCTGGACTATTCACCCCTAGTCTCAATTTAAAAAGATCCCCATGGCCACAGGGCCCCTGCCTGGGGGCTTGTCACCTCCCCCACCTTCTTCCTGAGTCACTTCTGCAGCCTTGCTCCCTAACCTGCCCCACAGCCTTGCCTGGATTTCTATCTCCCTGGCTTGGTGCCAGTTCCTCCAAGTCGATGGCACCTCCCTCCCTCTCAACCACTTGAGCAAACTCCAAGACATCTTCTTCCCCAACACCAGCAATTGTGCCAAGGGCCATTAGGCTCTCAGCATGACTATTTTTAGAGACCCCGTGTCTGTCACTGAAACCTTTTTTGTGGGAGACTATTCCTCCCATCTGCAACAGCTGCCCCTGCTGACGGCCCTTCTCTCCTCCCTCTCATCCCAGAGAAACAGGTCAGCTGGGAGCTCCTGCCCCCACTGCCTAGGGACCAACAGGGGCAGGAGGCAGTCACTGACCCCGAGAAGTTTGCATCCTGCACAGCTAGAGATCCTTTATTAAAAGCACACTGTTGGTTTCTGCTC * CB:Z:CCAACTCACATTGAAG XA:Z:XM-CB XM:Z:AGACAATCCGTA ic:i:1 im:Z:m84210_240422_080753_s1/229444864/ccs/5985_7265 is:i:1 it:Z:AGACAATCCGTACCAACTCACATTGAAG rc:i:1 RG:Z:e4927d21 zm:i:22487242 mg:f:98.1265 NM:i:24`. **Also, when I used test files, I got the final results without any errors**. All advice much appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:8404,error,errors,8404,,https://github.com/google/deepvariant/issues/877,1,['error'],['errors']
Availability,1:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15562,error,error,15562,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"2 07:51:54.272932 127450974123840 make_examples_core.py:301] Task 17/20: Created 18817 examples; I0822 07:52:09.283522 133276175411008 make_examples_core.py:301] Task 8/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00008-of-00020.gz.example_info.json; I0822 07:52:09.283617 133276175411008 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:52:09.283882 133276175411008 make_examples_core.py:301] Task 8/20: Found 17371 candidate variants; I0822 07:52:09.283904 133276175411008 make_examples_core.py:301] Task 8/20: Created 18820 examples. real 34m15.728s; user 624m43.553s; sys 2m24.932s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@20.gz"" --checkpoint ""/output/checkpoints/ckpt-679"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0822 07:52:10.812179 127086447671104 call_variants.py:563] Total 1 writing processes started.; I0822 07:52:10.813103 127086447671104 dv_utils.py:370] From /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0822 07:52:10.813141 127086447671104 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0822 07:52:10.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:4619,checkpoint,checkpoint,4619,,https://github.com/google/deepvariant/issues/869,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"2. Sequence obtain from data generation tools. (dwgsim) . **Setup**; - Operating system: Red Hat Enterprise Linux release 8.6 (Ootpa); - DeepVariant version: deepvariant1.6.0.sif; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); GRCh38| (Sequence obtain from data generation tools. (dwgsim) ，length,8-9kB, 150bp, PE, sequence may the same as reference sequence) . **Steps to reproduce:**; - Command: ; time singularity run ~/singularity/deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${bamSavePath}/${name}.sorted.bam \; --output_vcf ${vcf} \; --output_gvcf ${outputPath}/vcf/${name}/${name}.g.vcf.gz \; --num_shards $(nproc) \; --regions ${BED} \; --sample_name ${name} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" . - Error trace: (if applicable); I0729 14:44:37.339473 140223721211712 make_examples_core.py:301] Task 0/4: Preparing inputs; I0729 14:44:37.339473 140478861559616 make_examples_core.py:301] Task 3/4: Preparing inputs; I0729 14:44:37.350302 140710547908416 make_examples_core.py:301] Task 1/4: Preparing inputs; I0729 14:44:37.339477 139779121772352 make_examples_core.py:301] Task 2/4: Preparing inputs; I0729 14:44:37.476220 140223721211712 genomics_reader.py:222] Reading result/simulate_A/bam/chr6_CYP21A2_CYP21A1P__A_Q_8845_R_8585/chr6_CYP21A2_CYP21A1P__A_Q_8845_R_8585.sorted.bam with NativeSamReader; I0729 14:44:37.485832 140223721211712 make_examples_core.py:301] Task 0/4: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; I0729 14:44:37.533100 140223721211712 genomics_reader.py:222] Reading /lustre/home/acct-medfzx/medfzx-lkw/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:1202,Error,Error,1202,,https://github.com/google/deepvariant/issues/855,1,['Error'],['Error']
Availability,2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12130,error,error,12130,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.4",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1955,error,errors,1955,,https://github.com/google/deepvariant/issues/844,1,['error'],['errors']
Availability,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:2049,Down,Downloads,2049,,https://github.com/google/deepvariant/issues/231,7,"['Down', 'error']","['Downloads', 'error']"
Availability,"3 12:26:09.056452: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963197: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056463: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963211: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056474: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963212: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056489: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/9461690: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056506: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/8764719: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.086379: F deepvariant/allelecounter.cc:204] Check failed: offset + len <= read.aligned_quality_size() (496 vs. 0); Fatal Python error: Aborted. Current thread 0x00007b82cc097740 (most recent call first):; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2040 in candidates_in_region; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/GRCh38.p13.genome.fa -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:5126,error,error,5126,,https://github.com/google/deepvariant/issues/877,1,['error'],['error']
Availability,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:36017,error,error,36017,,https://github.com/google/deepvariant/issues/491,1,['error'],['error']
Availability,"3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 693, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:2036,error,error,2036,,https://github.com/google/deepvariant/issues/866,1,['error'],['error']
Availability,"3, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; range_tracker.sub_range_tracker(source_ix)):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 175, in read_records; record = _TFRecordUtil.read_record(file_handle); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 131, in read_record; buf = file_handle.read(buf_length_expected); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 240, in read; self._fetch_to_internal_buffer(num_bytes); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 199, in _fetch_to_internal_buffer; self._read_buffer.write(decompressed); MemoryError: out of memory. ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:5080,ERROR,ERROR,5080,,https://github.com/google/deepvariant/issues/91,1,['ERROR'],['ERROR']
Availability,"3-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3578,error,errors,3578,,https://github.com/google/deepvariant/issues/429,1,['error'],['errors']
Availability,"3952 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; 2020-09-24 03:47:45.236844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12321,error,error,12321,,https://github.com/google/deepvariant/issues/358,1,['error'],['error']
Availability,"3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1580,avail,available,1580,,https://github.com/google/deepvariant/issues/761,1,['avail'],['available']
Availability,"3:04.933572 137565708298048 make_examples_core.py:301] Task 7/16: Created 0 examples; I0203 17:23:09.199501 136895166957376 make_examples_core.py:301] Task 13/16: Writing example info to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:3138,checkpoint,checkpoint,3138,,https://github.com/google/deepvariant/issues/769,1,['checkpoint'],['checkpoint']
Availability,"3] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.; INFO:tensorflow:Found TPU system:; I0524 21:18:26.631762 140032543119168 tpu_system_metadata.py:159] Found TPU system:; INFO:tensorflow:*** Num TPU Cores: 8; I0524 21:18:26.631872 140032543119168 tpu_system_metadata.py:160] *** Num TPU Cores: 8; INFO:tensorflow:*** Num TPU Workers: 1; I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1; INFO:tensorflow:*** Num TPU Cores Per Worker: 8; I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8; INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INF",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:6491,Avail,Available,6491,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: True; I0105 15:56:02.220975 140372734228288 dv_utils.py:365] From /opt/models/wgs/example_info.json: Shape ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13436,down,downstream,13436,,https://github.com/google/deepvariant/issues/761,1,['down'],['downstream']
Availability,"4.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:4658,checkpoint,checkpoint,4658,,https://github.com/google/deepvariant/issues/611,1,['checkpoint'],['checkpoint']
Availability,"44992 genomics_reader.py:223] Reading /input/R9_Z-1707-003_cluster1_RC492.bam with NativeSamReader; I0511 12:24:32.453339 140409179444992 genomics_reader.py:223] Reading /input/R9_Z-1707-003_cluster1_RC492.bam with NativeSamReader; I0511 12:24:32.579413 140409179444992 make_examples.py:648] Writing examples to /tmp/tmpq5tvks3j/make_examples.tfrecord-00000-of-00001.gz; I0511 12:24:32.579596 140409179444992 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0511 12:24:32.587054 140409179444992 make_examples.py:648] 0 candidates (0 examples) [0.01s elapsed]; I0511 12:24:32.591045 140409179444992 make_examples.py:648] Found 0 candidate variants; I0511 12:24:32.591111 140409179444992 make_examples.py:648] Created 0 examples. real 0m3.165s; user 0m3.133s; sys 0m1.450s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpq5tvks3j/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpq5tvks3j/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" ). W0511 12:24:34.935784 140411820246784 call_variants.py:327] Unable to read any records from /tmp/tmpq5tvks3j/make_examples.tfrecord@1.gz. Output will contain zero records. real 0m2.355s; user 0m2.789s; sys 0m1.594s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fasta"" --infile ""/tmp/tmpq5tvks3j/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" ). I0511 12:24:37.234371 139970945300224 postprocess_variants.py:1083] Could not determine sample name and --sample_name is unset. Using the default sample name. Sample name: default; I0511 12:24:37.235468 139970945300224 postprocess_variants.py:1111] call_variants_output is empty. Writing out empty VCF.; I0511 12:24:37.235656 139970945300224 postprocess_variants.py:1139] Writing variants to VCF.; I0511 12:24:37.235709 139970945300224 postprocess_variants.py:723] Writing output to VCF file: /output/output.vcf.gz; I0511 12:24:37.236480 13997094530",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457:4514,checkpoint,checkpoint,4514,,https://github.com/google/deepvariant/issues/457,1,['checkpoint'],['checkpoint']
Availability,"48 47424450524992 make_examples_core.py:301] Task 19/48: 2809 candidates (2881 examples) [27.12s elapsed]; I0325 17:32:43.357604 47806383535936 make_examples_core.py:301] Task 23/48: 2286 candidates (2401 examples) [28.70s elapsed]; I0325 17:32:43.931203 47985428563776 make_examples_core.py:301] Task 35/48: 3282 candidates (3387 examples) [31.57s elapsed]; I0325 17:32:44.979849 47999988315968 make_examples_core.py:301] Task 31/48: 2600 candidates (2699 examples) [23.92s elapsed]; I0325 17:32:44.729335 47653137950528 make_examples_core.py:301] Task 30/48: 2895 candidates (3016 examples) [25.97s elapsed]; I0325 17:32:47.486382 47801829132096 make_examples_core.py:301] Task 2/48: 4049 candidates (4139 examples) [15.04s elapsed]; I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]; I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]; Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:8261,error,error,8261,,https://github.com/google/deepvariant/issues/794,2,"['error', 'fault']","['error', 'fault']"
Availability,"49:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_ID; PROBAND_ID=${Case_ID}_proband; MOTHER_ID=${Case_ID}_mother; FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/deeptrio/run_deeptrio \; 	--model_type=$SEQ_TYPE \; 	--ref=""/genomedir/$FASTA_FILE"" \; 	--reads_child=""/bamdir/$PROBAND_BAM"" \; 	--reads_parent1=""/bamdir/$FATHER_BAM"" \; 	--reads_parent2=""/bamdir/$MOTHE",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:7657,echo,echo,7657,,https://github.com/google/deepvariant/issues/518,1,['echo'],['echo']
Availability,"51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_examples.py:587] Task 9/30: Created 76 examples; I0924 03:47:35.441959 139746083813120 make_examples.py:587] Task 29/30: Found 78 candidate variants; I0924 03:47:35.442209 139746083813120 make_examples.py:587] Task 29/30: Created 78 examples. real	0m5.429s; user	2m1.568s; sys	0m23.089s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:1431,Error,Error,1431,,https://github.com/google/deepvariant/issues/358,1,['Error'],['Error']
Availability,"6 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:2252,error,error,2252,,https://github.com/google/deepvariant/issues/251,1,['error'],['error']
Availability,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12763,error,error,12763,,https://github.com/google/deepvariant/issues/614,3,"['error', 'failure']","['error', 'failures']"
Availability,"6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ------; > [builder 6/6] RUN ./build-prere",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8503,Error,Error,8503,,https://github.com/google/deepvariant/issues/608,2,"['Error', 'down']","['Error', 'downloading']"
Availability,6:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8405,error,error,8405,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"7 140710547908416 make_examples_core.py:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:11572,checkpoint,checkpoint,11572,,https://github.com/google/deepvariant/issues/855,1,['checkpoint'],['checkpoint']
Availability,"7552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt.; 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory.; 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory.; 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory.; I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1; I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt.; I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:123430,checkpoint,checkpoints,123430,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoints']
Availability,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ------; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; ------; executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:9320,ERROR,ERROR,9320,,https://github.com/google/deepvariant/issues/608,1,['ERROR'],['ERROR']
Availability,"852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3100,echo,echo,3100,,https://github.com/google/deepvariant/issues/151,1,['echo'],['echo']
Availability,88.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8276,down,download,8276,,https://github.com/google/deepvariant/issues/292,1,['down'],['download']
Availability,"9.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; Traceba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/250:1038,error,error,1038,,https://github.com/google/deepvariant/issues/250,1,['error'],['error']
Availability,"903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:10.817819 139872277903104 session_manager.py:500] Running local_init_op.; I0424 16:00:11.060626 139872277903104 session_manager.py:502] Done running local_init_op.; I0424 16:00:12.403780 139872277903104 modeling.py:413] Reloading EMA...; I0424 16:00:12.405867 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:48.634510 139872277903104 call_variants.py:402] Processed 1 examples in 1 batches [5816.472 sec per 100]. real	4m2.970s; user	5m54.674s; sys	1m14.107s; I0424 16:03:48.557898 140277446174464 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt""' returned non-zero exit status 247.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:6641,checkpoint,checkpoint,6641,,https://github.com/google/deepvariant/issues/304,1,['checkpoint'],['checkpoint']
Availability,"906 02:45:51.913708 257960059396112 make_examples_core.py:301] Common contigs are ['chr20']; I0906 02:45:51.914803 257960059396112 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I0906 02:45:51.916343 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.961755 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.961891 257960059396112 make_examples_core.py:301] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz; I0906 02:45:51.962091 257960059396112 make_examples_core.py:301] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz; I0906 02:45:51.962123 257960059396112 make_examples_core.py:301] Overhead for preparing inputs: 0 seconds; Fatal Python error: Segmentation fault. Current thread 0x0000ea9d01b4e010 (most recent call first):; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/realigner/window_selector.py"", line 116 in _variant_reads_threshold_selector; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/realigner/window_selector.py"", line 76 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/realigner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_v_9",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:3098,error,error,3098,,https://github.com/google/deepvariant/issues/879,2,"['error', 'fault']","['error', 'fault']"
Availability,"9168 tpu_system_metadata.py:160] *** Num TPU Cores: 8; INFO:tensorflow:*** Num TPU Workers: 1; I0524 21:18:26.631940 140032543119168 tpu_system_metadata.py:161] *** Num TPU Workers: 1; INFO:tensorflow:*** Num TPU Cores Per Worker: 8; I0524 21:18:26.631998 140032543119168 tpu_system_metadata.py:162] *** Num TPU Cores Per Worker: 8; INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:6819,Avail,Available,6819,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"9247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s; user	0m12.056s; sys	0m2.006s. ```. ----------------------------------------------------------------------------------; ----------------------------------------------------------------------------------. My system is Ubuntu 22.04. I have two GPUs. . **nvidia-smi** ; ``` ; Sat Feb 17 23:40:49 2024 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.147.05 Driver Version: 525.147.05 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Quadro RTX 4000 On | 00000000:17:00.0 Off ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:20545,error,error,20545,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,"9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:4651,Error,Error,4651,,https://github.com/google/deepvariant/issues/739,1,['Error'],['Error']
Availability,": Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run_deepvariant ; --model_type=WGS; --ref='/ref_dir/reference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/out_dir/test1.vcf.gz' ; --intermediate_results_dir='/out_dir/tmp/test1/' ; --num_shards='39' ; --make_examples_extra_args=""use_allele_frequency=true,population_vcfs=/popVCF_dir/UMAG1.POP.FREQ.vcf.gz"" ; --regions=/region_dir/regions_to_test.bed ; ```; - Error trace: (if applicable); ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/out_dir/tmp/test1/call_variants_output.tfrecord.gz"" --examples ""/out_dir/tmp/test1/make_examples.tfrecord@39.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/out_dir/tmp/test1/"". I0919 17:19:47.185331 46912500266816 call_variants.py:317] From /out_dir/tmp/test1/make_examples.tfrecord-00000-of-00039.gz.example_info.json: Shape of input examples: [100, 221, 8], Channels of input examples: [1, 2, 3, 4, 5, 6, 8, 19].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_v",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:2748,checkpoint,checkpoint,2748,,https://github.com/google/deepvariant/issues/568,1,['checkpoint'],['checkpoint']
Availability,": Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f5478ac499a: Pull complete ; b07098b67a6d: Pull complete ; 0accf0f55269: Pull complete ; ccc95462eb8f: Pull complete ; f1416983139e: Pull complete ; 2242c582e0cc: Pull complete ; 8f749be1be0b: Pull complete ; 03fdf02906f9: Pull complete ; ea2763a10d98: Pull complete ; fff529645086: Pull complete ; 42ad15be12fa: Pull complete ; 82830610edc8: Pull complete ; d1a85d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:1325,Error,Error,1325,,https://github.com/google/deepvariant/issues/462,1,['Error'],['Error']
Availability,:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_cod,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16140,error,error,16140,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,":534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_examples.py:587] Task 9/30: Created 76 examples; I0924 03:47:35.441959 139746083813120 make_examples.py:587] Task 29/30: Found 78 candidate variants; I0924 03:47:35.442209 139746083813120 make_examples.py:587] Task 29/30: Created 78 examples. real	0m5.429s; user	2m1.568s; sys	0m23.089s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0924 03:47:37.408303 140325876573952 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-24 03:47:37.413854: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-09-24 03:47:37.437208: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz; 2020-09-24 03:47:37.440001: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e41920 executing computations on platform Host. Devices:; 2020-09-24 03:47:37.440048: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-24 03:47:37.444991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-09-24 03:47:37.554617: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ea0f10 executing computations on platform CUDA. Devices:; 20",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:2515,checkpoint,checkpoint,2515,,https://github.com/google/deepvariant/issues/358,1,['checkpoint'],['checkpoint']
Availability,":59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; [[Node: train_op/control_dependency/_5647 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_14228_train_op/control_dependency"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]; Caused by op u'train_op/CheckNumerics', defined at:; File ""/tmp/Bazel.runfiles_ecWAzH/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 364, in <module>; tf.app.run(); File ""/usr/local/li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:3650,Error,Error,3650,,https://github.com/google/deepvariant/issues/69,1,['Error'],['Error']
Availability,":; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4425,error,error,4425,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,":Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based form",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1330,checkpoint,checkpoint,1330,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability,"; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; Traceback (most recent call last):; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); F",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:1310,Error,Error,1310,,https://github.com/google/deepvariant/issues/870,1,['Error'],['Error']
Availability,"; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2066,error,error,2066,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,"; `python ../bin/make_examples.zip \; --mode training \; --ref ""file/ucsc.hg19.chr20.unittest.fasta.gz"" \; --reads ""file/NA12878_S1.chr20.10_10p1mb.bam"" \; --confident_regions ""file/test_nist.b37_chr20_100kbp_at_10mb.bed"" \; --truth_variants ""file/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz"" \; --examples ""output/examples.tfrecord.gz""; `; **my-training-dataset.pbtxt file:**; `name: ""my-training-dataset""; tfrecord_path: ""/home/suanfa/Documents/wangpeng/testmake_examples/output/examples.tfrecord.gz""; num_examples: 1`. **The model_train script is:**; `python ../bin/model_train.zip \; --dataset_config_pbtxt ""./my-training-dataset.pbtxt"" \; --start_from_checkpoint ""/my/path/of/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt""; `. **The following error have happened while the model_train.zip is invoked:**; > I0502 10:58:51.903573 139632719935232 model_train.py:182] Initializing model from checkpoint at /home/suanfa/Documents/source/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt; 2018-05-02 10:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restoring parameters from /tmp/deepvariant/model.ckpt-0; I0502 10:58:57.455770 139632719935232 tf_logging.py:82] Restoring parameters from /tmp/deepvariant/model.ckpt-0; INFO:tenso",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:1320,checkpoint,checkpoint,1320,,https://github.com/google/deepvariant/issues/69,1,['checkpoint'],['checkpoint']
Availability,">=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or di",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1287,avail,available,1287,,https://github.com/google/deepvariant/issues/844,1,['avail'],['available']
Availability,"@pichuan hi, i tried running the visualizing example, but at this stage keeps giving this error below. what could have been the problem. thank you. <class 'tensorflow.core.example.example_pb2.Example'>; Traceback (most recent call last):; File ""visualize.py"", line 70, in <module>; visualize_example(example); File ""visualize.py"", line 48, in visualize_example; titles = [""reconstructed RGBA (label=%s)"" % get_label(example), ""read base"", ""base quality"", ""mapping quality"", ""strand"",; File ""visualize.py"", line 29, in get_label; return get_int64_list(example, 'label')[0]; IndexError: list index (0) out of range",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/110:90,error,error,90,,https://github.com/google/deepvariant/issues/110,1,['error'],['error']
Availability,A timeout error occurs,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/831:10,error,error,10,,https://github.com/google/deepvariant/issues/831,1,['error'],['error']
Availability,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1891,error,error,1891,,https://github.com/google/deepvariant/issues/223,1,['error'],['error']
Availability,"ATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the event that it is a public image.""; I0331 17:40:16.049175 47823917316800 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpl3fvinw4; I0331 17:40:24.867229 47384002755264 make_examples.py:386] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0331 17:40:25.244051 47384002755264 genomics_reader.py:223] Reading /scratch/moldach/bin/DEEPVARIANT/MADDOG/maddog_bam_trim_bwaMEM_sort_dedupped.bam with NativeSamReader; I0331 17:40:25.256583 47384002755264 make_examples.py:535] Preparing inputs; I0331 17:40:25.453527 47384002755264 genomics_re",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:2494,error,error,2494,,https://github.com/google/deepvariant/issues/292,1,['error'],['error']
Availability,Add missing import to avoid build failure,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/68:34,failure,failure,34,,https://github.com/google/deepvariant/pull/68,1,['failure'],['failure']
Availability,Adding DNA Sequencing Error Correction tutorial blog post.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/144:22,Error,Error,22,,https://github.com/google/deepvariant/pull/144,1,['Error'],['Error']
Availability,"After creating the directories and downloading the required files in their respective folders, I used the single command run_deeppvariant script. But it not's working. I'm attaching the screenshot. . - Operating system: MacOS; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina seq, short reads. <img width=""1510"" alt=""Screenshot 2022-12-05 at 12 09 45"" src=""https://user-images.githubusercontent.com/75676816/205648651-e8ad6b73-7139-4fa6-9b5d-b496cdcf7bc2.png"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/596:35,down,downloading,35,,https://github.com/google/deepvariant/issues/596,1,['down'],['downloading']
Availability,An error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/444:3,error,error,3,,https://github.com/google/deepvariant/issues/444,1,['error'],['error']
Availability,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:118,avail,available,118,,https://github.com/google/deepvariant/issues/248,1,['avail'],['available']
Availability,"Attempting to run variant analysis on bam file aligned to GRCh38.p13 using GRCh38.p13 as the reference but receiving the error below from make_examples. Running in Google Cloud with the following script.sh:. #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=mbh-deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:121,error,error,121,,https://github.com/google/deepvariant/issues/225,1,['error'],['error']
Availability,Availability of DeepVariant RNA-seq GTEx model,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/605:0,Avail,Availability,0,,https://github.com/google/deepvariant/issues/605,1,['Avail'],['Availability']
Availability,"BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1932,checkpoint,checkpoint,1932,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability,Bazel build error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:12,error,error,12,,https://github.com/google/deepvariant/issues/441,1,['error'],['error']
Availability,CUDA Errors in call_variants step,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:5,Error,Errors,5,,https://github.com/google/deepvariant/issues/321,1,['Error'],['Errors']
Availability,Can't Download print_F1.py,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/76:6,Down,Download,6,,https://github.com/google/deepvariant/issues/76,1,['Down'],['Download']
Availability,Cannot identify error cause in make_examples stage,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:16,error,error,16,,https://github.com/google/deepvariant/issues/207,1,['error'],['error']
Availability,"Checkpoint ""Model files do not exist"" when testing custom model",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:0,Checkpoint,Checkpoint,0,,https://github.com/google/deepvariant/issues/866,1,['Checkpoint'],['Checkpoint']
Availability,Checkpoint and examples have mismatched number of channels,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:0,Checkpoint,Checkpoint,0,,https://github.com/google/deepvariant/issues/625,1,['Checkpoint'],['Checkpoint']
Availability,Checkpointing / resuming analysis,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749:0,Checkpoint,Checkpointing,0,,https://github.com/google/deepvariant/issues/749,1,['Checkpoint'],['Checkpointing']
Availability,Conda: dv_make_example.py execution error.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314:36,error,error,36,,https://github.com/google/deepvariant/issues/314,1,['error'],['error']
Availability,CondaError: Downloaded bytes did not match Content-Length,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228:12,Down,Downloaded,12,,https://github.com/google/deepvariant/issues/228,1,['Down'],['Downloaded']
Availability,Cram support errors,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307:13,error,errors,13,,https://github.com/google/deepvariant/issues/307,1,['error'],['errors']
Availability,D:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15270,error,error,15270,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1567,checkpoint,checkpoints,1567,,https://github.com/google/deepvariant/issues/857,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,DataLoss Error with Tensorflow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:9,Error,Error,9,,https://github.com/google/deepvariant/issues/564,1,['Error'],['Error']
Availability,"Dear All,. I am trying to run gcloud alpha genomics but have recurrently encountered the same issues about authentification and docker run. . The bash file for Deep Variant and error logs are below:; BASH file [https://storage.googleapis.com/wgs-test-shan/test_samples/deepVariant.sh](url); YAML file [https://storage.googleapis.com/wgs-test-shan/test_samples/deepvariant_wes_pipeline.yaml](url); LOG file [https://storage.googleapis.com/wgs-test-shan/test_samples/runner_logs/ENjW7s2JLBjf3aql19nvyv8BIKeM6-b_FyoPcHJvZHVjdGlvblF1ZXVl-stderr.log](url). I have contacted Cloud support center and obtained suggestions as below. However this did not mend the problem. What is your suggestion? ; [https://enterprise.google.com/supportcenter/managecases#Case/001f200001TaEgT/U-14552728; ](url); Thank you.; I will appreciate your help.; Best,; Shan",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27:177,error,error,177,,https://github.com/google/deepvariant/issues/27,1,['error'],['error']
Availability,"Dear DeepVariant Team,. I have a short question. Are there any plans for calling MNPs in near future? This would be really helpfull for variant annotations further downstream. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238:164,down,downstream,164,,https://github.com/google/deepvariant/issues/238,1,['down'],['downstream']
Availability,"Dear Deepvariant team,. I was attempting to run Deepvariant on GCP by following the sample scripts from the tutorials, but it failed. I have checked the configuration regarding the Compute Engine quota and it should meet the requirements (i.e. CPU, Persistent Disk and In-use IP addresses). The error message from the log is like:; ""RuntimeError: Job failed with error ""run"": operation ""projects/deepvariant-phh/operations/7761698599878123803"" failed: executing pipeline: Execution failed: action 2: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION)"". I have read some of the related discussed issues but still can't solve my problem. The log files and my script file are attached. Your help is appreciated. . [staging_temp%2Frunner_logs_20181118_014355.log](https://github.com/google/deepvariant/files/2592663/staging_temp.2Frunner_logs_20181118_014355.log); [log.txt](https://github.com/google/deepvariant/files/2592666/log.txt). [script.txt](https://github.com/google/deepvariant/files/2592665/script.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:295,error,error,295,,https://github.com/google/deepvariant/issues/120,2,['error'],['error']
Availability,"Dear Developer,. I am going to use DeepVariant for variant calling. I have followed the instruction provided [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), to install the tool. After downloading the all needed sample data I run the command:; ```; sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Comman",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:225,down,downloading,225,,https://github.com/google/deepvariant/issues/217,2,"['down', 'error']","['downloading', 'error']"
Availability,"Dear Developers,. I compared the Snp calling results between V1.5 and V1.6 with a trio from a non-model species (a pair of parents and offspring). I used percentages of sites with violations of Mendelian error as a proxy. Results from V1.5 (WGS default model): 24%; Results from V1.6 (WGS default model): 40%. Also, results from V1.6 with the SLIM model (WGS default) from V1.5 show 24% of Mendelian errors. All other parameters and settings are the same except for the version.; It seems the newly trained model from V1.6 has a huge influence, and I'm not sure if this is a good sign (with significantly more Mendelian errors). Could you please look into this?. Any help and discussion would be appreciated. Thank you; Zuyao",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/726:204,error,error,204,,https://github.com/google/deepvariant/issues/726,3,['error'],"['error', 'errors']"
Availability,"Dear Developers,. Thank you very much for the amazing tool! I recently encountered an issue while running DeepVariant on a GIAB sample. I managed to run DeepVariant open another sample successfully so I believe there may be something wrong in the current sample input. It would be most helpful and appreciated if you could kindly take a look at the error messages. **Setup**; - Operating system: CentOS 7 x86_64; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Singularity (v3.10.0); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:349,error,error,349,,https://github.com/google/deepvariant/issues/833,2,"['down', 'error']","['downsampled', 'error']"
Availability,"Dear Devs, . I am currently training a model (starting from wgs.1.6.1) for use in a fish species. The programs are running well, I have confident regions and truth variants defined, and am currently tuning hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such traini",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:395,reliab,reliable,395,,https://github.com/google/deepvariant/issues/904,1,['reliab'],['reliable']
Availability,"Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm; ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this ; version of SciPy (detected version 1.24.2; warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>; from deepvariant import modeling; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>; from tensorflow.python.tpu import tpu_config ; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>; from tensorflow_estimator.python.estimator.tpu.tpu_config import *; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1 import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1.estimator import experimental; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>; from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder; File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>; from tensorflow_estimator.python.estimator import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>; from tensorflow.python.saved_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/673:83,error,error,83,,https://github.com/google/deepvariant/issues/673,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:95,error,error,95,,https://github.com/google/deepvariant/issues/679,1,['error'],['error']
Availability,"Dear all,; Thanks for a fantastic tool! I'm trying to further train the deepvariant models on a truth VCF for a non-human genome. When I run the make_examples script in training mode with my truth vcf and confident regions BED, I get the following error:; ```; ValueError: ('truth_variant needs genotypes to be used for labeling', reference_bases: ""A""; alternate_bases: ""G""; end: 1977; reference_name: ""NC_000962.3""; start: 1976; ); ```; I'm wondering if the truth VCF needs to be in a particular format? Or is the problem because I am working with a haploid genome and thus my truth VCF is haploid?. Thank you in advance!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/64:248,error,error,248,,https://github.com/google/deepvariant/issues/64,1,['error'],['error']
Availability,"Dear all. I am new to deepvariant. We are trying to use deepvariant on a HPC cluster with singularity.; We installed nvidia and cuda drivers through conda, and tested it with other python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:963,Error,Error,963,,https://github.com/google/deepvariant/issues/761,1,['Error'],['Error']
Availability,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/313:163,fault,fault,163,,https://github.com/google/deepvariant/issues/313,2,"['error', 'fault']","['error', 'fault']"
Availability,"Dear deepvariant team,. we are currently playing a bit around with the GPU docker version of deepvariant 0.10.0. I've noticed the following errors in the ""call_variants"" step and I'm not sure if the GPU is used correctly in this step (""cannot find working devices"" ???) or if this is just a warning because the calling went through without any abort:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp7l6e69ft/make_examples.tfrecord@25.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0703 17:18:45.593843 140322304501504 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-07-03 17:18:45.625121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-07-03 17:18:45.661204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz; 2020-07-03 17:18:45.667182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f919f0 executing computations on platform Host. Devices:; 2020-07-03 17:18:45.667206: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-07-03 17:18:45.674475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:140,error,errors,140,,https://github.com/google/deepvariant/issues/321,2,"['checkpoint', 'error']","['checkpoint', 'errors']"
Availability,"Dear developers! I am very curious to use DeepVariant on our in house data. In trying to do so, I stumbled upon an error I cannot seem to circumvent. . **Problem:**; I am trying to run my bamfile that originated from a pacbio LAA output, mapped with minimap2. I receive the error that it's unable to read any records. As I got the warGning (lol!) that --'add_hp_channel' is set but not 'parse_sam_aux_fields'. . **Initial command:**; sudo docker run -v ""2021-05-11_deepvariant_PB"":""/input/"" -v ""2021-05-11_deepvariant_PB/output_DV"":""/output/"" google/deepvariant:""1.1.0"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/ref.fasta --reads=/input/R9_Z-1707-003_cluster1_RC492.bam --output_vcf=/output/output.vcf.gz. **What I tried:**; I tried to rerun with the following extra argument: --make_examples_extra_args=""parse_sam_aux_fields=true"".; This gives me the ValueError from run_deepvariant.py that it is in conflict with the sort_by_haplotypes flag, eventhough I didn't use it. Then, I tried to add both arguments: --make_examples_extra_args=""sort_by_haplotypes=false,parse_sam_aux_fields=true"", but this gives the same ValueError. ; `ValueError: The extra_args ""parse_sam_aux_fields"" conflicts with other flags. Please fix and try again. Starting in v1.1.0, if you are running with PACBIO and want to use HP tags, please use the new --use_hp_information flag instead of using --make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`. I also tried to run the command with --sample_name=Z-1707-003_cluster1_RC492_phase0 (the RG for the bamfile), which does not give the warning anymore, but still leaves me with an empty vcf. **Tool stderr for the initial command:**; ```; I0511 12:24:29.658635 140614860437248 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpq5tvks3j. ***** Intermediate results will be written to /tmp/tmpq5tvks3j in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457:115,error,error,115,,https://github.com/google/deepvariant/issues/457,2,['error'],['error']
Availability,"Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:89,error,error,89,,https://github.com/google/deepvariant/issues/722,2,"['error', 'mainten']","['error', 'maintenance']"
Availability,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/405:228,avail,available,228,,https://github.com/google/deepvariant/issues/405,1,['avail'],['available']
Availability,"Dear,. I tried to use the singularity to run the DeepVariant docker with the following command:. BIN_VERSION=""0.10.0""; OUTPUT_DIR=$fpath; INPUT_DIR=$fpath; REF=$refpath ;. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ -B ${INPUT_DIR}:/input -B ${OUTPUT_DIR}:/output -B ${REF}:/ref \; 		docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \ ; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=150 ;. unfortunately, it's not working and give me the following error:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; line 82: --ref=/ref/human_g1k_v37.fasta: No such file or directory. FYI: Same Structure fro the Docker itself is working.; 	BIN_VERSION=""0.10.0""; 		OUTPUT_DIR=$fpath; 		INPUT_DIR=$fpath; 		REF=$refpath ;; 		sudo docker run -v ${INPUT_DIR}:/input -v ${OUTPUT_DIR}:/output -v ${REF}:/ref \; 		gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=45 ;; 	. Kind regards ; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/322:657,error,error,657,,https://github.com/google/deepvariant/issues/322,2,['error'],['error']
Availability,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:17,error,error,17,,https://github.com/google/deepvariant/issues/325,2,['error'],['error']
Availability,"Dears,; I'm working on Ubuntu 16.04.5 LTS, and Docker API version 1.39,; I have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:80,down,downloaded,80,,https://github.com/google/deepvariant/issues/223,1,['down'],['downloaded']
Availability,Deep variant error: FAILED_PRECONDITION,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118:13,error,error,13,,https://github.com/google/deepvariant/issues/118,1,['error'],['error']
Availability,DeepTrio Quickstart Test Command error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632:33,error,error,33,,https://github.com/google/deepvariant/issues/632,1,['error'],['error']
Availability,DeepTrio and type 2 error question,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450:20,error,error,20,,https://github.com/google/deepvariant/issues/450,1,['error'],['error']
Availability,DeepTrio postprocess_variants failure,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:30,failure,failure,30,,https://github.com/google/deepvariant/issues/429,1,['failure'],['failure']
Availability,"DeepVariant fails to run with test data, giving error:; ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**; running from HPC; OS info:; `cat /etc/os-release`; output:. ```; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. - DeepVariant version: **1.6.1**; - Installation method (Docker, built from source, etc.): **Docker**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**; - Command: ; ``` ; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. - Error trace: (if applicable). ```; I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds; 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:48,error,error,48,,https://github.com/google/deepvariant/issues/812,1,['error'],['error']
Availability,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/352:405,Error,Error,405,,https://github.com/google/deepvariant/issues/352,1,['Error'],['Error']
Availability,Deepvariant DataLoss Error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175:21,Error,Error,21,,https://github.com/google/deepvariant/issues/175,1,['Error'],['Error']
Availability,"Describe the issue:**; I previously used the following PopVCF [model.ckpt](https://console.cloud.google.com/storage/browser/brain-genomics-public/research/allele_frequency/pretrained_model_WGS;tab=objects?pli=1&prefix=&forceOnObjectsSortingFiltering=false) with `run_deepvariant` v.1.1 while including a PopVCF channel during `make_examples`. However, that model does not include a channel for `insert_size` as their work predates v1.4. . With the default extra channel for `'insert_size'` in v1.4, and `make_examples` having numerous options to include additional channels:; ```; --[no]use_allele_frequency: If True, add another channel for pileup images to represent allele frequency information gathered from population call sets.; (default: 'false'); --[no]add_hp_channel: If true, add another channel to represent HP tags per read.; (default: 'false'); --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. Are there `model-ckpt` files for these channel options available somewhere to provide `call_variants` via:; ```; --checkpoint: Required. Path to the TensorFlow model checkpoint to use to evaluate candidate variant calls.; ```. If so, do they include one additional channel or permutations of multiple channels?. If not, is there an alternative way to have `run_deepvariant` use different channels than what the default checkpoint contains during `call_variants`? For example, I am currently unable to include both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:922,Avail,Available,922,,https://github.com/google/deepvariant/issues/568,1,['Avail'],['Available']
Availability,"Describe the issue:; Hello, i am trying to recreate the steps done in this post: https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/; I have a bam file with phasing information which a got by running whatshap on the data and now i want use deepvariant for a second time just like the steps in the post.; I have a illumina bam file which i phased with nanopore data. I run this command:; singularity exec --bind /usr/lib/locale/; docker://google/deepvariant:${BIN_VERSION}; /opt/deepvariant/bin/run_deepvariant; --model_type PACBIO; --ref reference/GRCh38_no_alt_analysis_set.fasta; --reads whatshap/HG003.GRCh38.chr20.haplotagged.bam; --use_hp_information; --output_vcf deepvariant2/output.vcf.gz; --num_shards $(nproc); --regions chr20. And get this error :; NotImplementedError: The --use_hp_information flag has been deprecated. DeepVariant now phases internally for PacBio mode. I get what deprecated means but what is the thing i have to do to specify to deepvariant that my bam file is phased because i dont think it is using the haplotagged info in the bam file now?. Setup. DeepVariant version: latest; Installation method : source; Type of data: bam file with phasing information by whatshap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/823:776,error,error,776,,https://github.com/google/deepvariant/issues/823,1,['error'],['error']
Availability,"Describe the issue:; I can't get vcf output after bind mount a root directory. Setup; - Operating system: Windows 11, but mount an Ubuntu VM through multipass; - Type of data: fasta, bam and vcf file. Steps to reproduce:; - Command:; #Configure the DeepVariant environment variables (missing input directory,...); BIN_VERSION=""1.5.0"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; PWD=/mountpoint/fastQ; INPUT_DIR=""${PWD}/testdata_input""; mkdir -p ${INPUT_DIR}; OUTPUT_DIR=""${PWD}/04.deepvariant_out""; mkdir -p ""${OUTPUT_DIR}"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; - Error trace: ; ; It displays: Task reading input the .bam file but it ends up with 0 candidates.; I suppose it can read the input files. Does the quick start test work on your system?; This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675:1002,Error,Error,1002,,https://github.com/google/deepvariant/issues/675,1,['Error'],['Error']
Availability,"Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); INFO:tensorflow:Calling model_fn.; I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `laye",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:9181,Avail,Available,9181,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,Difference between WES/WGS error models,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/329:27,error,error,27,,https://github.com/google/deepvariant/issues/329,1,['error'],['error']
Availability,Docker pull error.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/221:12,error,error,12,,https://github.com/google/deepvariant/issues/221,1,['error'],['error']
Availability,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:39,error,error,39,,https://github.com/google/deepvariant/issues/465,3,['error'],['error']
Availability,DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1712,Error,Error,1712,,https://github.com/google/deepvariant/issues/717,1,['Error'],['Error']
Availability,"During make_examples, there is a validation step to be sure contigs; reasonably overlap. This excludes some contigs, like chrM and extra; chromosomes but does it inconsistently. The contigs get excluded from; the list to use but then not during validation. This leads to errors on; small test datasets (bcbio has a chr22/chrM dataset that exposes this,; chrM is removed and then only 50% of the bases overlap so it fails).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18:271,error,errors,271,,https://github.com/google/deepvariant/pull/18,1,['error'],['errors']
Availability,"EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:1517,ERROR,ERROR,1517,,https://github.com/google/deepvariant/issues/91,1,['ERROR'],['ERROR']
Availability,Error - reference genome files don't exist,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:0,Error,Error,0,,https://github.com/google/deepvariant/issues/543,1,['Error'],['Error']
Availability,Error about expected cur_seq.size() < Max_READ_LEN,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/798:0,Error,Error,0,,https://github.com/google/deepvariant/issues/798,1,['Error'],['Error']
Availability,Error during training with V1.6.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:0,Error,Error,0,,https://github.com/google/deepvariant/issues/722,1,['Error'],['Error']
Availability,Error encountered while running on downsampled BAM,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:0,Error,Error,0,,https://github.com/google/deepvariant/issues/833,2,"['Error', 'down']","['Error', 'downsampled']"
Availability,Error in pbc_varicall,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659:0,Error,Error,0,,https://github.com/google/deepvariant/issues/659,1,['Error'],['Error']
Availability,Error in running DeepVariant in Docker on CentOS 7,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:0,Error,Error,0,,https://github.com/google/deepvariant/issues/226,1,['Error'],['Error']
Availability,"Error in trying RNA-Seq case study , it aborted",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:0,Error,Error,0,,https://github.com/google/deepvariant/issues/581,1,['Error'],['Error']
Availability,Error on Post processing from 2 different gvcf sources,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:0,Error,Error,0,,https://github.com/google/deepvariant/issues/413,1,['Error'],['Error']
Availability,Error on testing deepvariant for WES,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:0,Error,Error,0,,https://github.com/google/deepvariant/issues/743,1,['Error'],['Error']
Availability,Error running DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/782:0,Error,Error,0,,https://github.com/google/deepvariant/issues/782,1,['Error'],['Error']
Availability,Error running DeepVariant on non-human model organism,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:0,Error,Error,0,,https://github.com/google/deepvariant/issues/292,1,['Error'],['Error']
Availability,Error running DeepVariant v1.1.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757:0,Error,Error,0,,https://github.com/google/deepvariant/issues/757,1,['Error'],['Error']
Availability,Error running DeepVariant via singularity,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783:0,Error,Error,0,,https://github.com/google/deepvariant/issues/783,1,['Error'],['Error']
Availability,Error running KMC in Giraffe case study,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/767:0,Error,Error,0,,https://github.com/google/deepvariant/issues/767,1,['Error'],['Error']
Availability,Error running bioconda installed version,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:0,Error,Error,0,,https://github.com/google/deepvariant/issues/664,1,['Error'],['Error']
Availability,Error running deepvariant from singularity in cluster,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:0,Error,Error,0,,https://github.com/google/deepvariant/issues/679,1,['Error'],['Error']
Availability,Error running example Bam File on hg19 complete genome,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/54:0,Error,Error,0,,https://github.com/google/deepvariant/issues/54,1,['Error'],['Error']
Availability,Error running example with hg19 genome,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/53:0,Error,Error,0,,https://github.com/google/deepvariant/issues/53,1,['Error'],['Error']
Availability,Error running quickstart test command,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262:0,Error,Error,0,,https://github.com/google/deepvariant/issues/262,1,['Error'],['Error']
Availability,Error while running tests on Calling variants in non-autosomal contigs,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853:0,Error,Error,0,,https://github.com/google/deepvariant/issues/853,1,['Error'],['Error']
Availability,Error while using deepvariant with a bam file that is phased,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/823:0,Error,Error,0,,https://github.com/google/deepvariant/issues/823,1,['Error'],['Error']
Availability,Error: validating pipeline: zones and regions cannot be specified together,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:0,Error,Error,0,,https://github.com/google/deepvariant/issues/96,1,['Error'],['Error']
Availability,Errors on testing DeepTrio on PacBio samples,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/742:0,Error,Errors,0,,https://github.com/google/deepvariant/issues/742,1,['Error'],['Errors']
Availability,Errors running DeepVariant with Singularity,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/780:0,Error,Errors,0,,https://github.com/google/deepvariant/issues/780,1,['Error'],['Errors']
Availability,Errors when building with Ubuntu 18.04,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:0,Error,Errors,0,,https://github.com/google/deepvariant/issues/98,1,['Error'],['Errors']
Availability,"F_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9.0; ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.9.0; ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.9.0; ++ export DV_GPU_BUILD=0; ++ DV_GPU_BUILD=0; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/home/viniws/anaconda3/bin/python; ++ PYTHON_BIN_PATH=/home/viniws/anaconda3/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; ./build_and_test.sh: line 39: bazel: command not found; + PATH=/home/viniws/bin:/home/viniws/anaconda3/bin:/home/viniws/anaconda3/bin:/home/viniws/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; ./build_and_test.sh: line 54: bazel: command not found; ```. I don't know where to start troubleshooting this. In the first error, apparently the script couldn't retrieve my Ubuntu version, but I don't know where to edit it to place it there manually.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:4318,error,error,4318,,https://github.com/google/deepvariant/issues/98,1,['error'],['error']
Availability,Failed to retrieve block: error reading file for gvcf postprocess variants for WGS,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164:26,error,error,26,,https://github.com/google/deepvariant/issues/164,1,['error'],['error']
Availability,Fatal Python error: Aborted,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:13,error,error,13,,https://github.com/google/deepvariant/issues/877,1,['error'],['error']
Availability,Fatal Python error: Segmentation fault,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:13,error,error,13,,https://github.com/google/deepvariant/issues/794,6,"['error', 'fault']","['error', 'fault']"
Availability,Fatal: python Bus Error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:18,Error,Error,18,,https://github.com/google/deepvariant/issues/646,1,['Error'],['Error']
Availability,"File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write; self._prewrite_check(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check; compat.as_bytes(self.__name), compat.as_bytes(self.__mode)); tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system; ```. I guess since the model resides in the container, it can't be updated as singularity is read-only without sudo. Would there be some way of passing the `--checkpoint ""/opt/models/pacbio/model.ckpt""` parameter, but allowing it to be saved elsewhere?. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404:2509,checkpoint,checkpoint,2509,,https://github.com/google/deepvariant/issues/404,1,['checkpoint'],['checkpoint']
Availability,"Fix an error in description: In v1.0 PacBio model, we actually only u…",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/382:7,error,error,7,,https://github.com/google/deepvariant/pull/382,1,['error'],['error']
Availability,GPU mode error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452:9,error,error,9,,https://github.com/google/deepvariant/issues/452,1,['error'],['error']
Availability,Genotype error in VCF result,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655:9,error,error,9,,https://github.com/google/deepvariant/issues/655,1,['error'],['error']
Availability,"Germline deep variant 0.8.0.; I have verified that the input mapping, the bed file, and the ref genome are all based on the same reference. The same script was used for several other samples and they completed without errors. ```; I1030 18:44:39.325231 140247813654272 make_examples.py:1164] Writing MakeExamplesRunInfo to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz.run_info.pbtxt; I1030 18:44:39.367234 140247813654272 make_examples.py:1167] Found 484 candidate variants; I1030 18:44:39.367513 140247813654272 make_examples.py:1168] Created 496 examples. real	5m38.670s; user	85m59.188s; sys	1m53.684s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""ref.fa"" --reads ""input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero exit status 16. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232:218,error,errors,218,,https://github.com/google/deepvariant/issues/232,1,['error'],['errors']
Availability,"Getting error while running deepvariant: ERROR[12893] error waiting for container: unexpected EOF. Command: ; docker run -v ""/home/input"":""/input"" -v ""/home/output/"":""/output"" gcr.io/deepvariant-docker/deepvariant:1.2.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fa --reads=/input/sorted_printReads_bwa.bam --output_vcf=/output/ERR194147_output.vcf.gz --num_shards=2 --intermediate_results_dir=/output/ --output_gvcf=/output/ERR194147_output.g.vcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482:8,error,error,8,,https://github.com/google/deepvariant/issues/482,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Given that academic HPCs do not have root privileges it is not possible to run docker and one needs to use singularity. I think it would helpful to your user-base if you added a section on singularity to either the `deepvariant-quick-start.md` or `README.md`. . I had never used containers before today so the following would have been very helpful for me getting your tool up-and-running quicker:. ## Download an existing container from Docker Hub; ```; singularity build deepvariant.simg docker://google/deepvariant; ```. ## Run Deep Variant with Singularity in one command; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Respectfully,; Matthew",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287:402,Down,Download,402,,https://github.com/google/deepvariant/issues/287,1,['Down'],['Download']
Availability,"Good morning,; i'm Ilaria, a PhD student. ; I have a problem with the command bcftools consensus: ; bcftools consensus newnodup.vcf.gz - f../../../referencegenome_genome -o consensus.fasta.; I have the file consensus.fasta but with this error:; [E::vcf_parse_format] Incorrect number of FORMAT fields at NC_059157.1:24900; [E::vcf_parse_format] Incorrect number of FORMAT fields at NC_059158.1:490105; etc.. So, how can i fix the problem? ; I used Sniffles for calling variant. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/817:237,error,error,237,,https://github.com/google/deepvariant/issues/817,1,['error'],['error']
Availability,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394:311,error,error,311,,https://github.com/google/deepvariant/issues/394,1,['error'],['error']
Availability,"Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/605:224,avail,available,224,,https://github.com/google/deepvariant/issues/605,3,"['avail', 'down']","['available', 'downloadable']"
Availability,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:288,error,error,288,,https://github.com/google/deepvariant/issues/236,3,['error'],['error']
Availability,"Hello deepvariant community,; I am running deepvariant analysis on whole genome sequencing data. The first part of the analysis, the ""make_examples"" runs without error. However, in the ""call_variants"" step, I get the following error: . ![Screen Shot 2019-04-16 at 17 10 54](https://user-images.githubusercontent.com/9975286/56221509-aacef300-606a-11e9-9302-e3c7c2a9eed2.png). My code is as follows:. N_SHARDS=""8"" \; seq 0 $((N_SHARDS-1)) | parallel -k --line-buffer \; python ../bin/make_examples.zip \; --mode calling \; --ref /Volumes/workspace/input/reference/GRCh38/Homo_sapiens_hg38.fa \; --reads /Volumes/workspace/input_march_mac7/AS-196110-LR-30685_HISATmapping/Aligned.out.bam \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@${N_SHARDS}.gz \; --gvcf /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.gvcf.tfrecord@${N_SHARDS}.gz \; --task {}. python ../bin/call_variants.zip \; --outfile /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.vcf.gz \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@8.gz \; --checkpoint /Volumes/workspace/input/models/WGS/model.ckpt \; --batch_size 32. However, when I run the same scripts on a BAM file generated using reads extracted from chromosome 1, all the steps run successfully. Please let me know where I could be going wrong with the commands.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175:162,error,error,162,,https://github.com/google/deepvariant/issues/175,3,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error?. My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/139:389,error,error,389,,https://github.com/google/deepvariant/issues/139,3,"['avail', 'error']","['available', 'error']"
Availability,"Hello ！; ; ![image](https://user-images.githubusercontent.com/34881972/34375365-21791992-eb21-11e7-913e-5796551352d2.png); The picture shows the error,how can I solve this problem?; Thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/24:145,error,error,145,,https://github.com/google/deepvariant/issues/24,1,['error'],['error']
Availability,"Hello! ; so some quick background, I am interested in SNP that are unique (= private) to each sample of my cohort. I ran DeepVariant on each sample, then GLnexus as per the best practices recommendations. I extracted unique variants using bcftools --private option. Then, I wanted to do some filtering on GQ. . Here is the GQ distribution on one of the individual vcf file (so before joint-calling), I don't have much to say about it, it makes sense ; ![GQ](https://user-images.githubusercontent.com/23341393/87663191-bb858180-c763-11ea-9579-22ab38004f05.PNG). However, here is the GQ distribution of the --private SNPs ; ![GQ_private](https://user-images.githubusercontent.com/23341393/87665589-6fd4d700-c767-11ea-99db-19a2cbbd6e4a.PNG). First, there are some values that are NA due to SNPs that have . as GQ value. That's all right, errors in sequencing / mapping I guess. In fact, my reasoning is that the --private option will enrich the SNP set in all the errors that are unique to each sequencing data set. Therefore, I was expecting that the GQ distribution would be shifted to the left. However, what we see is that, not only is it shifted to the left but the shape of the distribution is also changed. ; So I would like to know more about how GQ is exactly computed by DeepVariant. And why does the GQ seems to abruptly peak at 11-12. I also read on your blog that you consider ""high quality variants"" as the ones with a GQ of 20. Of course, owing to the distribution of GQ for the private set, setting a GQ threshold at 20 will make a big difference, as seen on this plot ; ![GQ_range_subs_cropped_for_github](https://user-images.githubusercontent.com/23341393/87666119-57b18780-c768-11ea-96ea-90e7f3b58191.PNG). Thanks a lot for your insight!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/326:835,error,errors,835,,https://github.com/google/deepvariant/issues/326,2,['error'],['errors']
Availability,"Hello! I'm experiencing an issue when trying to run make_examples. Instead of Docker we're using Singularity, and deepvariant has run before with just calling the run_deepvariant.py. . For example, this is what has worked for us in the past in our environment: . > module load singularity; > source activate $condapath/DeepVariant. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/run_deepvariant.py [...]. When trying to run make_examples, this code:. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/deepvariant/make_examples.py [...] . is now throwing this error code: . > Traceback (most recent call last):; > File ""/$softwarepath/deepvariant/deepvariant/make_examples.py"", line 35, in <module>; > from deepvariant import dv_constants; > ModuleNotFoundError: No module named 'deepvariant'. Does this mean there is a problem with our install? Any ideas or suggestions? . Thank you very much for any light you can shed on this issue!. Best, ; Haley",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/771:676,error,error,676,,https://github.com/google/deepvariant/issues/771,1,['error'],['error']
Availability,"Hello!; I've tried to run DeepVariant1.1.0 on hundreds of WES data. However, I mistakenly specified the ""model_type"" parameter as ""PACBIO"" instead of ""WES"".The program generated gvcf files normally and the gvcf files can be merged by GLnexus.; I wonder how serious is this error? Do I have to run the program again?; Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/567:273,error,error,273,,https://github.com/google/deepvariant/issues/567,1,['error'],['error']
Availability,"Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```; #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}""; INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz; OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz; BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log; ```; and the output tells me . ```; ValueError: NOT_FOUND: Could not open /input/BAM; [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```; the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685:120,avail,avail,120,,https://github.com/google/deepvariant/issues/685,3,"['avail', 'error']","['avail', 'available', 'error']"
Availability,"Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. L",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:146,error,errors,146,,https://github.com/google/deepvariant/issues/797,2,['error'],"['error', 'errors']"
Availability,"Hello, . I working through the DeepVariant set up workflow under the docker umbrella, but the variable ""ref"" indicating the path of the reference .fasta is not responding properly. . sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \ ; **<cmd-b>FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.</cmd-b>**; Pass --helpshort or --helpfull to see help on flags.; shaba033$ <cmd-b> --ref=/input/ucsc.hg19.chr20.unittest.fasta </cmd-b>\; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=4; -bash: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/181:420,error,error,420,,https://github.com/google/deepvariant/issues/181,1,['error'],['error']
Availability,"Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:396,down,downloaded,396,,https://github.com/google/deepvariant/issues/793,3,"['down', 'error']","['downloaded', 'error', 'errors']"
Availability,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454:254,avail,available,254,,https://github.com/google/deepvariant/issues/454,1,['avail'],['available']
Availability,"Hello, . Previously I had an issue where the parameters I was using were not producing checkpoints in the model training step. I know that choosing parameters has a component of guesswork and iteration, and I was wondering if there are recommendations anywhere on how to choose a starting point for model training parameters, or if there are descriptions somewhere of what changing a particular parameter is likely to do. In the run described below, I am attempting to train the model on an individual using a second individual for the training data and a third individual for the validation data, but my goal is to use multiple individuals for both the training and validation sets, akin to the project described [here](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). . Secondly, I've been having an issue lately where I submit scripts to my computing cluster, and though they are granted resources and produce a log file, the log file is empty after several days of the code running indicating no progress has been made or that the program has even initialized. I am also asking my cluster resources about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:87,checkpoint,checkpoints,87,,https://github.com/google/deepvariant/issues/840,1,['checkpoint'],['checkpoints']
Availability,"Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:54,error,error,54,,https://github.com/google/deepvariant/issues/543,3,"['Down', 'error']","['Download', 'error']"
Availability,"Hello, ; I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this?. `docker pull google/deepvariant:deeptrio-1.5.0`; ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/718:161,error,error,161,,https://github.com/google/deepvariant/issues/718,1,['error'],['error']
Availability,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:178,ERROR,ERROR,178,,https://github.com/google/deepvariant/issues/189,6,['ERROR'],['ERROR']
Availability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:115,error,error,115,,https://github.com/google/deepvariant/issues/123,6,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists.; I've googled all over and still can't solve the problem. please help me!. command：; ```; INPUT_DIR=/path1/4_Test/qingjiang/dpv; OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/QJref.fa \; --reads=""${INPUT_DIR}""/input.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; ```; error：; ```; I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s; user 0m0.211s; sys 0m0.371s; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613:16,error,error,16,,https://github.com/google/deepvariant/issues/613,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hello, I read you essay ""A universal sNP and small-indel variant caller using deep neural networks"" in nature biotechnology. It's so good. I want to download the Platinum Genomes Project NA12878 data. I check it in the supplementary information. In supplementary note 11, there is a link[https://cloud.google.com/genomics/data/platinum-genomes.]( NA12878 Platinum Genomes BAM file). However, it's missing. So I want to ask if there is a new web that I can download it.; Thank you!!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/852:149,down,download,149,,https://github.com/google/deepvariant/issues/852,2,['down'],['download']
Availability,"Hello, I trained a customized model, and am now trying to test it. However, when I try to run it, it says that the model files in the checkpoint do not exist. . Here is the command I tried to run: . > module load apptainer; > ; > apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; > --model_type WGS \; > --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/2fullindividualmodel/checkpoints/ckpt-14902"" \; > --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/Bactrocera_dorsalis_rearing_male_mt_chr_unpl.fasta"" \; > --reads ""${filesdir}_mapped/${sample}.bam"" \; > --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/modeltestout/2fullindividualmodeltest/${sample}.vcf.gz"". Here are the contents of the checkpoints folder for this training: . > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jun 29 01:06 ..; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 .; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:134,checkpoint,checkpoint,134,,https://github.com/google/deepvariant/issues/866,3,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"Hello, I'm getting error below - please let me know if I can fix it in anyway.. . [root@localhost processed]# docker run -v /home/imusayev/Documents/RNAseq:/input -v /home/imusayev/Documents/RNAseq:/output google/deepvariant:1.4.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --regions=chr16:56191390-56357444 --ref=/input/hg38/hg38.fa --reads=/input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam --output_vcf=/output/MUT60d.sorted.bam.vcf --output_gvcf=/output/MUT60d.sorted.bam.gvcf --call_variants_extra_args=use_openvino=true --num_shards=2 --logging_dir=/output/logs; Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. I1214 05:45:20.217978 140442762327872 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmpiy9bfzyx. ***** Intermediate results will be written to /tmp/tmpiy9bfzyx in docker. ****. ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hg38/hg38.fa"" --reads ""/input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam"" --examples ""/tmp/tmpiy9bfzyx/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpiy9bfzyx/gvcf.tfrecord@2.gz"" --regions ""chr16:56191390-56357444"" --task {}. I1214 05:45:33.914664 140555214505792 genomics_reader.py:222] Reading /input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam with NativeSamReader; I1214 05:45:33.947415 140555214505792 make_examples_core.py:243] Task 1/2: Preparing inputs; I1214 05:45:34.038768 140555214505792 genomics_reader.py:222] Reading /input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam with NativeSamReader; I1214 05:45:34.078675 140555214505792 make_examples_core.py:243] Task 1/2: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18',",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:19,error,error,19,,https://github.com/google/deepvariant/issues/597,1,['error'],['error']
Availability,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312:739,down,downsampling,739,,https://github.com/google/deepvariant/issues/312,2,['down'],['downsampling']
Availability,"Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:195,error,error,195,,https://github.com/google/deepvariant/issues/511,1,['error'],['error']
Availability,"Hello, more of a question than an issue: what does the ""Could not create PileupImage for candidate"" mean during make_examples? What triggers it?. **Setup**; - DeepVariant version: 1.1.0 and 1.3.0; - Installation method: Singularity; - Type of data: illumina on a Pinus genome (big, repetitive genome). **Error trace** ; ```; W1203 19:21:43.514668 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:9 ; W1203 19:21:43.515001 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:83 ; W1203 19:21:43.515132 139865530996480 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_1:91 ; W1203 19:21:48.118362 140507900241664 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_4:129804; W1203 19:21:51.183064 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:82 ; W1203 19:21:51.183443 139737683482368 make_examples.py:1855] Could not create PileupImage for candidate at scaffold_19:106 ; ```. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/512:304,Error,Error,304,,https://github.com/google/deepvariant/issues/512,1,['Error'],['Error']
Availability,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/422:80,error,error,80,,https://github.com/google/deepvariant/issues/422,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:526,checkpoint,checkpoint,526,,https://github.com/google/deepvariant/issues/392,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Hello,. I am facing the below error when running built_and_test.sh. I have protocol buf built and installed in my home directory. (19:11:12) ERROR: /uufs/chpc.utah.edu/common/home/u1142888/deepvariant/bazel_tmp/_bazel_u1142888/bc41070ad1d30708841b968fbd6bc540/external/com_google_protobuf/BUILD:104:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): gcc failed: error executing command . I have built the latest protobuf from source following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) and have installed it under $HOME/protobuf and updated my PATH and LD_LIBRARY_PATH accordingly. Looking for some pointers on how to resolve this issue by tweaking the build procedure. Thank you,; Ram",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94:30,error,error,30,,https://github.com/google/deepvariant/issues/94,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:190,down,downloaded,190,,https://github.com/google/deepvariant/issues/93,3,"['ERROR', 'down', 'error']","['ERROR', 'downloaded', 'error']"
Availability,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/443:186,error,error,186,,https://github.com/google/deepvariant/issues/443,2,"['echo', 'error']","['echo', 'error']"
Availability,"Hello,. I get an error about a missing index file for the used fasta. But it is available in the specified folder. BIN_VERSION=""0.10.0"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fasta --reads=/input/input.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=15. I0529 12:24:27.041167 139828907894528 make_examples.py:535] Task 5/15: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fasta.fai: No such file or directory. Do you have any idea why this happens?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/310:17,error,error,17,,https://github.com/google/deepvariant/issues/310,2,"['avail', 'error']","['available', 'error']"
Availability,"Hello,. I have a trouble to control the %cpu in Linux when deepvariant run call_variant.py step.; It makes %cpu is too high to affect another user to run his program. ![Pasted image 20240201133331(1)](https://github.com/google/deepvariant/assets/91660863/8f6a76bf-5d27-4a27-8175-417a644f396e). How could I make it lower than this picture show?. Please look at my picture, And put my code down. . docker run \; -u ""$(id -u)"":""$(id -g)"" \; -v ""${hg19}"":""/home/luohaosen/ref"" \; -v ""${INPUT_DIR}"":""/home/luohaosen/input"" \; -v ""${OUTPUT_DIR}"":""/home/luohaosen/output"" \; luohaosen/deepvariant:v1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/home/luohaosen/ref/ucsc.hg19.fa \; --reads=/home/luohaosen/input/TKQX230060435-1A.MarkDuplicates.bam \; --output_vcf=/home/luohaosen/output/TKQX230060435.deepvariant.vcf.gz \; --intermediate_results_dir=/home/luohaosen/output/intermediate_results_dir \; --num_shards=$num_shards. (Note：luohaosen//deepvariant:v1 is based on google/deepvariant:1.6.0，It is almost the same as origin google/deepvariant:1.6.0 docker image)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/768:388,down,down,388,,https://github.com/google/deepvariant/issues/768,1,['down'],['down']
Availability,"Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:703,down,download,703,,https://github.com/google/deepvariant/issues/166,1,['down'],['download']
Availability,"Hello,. I tried running ""_run_deepvariant_keras.py_"" script from the latest release of deepvaraint and faced some issues while running the keras-based call variant module. . I used the following command to run:. `; python bazel-out/k8-opt/bin/deepvariant/run_deepvariant_keras.py --model_type=WGS --ref=ref.fa --reads=reads.bam --regions ""chr19"" --output_vcf=${OUTPUT_DIR}/output.vcf.gz --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz --intermediate_results_dir ${OUTPUT_DIR}/intermediate_results_dir --num_shards=10; `. The above command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:635,error,error,635,,https://github.com/google/deepvariant/issues/636,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Hello,. I want to suggest here to left-aligne indel in the DeepVariant output VCF / gVCF to avoid the issue described below. I'm running DeepVariant v1.1.0 on a set of samples sequenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/487:580,down,downstream,580,,https://github.com/google/deepvariant/issues/487,2,['down'],['downstream']
Availability,"Hello,. I'm trying to apply deepvariant v1.0.0 to a small cohort to generate multi-sample VCF. I've followed the steps described in your tutorial on multi-sample calling, so:; 1. generate single sample g.vcf with deepvariant; 2. merge g.vcf using GLnexus. However, I'm confused by the output in the multi-sample VCF. Looking at the variants I've found several positions where one or more of the samples are reported as `0/0` genotype, but with DP = 0. In this case, I assume the position has no reads in the data and so I'm expecting a missing genotype (`./.`) not a hom ref. This error can really mess up segregation analysis.; Trying to understand what is going on, I've looked at the single g.vcf generated by deepvariant for some of this position and I've noticed that the errors seem related to variants output as a hom ref block with MIN_DP zero in the g.vcf file. See the following example:. **multi-sample VCF:**; ```; chr1 72787 chr1_72787_C_T C T 18 . AF=0.5;AQ=18 GT:DP:AD:GQ:PL:RNC 0/0:0:0,0:1:0,3,29:.. 1/1:2:0,2:6:18,9,0:..; ```; **g.vcf sample1:**; ```; chr1 72121 . A <*> 0 . END=73000 GT:GQ:MIN_DP:PL 0/0:1:0:0,3,29; ```; **g.vcf sample2:**; ```; chr1 72787 . C T,<*> 18.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:10:2:0,2,0:1,0:18,9,0,990,990,990; ```. Instead, if the g.vcf line is like the one below the genotype is correctly reported as missing in the multi-sample VCF.; ```; chr1 20595 . A <*> 0 . END=20595 GT:GQ:MIN_DP:PL ./.:0:4:17,0,77; ```. So a couple of questions:; 1. Is this behavior expected for deepvariant or is it a kind of bug?; 2. How to interpret a g.vcf block like the one above with MIN_DP zero? Does it mean that some of the positions in this block have actually non-zero coverage or all of them have zero? In the first case, I suggest splitting the block when a position has zero coverage; in the second case, it is probably better to output the block with `./.` genotype.; 3. How can I interpret `0/0` genotypes with zero DP in the multi-sample VCF? Currently, I assume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346:581,error,error,581,,https://github.com/google/deepvariant/issues/346,2,['error'],"['error', 'errors']"
Availability,"Hello,. I'm trying to run DeepVariant using ultima data (cram file).; I get information that deepvariant tool provides --enable_joint_realignment and --p_error in DeepVariant 1.5.0 release page.; But, I got error message when I am trying to use --enable_joint_realignment options.. Can I get some advice which custom channels or options I should use to run deepvariant using ultima data?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711:207,error,error,207,,https://github.com/google/deepvariant/issues/711,1,['error'],['error']
Availability,"Hello,. I'm trying to run Deepvariant using singularity. I just followed the ""Notes on Singularity"" section in quick start test (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:225,error,error,225,,https://github.com/google/deepvariant/issues/746,1,['error'],['error']
Availability,"Hello,. I've been trying to set up the **google/deepvariant:1.6.1-gpu** or **google/deepvariant:latest-gpu** image on a GPU instance, but I've encountered the error message mentioned below when running the **run_deepvariant** or **train** scripts, and despite generating the flags (screenshot) as expected, I believe those incompatible/missing TensorRT libraries are preventing these scripts from using the GPU. **Command used:** ; ` sudo docker run --runtime=nvidia --gpus 1 google/deepvariant:1.6.1-gpu train --help; `. **Error message:**; ```; 2024-05-08 15:11:26.358196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-05-08 15:11:26.358229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. ![image](https://github.com/google/deepvariant/assets/169280348/fd17bf4e-0b6c-46b7-b5e8-74a3525d07a5)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819:159,error,error,159,,https://github.com/google/deepvariant/issues/819,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hello,. Please find below the description of an issue with the flag --output_gvcf_merged in DeepTrio.; Thanks for considering this request; Fred-07. **Describe the issue:**; Running the ""DeepTrio quick start"" commands with the additional flag --output_gvcf_merged produces no error but the""merged"" file is not created.; https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-quick-start.md; All other expected files are created. **Setup**; - Operating system: Linux 3.10.0-1160.71.1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method: singularity pull from docker, LSF as batch system; - Type of data: ""DeepTrio quick start"" data. **Steps to reproduce:**; - Command: additional flag; `--output_gvcf_merged ""${OUTPUT_DIR}""/ALL.g.vcf.gz`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/544:276,error,error,276,,https://github.com/google/deepvariant/issues/544,1,['error'],['error']
Availability,"Hello,. Thank you very much for providing the software. However, when I run deepvariant v1.1.0, I encounter the following file error. I did not encounter this error before. I hope you can help me solve this problem. Thank you very much for your assistance. Below is my commond:. BIN_VERSION=""1.1.0""; INPUT_DIR=""/data/lilab/hli1/Workspace/DATA/run_out_data/sorted_BAM/vs_F/6_2_vsF""; #DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/run_DeepVariant_6-20231219""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/input/F_unphased.Chr.v2.fa \; 	--reads=/input/6ccs_2_vsF.sorted.bam \; 	--output_vcf=/output/output.vcf.gz \; 	--output_gvcf=/output/output.g.vcf.gz \; 	--intermediate_results_dir /output/intermediate_results_dir \; 	--num_shards=30; [DeepVariant-v1.1.0.docx](https://github.com/google/deepvariant/files/13722568/DeepVariant-v1.1.0.docx)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757:127,error,error,127,,https://github.com/google/deepvariant/issues/757,2,['error'],['error']
Availability,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486:186,down,downstream,186,,https://github.com/google/deepvariant/issues/486,2,"['down', 'fault']","['downstream', 'faulty']"
Availability,"Hello,. some of you might remember me. I know Deepvariant works well in human and in some species like rice, if I recall well. In short, all species with (very) low heterozygousity. I wonder if you see a use for Deepvariant in other species, like, there are marine species that are so ancient, diverse, widespread, you can have 5% heterozygosity, in shorts, SNPs everywhere. In such cases, Deepvariant has a tendency to ""ditch"" apparently at random (Sample1 Chrom3:20456 called, Sample2 same position not called, despite obvious evidence from mapping and support from long reads). Probably because it didn't learn what to do with so many SNPs. You know the issue because of your mosquito blog spot. And I have seen other issues (including mine) talking about that. The issue is to have a gold standard like in human, or trio data like in the mosquito, you need specific conditions, it seems difficult to imagine this could be doable with, let's say, a deep sea coral (just random example, I don't actually know what's their genome like). . Could a synthetic dataset help here? What if we feed Deepvariant a genome we made up based on what we can observed visually? I am aware if we make an error it will learn errors, but I wanted your opinion, because the lack of high quality reference dataset for many species, seems to be a serious limitation for this kind of program. Thanks a lot. Since it's not the first time I bring this out, I understand if you would simply close this. Have a good week everyone.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872:1190,error,error,1190,,https://github.com/google/deepvariant/issues/872,2,['error'],"['error', 'errors']"
Availability,"Hello,; I am using docker command first time, so not much aware of it, i am trying to run deepvariant, tried out almost all possible method to make changes in my command, but i might be missing some minor error in my command, which i may not be able to rectify, The error i am getting is docker: invalid reference format.; See 'docker run --help'. (For this i followed the given parameter format in run_deepvariant "" --ref: Required. Genome reference to use. Must have an associated FAI index as well. Supports text or gzipped references. Should match the reference used to align the BAM file; provided to --reads.; ""); it would be great help if anyone help me to sort out this issue.; I am attaching my command here. docker run -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3 -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result -v /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker:/media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker google/deepvariant:{BIN_VERSION=""1.6.1""} python /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py --reads /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam --ref /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz --report_title MITO60_Stats --sample_name MITO60 --output_v",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829:205,error,error,205,,https://github.com/google/deepvariant/issues/829,2,['error'],['error']
Availability,"Hello,; I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:88,error,error,88,,https://github.com/google/deepvariant/issues/725,3,"['Error', 'checkpoint', 'error']","['Error', 'checkpoint', 'error']"
Availability,"Hello,; I've been trying to run deepvariant, but I keep getting the same error:. d not read base quality scores; 2020-02-10 20:52:23.436669: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/0_2762: Not found: Could not read base quality scores; 2020-02-10 20:52:23.436770: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/22518_26356: Not found: Could not read base quality scores; 2020-02-10 20:52:23.436874: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/2839_6783: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437269: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/14677_18575: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437378: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_18",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270:73,error,error,73,,https://github.com/google/deepvariant/issues/270,1,['error'],['error']
Availability,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/289:99,error,error,99,,https://github.com/google/deepvariant/issues/289,1,['error'],['error']
Availability,"Hello. OS: Scicore Cluster, Linux; Deep Variant version:1.2.0; Installation: Singularity; Instrument: Ilumina; Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:639,error,error,639,,https://github.com/google/deepvariant/issues/515,1,['error'],['error']
Availability,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:26,error,error,26,,https://github.com/google/deepvariant/issues/489,1,['error'],['error']
Availability,"Hello; I download 40 sample in 1kgp, first I use oqfe to BWA; Then I send the CRAM to Deepvariant to call variants.; But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis?; So if someone give me some explain or advice?; very Thanks!; ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/670:9,down,download,9,,https://github.com/google/deepvariant/issues/670,3,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,"Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5.; First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant.; I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp.; Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py.; The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below.; ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png); Can you give me a detailed explanation of this detail？ Thank you very much！ ; Finally, thank you very much for developing such a great tool！",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:36,down,download,36,,https://github.com/google/deepvariant/issues/616,1,['down'],['download']
Availability,"Helo everyone,. I tested DeepVariant 1.5.0 on quickstart-testdata, using WGS model and got the following error in call_variants.py script:. I0405 09:07:48.961947 140199643854656 call_variants.py:317] From ./examples.tfrecord-00000-of-00032.gz.example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6].; Traceback (most recent call last):; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" &&",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:105,error,error,105,,https://github.com/google/deepvariant/issues/625,1,['error'],['error']
Availability,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179:148,fault,fault,148,,https://github.com/google/deepvariant/issues/179,1,['fault'],['fault']
Availability,"Hey, I tried to run the docker run single command following step by step from your instructions on macOS. Unfortunately, I ran into an error that couldn't give me any idea what's wrong! I attached the screenshot, so you can see the error. ; <img width=""1512"" alt=""Screenshot 2022-12-01 at 14 28 42"" src=""https://user-images.githubusercontent.com/75676816/205064985-34617012-9e82-4734-88a6-0d702f7bd445.png"">. Thanks,; Anil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/593:135,error,error,135,,https://github.com/google/deepvariant/issues/593,2,['error'],['error']
Availability,"Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149:245,echo,echo,245,,https://github.com/google/deepvariant/issues/149,2,"['echo', 'error']","['echo', 'error']"
Availability,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:234,avail,available,234,,https://github.com/google/deepvariant/issues/269,1,['avail'],['available']
Availability,"Hi @pichuan,; I did come across various bits in different documents on how to train custom checkpoint but i don't have a confident on handle on it before I embark. - Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5); - Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?; - Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5); Thank you for guidance!; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765:91,checkpoint,checkpoint,91,,https://github.com/google/deepvariant/issues/765,1,['checkpoint'],['checkpoint']
Availability,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265:163,error,errors,163,,https://github.com/google/deepvariant/issues/265,1,['error'],['errors']
Availability,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. I’m looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/235:43,error,error,43,,https://github.com/google/deepvariant/issues/235,5,"['Error', 'error']","['Error', 'error']"
Availability,"Hi DeepVariant,; I am getting below errors on running DeepTrio on the provided PacBio samples using singularity:. ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""./output/HG002.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_child.tfrecord@128.gz"" --gvcf_outfile ""./output/HG002.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""./output/HG003.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent1.tfrecord@128.gz"" --gvcf_outfile ""./output/HG003.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""./reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""./output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --outfile ""./output/HG004.output.vcf.gz"" --cpus 0 --nonvariant_site_tfrecord_path ""./output/intermediate_results_dir/gvcf_parent2.tfrecord@128.gz"" --gvcf_outfile ""./output/HG004.g.vcf.gz"". Traceback (most recent call last):; File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/var/tmp/Bazel.runfiles_m2211dcw/runfiles/com_google_deepvariant/deepvariant/postpr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/742:36,error,errors,36,,https://github.com/google/deepvariant/issues/742,1,['error'],['errors']
Availability,"Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \; --mode calling \; --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs; 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options; options.exclude_calling_regions",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155:39,error,error,39,,https://github.com/google/deepvariant/issues/155,1,['error'],['error']
Availability,"Hi Developers,. I am getting the following error when running a WES case through version 0.7.0 of deep variant:. Deep variant error: cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored' code: FAILED_PRECONDITION. However, I am able to successfully run the same files though v0.6.1. I am unable to understand what the above error means. I looked at FAILED_PRECONDITION [here](https://cloud.google.com/genomics/reference/rest/Shared.Types/Code) but it was not helpful. I will appreciate if someone can point the mistake in the script for running v_0.7.0 of deepvariant. ; Operation id: ENLB84XvLBiClY23zqHw1vkBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl; I am attaching the complete error file: UDN668131.err. script for v_0.7.0: deepvariant_v0.7.0_UDN668131.sh.txt. scripts for v0.6.1: deepvariant_v0.6.1_UDN668131.sh.txt; deepvariant_v0.6.1_UDN668131.yaml.txt; [UDN668131_error.txt](https://github.com/google/deepvariant/files/2563031/UDN668131_error.txt); [deepvariant_v0.7.0_UDN668131.sh.txt](https://github.com/google/deepvariant/files/2563033/deepvariant_v0.7.0_UDN668131.sh.txt); [deepvariant_v0.6.1_UDN668131.sh.txt](https://github.com/google/deepvariant/files/2563036/deepvariant_v0.6.1_UDN668131.sh.txt); [deepvariant_v0.6.1_UDN668131.yaml.txt](https://github.com/google/deepvariant/files/2563037/deepvariant_v0.6.1_UDN668131.yaml.txt). Thanks,; Shruti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118:43,error,error,43,,https://github.com/google/deepvariant/issues/118,4,['error'],['error']
Availability,"Hi Developers,. I get the following error when I am trying to use the latest version (V0.7.0) of deepvariant for wes analysis. I encounter the same error even I used the example given [here](https://cloud.google.com/genomics/docs/tutorials/deepvariant). ; ./deepvariant_v0.7.0_UDN644883_wes_09202018.sh; ERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating pipeline: zones and regions cannot be specified together. I have attached my script for the reference. ; [deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt](https://github.com/google/deepvariant/files/2403088/deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt). I was able to run deepvariant for wes using V0.6.1 successfully however looking at the output vcf I 'think' the script is not restricting the variants to the bed file regions. ; [deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt](https://github.com/google/deepvariant/files/2403091/deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt); [deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt](https://github.com/google/deepvariant/files/2403092/deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt). Thanks,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:36,error,error,36,,https://github.com/google/deepvariant/issues/96,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi I want to run DeepVariant but i faced an error . **Describe the issue:**; `#!/bin/bash; BIN_VERSION=""0.10.0""; N_SHARDS=""4""; BASE=""/tera/home/phswin92/WGS/Variant_Call/DeepVariant""; INPUT_DIR=""$BASE/input""; mkdir $BASE/output; OUTPUT_DIR=""$BASE/output"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$INPUT_DIR/GRCh38_full_analysis_set_plus_decoy_hla.fa \; --reads=$INPUT_DIR/$2_Markdup_sort.bam \; --output_vcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.vcf.gz \; --output_gvcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.g.vcf.gz \; --num_shards=$N_SHARDS; `; this is my code. ![image](https://user-images.githubusercontent.com/68986997/91021146-4f355200-e62e-11ea-96bd-40bbe6a53c94.png); and this is my file in 'input' folder. error; `***** Running the command:*****; time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}. I0824 08:09:22.428504 140501281453824 make_examples.py:386] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. [E::hts_open_format] Failed to open file /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/337:44,error,error,44,,https://github.com/google/deepvariant/issues/337,2,['error'],['error']
Availability,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:659,avail,available,659,,https://github.com/google/deepvariant/issues/50,1,['avail'],['available']
Availability,"Hi Team,. I was trying to run below command for call_variants . docker run -it -v /gpfs/projects/bioinfo/najeeb/withKhalid/PMC/:/dv2/PMC01/ -v ${PWD}:/${PWD} -v /gpfs/data_jrnas1/ref_data/Homo_sapiens/GRCh37/Sequences/:/dv2/WholeGenomeSequence/ gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/call_variants --outfile ""$PWD/PMC01-01_AB082422_S5_L005_R1_001_output.tfrecord.gz"" --examples ""$PWD/PMC01-01_AB082422_S5_L005_R1_001.tfrecord.gz"" --checkpoint ${PWD}/models/model.ckpt. But for some reason , I am getting below error. I1108 10:29:03.150824 140295000123136 call_variants.py:283] Set KMP_BLOCKTIME to 0; 2018-11-08 10:29:03.161879: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 FMA; 2018-11-08 10:29:03.168258: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1108 10:29:03.176211 140295000123136 modeling.py:318] Initializing model with random parameters; W1108 10:29:03.177896 140295000123136 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpkXijQm; I1108 10:29:03.178158 140295000123136 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_session_config': None, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9889d6e750>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpkXijQm', '_train_distribute': None, '_save_summary_steps': 100}; I1108 10:29:03.178364 140295000123136 call_variants.py:341] Writing calls to /gpfs/project",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:458,checkpoint,checkpoint,458,,https://github.com/google/deepvariant/issues/117,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:396,error,error,396,,https://github.com/google/deepvariant/issues/154,1,['error'],['error']
Availability,"Hi all,. I am currently running some tests using deepvariant and getting error codes that I do not understand.; I hope you can help me finding the error. (Sorry for the long post, but the devil is probably in the details!!). Script:; #!/bin/bash; #set -euo pipefail; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/deep_output; STAGING_FOLDER_NAME=stage; OUTPUT_FILE_NAME=deeptest_FB4_chr20.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:73,error,error,73,,https://github.com/google/deepvariant/issues/129,2,['error'],['error']
Availability,"Hi all,. I have encountered the following error when running deepvaraint on a wgs file. ‘[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.’ I have googled it but I am not sure what is causing this issue and how to resolve it. I will appreciate your help. ; I have attached the full error log, bash and yaml files. . error:; code: 10; message: |-; 11: Docker run failed: ASS""\ncalls {\n info {\n key: ""AD""\n value {\n values {\n int_value: 8\n }\n values {\n int_value: 7\n }\n }\n }\n info {\n key: ""DP""\n value {\n values {\n int_value: 15\n }\n }\n }\n info {\n key: ""GQ""\n value {\n values {\n int_value: 12\n }\n }\n }\n…….; ……….; ………..Reading /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfReader\nI0806 01:37:07.984452 140434439055104 postprocess_variants.py:593] Writing output to VCF file: /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf\nI0806 01:37:08.052251 140434439055104 genomics_writer.py:118] Writing /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfWriter\n**[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.** See logs at gs://gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/deepvariant_staging_folder/logs/']]. Operation ID: ENqznd7QLBi1jMjtufCo3ckBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl. Best,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University. [UDN631726_gvcf_error_08052018.log](https://github.com/google/deepvariant/files/2263671/UDN631726_gvcf_error_08052018.log); Since GIT does not allow me to attach .sh or .yaml files, I am saving them as text files and attaching.; [deepvariant_v0.6.1_UDN631726.yaml.txt](https://github.com/google/deepvariant/files/2263680/deepvariant_v0.6.1_UDN631726.yaml.txt); [deepvariant_v0.6.1_UDN631726.sh.txt](https://github.com/google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86:42,error,error,42,,https://github.com/google/deepvariant/issues/86,3,['error'],['error']
Availability,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:142,error,error,142,,https://github.com/google/deepvariant/issues/263,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi all,. This is not strictly a DeepVariant issue, but it is an issue I ran into while following the Giraffe case study using exome FASTQs from [here](https://www.internationalgenome.org/data-portal/sample/NA12878) (specifically, `SRR1518158_*.fastq.gz`). I'm running this on a DNAnexus cloud workstation (`mem1_ssd1_v2_x8`) and using the same version of KMC used in the case study. I am trying to run KMC using the following command:. ```; TMPDIR=$(mktemp -d); time ./kmc -k29 -okff -t8 sra.fq.paths ./sra.fq $TMPDIR; ```. where `sra.fq.paths` is the result of `ls SRR1518158_*.fastq.gz > sra.fq.paths`. The error message is simply `Error: unknown exception`. Have you seen this before? I thought I'd ask here before filing an issue with the KMC repo. Thanks,; Samantha",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/767:609,error,error,609,,https://github.com/google/deepvariant/issues/767,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi deepvariant developer,; I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome.; I used the following commands in docker:. python ../binaries/make_examples.zip \; --mode calling \; --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \; --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \; --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \; --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \; --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \; --task 0. python ../bin/call_variants.zip \; --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \; --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \; --checkpoint ../models/model.ckpt \; --batch_size 32. python ../bin/postprocess_variants.zip \; --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \; --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \; --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version?. Many Thanks in advance; Ferdinand",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/158:1060,checkpoint,checkpoint,1060,,https://github.com/google/deepvariant/issues/158,1,['checkpoint'],['checkpoint']
Availability,"Hi everyone,. I tested DeepVariant 1.1.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:; `I0519 14:15:32.843033 139847781275392 call_variants.py:338] Shape of input examples: [100, 221, 8]; Traceback (most recent call last):; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:109,error,error,109,,https://github.com/google/deepvariant/issues/458,1,['error'],['error']
Availability,"Hi i have deepvariant 1.5.0 version singularity SIF file,; I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below; #nextflow.config; ```; singularity {; process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'; cacheDir = ""/data/shared/clinical/LongRead/cache/""; singularity.enabled = true; singularity.autoMounts = true; SINGULARITY_BINDPATH = ""/data""; }. conda; {; enabled = true; cacheDir = ""/data/shared/clinical/LongRead/Programs/""; }; params; {; path=""/data/shared/clinical/LongRead/""; ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""; pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi""; at=18; st=6; data_input=""/data/shared/clinical/LongRead/Data/""; }; ```; #deepvariant.nf; ```; process pbc_varicall {; publishDir ""/data/shared/clinical/LongRead/Data/resources/""; container 'docker://google/deepvariant:1.5.0'. input:; path 'fa'; output:; file ""*""; path 'm84011_220902_175841_NF_sif.vcf.gz'. script:; """"""; run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40; """"""; }. workflow {; fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""); pbc_varicall(fa); }; ```; after running for several hours i do not get any output, instead during run , i get msg as; `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `; this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659:1593,error,error,1593,,https://github.com/google/deepvariant/issues/659,1,['error'],['error']
Availability,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170:88,error,error,88,,https://github.com/google/deepvariant/issues/170,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/444:69,error,error,69,,https://github.com/google/deepvariant/issues/444,1,['error'],['error']
Availability,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container 🤷‍♂️. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:161,down,downloaded,161,,https://github.com/google/deepvariant/issues/199,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/436:68,avail,available,68,,https://github.com/google/deepvariant/issues/436,2,"['avail', 'down']","['available', 'download']"
Availability,"Hi, . I am running DeepVariant using the complete version of the genome ( hg19 with all chromsomes) and the example BAM file provided by google ( NA12878_S1.chr20.10_10p1mb.bam). I get an error when postprocessing. The command executed is: ; ```; /opt/deepvariant/bin/postprocess_variants --ref ""hg19.fa.gz"" --infile call_variants_output.tfrecord --outfile ""NA12878_S1.chr20.10_10p1mb.bam.vcf""; ```. and the make_file is called using:; ```; --regions chr20:10,000,000-10,010,000; ```. and I get the error:. ```; 2018-03-06 12:06:27.034036: I deepvariant/postprocess_variants.cc:87] Read from: call_variants_output.tfrecord; 2018-03-06 12:06:27.034770: I deepvariant/postprocess_variants.cc:96] Done reading: call_variants_output.tfrecord. #entries in single_site_calls = 82; 2018-03-06 12:06:27.034797: I deepvariant/postprocess_variants.cc:100] Total #entries in single_site_calls = 82; 2018-03-06 12:06:27.034805: I deepvariant/postprocess_variants.cc:102] Start SortSingleSiteCalls; 2018-03-06 12:06:27.034819: F deepvariant/core/utils.cc:84] Check failed: pos_in_fasta != contig_name_to_pos_in_fasta.end() Reference name chr20 not in contig info. ```. I appreciate any kind of help.; Thanks,; Luisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/54:188,error,error,188,,https://github.com/google/deepvariant/issues/54,2,['error'],['error']
Availability,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:178,recover,recover,178,,https://github.com/google/deepvariant/issues/214,2,['recover'],['recover']
Availability,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314:229,error,error,229,,https://github.com/google/deepvariant/issues/314,1,['error'],['error']
Availability,"Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:940,error,errors,940,,https://github.com/google/deepvariant/issues/19,1,['error'],['errors']
Availability,"Hi, . when I run deepVariant, using 2 shards for the makeExample, the call_variants fails.; The error is : ; ```; ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. One confusing thing that happens: ; The error only shows up only when running with my own BAM file ( mapped to hg19 ) but when I run deepVariant with the example BAM file provided by Google this does not happen. I would appreciate any kind of help. Thanks a lot, ; Luisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52:96,error,error,96,,https://github.com/google/deepvariant/issues/52,2,['error'],['error']
Availability,"Hi, ; I have no problem with running deepvariant on example BAM on google cloud, but the following error pops out when I switch to my BAM. ```; [05/03/2018 23:30:18 INFO discovery.py] URL being requested: GET https://genomics.googleapis.com/v1alpha2/operations?filter=projectId+%3D+isb-cgc-06-0004+AND+labels.job-id+%3D+call-varia--root--180503-233007-45&alt=json&pageSize=256; call-varia--root--180503-233007-45: FAILURE; [u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/03/2018 23:30:18 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-XX-XXXX', '--logging', 'gs://XXXXX/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:99,error,error,99,,https://github.com/google/deepvariant/issues/70,8,"['ERROR', 'Error', 'FAILURE', 'error']","['ERROR', 'Error', 'FAILURE', 'error']"
Availability,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228:160,error,errors,160,,https://github.com/google/deepvariant/issues/228,3,"['Down', 'down', 'error']","['Downloaded', 'downloaded', 'errors']"
Availability,"Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:355,error,error,355,,https://github.com/google/deepvariant/issues/172,1,['error'],['error']
Availability,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:171,ERROR,ERROR,171,,https://github.com/google/deepvariant/issues/355,2,"['ERROR', 'avail']","['ERROR', 'avail']"
Availability,"Hi, I am trying to call some targeted mutations from multiple targeted regions and chromosomes by Deepvariant. I followed the manual of make_examples.py to separate the multiple regions by space(https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/make_examples.py#L116), but it showed an error like the following:; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/Homo_sapiens_assembly19.fasta --reads=/input/proper.bam ; --regions=""3:178936057-178936106 3:178952054-178952106"" ; --output_vcf=/output/outputch3.vcf.gz --output_gvcf=/output/outputchr3.g.vcf.gz ; --num_shards=1 --make_examples_extra_args vsc_min_fraction_snps=0.004; E0416 22:33:24.439428 140413429749504 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xs3AuO/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; I wonder whether this issue is due to the chromosome index of my sample is 1-22 without ‘chr’ and how to run it with multiple regions and multiple chromosomes. Another issue is that when I changed the region to the bed file which includes all targeted regions, the variants called are all false-positive and none true variants included. . Furthermore, my sample is from deep sequencing on targeted regions. I have followed issue #62 to change the pileup_image_hight and downsample_fraction options, but the results are not improved. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/305:332,error,error,332,,https://github.com/google/deepvariant/issues/305,3,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:13,error,error,13,,https://github.com/google/deepvariant/issues/184,1,['error'],['error']
Availability,"Hi, I got the error in below. ```; # docker run \; -v ${HOME}:${HOME} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --logging_level DEBUG \; --mode calling \; --ref ""${REF}"" \; --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; > tf.app.run(); > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; > _sys.exit(main(argv)); > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options; > with sam.SamReader(flags_obj.reads) as sam_reader:; > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; > self._reader = self._native_reader(input_path, **kwargs); > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader; > return NativeSamReader(input_path, **kwargs); > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; > random_seed=random_seed)); > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header; ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header; ![2019-01-15 15 24 38](https://user-images.githubusercontent.com/4966343/51162515-d27eb5",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/141:14,error,error,14,,https://github.com/google/deepvariant/issues/141,1,['error'],['error']
Availability,"Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose?; 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features.; 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed?. Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790:710,avail,available,710,,https://github.com/google/deepvariant/issues/790,1,['avail'],['available']
Availability,"Hi, I run this command to call variant on my bam file:; ```; sudo docker run \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/home/thanh/ref37/hg19.fa \; --reads=/home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam \; --regions=$/home/thanh/RB1_Project/VBDI-VAR/RB1_padding_100.bed \; --output_vcf=/home/thanh/Procedure_practice/17D2627041/VCF/17D2627041_De_hg19.vcf \; --output_gvcf=/home/thanh/Procedure_practice/17D2627041/VCF/17D2627041_De_hg19.g.vcf \; --num_shards=4 ; ```; it resulted in this error; ```; ValueError: Not found: Could not open /home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam; I0305 09:55:16.252675 139778511243008 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. [E::hts_open_format] Failed to open file /home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1399, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 388, in default_options; with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return Native",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/281:587,error,error,587,,https://github.com/google/deepvariant/issues/281,1,['error'],['error']
Availability,"Hi, I try to setup deepvariant on my ubuntu, following instructionsin section Download binaries, models, and test data, listed here.; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.mddy. I get httplib.ResponseNotReady error.; ![screenshot from 2017-12-11 13-31-30](https://user-images.githubusercontent.com/16382685/33831300-c7d8b290-de77-11e7-957f-748aadf16347.png). I was able to run the repository docker, but I would like to use this variant caller on machine without internet access. Is this possible?. Kind regards,. --; Tomasz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/11:78,Down,Download,78,,https://github.com/google/deepvariant/issues/11,2,"['Down', 'error']","['Download', 'error']"
Availability,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:342,error,error,342,,https://github.com/google/deepvariant/issues/441,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find?. ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf; I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter; [E::fai_retrieve] Failed to retrieve block: error reading file; Traceback (most recent call last):; File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main; header=header); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf; for variant in variant_generator:; File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants; nonvariant.end, fasta_reader); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template; ranges.make_range(retval.reference_name, start, start + 1)); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 90, in query; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164:30,error,error,30,,https://github.com/google/deepvariant/issues/164,2,['error'],['error']
Availability,"Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/595:321,error,error,321,,https://github.com/google/deepvariant/issues/595,2,['error'],['error']
Availability,"Hi, My question is regarding the following comment: https://github.com/google/deepvariant/blob/43e54207bbf37dc511d3ea092cc84cd0476e5efe/deepvariant/haplotypes.py#L335. Does ""biallelic deletion ... and inside it a biallelic SNP"" refer to something like this?; ```; REF: ACATTACGATC; READ1 AC----CGA; READ2 ACAGTACG; ```; where we have the combined variant; ref = CATTA -> C/CAGTA. In this case, following the comment, it seems the recommended variant representation is (assuming reference starts at position 1 (1-based index for VCF)); ```; POS REF ALT GT field; 2 CATTA C 0/1; 4 T G 0/1; ```. However, I am wondering whether this is a valid VCF representation, since at position 4, we do not have the ref allele, but 0/1 indicates that ref allele is present at position 4. Apologize if this is down to my misunderstanding of the VCF format.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/247:794,down,down,794,,https://github.com/google/deepvariant/issues/247,1,['down'],['down']
Availability,"Hi, With the latest version of deeptrio, downloaded 3 days ago, run on singularity cpu, I got an out of memory error (despite having 64 Gb ram) on the post-process step of two parents. Since the run took 3 days, I don't want to have to start from scratch. I found the tmp directory containing all the make_examples files. How do I restart from the postprocess variants step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/803:41,down,downloaded,41,,https://github.com/google/deepvariant/issues/803,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Hi, i am new to deepvariant and kind of learning some programming languages too as a beginner, i try to run these commands on my ubuntu 16.04 and had the following errors, both with Build_and_test.sh, Build-prereq.sh. please i need help on how to fix these errors and get deepvariant running. thank you. [sudo] password for solokopi: ; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ [[ 0 = \1 ]]; ++ export DV_CPP_TENSORFLOW_TAG=r1.9; ++ DV_CPP_TENSORFLOW_TAG=r1.9; ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:164,error,errors,164,,https://github.com/google/deepvariant/issues/89,2,['error'],['errors']
Availability,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381:250,avail,available,250,,https://github.com/google/deepvariant/issues/381,2,['avail'],['available']
Availability,"Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150:237,echo,echo,237,,https://github.com/google/deepvariant/issues/150,3,['echo'],['echo']
Availability,"Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue?. **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command?. This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167:240,error,error,240,,https://github.com/google/deepvariant/issues/167,1,['error'],['error']
Availability,"Hi,. I am able to run the command below when the bash file is located in the same folder where the sif file was pulled. . ```; singularity run -B /scratch \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** ; ```; Can I run the bash file if it is located in a folder other than the folder where the sif file is located? I tried adding `cd ${SINGULARITY_DIR}` in the bash file but it returns an error:. ```; INFO: /etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html); INFO: Using cached SIF image; 2024-03-05 05:27:15.547122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; --ref is required.; Pass --helpshort or --helpfull to see help on flags.; /var/spool/slurmd/job29299921/slurm_script: line 24: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory; /var/spool/slurmd/job29299921/slurm_script: line 30: --make_examples_extra_args=split_skip_reads=true,channels='': command not found; ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783:951,error,error,951,,https://github.com/google/deepvariant/issues/783,1,['error'],['error']
Availability,"Hi,. I am getting an error at postprocess variants. . The command executed is:. ```; /opt/deepvariant/bin/postprocess_variants --ref ""hg19.fa.gz"" --infile call_variants_output.tfrecord --outfile ""NA12878_S1.chr20.10_10p1mb.bam.vcf""; ```. And the ouput is:; ```; 2018-03-06 11:34:21.456020: I deepvariant/postprocess_variants.cc:87] Read from: call_variants_output.tfrecord; 2018-03-06 11:34:21.457925: I deepvariant/postprocess_variants.cc:96] Done reading: call_variants_output.tfrecord. #entries in single_site_calls = 289; 2018-03-06 11:34:21.457943: I deepvariant/postprocess_variants.cc:100] Total #entries in single_site_calls = 289; 2018-03-06 11:34:21.457949: I deepvariant/postprocess_variants.cc:102] Start SortSingleSiteCalls; 2018-03-06 11:34:21.457957: F deepvariant/core/utils.cc:84] Check failed: pos_in_fasta != contig_name_to_pos_in_fasta.end() Reference name chr20 not in contig info.; ```. Any idea why I cannot change the genome to run the example?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/53:21,error,error,21,,https://github.com/google/deepvariant/issues/53,1,['error'],['error']
Availability,"Hi,. I am trying to run the training step, and met the following error. Any possible reason for this? I have checked the bam file and reference are match well. Thank you!. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/tmp/Bazel.runfiles_OK9hEl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373; bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MIS; SING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/106:65,error,error,65,,https://github.com/google/deepvariant/issues/106,1,['error'],['error']
Availability,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291:260,down,download,260,,https://github.com/google/deepvariant/issues/291,1,['down'],['download']
Availability,"Hi,. I am using [Pepper-MARGIN-DeepVariant r0.7](https://github.com/kishwarshafin/pepper) with custom made models for Pepper-SNP, Pepper-HP and DeepVariant. As far as I know, this release uses DeepVariant 1.2. I have run this pipeline successfully for a small cohort of about 100 genomes but when merging the GVCF files, [GLnexus 1.4.1](https://github.com/dnanexus-rnd/GLnexus) (with config `DeepVariant`) complains that at least one variant is missing PL values. I tracked down the issue to one GVCF were the record is:; ```; CHR	POS	.	A	G,<*>	9.9	NoCall	.	GT:GQ:DP:AD:VAF:PL	./.:0:5:0,0,0:0,0:0,0,990,990,990; ```; We can see that this GVCF record has 5 PL values where there should be 6. The corresponding record in the VCF file is:; ```; CHR POS .	A	G	9.9	refCall	.	GT:GQ:DP:AD:VAF:C	./.:0:5:0:0:DV; ```; The VCF record indicates that the variant call was issued by DeepVariant. . Any ideas what could be the issue here?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521:474,down,down,474,,https://github.com/google/deepvariant/issues/521,1,['down'],['down']
Availability,"Hi,. I am wondering if the DV team has done an evaluation on how the availability of BAQ (the `BQ:Z:...` tag) affects the quality of calls generated from HiFi reads. Thank you!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/565:69,avail,availability,69,,https://github.com/google/deepvariant/issues/565,1,['avail'],['availability']
Availability,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399:396,error,error,396,,https://github.com/google/deepvariant/issues/399,2,['error'],['error']
Availability,"Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```; singularity run -B /scratch \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** ; ```. The error log is attached. ; [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/782:737,error,error,737,,https://github.com/google/deepvariant/issues/782,3,['error'],['error']
Availability,"Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:; ```; time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=0 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512 \; --debug 'true'; ```. I received an error regarding about the checkpoint: ```No checkpoint found.```; I also attached my log for training step here: ; [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:898,error,error,898,,https://github.com/google/deepvariant/issues/802,4,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Hi,. I got the following error: . I'm using Docker version 1.1.0; gpu NVIDIA GeForce RTX 3090. Any suggestion or advice?. Thanks in advance.; Amin. `2021-05-06 16:56:50.765879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; I0506 16:56:52.008759 140393620989696 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2021-05-06 16:56:52.013998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-05-06 16:56:52.046181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2021-05-06 16:56:52.053674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47507d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.053727: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-05-06 16:56:52.058754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1; 2021-05-06 16:56:52.188018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b9240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.188089: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6; 2021-05-06 16:56:52.191811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ; pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6; coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s; 2021-05-06 16:56:52.191885: I tensorflow/st",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452:25,error,error,25,,https://github.com/google/deepvariant/issues/452,1,['error'],['error']
Availability,"Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**; Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**; - Operating system: CentOS; - DeepVariant version: v1.2; - Installation method (Docker, built from source, etc.): Singularity v3.5.2; - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=""PACBIO"" \; --regions ""$CHROM""; [ ... otherwise default options ...]; ```; - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`); - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**; can't be used to reproduce the problem. **Any additional context:**; In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this; ```; I0112 10:31:04.917984 47443049531200 \; make_examples_core.py:236] \; Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]; ```. Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/603:772,Error,Error,772,,https://github.com/google/deepvariant/issues/603,1,['Error'],['Error']
Availability,"Hi,. I have trouble running DeepVariant v 1.6.0 with Singularity. I tried running the following:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} ; ```; But I get the following errors:; ```; /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory; /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found; ```. I've downloaded the model.ckpt files:; ```; ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model; model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta; ```; I downloaded them with:; ```; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. Pleas",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/780:696,error,errors,696,,https://github.com/google/deepvariant/issues/780,1,['error'],['errors']
Availability,"Hi,. I just wonder if Is there a way to disable downsampling? I work with high depth data from targeted tumor panel. The depth could be as high as 20,000x. With deepvariant 8.0, we miss some variants with low maf such as 0.05, 0.02... Thanks a lot for rhe help!. Best,. Sean",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/176:48,down,downsampling,48,,https://github.com/google/deepvariant/issues/176,1,['down'],['downsampling']
Availability,"Hi,. I tested DeepVariant 1.5.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:. ```; 2023-04-07 15:47:02.393512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0407 15:47:04.372007 140275094697792 call_variants.py:317] From ./examples.tfrecord-00000-of-00032.gz.example_info.json: Shape of input examples: [100, 221, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0407 15:47:04.374644 140275094697792 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; Traceback (most recent call last):; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:100,error,error,100,,https://github.com/google/deepvariant/issues/628,1,['error'],['error']
Availability,"Hi,. I want to load pre-trained DeepVariant model (DeepVariant-inception_v3-0.7.0+data-wgs_standard).; However, loading model.ckpt.meta file produced some error.; Analyzing environment was google collaboratory (python2, GPU). My purpose is to use the pre-trained model in Keras. I used model.ckpt.meta file as follows:. `import tensorflow as tf`; `pretrian_model_path='/content/drive/My Drive/DeepVariant-inception_v3-0.7.0+data-wgs_standard'`; `saver = tf.train.import_meta_graph(pretrian_model_path + '/model.ckpt.meta', clear_devices=True)`. This produced:. > ValueErrorTraceback (most recent call last); <ipython-input-9-8883daf94bd3> in <module>(); 1 with tf.Session() as sess:; ----> 2 saver = tf.train.import_meta_graph(pretrian_model_path + '/model.ckpt.meta', clear_devices=True); >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs); 1672 """""" # pylint: disable=g-doc-exception; 1673 return _import_meta_graph_with_return_elements(; -> 1674 meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]; 1675 ; 1676 ; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs); 1694 import_scope=import_scope,; 1695 return_elements=return_elements,; -> 1696 **kwargs)); 1697 ; 1698 saver = _create_saver_from_imported_meta_graph(; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements); 804 input_map=input_map,; 805 producer_op_list=producer_op_list,; --> 806 return_elements=return_elements); 807 ; 808 # Restores all the other collections.; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127:155,error,error,155,,https://github.com/google/deepvariant/issues/127,1,['error'],['error']
Availability,"Hi,. I was trying to train a PacBio HiFi model for bacteria (surprisingly the default DeepVariant 1.3.0 PacBio model seems to work pretty well), but I run into an error with the `model_train` binary such that this command works:. ```bash; GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```. But, when I try the following. ```bash; GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```; I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint?. ```python; Warm-starting variables only in TRAINABLE_VARIABLES.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_eh_1d4e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:163,error,error,163,,https://github.com/google/deepvariant/issues/500,1,['error'],['error']
Availability,"Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696:133,down,download,133,,https://github.com/google/deepvariant/issues/696,1,['down'],['download']
Availability,"Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:; docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0""; zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads /reads.bam \; --examples ""${sample_id}.examples.tfrecord${numShards}"" \; --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```; Traceback (most recent call last):; File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options; flags_obj.gvcf or ''); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs; raise ValueError('Output is not sharded but shard > 0', shard); Val",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/143:330,echo,echo,330,,https://github.com/google/deepvariant/issues/143,4,"['echo', 'error']","['echo', 'error']"
Availability,"Hi,. If I run the following command:; ```bash; dv_make_examples.py \; --cores 16 \; --sample ISO_349.bam \; --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz \; --reads ISO_349.bam \; --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz \; --logdir logs \; --examples ISO_349_shardedExamples; ```. I get the following output/error:; ```bash; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s 2019-06-13 12:05:15.630247: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I0613 12:05:15.631112 47747793808000 genomics_reader.py:213] Reading ISO_349.bam with NativeSamReader; Traceback (most recent call last):; File ""/state/part",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191:339,error,error,339,,https://github.com/google/deepvariant/issues/191,1,['error'],['error']
Availability,"Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/148:202,echo,echo,202,,https://github.com/google/deepvariant/issues/148,2,"['echo', 'error']","['echo', 'error']"
Availability,"Hi,. Thank you for the great program. I followed the steps in the DeepVariant PacBio model case study (https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md) and generated a BAM file as below. . minimap2 -t 8 -ax map-hifi --secondary=no -Y -R '@RG\tID:N006942-GRCh38\tSM:ELD144989A-D01\tPU:ELD144989A-D01-CCS' -r7k --MD /data/GRCh38_no_alt_analysis_set.fasta / data/N006942-20231016.fq.gz | samtools sort -@ 8 -O BAM -o ./N006942-20231016.bam; samtools index ./N006942-20231016.bam. However, when I tried to “Run DeepVariant on chromosome 20 alignments” as described, I encountered unrecognized parameters and had to modify the command for PBSPro as below. #PBS -q gpuvolta; #PBS -l ncpus=48; #PBS -l ngpus=4; #PBS -l mem=384GB. module load singularity; module load parabricks/4.2.1. REF_FA='/data/GRCh38_no_alt_analysis_set.fasta'; INPUT_BAM='/data/N006942-20231016.bam'; OUTPUT_VCF='/data/test_output.vcf'. # Run DeepVariant; ulimit -u 100000; singularity run /apps/parabricks/4.2.1/image/clara-parabricks_4.2.1-1.sif pbrun deepvariant \; --ref ${REF_FA} \; --in-bam ${INPUT_BAM} \; --out-variants ${OUTPUT_VCF} \; --run-partition \; --num-cpu-threads-per-stream 12 \; --gpu-num-per-partition 1. I'm now facing an error related to cur_seq.size() < MAX_READ_LEN. [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/region.cpp:3442] Too many sequences - 17549 (max 512, expected cur_seq.size() < MAX_READ_LEN, exiting.; [PB ^[[31mError^[[0m 2024-Mar-30 15:00:13][src/ssw_gpu.cu:439] cudaSafeCall() failed: driver shutting down, exiting. Could you provide any insights or suggestions on this issue?. Cheers,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/798:1256,error,error,1256,,https://github.com/google/deepvariant/issues/798,2,"['down', 'error']","['down', 'error']"
Availability,"Hi,. Thank you for this great tool. ; I have a question with regard to SNV calling in paraloguous regions for WES/WGS illumina models. ; As i understand, by default, candidates variants in regions with mapq = 0 are not analyzed. There are attempts to deal with with issue for other callers data https://www.nature.com/articles/s41467-023-42531-9?fromPaywallRec=true. . My question is whether you have data on this issue and if setting the mapq to zero would be advised or it is better advised to use masked genomes. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/900:500,mask,masked,500,,https://github.com/google/deepvariant/issues/900,1,['mask'],['masked']
Availability,"Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```; # works ; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; # fails; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624:44,avail,available,44,,https://github.com/google/deepvariant/issues/624,4,"['avail', 'down', 'error']","['available', 'download', 'error']"
Availability,"Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like; ```; make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference.; ```; make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728; third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'; genomics_reader.py:222] Reading sample.cram with NativeSamReader; ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741:338,error,error,338,,https://github.com/google/deepvariant/issues/741,1,['error'],['error']
Availability,"Hi,; I am doing a genomic analysis of bats, and I have WGS for about 13 individuals but with unknown ancestry or relatedness, many from different populations.; I read that DeepVariant is one of the best genotype-callers, but I would like to consult first about the reliability and confidence on calling variants on a non-human system (yet mammalian..??). I read the blog about doing the mosquito-specific model, but the problem, as stated, is that I don't have trios data to train a model, nor I think that I can get it (nor samples or money), which is a very hard request.; I would appreciate a lot your comments and suggestions on what can I do, or if DeepVariant can confidently call variants. Thanks;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878:265,reliab,reliability,265,,https://github.com/google/deepvariant/issues/878,1,['reliab'],['reliability']
Availability,"Hi,; I am running into an error when I try to use make_examples with a training VCF. The error does not occur when I run make_examples in training mode, without a truth VCF, so I believe the error occurs there. I call make examples with the following:; `/opt/deepvariant/bin/make_examples --mode training --ref refs/${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions refs/confidence.bed `. I am using a simple test VCF attached here and I get the following error:; `; ValueError: Invalid argument: Invalid interval: reference_name: ""NC_000962.3"" start: -1 end: 22; `; Could you point me towards how to debug this issue? Thank you! ; [test.vcf.gz](https://github.com/google/deepvariant/files/1985941/test.vcf.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71:26,error,error,26,,https://github.com/google/deepvariant/issues/71,4,['error'],['error']
Availability,"Hi,; I saw the file about Improve DeepVariant for BGISEQ germline variant calling:; https://github.com/SVAI/RecausalNucleotideNetworks/; There is an introduction how to pick a model in the page 11th of this file. When I try to pick a model, I can't download the python script print_F1.py frome https://gist.github.com/pichuan/fde73e263dba51230791d56e0b9396e1. How can I get print_F1.py with other way?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/76:249,down,download,249,,https://github.com/google/deepvariant/issues/76,1,['down'],['download']
Availability,"Hi,; I was wondering if the RAW data used to train the ONT model on R10.4.1 chemistry is publicly available. The ones on the GIAB FTP server are all R9.4.1. If so, could you point me to where these files are located?; Thank you in advance for any help :)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/821:98,avail,available,98,,https://github.com/google/deepvariant/issues/821,1,['avail'],['available']
Availability,"Hi,; I'm trying to run make_examples with this data:; ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai/; using chm1.fa as the reference from here:; http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz. ```; OUTPUT_DIR=""output""; mkdir -p ""${OUTPUT_DIR}"". BASE=""/home/guy/Data/NA12878_PacBio_MtSinai"". REF=""${BASE}/chromFa/chr1.fa"". BAM=""${BASE}/sorted_final_merged.bam"". MODEL=""${MODEL_NAME}/model.ckpt"". python bazel-bin/deepvariant/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr1:1,000,000-2,000,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```; and the error I get:; ```; Traceback (most recent call last):; File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1118, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1108, in main; make_examples_runner(options); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1037, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 764, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 802, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/75:654,error,error,654,,https://github.com/google/deepvariant/issues/75,1,['error'],['error']
Availability,"Hi,; I'm trying to train the network on my local machine with long reads but I get this error when trying to make examples out of the reads:. ```; 2018-12-10 17:04:46.071140: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.071218 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1210 17:04:46.072298 140037815584512 make_examples.py:1024] Preparing inputs; 2018-12-10 17:04:46.072535: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.072572 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; I1210 17:04:46.072917 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128:88,error,error,88,,https://github.com/google/deepvariant/issues/128,1,['error'],['error']
Availability,"Hi,; I'm using the docker Deepvariant 1.0.0 via singularity, and getting this error:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_nug2d3l9/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>; import tensorflow as tf; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>; from tensorflow_core import *; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 33, in <module>; import distutils as _distutils; File ""/usr/local/lib/python3.6/dist-packages/_distutils_hack/__init__.py"", line 82, in create_module; return importlib.import_module('._distutils', 'setuptools'); File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/setuptools/__init__.py"", line 6, in <module>; import distutils.core; File ""/usr/local/lib/python3.6/dist-packages/_distutils_hack/__init__.py"", line 82, in create_module; return importlib.import_module('._distutils', 'setuptools'); File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module; return _bootstrap._gcd_import(name[level:], package, level); ModuleNotFoundError: No module named 'setuptools._distutils'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/385:78,error,error,78,,https://github.com/google/deepvariant/issues/385,1,['error'],['error']
Availability,"Hi,; I've been encountering an issue when using the `openvino` extra arg for calling variants. When I do not use this flag, things progress as expected. When using the extra flag, both via `--call_variants_extra_args ""use_openvino=true""` to *run_deepvariant* as well as the `--use_openvino` to *call_variants*, there is the following error about being unable to save the new model due to read-only system. This is running V1.1.0 via singularity on a linux system, using the pacbio hifi model. . ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Traceback (most recent call last):; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404:334,error,error,334,,https://github.com/google/deepvariant/issues/404,1,['error'],['error']
Availability,"Hi,; Is there any additional documentation available on the new single-step phasing method? It would be very helpful to have an overview of how that compares to the previous WhatsHap-based process in terms of how the phasing information is generated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/557:43,avail,available,43,,https://github.com/google/deepvariant/issues/557,1,['avail'],['available']
Availability,"Hi,; This is all done using the singularity containers of the 1.1.0 version, on a linux system. . I encountered one error and one potential error when experimenting with DeepTrio. The first potential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416:116,error,error,116,,https://github.com/google/deepvariant/issues/416,4,['error'],['error']
Availability,"Hi,; When using the --gpus 1 flag for deeptrio with the gpu deeptrio docker, I get this error:; 2024-03-30 20:17:58.355690: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; FATAL Flags parsing error: Unknown command line flag 'gpus'; Pass --helpshort or --helpfull to see help on flags. Does deeptrio not know how to handle the --gpus flag? If so, then how can deeptrio be run with gpus?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/799:88,error,error,88,,https://github.com/google/deepvariant/issues/799,3,['error'],"['error', 'errors']"
Availability,"Hi. DeepVariant requires more than 60 minutes running time. I am encountering a problem of a termination of googleCloud shell after 60 minutes. I am trying to find a way to extent the running time of an idle. So far I could not find any relevant solution. Please let me know hot to prevent the googleCloud termination in order to successfully execute DeepVariant? Thank you a lot. . I tried to use screen mode. When the screen mode was used, I got an error running following command (branch 0.7):; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {} \; ) >""${LOG_DIR}/make_examples.log"" 2>&1. I got an error: -bash: /make_examples.log: Permission denied. Then I went out of the screen mode and try to run same command again and the error was still there.; Thank you a lot for the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/88:451,error,error,451,,https://github.com/google/deepvariant/issues/88,3,['error'],['error']
Availability,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: línea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/309:98,error,errors,98,,https://github.com/google/deepvariant/issues/309,1,['error'],['errors']
Availability,"Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash; singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:707,error,error,707,,https://github.com/google/deepvariant/issues/776,2,['error'],['error']
Availability,Hi. I started using the new version of deepvariant (0:10:0-gpu) and have been getting the following error: . parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. It seems like running deepvariant leaves behind temporary files that are taking up a lot of disk space and I was wondering if there is a post run cleaning you recommend to remove these sort of items or some flags to do it?. Thanks for the help!,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296:100,error,error,100,,https://github.com/google/deepvariant/issues/296,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hi. I tried to investigate the ADs and VAFs of sites with label '0' in the training data.; In most cases, hom-ref sites have very low ADs, which is generated by sequencing errors. However, about 3% of the total examples (about 20000) have AD values greater than 10, yet still labeled as hom-ref. For example, in the BAM I'm studying, the following site. 4:39708091-39708091 C->CT DP:AD:VAF 46:28,15:0.326087. is found, and labeled as hom-ref. Most of these sites (about 98% of which) are short-tandem-repeats (STR), the reference bases are ; CTTTTTTTTTTTTTTTTTCCCAGATGGAAT; at the mentioned site. It seems the GIAB high-confidence VCFs have missed quite a few STR sites (which are within the high-confidence regions). My question is, would these mislabeled sites pollute the training set of DeepVariant, and lead to a higher FN rate? Could this be fixed by, for example, adding an STR/non-STR channel?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/317:172,error,errors,172,,https://github.com/google/deepvariant/issues/317,1,['error'],['errors']
Availability,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:38,error,error,38,,https://github.com/google/deepvariant/issues/207,3,"['error', 'failure']","['error', 'failure']"
Availability,"Hi. Thank you for the great caller. . Recently I tried to use Deepvariant by using Docker image(v0.7.0). When I run call_variants or make_exmaple it complains that the Tensorflow in the docker image is not compled to use AVS and it takes quite longer time than the estimated time in ""DeepVariant exome case study"" tutorial. Is the Tensorflow in the docker image is not compled with AVX options or should I have to set up something else?. `(time docker run -v /data:/data gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/call_variants --outfile $PWD/HG002.cvo.tfrecord.gz --examples $PWD/examples.tfrecord.gz --checkpoint /cluster/ngs/DeepVariantModels/DeepVariant-inception_v3-0.7.0+data-wes_standard/model.ckpt); I1010 00:12:34.233438 140187141920512 call_variants.py:283] Set KMP_BLOCKTIME to 0; 2018-10-10 00:12:34.308707: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/100:626,checkpoint,checkpoint,626,,https://github.com/google/deepvariant/issues/100,1,['checkpoint'],['checkpoint']
Availability,"Hi. when I try to pull the deepvariant docker image via singularity using the following command:; singularity pull docker://google/deepvariant:""1.3.0""; it return the following error:; WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; ERROR Authentication error, exiting.; Cleaning up...; ERROR: pulling container failed!. could you please help. I don't have the permission to install docker.; Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513:176,error,error,176,,https://github.com/google/deepvariant/issues/513,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi.; I read this post and came here. [https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction). I'd like to apply the process to my pipeline. But the above example ingores indel errors and considers only regions with no known variants. Does this repo has sequencing error correction part?; If then, can I use the part only?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/760:140,error,error-correction,140,,https://github.com/google/deepvariant/issues/760,4,['error'],"['error', 'error-correction', 'errors']"
Availability,"Hi.; I'm running the pipeline on a CRAM file. I read that the pipeline works with CRAM files, so I guess that's not the issue.; Can you assist in any way?; Thanks. **Setup**; - Operating system: Ubuntu 20.04.5 LTS; - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: I have no information about the sequencing instrument, the reference genome was GRCh38. **Steps to reproduce:**; - Command:; ```; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""input"":""/input"" \; -v ""output"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/GCF_000001405.26_GRCh38_genomic1.fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; retu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:850,Error,Error,850,,https://github.com/google/deepvariant/issues/588,1,['Error'],['Error']
Availability,"Hi;; I would like to build DeepVariant on CentOS 7. I have installed dependencies of the Centos version corresponding to run-prereq.sh. But I also cann't run copying binaries on my local machines which have installed CentOS 7. The wrror message indicate it cann't find some dependent package in environment.; the error message:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_w1ath6/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: libbz2.so.1.0: cannot open shared object file: No such file or directory; ```; I know we can use docker to run DeepVariant on CentOS 7. But There are some reason why I cann't use docker to run DeepVariant. Did you try to build DeepVariant for CentOS 7. Or, you know who have build DeepVariant with some way on CentOS 7. If you know, can give me some advice？; Thanks a lot,; Simon.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/95:313,error,error,313,,https://github.com/google/deepvariant/issues/95,1,['error'],['error']
Availability,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:73,error,error,73,,https://github.com/google/deepvariant/issues/345,1,['error'],['error']
Availability,"I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10.; I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed.; On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ?. Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not; unavailable for this platform. Please install CLIF at; https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**.; A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160:562,error,error,562,,https://github.com/google/deepvariant/issues/160,4,"['avail', 'error']","['available', 'error']"
Availability,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:202,error,errors,202,,https://github.com/google/deepvariant/issues/98,1,['error'],['errors']
Availability,"I am following the instructions under:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. 1. start GCE image : Ubuntu 16.04 with 100GB. git clone https://github.com/google/deepvariant; cd deepvariant; ./build-prereq.sh; ./build_and_test.sh; ```; ...; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (17:54:59) INFO: Current date is 2017-12-22; (17:55:18) ERROR: /home/<mypath>/0fcc5a420905d68918d80793ee59fab4/external/com_goo; glesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start; with either '//', ':', or '@'. Us; e --incompatible_load_argument_is_label=false to temporarily disable this check. ... (17:55:26) ERROR: Analysis of target '//deepvariant/testing:gunit_extras' failed; build aborted: Loading failed; (17:55:26) INFO: Elapsed time: 27.289s; (17:55:26) FAILED: Build did NOT complete successfully (50 packages loaded); (17:55:26) ERROR: Couldn't start the build. Unable to run tests; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/20:983,ERROR,ERROR,983,,https://github.com/google/deepvariant/issues/20,3,['ERROR'],['ERROR']
Availability,"I am running deep variant example code using gpu but getting this error:. Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 5, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/rmehmood/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 51, in <module>; from google.protobuf.pyext import _message; ImportError: cannot import name '_message' from 'google.protobuf.pyext' (/home/rmehmood/.local/lib/python3.8/site-packages/google/protobuf/pyext/__init__.py). need kind help, many thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/754:66,error,error,66,,https://github.com/google/deepvariant/issues/754,1,['error'],['error']
Availability,"I am running on BAM generated with BWA. The fastq that was used had one reads file with less sequences. Docker latest image. . /opt/deepvariant/bin/run_deepvariant --model_type WGS --num_shards 5 --output_vcf 19CT030668_deepvariant.vcf --reads 19CT030668.bam --ref ../human_g1k_v37.fasta --sample_name 19CT030668. The processe starts but when it gets to end of the BAM it exits. . 2020-12-11 15:45:32.928477: W third_party/nucleus/io/sam_reader.cc:532] Could not read base quality scores A00215:130:HK3H2DSXX:2:2549:4869:24768: Not found: Could not read base quality scores; 2020-12-11 15:45:32.931412: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (1 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f38280a8700 (most recent call first):; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 76 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>; p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/396:721,error,error,721,,https://github.com/google/deepvariant/issues/396,1,['error'],['error']
Availability,"I am running the Docker google/deepvariant:latest-deeptrio. **Setup**; - Operating system: Docker Desktop 3.0 (Windows 10); - DeepVariant version: DeepTrio version 1.1.0. Trio WGS data HG37 (bams aligned with BWA MEM). Command: ; /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=human_g1k_v37.fasta --reads_child=DS187706-DS187706_39743_S1_261735.bam --reads_parent1=19CT021737-19CT021737-20190619_64683_S7_346963.bam --reads_parent2=19CT021740-19CT021740-20190619_64686_S8_346962.bam --output_vcf_child DS187706_proband.vcf --output_vcf_parent1 19CT021737_mother.vcf --output_vcf_parent2 19CT021740_father.vcf --sample_name_child 'DS187706_proband' --sample_name_parent1 '19CT021737_mother.vcf' --sample_name_parent2 '19CT021740_father' --num_shards 6. - Error trace: (if applicable); parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 438m45.227s; user 0m12.742s; sys 0m12.847s; I1220 22:21:36.502776 140568143795968 run_deeptrio.py:584] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 600, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 582, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""human_g1k_v37.fasta"" --reads_parent1 ""19CT021737-19CT021737-20190619_64683_S7_346963.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400:773,Error,Error,773,,https://github.com/google/deepvariant/issues/400,3,['Error'],['Error']
Availability,"I am trying to build the docker image of `deepvariant` but running into problems. I'm on `r1.7` branch and my branch is up to date. I am also facing the same issues on `r1.6.1` branch as well. The command I am using is the following -. ```; docker build --build-arg=FROM_IMAGE=nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 \; --build-arg=DV_GPU_BUILD=1 \; -t deepvariant_gpu .; ```. I am building this docker image on my laptop (M3 macbook). #### PackagesNotFoundError error. The first error I get is -. ```; 1.247 Platform: linux-aarch64; 1.247 Collecting package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:467,error,error,467,,https://github.com/google/deepvariant/issues/902,5,"['Error', 'avail', 'error']","['Error', 'available', 'error']"
Availability,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:242,error,error,242,,https://github.com/google/deepvariant/issues/66,6,"['down', 'echo', 'error']","['download', 'echo', 'error']"
Availability,"I am trying to run this PacBio use case and encountered following error: ; https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. Any hint/info to solve this issue? . ```; INFO: Using cached SIF image; I0828 10:16:33.630316 22957909862208 run_deepvariant.py:342] Re-using the directory for intermediate results in /scratch-local/tahmad.1459036/tmpcy60f694. ***** Intermediate results will be written to /scratch-local/tahmad.1459036/tmpcy60f694 in docker. ****. ***** Running the command:*****; time seq 0 71 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/scratch-local/tahmad.1459036/tmpcy60f694/make_examples.tfrecord@72.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. 2022-08-28 10:16:41.685530: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:41.685937 22858289674048 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; I0828 10:16:41.693562 22858289674048 make_examples_core.py:243] Task 0/72: Preparing inputs; 2022-08-28 10:16:41.685378: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.9 SO:coordinate pb:3.0.7; I0828 10:16:41.685891 23090607179584 genomics_reader.py:222] Reading input/HG003.GRCh38.chr20.pFDA_truthv2.bam with NativeSamReader; I0828 10:16:41.693572 23090607179584 make_examples_core.py:243] Task 51/72: Preparing inputs; 2022-08-28 10:16:41.910178: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignor",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/559:66,error,error,66,,https://github.com/google/deepvariant/issues/559,1,['error'],['error']
Availability,I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137:131,error,error,131,,https://github.com/google/deepvariant/issues/137,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am trying to use deepvariant to call variants using a TPU Node v3-8, but I am running into a persistent issue. Here is the command I am using:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```; However, I am seeing the following error in the call variants step.; ```bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token; I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]; I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6].; 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:680,error,error,680,,https://github.com/google/deepvariant/issues/537,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"I am using Ubuntu 16. I got binaries from file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip. **I run run-prereq.sh first and warning message appears:** ; Cloning into 'tensorflow'...; Switched to a new branch 'r1.9'; Extracting Bazel installation...; WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"". **When I run ./build_and_test.sh command, an error appears:**; ./build_and_test.sh: line 54: bazel: command not found. **When I run ./run-prereq.sh command, it stops at ""unable to re-open stdin:""** ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: ; debconf: unable to initialize frontend: Dialog; debconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.); debconf: falling back to frontend: Readline; debconf: unable to initialize frontend: Readline; debconf: (This frontend requires a controlling tty.); debconf: falling back to frontend: Teletype; dpkg-preconfigure: unable to re-open stdin: . As I can see the problem is bazel installation and already some ways of resolving the problem were suggested - one of suggestion was to change .txt.sh file, another one to manually install bazel package (which seems to me regarding instructions on bazel site not a straightforward approach). I am running DeepVariant on a cluster, therefore would be very grateful for any more straightforward; suggestion. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/92:96,down,download,96,,https://github.com/google/deepvariant/issues/92,3,"['down', 'error']","['down', 'download', 'error']"
Availability,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262:91,error,error,91,,https://github.com/google/deepvariant/issues/262,2,"['Error', 'error']","['Error', 'error']"
Availability,I download the source code and then simply ran:. `./build-prereq.sh`. I got the error: '/tmp/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl'. ![screen shot 2017-12-07 at 11 24 47 am](https://user-images.githubusercontent.com/7627987/33692414-469c85d2-db41-11e7-909e-1a5b53356481.png),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5:2,down,download,2,,https://github.com/google/deepvariant/issues/5,2,"['down', 'error']","['download', 'error']"
Availability,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/362:2,down,downloaded,2,,https://github.com/google/deepvariant/issues/362,5,"['Error', 'down', 'error']","['Error', 'downloaded', 'error']"
Availability,"I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized wi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:1139,mainten,maintenance,1139,,https://github.com/google/deepvariant/issues/841,1,['mainten'],['maintenance']
Availability,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:20,error,error,20,,https://github.com/google/deepvariant/issues/255,2,['error'],['error']
Availability,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338:411,error,error,411,,https://github.com/google/deepvariant/issues/338,1,['error'],['error']
Availability,"I have a few queries about your output:-. 1) Can we generate VCF file with each position information?. 2) Is it possible to convert g.VCF to VCF since commonly used software used VCF format? What does QUAL ""0"" mean in VCF? Some FILTER column with QUAL 0 is marked as ""RefCall"" while some as ""."" Can you please explain how is filter decided in VCF?. 3); #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 1 | . | A | <*> | 0 | . | END=587 | GT:GQ:MIN_DP:PL | 0/0:1:0:0,0,0; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 588 | . | A | <*> | 0 | . | END=588 | GT:GQ:MIN_DP:PL | ./.:0:1:29,3,0. Both the row has QUAL value ""0"". Then why is first row has given homozygous call and the second row with now call?. 4) I have observed that there are **three options available in filter column**. RefCall:- Which would mean Homozygous reference ; PASS:- Can be homozygous/Heterozygous; . :- what does this mean?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/318:841,avail,available,841,,https://github.com/google/deepvariant/issues/318,1,['avail'],['available']
Availability,"I have encountered the following error in several PacBio HiFi samples while running the docker image of deepvariant 1.4.0:. > F deepvariant/allelecounter.cc:872] Check failed: left_padding + right_padding < counts_.size() (5000 vs. 4022); Fatal Python error: Aborted. Deepvariant was run while enabling read normalization:. docker run -v ""input_path"":/input -v ""output_path"":/output google/deepvariant:1.4.0 /opt/deepvariant/bin /run_deepvariant --model_type=PACBIO --make_examples_extra_args=""normalize_reads=true"" --ref=/input/reference.fa --reads=/input/sample.bam --output_vcf=/output/sample.vcf --output_gvcf=/output/sample.gvcf --num_shards=16 --logging_dir=/output/logs. I know it is discouraged to enable read normalization due to potential excessive computational times, but I need it to make sure that I am capturing INDELs on the same conditions as my Illumina left-aligned samples. Any ideas on how to solve this issue?; Thank you,; Eugenio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/762:33,error,error,33,,https://github.com/google/deepvariant/issues/762,2,['error'],['error']
Availability,"I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing; BIN_VERSION=""1.5.0""; ```dockerfile; docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=test_data/Aligned.sortedByCoord.out.bam \; --output_vcf=output/output.vcf.gz \; --num_shards=30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --logging_dir=output/logs \; --intermediate_results_dir output/intermediate_results_dir; ``` ; Please let me know if any error in the command i ran",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/775:671,error,error,671,,https://github.com/google/deepvariant/issues/775,1,['error'],['error']
Availability,"I have searched at the [Evaluation regions for HG002 · Issue #15 · google/deepvariant](https://github.com/google/deepvariant/issues/15), but the website is not available. Here is the non-intersected BED file:. https://dl.dnanex.us/F/D/y61fXf4qz5G8F4QvXz14bkYQ5Q9Yy5q5JPf1K3kg/agilent_sureselect_human_all_exon_v5_b37_targets.bed. Would you please offer the reachable site of the file ""agilent_sureselect_human_all_exon_v5_b37_targets.bed""?. Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/237:160,avail,available,160,,https://github.com/google/deepvariant/issues/237,1,['avail'],['available']
Availability,"I have the problem that I can't enable the GPU when I run the Docker. I am using a NVIDIA P100. nvidia-docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all -it ... This is how I call call_variant ; `(time /opt/deepvariant/bin/call_variants --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ""${EXAMPLES}"" --checkpoint ""${MODEL}"" --execution_hardware accelerator) >""${LOG_DIR}/call_variants.log"" 2>&1`. The output from the call_variant log: . > 2018-06-23 22:47:42.743518: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; WARNING: Logging before flag parsing goes to stderr.; I0623 22:47:43.324297 140315677046528 call_variants.py:329] Initializing model from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; INFO:tensorflow:Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; I0623 22:47:44.415543 140315677046528 tf_logging.py:82] Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 379, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 335, in call_variants; 'execution_hardware is set to accelerator, but no accelerator '; __main__.ExecutionHardwareError: execution_hardware is set to accelerator, but no accelerator was found; real	0m6.241s; user	0m6.872s; sys	0m2.256s. When I run the docker and check for the GPU with nvidia-smi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/81:308,checkpoint,checkpoint,308,,https://github.com/google/deepvariant/issues/81,1,['checkpoint'],['checkpoint']
Availability,"I have try to install the DeepVariant, but failed. I cannot install the GoogleCloud at first, and then cannot also install the software. I come from China. Any solutions to the problem. I hope you can give some help. Thank you. I downloaded a versioned archive for Cloud SDK. When installing googlecloud, some module error pops. ; File ""xxxxx/install.py"", line 8, in <module> import bootstrapping; File ""xxxxx/install.py"", line 9, in <module> import setup; File ""xxxxx/install.py"", line 38, in <module> from googlecloudsdk.core.util import platforms; ImportError: No module named googlecloudsdk.core.util; I also tried the apt-get install, however due to network striction, I cannot install it by online command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17:230,down,downloaded,230,,https://github.com/google/deepvariant/issues/17,2,"['down', 'error']","['downloaded', 'error']"
Availability,"I have used the following commandline for running deepvariant. sudo docker run \; -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/"":""/input"" \; -v ""/media/eniac/WD1/Hifi_Assemblies/mapping/docker_out:/output"" \; google/deepvariant:1.2.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/G1.asm.hic.hap2.p_ctg.fa \; --reads=/input/G1_hap2.bam \; --output_vcf=/output/output.vcf \; --num_shards=4. Deepvariant docker version is unable to find the indexed file. But there is an index file located in the specified directory. *****Error Message *******; I1104 14:33:59.843885 140094834329408 run_deepvariant.py:344] Re-using the directory for intermediate results in /tmp/tmp6zw47x4_; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.363862 139708777084736 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364024 139708777084736 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.363985 139750224725824 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364139 139750224725824 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Could not retrieve index file for '/input/G1_hap2.bam'; I1104 14:34:02.364016 140243238704960 genomics_reader.py:222] Reading /input/G1_hap2.bam with NativeSamReader; W1104 14:34:02.364171 140243238704960 make_examples_core.py:273] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; [E::idx_find_and_load] Co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/492:550,Error,Error,550,,https://github.com/google/deepvariant/issues/492,1,['Error'],['Error']
Availability,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:7,down,downloaded,7,,https://github.com/google/deepvariant/issues/222,1,['down'],['downloaded']
Availability,"I just started using DeepVariant 0.7.0 and I have gotten it to complete on a few exome runs. Out of the 4 exome runs I tested, I used 64 shards for the make_examples step. For 3 of the exomes, the make_examples step seemed to take about 10-15 minutes per shard. For a 4th exome, the make_examples step for one of the shards was taking much longer than 10-15 minutes; after 14 hours it was still running and I manually killed it. Now I have moved onto a whole-genome sequencing run for testing and the same thing is happening; 63/64 shards complete in about 1hour, but a straggling job has been running for 22 hours now.; ; * Have you run into this problem before and do you have suggestions for debugging?; * It occurred to me that there could be some ultra-high coverage region in my BAM file that could be slowing things down. But, I am looking for a way to investigate this. Is there an easy way to determine which region of the genome is in a particular shard?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105:823,down,down,823,,https://github.com/google/deepvariant/issues/105,1,['down'],['down']
Availability,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378:128,checkpoint,checkpoint,128,,https://github.com/google/deepvariant/issues/378,3,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230:539,down,downsampled,539,,https://github.com/google/deepvariant/issues/230,1,['down'],['downsampled']
Availability,"I managed to start training on a GPU, but it took too much time. Now, I am attempting to train DeepVariant on a TPU v3-8 VM. However, the most recent tutorial I found is for version 0.9, and I am unsure how to proceed with version 1.6.1. The command I used is the following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:825,error,error,825,,https://github.com/google/deepvariant/issues/841,1,['error'],['error']
Availability,"I need to run DeepVariant on ARM64 architecture. ; Is there an available Docker image for this, or what steps should I take to achieve it?; Thank you ~",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/834:63,avail,available,63,,https://github.com/google/deepvariant/issues/834,1,['avail'],['available']
Availability,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:840,error,error,840,,https://github.com/google/deepvariant/issues/488,1,['error'],['error']
Availability,"I ran an eval on chr20 of HG002 for PacBio CCS Sequel 2 data downloaded from https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/. The results are different from that given in the following blog:; https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/. Specifically, the indel results are improved by ~4% (F1 is 0.982) and SNV results are largely unchanged. I wonder what is the reason for the difference in the results. Are the two datasets (the one I mentioned, and the one in the blog) the same? If they are the same, has the model been improved since the publication of the blog?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/201:61,down,downloaded,61,,https://github.com/google/deepvariant/issues/201,1,['down'],['downloaded']
Availability,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431:127,error,error,127,,https://github.com/google/deepvariant/issues/431,2,"['Error', 'error']","['Error', 'error']"
Availability,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:53,error,errors,53,,https://github.com/google/deepvariant/issues/226,1,['error'],['errors']
Availability,"I ran the benchmarking using RTGtool for BWA-MEM + deepvariant 1.5.0 with 35x Illumina WGS data of HG002 (used in pFDA challenge) and compared the results with pFDA v2 challenge submission (BSODP here https://doi.org/10.18434/mds2-2336). I observed that the deepvariant 1.5.0 that I run has improved precision but lower recall. . pFDA v2 submission: `Precision: 0.9940, Recall: 0.9982 and F-score:0.9961`; deepvariant v1.5.0: `Precison: 0.9991 , recall: 0.9934, and -score: 0.9962`. I am just wondering if the pFDA submission has any extra filtering used after running deepvariant. Is there any publicly available deepvaraint v1.5.0 vcf for HG002?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/626:604,avail,available,604,,https://github.com/google/deepvariant/issues/626,1,['avail'],['available']
Availability,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:141,error,error,141,,https://github.com/google/deepvariant/issues/530,1,['error'],['error']
Availability,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383:59,checkpoint,checkpoint,59,,https://github.com/google/deepvariant/issues/383,7,['checkpoint'],"['checkpoint', 'checkpoint-first']"
Availability,I read DeepVariant essay and want to download some data.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/852:37,down,download,37,,https://github.com/google/deepvariant/issues/852,1,['down'],['download']
Availability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:536,error,error,536,,https://github.com/google/deepvariant/issues/102,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"I thought I should be able to call variants from short reads aligned to the human genome (hg19) with hisat2. So I gave it a try. Windows 10 Docker desktop. . `/opt/deepvariant/bin/run_deepvariant --model_type=WGS --output_vcf=CDAR-016_run0124_deepvariant.vcf --reads=CDAR-016_human37_align.bam --ref=./CFM032/human_g1k_v37.fasta --novcf_stats_report --num_shards=5 --call_variants_extra_args ""use_openvino=true""`. Error:; ```; I0123 08:06:11.771944 139769488508672 make_examples.py:535] Task 1/5: 9704 candidates (9978 examples) [651.40s elapsed]; I0123 08:07:25.050816 140120745805568 make_examples.py:535] Task 3/5: 9400 candidates (9710 examples) [246.46s elapsed]; I0123 08:09:37.633983 140665948894976 make_examples.py:535] Task 4/5: 6501 candidates (6671 examples) [1447.91s elapsed]; I0123 08:10:30.871732 140590877263616 make_examples.py:535] Task 2/5: 7503 candidates (7798 examples) [1754.74s elapsed]; I0123 08:14:47.526611 139645822764800 make_examples.py:535] Task 0/5: 8701 candidates (9015 examples) [4857.13s elapsed]; I0123 08:16:17.660391 140590877263616 make_examples.py:535] Task 2/5: 7600 candidates (7895 examples) [346.79s elapsed]; I0123 08:17:25.425076 140120745805568 make_examples.py:535] Task 3/5: 9501 candidates (9813 examples) [600.37s elapsed]; I0123 08:23:17.762356 140590877263616 make_examples.py:535] Task 2/5: 7700 candidates (8000 examples) [420.10s elapsed]; I0123 08:28:08.231647 140590877263616 make_examples.py:535] Task 2/5: 7800 candidates (8102 examples) [290.47s elapsed]; I0123 08:33:41.777438 139769488508672 make_examples.py:535] Task 1/5: 9800 candidates (10074 examples) [1650.01s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ./CFM032/human_g1k_v37.fasta --reads run0124_lane9_indexUDI0001-UDI0001=CDAR-016_human37_align.bam --examples /tmp/tmpfh_b9v77/make_examples.tfrecord@5.gz --task 1. real 2480m8.473s; user 2463m44.983s; sys 2m30.474s; I0123 08:51:28.346467 139763485337344 run_deepvariant.py:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412:414,Error,Error,414,,https://github.com/google/deepvariant/issues/412,1,['Error'],['Error']
Availability,I tried running DeepVariant-0.5.0+cl-183695032 on the quickstart dataset with the following command:; ```; python call_variants.zip --dataset_config_p; btxt a.pbtxt --checkpoint DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard/model.ckpt.index; ```. However it exits with the following error:; ```; KeyError: 'InceptionV3/Logits/Conv2d_1c_1x1/weights; ```,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/51:167,checkpoint,checkpoint,167,,https://github.com/google/deepvariant/issues/51,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"I tried to build deepvariant on a local ubuntu server.; With GCP support turned off, so far I am stuck with an error after build_and_test.sh . `(13:58:12) ERROR: /root/deepvariant/deepvariant/core/python/BUILD:174:1: CLIF wrapping deepvariant/core/python/hts_verbose.clif failed (Exit 4): pyclif failed: error execut; ing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/genomics && \; exec env - \; bazel-out/host/bin/external/clif/pyclif --modname deepvariant.core.python.hts_verbose -c bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose.cc -g bazel-out/k8-; opt/genfiles/deepvariant/core/python/hts_verbose.h -i bazel-out/k8-opt/genfiles/deepvariant/core/python/hts_verbose_init.cc --prepend /root/opt/clif/python/types.h -Iextern; al/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/htslib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -; Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Ie; xternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_python -Ibazel-out/k8-opt/genfiles/external/protobuf_ar; chive -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/genfiles/external/local_config_python/python_include -Iexternal/protobuf_archive/src -Ibazel-out/k8-o; pt/genfiles/external/protobuf_archive/src '-f-Iexternal/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/hts; lib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/; k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Iexternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_py;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12:111,error,error,111,,https://github.com/google/deepvariant/issues/12,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432:158,error,error,158,,https://github.com/google/deepvariant/issues/432,3,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"I tried to run deeptrio using the suggested command:. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0rc2-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio. However, there was an error showing that. /opt/nvidia/nvidia_entrypoint.sh: line 67: /opt/deepvariant/bin/run_deeptrio: No such file or directory. I shelled into the sif file and couldn't find /opt/deepvariant/bin/run_deeptrio. Is this file missing in container or shall I use another executable?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745:222,error,error,222,,https://github.com/google/deepvariant/issues/745,1,['error'],['error']
Availability,"I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. ; <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606:116,error,errors,116,,https://github.com/google/deepvariant/issues/606,2,['error'],"['error', 'errors']"
Availability,"I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:86,down,download,86,,https://github.com/google/deepvariant/issues/641,1,['down'],['download']
Availability,"I used deepvariant to call variant on HIFI and ONT sequencing data, and merged the generated gvcf files. When I used gatk to merge, an error occurred. When using glnexus to merge gvcf, its config options are only DeepVariantWGS and DeepVariantWES. Can you provide me with some help and suggestions about merging gvcf files? grateful.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/710:135,error,error,135,,https://github.com/google/deepvariant/issues/710,1,['error'],['error']
Availability,"I want to change the source code of DeepVariant on Ubuntu 20.04, so I need to build it from source. I run the ./build-prereq.sh and meet the question. My user is root. Does anyone can help me, thank you very much. The question is below:. error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [57 lines of output]; [proxychains] DLL init: proxychains-ng 4.16; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); [proxychains] DLL init: proxychains-ng 4.16; [proxychains] DLL init: proxychains-ng 4.16; ; Error compiling Cython file:; ------------------------------------------------------------; ...; for i in range(1, RK_STATE_LEN):; self.rng_state.key[i] = val[i]; self.rng_state.pos = i; ; self._bitgen.state = &self.rng_state; self._bitgen.next_uint64 = &mt19937_uint64; ^; ------------------------------------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:238,error,error,238,,https://github.com/google/deepvariant/issues/727,3,"['Error', 'error']","['Error', 'error']"
Availability,"I want to change the source, so I choose to build it from source. I run the build-prereq.sh and meet the folling error.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:113,error,error,113,,https://github.com/google/deepvariant/issues/730,1,['error'],['error']
Availability,"I want to use deepvariant only on ucsc_hg19's chr20. So I execute:; ```; docker run \; -v ""${PWD}"":""/input"" \; -v ""${PWD}/vg_deepvariant"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc_hg19.fa.gz \; --reads=/output/chr20_aln.sort.bam \; --regions ""chr20"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; My bam file is from vg, and I want to use deepvariant to call variant. I check the header of bam and reference, they are matching.; The more detail error information:; `ValueError: Reference contigs span 3137161264 bases but only 63025520 bases (2.01%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ; ""chrM"" is 16571 bp and IS MISSING, ; ""chr1"" is 249250621 bp and IS MISSING, ; ""chr2"" is 243199373 bp and IS MISSING, ; ""chr3"" is 198022430 bp and IS MISSING, ; ""chr4"" is 191154276 bp and IS MISSING, ; ""chr5"" is 180915260 bp and IS MISSING, ; ""chr6"" is 171115067 bp and IS MISSING, ; ""chr7"" is 159138663 bp and IS MISSING, ; ""chr8"" is 146364022 bp and IS MISSING, ; ""chr9"" is 141213431 bp and IS MISSING, ; ""chr10"" is 135534747 bp and IS MISSING, ; ""chr11"" is 135006516 bp and IS MISSING, ; ""chr12"" is 133851895 bp and IS MISSING, ; ""chr13"" is 115169878 bp and IS MISSING, ; ""chr14"" is 107349540 bp and IS MISSING, ; ""chr15"" is 102531392 bp and IS MISSING, ; ""chr16"" is 90354753 bp and IS MISSING, ; ""chr17"" is 81195210 bp and IS MISSING, ; ""chr18"" is 78077248 bp and IS MISSING, ; ""chr19"" is 59128983 bp and IS MISSING, ; ""chr20"" is 63025520 bp and matched, ; ""chr21"" is 48129895 bp and IS MISSING, ; ""chr22"" is 51304566 bp and IS MISSING, ; ""chrX"" is 155270560 bp and IS MISSING, ; ""chrY"" is 59373566 bp and IS MISSING, ; ""chr1_gl000191_random"" is 106433 bp and IS MISSING, ; ""chr1_gl000192_random"" is 547496 bp and IS MIS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/851:634,error,error,634,,https://github.com/google/deepvariant/issues/851,1,['error'],['error']
Availability,"I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04; - DeepVariant version: 1.4.0; - Building docker image locally. **Steps to reproduce:**; - Command: `docker build .` in source directory (no modifications); - Error trace: ; ; ```; #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting; #16 1484.7 Extracting Bazel installation...; #16 1487.8 Starting local Bazel server and connecting to it...; #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc); #16 1489.8 (21:51:01) INFO: Options provided by the client:; #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:119,down,download,119,,https://github.com/google/deepvariant/issues/608,2,"['Error', 'down']","['Error', 'download']"
Availability,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/329:21,error,error,21,,https://github.com/google/deepvariant/issues/329,2,['error'],['error']
Availability,"I would like to limit number of vCores used by DeepVariant on my server (4 cpu cores), say 1 cpu cores for call_variants. Based on [https://github.com/google/deepvariant/issues/42](https://github.com/google/deepvariant/issues/42), I try to add those configs (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) into [https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L306](https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L306). However, I failed to limit the number of vCores. Here is my command and the status of the process.; ; Command:. ~/deepvariant$ time ~/deepvariant/bazel-bin/deepvariant/call_variants \; --outfile=${OUTPUT_DIR}/HG002.cvo.tfrecord.22.gz \; --examples=${OUTPUT_DIR}/HG002.examples.tfrecord.22.gz \; --checkpoint=""${MODEL}"" \; --execution_hardware=""cpu"" \; --num_readers=1. And the process list by `top`:. PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 1767 chungts+ 20 0 6622252 2.838g 103516 S 387.0 19.3 3:35.57 /usr/bin/python /tmp/Bazel.runfiles_dnbQVK/runfiles/com_google_deepvariant/+. and the status of the process:. ~/deepvariant$ cat /proc/1767/status; Name:	python; Umask:	0002; State:	S (sleeping); Tgid:	1767; Ngid:	0; Pid:	1767; PPid:	1766; TracerPid:	0; Uid:	1002	1002	1002	1002; Gid:	1003	1003	1003	1003; FDSize:	64; Groups:	4 20 24 25 29 30 44 46 109 110 1000 1001 1003; NStgid:	1767; NSpid:	1767; NSpgid:	1766; NSsid:	2233; VmPeak:	 6967476 kB; VmSize:	 6621996 kB; VmLck:	 0 kB; VmPin:	 0 kB; VmHWM:	 3321704 kB; VmRSS:	 2976936 kB; RssAnon:	 2873420 kB; RssFile:	 103516 kB; RssShmem:	 0 kB; VmData:	 5083652 kB; VmStk:	 148 kB; VmExe:	 2936 kB; VmLib:	 362804 kB; VmPTE:	 6888 kB; VmSwap:	 0 kB; HugetlbPages:	 0 kB; CoreDumping:	0; Threads:	12; Cpus_allowed:	f; Cpus_allowed_list:	0-3; voluntary_ctxt_switches:	96; nonvoluntary_ctxt_switches:	135. Is there anything missing?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90:793,checkpoint,checkpoint,793,,https://github.com/google/deepvariant/issues/90,1,['checkpoint'],['checkpoint']
Availability,"I would like to training a new model with two different dataset. There are about 8,500,00 training examples. But I used the python script shuffle_tfrecords_beam.py to shuffling these examples. I get the error that the code uses too mach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:203,error,error,203,,https://github.com/google/deepvariant/issues/91,4,"['echo', 'error']","['echo', 'error']"
Availability,"I would like to view and save the pileup images generated. . I was trying to get the features (image/encoded, and image/shape) from the example genereated, and decode then save it using the following code. ````; for candidate in candidates:; for example in self.create_pileup_examples(candidate):; features_temp = tf.parse_single_example(example_temp, features={'image/encoded': tf.FixedLenFeature([], tf.string),""image/shape"": tf.FixedLenFeature([], tf.int64)}); ; image_temp = features_temp['image/encoded'] ; shape_temp = features_temp['image/shape'] . image_temp1 = tf.reshape(tf.decode_raw(image_temp, tf.uint8), shape_temp). fh = open(filename_temp, ""wb""); fh.write(image_temp1); fh.close(); ````. and got the following error. ````; TypeError: Failed to convert object of type <class 'tensorflow.core.example.example_pb2.Example'> to Tensor.; ````; Is there a neat way to save all pileup images generated when we run the ""make example"" code?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/229:726,error,error,726,,https://github.com/google/deepvariant/issues/229,1,['error'],['error']
Availability,"I would rather not install the DeepVariant dependencies into my global python environment. When the python dependencies are installed into a virtual environment, make_examples.zip cannot find tensorflow. The steps below show the error. Any suggestions?. ```; $ mkvirtualenv -p /usr/bin/python2.7 DeepVariant.2.7; (DeepVariant.2.7) $ cd bin; bash run-prereq.sh; cd -. (DeepVariant.2.7) $ python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_r1oZvM/runfiles/genomics/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; ImportError: No module named tensorflow; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25:229,error,error,229,,https://github.com/google/deepvariant/issues/25,1,['error'],['error']
Availability,"I'm attempting to train DeepVariant on a Google VM. However, when I run the training command, I encounter the error below. All the paths are correct (of course I have edited the pbtxt file to match the path were shuffled example are stored), and everything works when I run the command on my local machine with my GPU or on our cluster with CPUs. The only difference is that the shuffled examples are stored on a mounted bucket with gcsfuse when I run the command on the Google VM. The bucket is mounted using the following command: `sudo gcsfuse -o allow_other`. I have also tried to use singularity, but I got the same error. I've tried to find a solution through various online resources, but nothing has helped so far. `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 121, in train; tune_dataset_config = data_providers.read_dataset_config(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/data_providers.py"", line 634, in read_dataset_config; dataset_config = text_format.Parse(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 648, in Parse; return ParseLines(text.split(b'\n' if isinstance(text, bytes) else u'\n'),; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 722, in ParseLines; return parser.ParseLines(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837:110,error,error,110,,https://github.com/google/deepvariant/issues/837,2,['error'],['error']
Availability,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/370:347,avail,available,347,,https://github.com/google/deepvariant/issues/370,1,['avail'],['available']
Availability,"I'm currently attempting to run Deepvariant 0.10.0 utilizing a cram file with a reference file. Everything else appears to be properly coded but it seems to fail upon reading the fa file. After failing it tries to fallback to an fai file that doesn't exist. I've tried it both compressed and uncompressed and it still seems to give the same error. Any ideas on what I should do here? . For reference Here is the command i'm running from Docker; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); ; and here is the error i'm seeing right before crashing:; I0509 06:55:24.592304 140264323651328 genomics_reader.py:223] Reading /input/2009617.cram with NativeSamReader; I0509 06:55:24.597789 140264323651328 make_examples.py:535] Task 2/16: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fa.gz.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1358, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1264, in processing_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307:341,error,error,341,,https://github.com/google/deepvariant/issues/307,2,['error'],['error']
Availability,"I'm trying to build deepvariant on debian_version stretch/sid, 3.10.0-327.3.1.el7.x86_64 #1 SMP Wed Dec 9 14:09:15 UTC 2015, and the build_and_test script is failing. The build-prereq.sh command runs successfully; however, build_and_test.sh throws this error:; In file included from external/htslib/hts.c:45:0:; external/htslib/hts_internal.h:31:32: fatal error: textutils_internal.h: No such file or directory. Yet the file is found here:; .cache/bazel/_bazel_root/5b3dfb1a5a17f553ec98d93bc2cea6e8/execroot/com_google_deepvariant/external/htslib/textutils_internal.h. I get the same error if I try to build on CentOS 7.2.1511 (Core). Please advise.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/134:253,error,error,253,,https://github.com/google/deepvariant/issues/134,3,['error'],['error']
Availability,"I've been trying out the new v1.4 docker image through singularity, but have been running into openvino issues again (#404 and #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541:174,error,error,174,,https://github.com/google/deepvariant/issues/541,1,['error'],['error']
Availability,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:193,error,error,193,,https://github.com/google/deepvariant/issues/292,2,['error'],['error']
Availability,"I've never encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/372:28,error,error,28,,https://github.com/google/deepvariant/issues/372,1,['error'],['error']
Availability,"I've setup deepvariant via `gsutil` as described under https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md. When running the first step `make_examples` as indicated in the reference within the docker container, it runs without any complaints, and terminates after around a second. However it does not create an output file nor does it output any error, nor does it care if I provide invalid input file names. The downstream tools `call_variants` and `postprocess_variants` behave similar, they run without error, but accept any input arguments (including invalid ones) and fail to create any output. Any help would be appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/39:369,error,error,369,,https://github.com/google/deepvariant/issues/39,3,"['down', 'error']","['downstream', 'error']"
Availability,"I've successfully run deepvariant with test data. But I keep getting the following error when extracting pileup images from my own provided BAM file. What could be the problem, please?. ```; I1003 20:27:32.183320 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1-1000 [1000 bp] [1.62s elapsed]; I1003 20:27:32.185085 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1001-2000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.186733 140083390310144 make_examples.py:825] Found 0 candidates in chr1:2001-3000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.188343 140083390310144 make_examples.py:825] Found 0 candidates in chr1:3001-4000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.189908 140083390310144 make_examples.py:825] Found 0 candidates in chr1:4001-5000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.191494 140083390310144 make_examples.py:825] Found 0 candidates in chr1:5001-6000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.193065 140083390310144 make_examples.py:825] Found 0 candidates in chr1:6001-7000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.194626 140083390310144 make_examples.py:825] Found 0 candidates in chr1:7001-8000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.196187 140083390310144 make_examples.py:825] Found 0 candidates in chr1:8001-9000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.197738 140083390310144 make_examples.py:825] Found 0 candidates in chr1:9001-10000 [1000 bp] [0.00s elapsed]; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1090, in make_examples_runner; c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99:83,error,error,83,,https://github.com/google/deepvariant/issues/99,1,['error'],['error']
Availability,"II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vcf header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GLnexusVersion=v1.4.1-0-g68e25e5; ##GLnexusConfigName=DeepVariant; ##GLnexusConfigCRC32C=2932316105; ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, snv_prior_calibration: 0.600000024, indel_prior_calibration: 0.449999988, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_format: BCF, liftover_fields: [{orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:1963,Error,Error,1963,,https://github.com/google/deepvariant/issues/633,2,"['Error', 'error']","['Error', 'errors']"
Availability,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created?. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:2596,error,error,2596,,https://github.com/google/deepvariant/issues/515,2,['error'],['error']
Availability,Image /mnt overriding my machine's /mnt causing errors,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:48,error,errors,48,,https://github.com/google/deepvariant/issues/530,1,['error'],['errors']
Availability,Import meta graph error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127:18,error,error,18,,https://github.com/google/deepvariant/issues/127,1,['error'],['error']
Availability,"In the hybrid model case study, the example bam files to download [here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-hybrid-case-study.md#download-hg003-chr20-bam) are labeled `HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam`. Have these bam files been phased in the same way as in the pacbio case study? If so, is the merged bam file able to be phased via `whatshap`, or is the pacbio bam phased independently and then merged with the short read bam. Digging through the `run_deepvariant.py` script makes it seem like the phasing information only pertains to the pacbio model specifically, but just wanted to check. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/410:57,down,download,57,,https://github.com/google/deepvariant/issues/410,2,['down'],"['download', 'download-']"
Availability,Installation error with intel-tensorflow,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/263:13,error,error,13,,https://github.com/google/deepvariant/issues/263,1,['error'],['error']
Availability,"Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:; ```; micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call; with Popen(*popenargs, **kwargs) as p:; File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__; restore_signals, start_new_session); File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```. I noticed inside the bazel .zip files the python binary is hard-coded:; /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```; micro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:113,error,error,113,,https://github.com/google/deepvariant/issues/664,1,['error'],['error']
Availability,"Is there a plan to support nVidia GPU Cuda 11 version in dockers available in the dockerhub? . Current version of `google/deepvariant:latest-gpu` uses Cuda 10, which does not work on newest nVidia GPUs.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/430:65,avail,available,65,,https://github.com/google/deepvariant/issues/430,1,['avail'],['available']
Availability,Is there a way to disable downsampling?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/176:26,down,downsampling,26,,https://github.com/google/deepvariant/issues/176,1,['down'],['downsampling']
Availability,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_Диссертация/Exp/Рез/новая_папка/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_Диссертация/Exp/Рез/новая_папка/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253:803,error,error,803,,https://github.com/google/deepvariant/issues/253,2,['error'],['error']
Availability,Is there any option to use sequencing error correction part only?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/760:38,error,error,38,,https://github.com/google/deepvariant/issues/760,1,['error'],['error']
Availability,"It was great to see haploid support added with version 1.6. However, it took me some time to understand that I had it working correctly as my expectation was that the genotypes would be represented as hemizygous (0 or 1) rather than homozygous (0/0 or 1/1) in the specified regions. Would it be useful to add a clear statement in the documentation regarding the current representation in ""haploid"" regions, and to possibly consider adding an option in postprocess_variants that allowed an alternative output with hemizygous genotypes? I understand that some tools used in downstream applications may have problems with hemizygous genotypes, and therefore the desirability of representing them as homozygous. However there are also downstream applications where true hemizygote representation has significant value. For us this would be the ability to generate accurate AF, AC and AN values after aggregating into multisample vcfs, but I am sure there are others that would benefit from a more accurate representation of genotypes on the true X and Y chromosomes.; Many thanks for considering this and for providing this very useful tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/751:572,down,downstream,572,,https://github.com/google/deepvariant/issues/751,2,['down'],['downstream']
Availability,"I’m new to working with computers tools like DeepVariant. I’m trying to build DeepVariant using Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:385,Error,Error,385,,https://github.com/google/deepvariant/issues/871,1,['Error'],['Error']
Availability,Job failed with error (reason: FAILED_PRECONDITION),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:16,error,error,16,,https://github.com/google/deepvariant/issues/120,1,['error'],['error']
Availability,LD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_goog,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8976,error,error,8976,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Left-normalization error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/762:19,error,error,19,,https://github.com/google/deepvariant/issues/762,1,['error'],['error']
Availability,"MAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1628,error,error,1628,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,Merging vcf files error with glnexus:v1.2.7,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815:18,error,error,18,,https://github.com/google/deepvariant/issues/815,1,['error'],['error']
Availability,"Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1577,error,error,1577,,https://github.com/google/deepvariant/issues/116,2,['error'],['error']
Availability,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:291,echo,echo,291,,https://github.com/google/deepvariant/issues/737,2,"['Error', 'echo']","['Error', 'echo']"
Availability,"Not sure what is causing the issue but upon reaching this step DeepVariant failed. Any thoughts on how to fix? I tired to run it in a python2.7 environment and still it somehow is pulling from python 3.6 it seems. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:409,checkpoint,checkpoint,409,,https://github.com/google/deepvariant/issues/304,1,['checkpoint'],['checkpoint']
Availability,NumPy dependency error despite matching versions,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:17,error,error,17,,https://github.com/google/deepvariant/issues/610,1,['error'],['error']
Availability,"OJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLE",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1753,error,error,1753,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,"ON=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framewo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:1223,Error,Error,1223,,https://github.com/google/deepvariant/issues/342,1,['Error'],['Error']
Availability,"On Ubuntu 16.04 LTS, when I tried to build it from source with Python 3.6.2, it failed to compile `bazel-out/k8-py3-opt/genfiles/deepvariant/core/python/hts_verbose.cc`, and the error was `hts_verbose.cc:134:143: error: 'Py_InitModule3' was not declared in this scope`. After some investigations and it seems to be an incompatible issue with Python 3. . The relatively full stack is here:. (14:05:26) ERROR: xx/git/deepvariant/deepvariant/core/python/BUILD:174:1: C++ compilation of rule '//deepvariant/core/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd xx/.cache/bazel/xx/7e4d04a878642732d9b8bb40a634229e/execroot/genomics && \; exec env - \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=xx/anaconda/envs/Python36/bin/python \; PYTHON_LIB_PATH=xx/anaconda/envs/Python36/lib/python3.6/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 '-std=c++0x' -MD -MF bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.d '-frandom-seed=bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.o' -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/htslib -iquote bazel-out/k8-py3-opt/genfiles/external/htslib -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -iquote external/clif -iquote bazel-out/k8-py3-opt/genfiles/external/clif -iquote external/local_config_python -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_python -iquote external/protobuf_archive -iquote bazel-out/k8-py3-opt/genfiles/external/protobuf_archive -isystem external/htslib/htslib/hts",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/31:178,error,error,178,,https://github.com/google/deepvariant/issues/31,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Otherwise, build fails on gcc6 with the following error:; error: 'accumulate' is not a member of 'std'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/68:50,error,error,50,,https://github.com/google/deepvariant/pull/68,2,['error'],['error']
Availability,PACBIO model error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:13,error,error,13,,https://github.com/google/deepvariant/issues/446,1,['error'],['error']
Availability,Pacbio revio data Error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672:18,Error,Error,18,,https://github.com/google/deepvariant/issues/672,1,['Error'],['Error']
Availability,Protobuf error: raise ValueError('Expected to be using C++ protobuf implementation ',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:9,error,error,9,,https://github.com/google/deepvariant/issues/499,1,['error'],['error']
Availability,RNA-seq model availability for v1.5.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624:14,avail,availability,14,,https://github.com/google/deepvariant/issues/624,1,['avail'],['availability']
Availability,"RNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2221,Checkpoint,Checkpoint,2221,,https://github.com/google/deepvariant/issues/722,1,['Checkpoint'],['Checkpoint']
Availability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3334,avail,available,3334,,https://github.com/google/deepvariant/issues/60,1,['avail'],['available']
Availability,Returns error while evaluating the model on Arabidopsis Thailand Sample,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606:8,error,error,8,,https://github.com/google/deepvariant/issues/606,1,['error'],['error']
Availability,"Running DeepVariant/DeepTrio 1.4.0 using singularity, based on the provided docker images, with many regions results in a `'parallel: Error: Command line too long (<num> >= 65524)` error. My use case is processing .bam files aligned against `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz` (containing 2500+ contigs) in parallel. Splitting by contig would result in 2500+ chunks which introduces too much overhead so I group contigs together based on the number of reads. In practice this results in chunks for chr1, ... chr22, chrX, chrY, chrM and one chunk for the other ~2500 contigs. This results in a very long command-line arg for `--regions` and thus the error. What would you think of adding an additional `--regions-file` command-line argument similar to e.g. [bcftools](https://samtools.github.io/bcftools/bcftools.html) which accepts a text file containing one region per line?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/556:134,Error,Error,134,,https://github.com/google/deepvariant/issues/556,3,"['Error', 'error']","['Error', 'error']"
Availability,Running error with deepvariant_1.6.0-gpu.sif,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:8,error,error,8,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,"Running make_examples locally from the Docker container. It runs without errors on the NA12878_S1.chr20.10_10p1mb.bam dataset, but fails with my BAM file. See the stack trace below. My BAM file is too large to attach here. What do you recommend? Also what are the allowed values for --logging_level? Please advise. ```; # ./opt/deepvariant/bin/make_examples --logging_level DEBUG --mode calling --ref /dv2/reference/CFSAN000189.fasta --reads /dv2/samples/CFSAN000211/reads.sorted.bam --examples output.examples.tfrecord; WARNING: Logging before flag parsing goes to stderr.; I1228 21:10:23.407845 140668049200896 client.py:1004] Timeout attempting to reach GCE metadata service.; W1228 21:10:23.408325 140668049200896 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 969, in main; options = default_options(add_flags=True, flags=FLAGS); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 207, in default_options; sample_name = extract_sample_name_from_reads(flags.reads); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 406, in extract_sample_name_from_reads; raise ValueError('Expected a single sample, found {}'.format(samples)); ValueError: Expected a single sample, found set([]). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:73,error,errors,73,,https://github.com/google/deepvariant/issues/28,1,['error'],['errors']
Availability,Runtime error: merge_overlaps() got an unexpected keyword argument 'strict',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255:8,error,error,8,,https://github.com/google/deepvariant/issues/255,1,['error'],['error']
Availability,"S. **Describe the issue:**; When Running deep variant wes mode, there arised an assetion error when loading the weights of the model. **Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1100,checkpoint,checkpoint,1100,,https://github.com/google/deepvariant/issues/857,2,['checkpoint'],['checkpoint']
Availability,Segmentation fault DeepTrio v1.6 for ONT duo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724:13,fault,fault,13,,https://github.com/google/deepvariant/issues/724,1,['fault'],['fault']
Availability,Singularity fatal error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:18,error,error,18,,https://github.com/google/deepvariant/issues/522,1,['error'],['error']
Availability,Singularity tempfile/TMPDIR error: Tutorials fail,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:28,error,error,28,,https://github.com/google/deepvariant/issues/533,1,['error'],['error']
Availability,Singularity version error: ModuleNotFoundError,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/385:20,error,error,20,,https://github.com/google/deepvariant/issues/385,1,['error'],['error']
Availability,"Sorry it's not an issue of the package per se, but is it possible for you to supply the shell scripts below for building the package on RedHat systems? I tried to install the package with conda on a RedHat system, but it didn't work. When I tried to build the package from the sources, I ran into some issues. Many system libraries and utilities, such as zlib1g-dev, python3-distutils, are not available or possible in different names. Without or not familiar with a Ubuntu system, it's hard to figure out what would be the equivalent. It'd be enough to have the scripts working for one version of RedHat and it'd be much easier for people familiar with RedHat to modify the scripts for a different version of RedHat. Thanks. build_and_test.sh; build-prereq.sh; build_release_binaries.sh; run-prereq.sh; settings.sh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740:394,avail,available,394,,https://github.com/google/deepvariant/issues/740,1,['avail'],['available']
Availability,"Sorry to bother, but could you please provide me with the original path for downloading the data in the exome case study? I am interested in exploring some related data. Thank you very much. . Here's the code related to the data in case study:; ```; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/842:76,down,downloading,76,,https://github.com/google/deepvariant/issues/842,1,['down'],['downloading']
Availability,TMPDIR and locale settings error！please help me!,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613:27,error,error,27,,https://github.com/google/deepvariant/issues/613,1,['error'],['error']
Availability,Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesour,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15853,error,error,15853,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Tensorflow version of the model checkpoint,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/477:32,checkpoint,checkpoint,32,,https://github.com/google/deepvariant/issues/477,1,['checkpoint'],['checkpoint']
Availability,"Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>; from . import multiarray; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/795:154,error,error,154,,https://github.com/google/deepvariant/issues/795,2,['error'],['error']
Availability,"The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:78,avail,available,78,,https://github.com/google/deepvariant/issues/217,1,['avail'],['available']
Availability,"The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:81,avail,available,81,,https://github.com/google/deepvariant/issues/552,1,['avail'],['available']
Availability,"The code in [Download binaries, models, and test data](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#download-binaries-models-and-test-data) the of the DV quick start guide ran successfully. However, running [make_examples](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#make_examples) using the quickstart-testdata failed with the following error:; ```; Traceback (most recent call last):; attila-ThinkS:~/tools/deepvariant$ python bin/make_examples.zip \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/make_examples.py"", line 45, in <module>; from deepvariant import variant_caller; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/variant_caller.py"", line 50, in <module>; from deepvariant.python import variant_calling; ImportError: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41:13,Down,Download,13,,https://github.com/google/deepvariant/issues/41,3,"['Down', 'down', 'error']","['Download', 'download-binaries-models-and-test-data', 'error']"
Availability,"The errors (part of them). + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (00:49:22) INFO: Current date is 2018-01-27; (00:49:22) Loading:; (00:49:22) Loading: 0 packages loaded; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:96:; 1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=fals; e to temporarily disable this check.; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:98:; 1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:100; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:102; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:104; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:106; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:108; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:110; :1: name 're2_test' is not defined (did you mean 'ios_test'?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43:4,error,errors,4,,https://github.com/google/deepvariant/issues/43,9,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"The run-prereq.sh script used to build the image calls 'pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt' on line 273. The nvidia-tensorrt package should not be used. It now just points to whatever the latest tensorrt version is; https://github.com/NVIDIA/TensorRT/issues/2668. The result is that depending on when you build, you may be actually downloading different versions. The result is that the latest version requires Cuda 12 but the image has Cuda 11. This creates conflicts and is likely behind the singularity GPU issues coming up on the issues page. Instead you should specifically specify tensorrt==8.5.3.1 to get the cuda 11 version.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789:343,down,downloading,343,,https://github.com/google/deepvariant/issues/789,1,['down'],['downloading']
Availability,Training mode error: 'truth_variant needs genotypes to be used for labeling',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/64:14,error,error,14,,https://github.com/google/deepvariant/issues/64,1,['error'],['error']
Availability,Training: How to reevaluate all checkpoints?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378:32,checkpoint,checkpoints,32,,https://github.com/google/deepvariant/issues/378,1,['checkpoint'],['checkpoints']
Availability,Troubleshooting segmentation fault,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/313:29,fault,fault,29,,https://github.com/google/deepvariant/issues/313,1,['fault'],['fault']
Availability,"Trying to run but come across the following error. docker run -it -v $PWD/input:/dv2/input -v $PWD/models:/dv2/models \; > gcr.io/deepvariant-docker/deepvariant:$IMAGE_VERSION; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.4.0' locally; 0.4.0: Pulling from deepvariant-docker/deepvariant; Digest: sha256:72d3bd936dfbfbb707e648d7e6f0f8fb4318eb115aad0bfde9b43ff05fef8f19; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.4.0; root@720aed86585e:/# ; root@720aed86585e:/# ./opt/deepvariant/bin/make_examples \; > --mode calling \; > --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; > --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples output.examples.tfrecord \; > --regions ""chr20:10,000,000-10,010,000""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/make_examples.py"", line 966, in main; htslib_gcp_oauth.init(); File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/core/htslib_gcp_oauth.py"", line 79, in init; token = cloud_utils.oauth2_token(); File ""/tmp/Bazel.runfiles_PMLPk5/runfiles/genomics/deepvariant/core/cloud_utils.py"", line 58, in oauth2_token; credentials = oauth2_client.GoogleCredentials.get_application_default(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1271, in get_application_default; return GoogleCredentials._get_implicit_credentials(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1256, in _get_implicit_credentials; credentials = checker(); File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1187, in _implicit_credentials_from_gce; if not _in_gce_environment():; File ""/usr/local/lib/python2.7/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/14:44,error,error,44,,https://github.com/google/deepvariant/issues/14,2,"['Down', 'error']","['Downloaded', 'error']"
Availability,"UT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=G",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:1264,checkpoint,checkpoint,1264,,https://github.com/google/deepvariant/issues/288,1,['checkpoint'],['checkpoint']
Availability,"Upon running dv_make_examples.py -h , dv_call_variants.py -h , or dv_postprocess_variants.py -h , python told me there is a syntax error around a f-string. **Setup**; - Operating system: Centos; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): bioconda. **Steps to reproduce:**; - Command: dv_make_examples.py -h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:131,error,error,131,,https://github.com/google/deepvariant/issues/627,1,['error'],['error']
Availability,Version 0.8 run_examples giving error inside Docker,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:32,error,error,32,,https://github.com/google/deepvariant/issues/342,1,['error'],['error']
Availability,Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 h1234567_1 ; libgomp 11.2.0 h1234567_1 ; libstdcxx-ng 11.2.0 h1234567_1 ; libuuid 1.41.5 h5eee18b_0 ; markdown-it-py 3.0.0 pypi_0 pypi; mdurl 0.1.2 pypi_0 pypi; mkl 2023.1.0 h213fc3f_46344 ; mkl-service 2.4.0 py310h5eee18b_1 ; mkl_fft 1.3.8 py310h5eee18b_0 ; mkl_random 1.2.4 py310hdb19cb5_0 ; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.4.0 pypi_0 pypi; mock 5.1.0 pypi_0 pypi; msgpack 1.0.8 pypi_0 pypi; ncurses 6.4 h6a678d5_0 ; nest-asyncio 1.6.0 pypi_0 pypi; numpy 1.24.3 py310h5f9d8c6_1 ; numpy-base 1.24.3 py310hb5e798b_1 ; openssl 3.0.14 h5eee18b_0 ; opt-einsum 3.3.0 pypi_0 pypi; optax 0.2.3 pypi_0 pypi; orbax-checkpoint 0.5.23 pypi_0 pypi; packaging 24.1 pypi_0 pypi; pip 24.0 py310h06a4308_0 ; protobuf 3.13.0 pypi_0 pypi; pygments 2.18.0 pypi_0 pypi; python 3.10.14 h955ad1f_1 ; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h5eee18b_0 ; rich 13.7.1 pypi_0 pypi; scipy 1.14.0 pypi_0 pypi; setuptools 69.5.1 py310h06a4308_0 ; six 1.16.0 pypi_0 pypi; sortedcontainers 2.1.0 pypi_0 pypi; sqlite 3.45.3 h5eee18b_0 ; tbb 2021.8.0 hdb19cb5_0 ; tensorstore 0.1.64 pypi_0 pypi; tf-slim 1.1.0 pypi_0 pypi; tk 8.6.14 h39e8969_0 ; toolz 0.12.1 pypi_0 pypi; typing-extensions 4.12.2 pypi_0 pypi; tzdata 2024a h04d1e81_0 ; wheel 0.43.0 py310h06a4308_0 ; wrapt 1.16.0 pypi_0 pypi; xz 5.4.6 h5eee18b_1 ; zipp 3.19.2 pypi_0 pypi; zlib 1.2.13 h5eee18b_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:12990,checkpoint,checkpoint,12990,,https://github.com/google/deepvariant/issues/859,1,['checkpoint'],['checkpoint']
Availability,"WFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 28, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 15, in <module>; from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py"", line 15, in <module>; from tensorflow.core.framework import tensor_pb2 as tensorflow_dot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:5070,Error,Error,5070,,https://github.com/google/deepvariant/issues/812,1,['Error'],['Error']
Availability,"WGS samples. . ``` bash; singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```. While the run has not errored out, I do believe that there is an is",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:1057,checkpoint,checkpoint,1057,,https://github.com/google/deepvariant/issues/776,1,['checkpoint'],['checkpoint']
Availability,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203:1161,checkpoint,checkpoint-,1161,,https://github.com/google/deepvariant/issues/203,3,['checkpoint'],['checkpoint-']
Availability,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450:193,avail,available,193,,https://github.com/google/deepvariant/issues/450,3,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"What is the version of tensorflow for generating the checkpoint files (`index`, `meta`, `data`)?; And is there any way that I can load these checkpoints into a standalone tensorflow program and then dump it as a `.onnx` file?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/477:53,checkpoint,checkpoint,53,,https://github.com/google/deepvariant/issues/477,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:185,ERROR,ERROR,185,,https://github.com/google/deepvariant/issues/59,5,['ERROR'],['ERROR']
Availability,"When I run make-example.zip, the error shows up... Please fix it for me ;- ). ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/runfiles/protobuf_archive/python/google/protob; uf/pyext/_message.so)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/22:33,error,error,33,,https://github.com/google/deepvariant/issues/22,1,['error'],['error']
Availability,"When I tried to create a VM with 8 GPUs using this command line:; export IMAGE_FAMILY=""tf-latest-gpu""; export ZONE=""us-west1-a""; export INSTANCE_NAME=""deep""; export INSTANCE_TYPE=""n1-standard-8""; gcloud compute instances create $INSTANCE_NAME \; --zone=$ZONE \; --image-family=$IMAGE_FAMILY \; --image-project=deeplearning-platform-release \; --maintenance-policy=TERMINATE \; --accelerator=""type=nvidia-tesla-p100,count=8"" \; --machine-type=$INSTANCE_TYPE \; --boot-disk-size=200GB \; --metadata=""install-nvidia-driver=True"". I got this error:; ERROR: (gcloud.compute.instances.create) Could not fetch resource:; - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:; ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/136:345,mainten,maintenance-policy,345,,https://github.com/google/deepvariant/issues/136,3,"['ERROR', 'error', 'mainten']","['ERROR', 'error', 'maintenance-policy']"
Availability,"When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:58,error,error,58,,https://github.com/google/deepvariant/issues/859,1,['error'],['error']
Availability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:77,error,error,77,,https://github.com/google/deepvariant/issues/552,4,"['Error', 'avail', 'error']","['Error', 'available', 'error']"
Availability,"When i run train_model.zip, i get a error:; ![image](https://user-images.githubusercontent.com/15261087/33821907-8e07de38-de90-11e7-9461-dc54523691d4.png); Here is my command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/make_examples.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10:36,error,error,36,,https://github.com/google/deepvariant/issues/10,1,['error'],['error']
Availability,"When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. ; Here is part of the command I used:. `; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --reads_parent1=markduplicates/S_500061.md.bam \; --reads_parent2=markduplicates/S_500062.md.bam \; --reads_child=markduplicates/S_500063.md.bam \; --output_vcf_parent1 output/S_500061.output.vcf.gz \; --output_vcf_parent2 output/S_500062.output.vcf.gz \; --output_vcf_child output/S_500063.output.vcf.gz \; --sample_name_parent1 'S_500061' \; --sample_name_parent2 'S_500062' \; --sample_name_child 'S_500063' \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir \; --output_gvcf_parent1 output/S_500061.g.vcf.gz \; --output_gvcf_parent2 output/S_500062.g.vcf.gz \; --output_gvcf_child output/S_500063.g.vcf.gz \; --output_gvcf_merged output/FAM1.g.vcf.gz\; --dry_run=true \; --vcf_stats_report=true; `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687:49,error,error,49,,https://github.com/google/deepvariant/issues/687,2,['error'],['error']
Availability,"When using the provided docker image, google/deepvariant:1.1.0-gpu, an error is produced if --use_tpu is used with model_train. The command:. ```; docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:1.1.0-gpu \; /opt/deepvariant/bin/model_train \; --gcp_project="""" \; --tpu_name="""" \; --tpu_zone=""us-central1-a"" \; --use_tpu \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${OUTPUT_BUCKET}/training-tpu/"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""${INPUT_BUCKET}/wgs_model/model.ckpt"" ; ```. The error that prints is:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 303, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 291, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 236, in parse_and_run; FLAGS.gcp_project) if FLAGS.use_tpu else ''; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 391, in resolve_master; tpu=[tpu_name], zone=tpu_zone, project=gcp_project).get_master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:71,error,error,71,,https://github.com/google/deepvariant/issues/469,2,['error'],['error']
Availability,"With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc; [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file.; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc; [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'; ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```; # Copyright 2017 Google LLC.; #; # Redistribution and use in source and binary forms, with or without; # modification, are permitted provided that the following conditions; # are met:; #; # 1. Redistributions of source code must retain the above copyright notice,; # this list of conditions and the following disclaimer.; #; # 2. Redistributions in binary form must reproduce the above copyright; # notice, this list of conditions and the following disclaimer in the; # documentation and/or other materials provided with the distribution.; #; # 3. Neither the name of the copyright holder nor the names of its; # contributors may be used to endorse or promote products derived from this; # software without specific prior written permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:759,error,error,759,,https://github.com/google/deepvariant/issues/145,1,['error'],['error']
Availability,"Would like to train from basic tensorflow, without dockers and other API's. Is the TensorFlow code for training available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/456:112,avail,available,112,,https://github.com/google/deepvariant/issues/456,1,['avail'],['available']
Availability,"Your dockerhub repo is currently having issues when trying to pull it, regardless of how:. ```; $> docker pull google/deepvariant; Using default tag: latest; Error response from daemon: manifest for google/deepvariant:latest not found: manifest unknown: manifest unknown; ```. ```; $> singularity pull deepvariant.img docker://google/deepvariant:latest; FATAL: While making image from oci registry: failed to get checksum for docker://google/deepvariant:latest: Error reading manifest latest in docker.io/google/deepvariant: manifest unknown: manifest unknown; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/221:158,Error,Error,158,,https://github.com/google/deepvariant/issues/221,2,['Error'],['Error']
Availability,[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584],MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:13,ERROR,ERROR,13,,https://github.com/google/deepvariant/issues/870,1,['ERROR'],['ERROR']
Availability,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:4199,Error,Error,4199,,https://github.com/google/deepvariant/issues/413,2,['Error'],['Error']
Availability,"] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12872,checkpoint,checkpoint,12872,,https://github.com/google/deepvariant/issues/876,1,['checkpoint'],['checkpoint']
Availability,"] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:52:09.283882 133276175411008 make_examples_core.py:301] Task 8/20: Found 17371 candidate variants; I0822 07:52:09.283904 133276175411008 make_examples_core.py:301] Task 8/20: Created 18820 examples. real 34m15.728s; user 624m43.553s; sys 2m24.932s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@20.gz"" --checkpoint ""/output/checkpoints/ckpt-679"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0822 07:52:10.812179 127086447671104 call_variants.py:563] Total 1 writing processes started.; I0822 07:52:10.813103 127086447671104 dv_utils.py:370] From /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0822 07:52:10.813141 127086447671104 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0822 07:52:10.813338 127086447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:4951,down,downstream,4951,,https://github.com/google/deepvariant/issues/869,1,['down'],['downstream']
Availability,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1725,error,error,1725,,https://github.com/google/deepvariant/issues/214,1,['error'],['error']
Availability,"_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=hs37d5_PhiX.fa \; --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \; --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \; --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \; --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \; --num_shards=15; ```; I have also tried postprocessing with `group_variants`, which also produces a similar error.; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/postprocess_variants \; --group_variants=false \; --ref=hs37d5_PhiX.fa \; --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \; --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz; ```; - Error trace: (if applicable); ```; I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_; 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz; 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285; I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes; I0217 17:00:24.205343 47945364948800 postprocess_variants.py:1183] Transforming call_variants_output to variants.; I0217 17:00:24.205814 47945364948800 postprocess_variants.py:1204] Writing variants to VCF.; I0217 17:00:24.205858 47945364948800 postprocess_variants.py:774] Writing output to VCF file: ""...""/_deepvariant.vcf.gz; I0217 17:00:24.230250 47945364948800 genomics_writer.py:175] Writing ""...""/_deepvariant.vcf.gz with NativeVcfWriter; I0217 17:00:24.234843 47945364948800 postprocess_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:1994,Error,Error,1994,,https://github.com/google/deepvariant/issues/517,1,['Error'],['Error']
Availability,"_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3833,error,error,3833,,https://github.com/google/deepvariant/issues/797,1,['error'],['error']
Availability,"___________________; Layer (type) Output Shape Param # Connected to ; ==================================================================================================; input_1 (InputLayer) [(None, 100, 221, 7 0 [] ; )] ; ...; classification (Dense) (None, 3) 6147 ['dropout[0][0]'] ; ; ==================================================================================================; Total params: 21,810,083; Trainable params: 21,775,651; Non-trainable params: 34,432; __________________________________________________________________________________________________; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95.; I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified.; 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100].; 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:16105,checkpoint,checkpoint,16105,,https://github.com/google/deepvariant/issues/774,1,['checkpoint'],['checkpoint']
Availability,"_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4739,error,error,4739,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,"_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:51:54.272911 127450974123840 make_examples_core.py:301] Task 17/20: Found 17451 candidate variants; I0822 07:51:54.272932 127450974123840 make_examples_core.py:301] Task 17/20: Created 18817 examples; I0822 07:52:09.283522 133276175411008 make_examples_core.py:301] Task 8/20: Writing example info to /output/in",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2866,checkpoint,checkpoints,2866,,https://github.com/google/deepvariant/issues/869,1,['checkpoint'],['checkpoints']
Availability,"_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1984, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1879, in processing_regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:1495,error,error,1495,,https://github.com/google/deepvariant/issues/374,1,['error'],['error']
Availability,"_exoaulhd/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 555, in call_variants; model = modeling.inceptionv3(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 312, in inceptionv3; backbone = add_l2_regularizers(; File ""/work/tmp_dir/Bazel.runfiles_exoaulhd/runfiles/com_google_deepvariant/deepvariant/keras_modeling.py"", line 99, in add_l2_regularizers; model.save_weights(tmp_weights_path); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 562, in __init__; fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid; fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create; OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'); Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 711, in main; for line in proc.stdout:; KeyboardInterrupt; ```; Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:3954,error,error,3954,,https://github.com/google/deepvariant/issues/725,2,['error'],['error']
Availability,_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_cod,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12974,error,error,12974,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel; return hook(metadata_directory, config_settings); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel; self.run_setup(); File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 248, in run_setup; super(_BuildMetaLegacyBackend,; File ""/tmp/pip-build-env-u8h8jp6y/overlay/lib/python3.10/site-packages/setuptools/build_meta.py"", line 142, in run_setup; exec(compile(code, __file__, 'exec'), locals()); File ""setup.py"", line 499, in <module>; setup_package(); File ""setup.py"", line 479, in setup_package; generate_cython(); File ""setup.py"", line 274, in generate_cython; raise RuntimeError(""Running cythonize failed!""); RuntimeError: Running cythonize failed!; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; /*********************************************/; My conda environment contains the following libraries:; conda list; # packages in environment at /opt/miniconda3/envs/deepvariant:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:11307,error,error,11307,,https://github.com/google/deepvariant/issues/859,1,['error'],['error']
Availability,_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_cod,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16423,error,error,16423,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca""; I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7. **Any additional context:**; DeepVariant 1.4 added an additional default channel. This appears to have broken all previously trained models. Using the convenient ""run_deepvariant"" script with the channels=blank options does not result in make_examples generating input examples with only six features. ***Desired Output:***; An option in run_deepvariant that will allow for the creation of example files with the previously standard six input channels.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/563:2586,checkpoint,checkpoint,2586,,https://github.com/google/deepvariant/issues/563,3,['checkpoint'],['checkpoint']
Availability,"_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model; I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0; I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop; name=eval_name); File ""/usr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:3865,checkpoint,checkpoint,3865,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoint']
Availability,"` having numerous options to include additional channels:; ```; --[no]use_allele_frequency: If True, add another channel for pileup images to represent allele frequency information gathered from population call sets.; (default: 'false'); --[no]add_hp_channel: If true, add another channel to represent HP tags per read.; (default: 'false'); --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. Are there `model-ckpt` files for these channel options available somewhere to provide `call_variants` via:; ```; --checkpoint: Required. Path to the TensorFlow model checkpoint to use to evaluate candidate variant calls.; ```. If so, do they include one additional channel or permutations of multiple channels?. If not, is there an alternative way to have `run_deepvariant` use different channels than what the default checkpoint contains during `call_variants`? For example, I am currently unable to include both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run_deepvariant ; --model_type=WGS; --ref='/ref_dir/reference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/out_dir/test1.vcf.gz' ; --intermediate_results_dir='/out_dir/tmp/test1/' ; --num_shards='39' ; --make_examples_extra_args=""use_allele_frequency=true,population_vcfs=/popVCF_dir/UMAG1.POP.FREQ.vcf.gz"" ; --regions=/region_dir/regions_to_test.bed ; ```; - Error ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:1503,checkpoint,checkpoint,1503,,https://github.com/google/deepvariant/issues/568,1,['checkpoint'],['checkpoint']
Availability,"`Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_OTW1G5/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_OTW1G5/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_OTW1G5/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_OTW1G5/runfiles/genomics/deepvariant/make_examples.py"", line 884, in processing_regions_from_options; options.truth_variants_filename).contigs; File ""/tmp/Bazel.runfiles_OTW1G5/runfiles/genomics/deepvariant/core/genomics_io.py"", line 161, in make_vcf_reader; index_mode=index_mode, desired_format_entries=desired_vcf_fields)); ValueError: Not found: No index found for /training-case-study%2FBGISEQ-HG001%2FHG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf` . i had been getting this error trying to train a data. i have got the index file for this same vcf file but still gets this error. please what could be wrong? thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111:1192,error,error,1192,,https://github.com/google/deepvariant/issues/111,2,['error'],['error']
Availability,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:778,checkpoint,checkpoint,778,,https://github.com/google/deepvariant/issues/435,1,['checkpoint'],['checkpoint']
Availability,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:53,checkpoint,checkpoints,53,,https://github.com/google/deepvariant/issues/611,1,['checkpoint'],['checkpoints']
Availability,"`model_train` binary such that this command works:. ```bash; GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```. But, when I try the following. ```bash; GCS_PRETRAINED_MODEL=""gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-pacbio_standard/model.ckpt"". singularity exec --nv ~/bin/deepvariant-1.3.0-gpu.simg \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""examples/training_set.dataset_config.pbtxt"" \; --train_dir=""training"" \; --model_name=""inception_v3"" \; --number_of_steps=900 \; --save_interval_secs=300 \; --start_from_checkpoint=""${GCS_PRETRAINED_MODEL}""; ```; I get the following error presumably because the `model_train` binary does make the correct tensor shape for the PacBio model's checkpoint?. ```python; Warm-starting variables only in TRAINABLE_VARIABLES.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 298, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 283, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_eh_1d4ep/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:1216,error,error,1216,,https://github.com/google/deepvariant/issues/500,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:1767,checkpoint,checkpoint,1767,,https://github.com/google/deepvariant/issues/458,2,['checkpoint'],['checkpoint']
Availability,"able Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:7495,Avail,Available,7495,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"able Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:7833,Avail,Available,7833,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"ailable Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XL",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:8844,Avail,Available,8844,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:4537,error,error,4537,,https://github.com/google/deepvariant/issues/345,2,['error'],['error']
Availability,"ake_examples.py"", line 1108, in main; make_examples_runner(options); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1037, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 764, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 802, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 460, in realign_reads; candidate_windows = self.call_window_selector(region, reads); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 360, in call_window_selector; region.start),; File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 249, in process_reads; for ref_pos in self.process_read(ref, read, ref_offset):; File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 162, in process_read; self._process_soft_clip(cigar, ref, read, ref_pos, read_pos)); File ""/home/guy/deepvariant-0.6.1/bazel-bin/deepvariant/make_examples.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 89, in _process_soft_clip; if read.aligned_quality[read_pos] >= self.config.min_base_quality:; IndexError: list index (0) out of range. Process finished with exit code 1; ```. What might be the cause of the error?; Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/75:3128,error,error,3128,,https://github.com/google/deepvariant/issues/75,1,['error'],['error']
Availability,"ake_examples_core.py:243] Task 0/2: Found 77 candidate variants; I1214 06:02:02.320545 140052653934400 make_examples_core.py:243] Task 0/2: Created 89 examples; I1214 06:10:23.735674 140555214505792 make_examples_core.py:243] Task 1/2: Writing example info to /tmp/tmpiy9bfzyx/make_examples.tfrecord-00001-of-00002.gz.example_info.json; I1214 06:10:23.736027 140555214505792 make_examples_core.py:1883] example_shape = [100, 221, 7]; I1214 06:10:23.736302 140555214505792 make_examples_core.py:1884] example_channels = [1, 2, 3, 4, 5, 6, 19]; I1214 06:10:23.748708 140555214505792 make_examples_core.py:243] Task 1/2: Found 77 candidate variants; I1214 06:10:23.748840 140555214505792 make_examples_core.py:243] Task 1/2: Created 84 examples. real	25m4.324s; user	39m40.647s; sys	0m24.239s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpiy9bfzyx/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpiy9bfzyx/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/tmpiy9bfzyx"" --use_openvino; I1214 06:10:30.972710 140363019278144 call_variants.py:317] From /tmp/tmpiy9bfzyx/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1214 06:10:30.995939 140363019278144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-12-14 06:10:31.060396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:6677,checkpoint,checkpoint,6677,,https://github.com/google/deepvariant/issues/597,1,['checkpoint'],['checkpoint']
Availability,al/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesou,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13541,error,error,13541,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,al/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13822,error,error,13822,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"al/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12866,error,errors,12866,,https://github.com/google/deepvariant/issues/358,1,['error'],['errors']
Availability,"al/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>> && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel; 52 |; --------------------; ERROR: failed to solve: process ""/bin/sh -c ./build-prereq.sh && PATH=\""${HOME}/bin:${PATH}\"" ./build_release_bi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:2158,error,error,2158,,https://github.com/google/deepvariant/issues/902,1,['error'],['error']
Availability,"aligner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --channels insert_size --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m5.584s; user 0m13.426s; sys 0m0.563s; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; I am running this on an AWS graviton4 machine (aarch64 architecture). The Dockerfile does not work for me directly. I had to run `build-prereq.sh` and resolve all the errors manually within a docker container. But I did run build_and_test.sh to make sure all tests passed and the binaries were also built successfully.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:5488,error,errors,5488,,https://github.com/google/deepvariant/issues/879,1,['error'],['errors']
Availability,"all_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/wgs/model.ckpt --use_openvino --batch_size 512 --include_debug_info --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./NA12878_S1.vcf --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" --gvcf_outfile ./NA12878_S1.gvcf.gz ) > ./postprocess_variants.log 2>&1. Is there something I am missing?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:2872,checkpoint,checkpoint,2872,,https://github.com/google/deepvariant/issues/625,1,['checkpoint'],['checkpoint']
Availability,"and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:26.990784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:27.004992 140619855705920 run_deepvariant.py:519] Re-using the directory ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:2264,error,errors,2264,,https://github.com/google/deepvariant/issues/761,1,['error'],['errors']
Availability,"ant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:2855,avail,available,2855,,https://github.com/google/deepvariant/issues/774,1,['avail'],['available']
Availability,"ant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:1748,avail,available,1748,,https://github.com/google/deepvariant/issues/248,1,['avail'],['available']
Availability,"art-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:1273,avail,available,1273,,https://github.com/google/deepvariant/issues/226,1,['avail'],['available']
Availability,"ate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. ; **Steps to reproduce:**; - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :; ; ```; E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'; 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0; 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX; PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M; 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:2853,error,error,2853,,https://github.com/google/deepvariant/issues/666,1,['error'],['error']
Availability,"attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: F",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:3354,checkpoint,checkpoint,3354,,https://github.com/google/deepvariant/issues/129,1,['checkpoint'],['checkpoint']
Availability,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:2696,error,errors,2696,,https://github.com/google/deepvariant/issues/419,1,['error'],['errors']
Availability,"bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:1973,echo,echo,1973,,https://github.com/google/deepvariant/issues/60,1,['echo'],['echo']
Availability,"bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepvariant_log.txt 2>&1. Command exit status:; 127. Command output:; (empty). Command error:; docker: Error response from daemon: open /var/lib/docker/overlay2/fe3663cd03e849890d83be14603f217249f3f43f9585b554df599d0318909f21/.tmp-committed2046174062: no such file or directory.; See 'docker run --help'. Work dir:; /home/ubuntu/dd/nextflow2/work/ea/9ecd306270fe3f00d9b73f8261fe89. Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`. -- Check '.nextflow.log' file for details",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:2761,error,error,2761,,https://github.com/google/deepvariant/issues/883,2,"['Error', 'error']","['Error', 'error']"
Availability,"blishDir ""${params.outdir}/6.variantM"", mode: 'copy'; cpus 16. input:; tuple val(sample_id), path(read_files); val(params.reference); val(params.bed_file); ; output:; tuple val(sample_id), path(""${sample_id}_raw.vcf.gz""), path(""${sample_id}_raw.gvcf.gz""), emit: raw_vcfs. script:; """"""; sudo docker run \; -v ""${PWD}"":""${PWD}"" \; google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${PWD}/${params.reference} \; --reads ${PWD}/output/4.markDuplicate/${sample_id}_sorted_md.bam \; --regions ${PWD}/${params.bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:1691,ERROR,ERROR,1691,,https://github.com/google/deepvariant/issues/883,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"bove command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored:; <tf.Variable 'classification/kernel:0' shape=(2048, 3) dtype=float32, numpy=_. ------------; Please note that I used the same checkpoint path as I was using while running _run_deepvariant.py_.; Could you please help me to resolve the issue and suggest if I am missing anything while using _call_varaint_keras.py_?. Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:2040,checkpoint,checkpoint,2040,,https://github.com/google/deepvariant/issues/636,3,['checkpoint'],['checkpoint']
Availability,"buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9771,avail,available,9771,,https://github.com/google/deepvariant/issues/489,1,['avail'],['available']
Availability,"by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2946,checkpoint,checkpoints,2946,,https://github.com/google/deepvariant/issues/797,1,['checkpoint'],['checkpoints']
Availability,"c.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:2558,error,error,2558,,https://github.com/google/deepvariant/issues/678,1,['error'],['error']
Availability,call_variant step 0.7.1 and 0.7.2rc - job failed with error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:54,error,error,54,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,"call_variants_output to variants.; I0217 17:38:26.135714 139808123168576 postprocess_variants.py:1318] Using 60 CPUs for parallelization of variant transformation.; I0217 17:39:46.024054 139808123168576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: SRR1572254; I0217 17:39:46.024466 139808123168576 postprocess_variants.py:1216] --sample_name is set but was not used.; I0217 17:53:05.493927 139808123168576 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 7.4284539779027305 minutes; [E::hts_open_format] Failed to open file ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz"" : No such file or directory; 2024-02-17 17:53:05.496309: E third_party/nucleus/io/merge_variants.cc:115] opening writer failedCould not open variants_path: /public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz; 2024-02-17 17:53:05.496469: F ./third_party/nucleus/core/statusor.h:230] Non-OK-status: status_ status: UNKNOWN: Could not open variants_path: /public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz; Fatal Python error: Aborted. Current thread 0x00007f279d84a740 (most recent call first):; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1391 in main; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419 in <module>. real	27m34.677s; user	18m45.707s; sys	4m47.747s; **Any additional context:**; ![image](https://github.com/google/deepvariant/assets/108465040/18d9cc45-ef3a-4f05-8a85-19eefac74034); call_variants may be have done ,but postprocess_variants Failed to open file ""file.vcf.gz"" : No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/773:3711,error,error,3711,,https://github.com/google/deepvariant/issues/773,1,['error'],['error']
Availability,ce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12693,error,error,12693,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,ce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16708,error,error,16708,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"cf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; Traceback (most recent call last):; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 220, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 157, in default_options; sa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:1961,ERROR,ERROR,1961,,https://github.com/google/deepvariant/issues/870,1,['ERROR'],['ERROR']
Availability,"ch of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘/mnt/cdw-genome’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further developme",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4810,error,error,4810,,https://github.com/google/deepvariant/issues/171,1,['error'],['error']
Availability,che/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googleso,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8691,error,error,8691,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:; ```; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta; output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta; output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta; output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:3290,checkpoint,checkpoint,3290,,https://github.com/google/deepvariant/issues/611,1,['checkpoint'],['checkpoint']
Availability,"cleus/util/ranges.py"", line 459, in bed_parser; for r in fin.iterate():; File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_pz6djil_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 102, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Unknown: BED record has invalid number of fields; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna --reads Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam --examples Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz --gvcf Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz --regions Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed --task 0. real	0m3.367s; user	0m2.683s; sys	0m0.545s; ```. **First lines:**; **First 10 lines of sorted marked duplicate bam is: ** ; BAM?P@HD	VN:1.6	SO:coordinate; @SQ	SN:NC_000001.11	LN:248956422; @SQ	SN:NT_187361.1	LN:175055; @SQ	SN:NT_187362.1	LN:32032; @SQ	SN:NT_187363.1	LN:127682; @SQ	SN:NT_187364.1	LN:66860; @SQ	SN:NT_187365.1	LN:40176; @SQ	SN:NT_187366.1	LN:42210; @SQ	SN:NT_187367.1	LN:176043; @SQ	SN:NT_187368.1	LN:40745. **First line of reference hg38 is:**; >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly. **First line of bed file is:**; NC_000001.11 65509 65625. I have got deepvariant and the above code to work for another dataset with a different bed file used - but I'm not sure why the ValueError: Unknown: BED record has invalid number of fields error is occurring. . Thanks!; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:18524,error,error,18524,,https://github.com/google/deepvariant/issues/542,1,['error'],['error']
Availability,code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18704,error,error,18704,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_goog,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17272,error,error,17272,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17555,error,error,17555,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Bui,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19836,error,error,19836,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20404,error,error,20404,,https://github.com/google/deepvariant/issues/19,7,"['ERROR', 'error']","['ERROR', 'error']"
Availability,compile from source error with version,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:20,error,error,20,,https://github.com/google/deepvariant/issues/189,1,['error'],['error']
Availability,"cription=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	default; ```. I then tried explicitly using /dev/stdin, this time I got errors; ```bash; cat ${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam | \; docker run \; --gpus all \; --rm \; -i \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/dev/stdin \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/output/output.vcf \; --num_shards=4 ; ```. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1325, in make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/288:2947,error,errors,2947,,https://github.com/google/deepvariant/issues/288,1,['error'],['errors']
Availability,"cs` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2574,checkpoint,checkpoint,2574,,https://github.com/google/deepvariant/issues/722,1,['checkpoint'],['checkpoint']
Availability,"cute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ol6r0lcv/runfiles/com_google_deepvariant/deepvariant/train.py"", line 109, in train; tf.tpu.experimental.initialize_tpu_system(resolver); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py"", line 113, in initialize_tpu_system; raise errors.NotFoundError(; tensorflow.python.framework.errors_impl.NotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; Registered devices: [CPU]; Registered kernels:; <no registered kernels>. [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:4822,error,errors,4822,,https://github.com/google/deepvariant/issues/841,1,['error'],['errors']
Availability,"d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 888, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 875, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4180,error,error,4180,,https://github.com/google/deepvariant/issues/129,1,['error'],['error']
Availability,"d), path(""${sample_id}_raw.vcf.gz""), path(""${sample_id}_raw.gvcf.gz""), emit: raw_vcfs. script:; """"""; sudo docker run \; -v ""${PWD}"":""${PWD}"" \; google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${PWD}/${params.reference} \; --reads ${PWD}/output/4.markDuplicate/${sample_id}_sorted_md.bam \; --regions ${PWD}/${params.bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepvariant_log.txt 2>&1. Command exit status:; 127. Command output:; (empty). Command error:; docker: Error response from daemon: open /var/lib/docker/overlay2/fe3663cd03e849890d83be14603f21",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:1882,ERROR,ERROR,1882,,https://github.com/google/deepvariant/issues/883,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,de_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12412,error,error,12412,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"ded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘/mnt/cdw-genome’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on th",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:6174,Error,Error,6174,,https://github.com/google/deepvariant/issues/171,2,"['Error', 'error']","['Error', 'error']"
Availability,deepvariant error on gpu,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/754:12,error,error,12,,https://github.com/google/deepvariant/issues/754,1,['error'],['error']
Availability,deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:74,error,error,74,,https://github.com/google/deepvariant/issues/598,2,['error'],['error']
Availability,"del_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_cpu/HG001.apptainer.cpu.output.vcf.gz \; --output_gvcf=output_apptainer_cpu/HG001.apptainer.cpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; ```; It was successful. Both vcf and gvcf were generated. 2. GPU version; I run the following command:; ```apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; ```. It seems there are some errors and GPU was not used. These are the output (part of the output were removed due to the limit of the characters of this post):. ```; ➜ t7 apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its content",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:1596,error,errors,1596,,https://github.com/google/deepvariant/issues/774,1,['error'],['errors']
Availability,dv_* file syntax error from bioconda installed 1.4.0 version DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627:17,error,error,17,,https://github.com/google/deepvariant/issues/627,1,['error'],['error']
Availability,"e are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:953,ERROR,ERROR,953,,https://github.com/google/deepvariant/issues/739,1,['ERROR'],['ERROR']
Availability,"e bazel-out/k8-opt/bin/external/snappy -iquote external/clif -iquote bazel-out/k8-opt/bin/external/clif -iquote external/local_config_python -iquote bazel-out/k8-opt/bin/external/local_config_python -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_python/python_include -isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++14' '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc -o bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o); Execution platform: @local_execution_config_platform//:platform; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:283:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:534:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:815:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };. I knew that ""nullptr"" might be change to ""NULL"" in pyth3.8. But how should I change the specific files?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441:4506,error,error,4506,,https://github.com/google/deepvariant/issues/441,3,['error'],['error']
Availability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isn’t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7309,error,error,7309,,https://github.com/google/deepvariant/issues/171,1,['error'],['error']
Availability,"e following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:1056,mainten,maintenance,1056,,https://github.com/google/deepvariant/issues/725,1,['mainten'],['maintenance']
Availability,"e thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:1935,error,error,1935,,https://github.com/google/deepvariant/issues/797,1,['error'],['error']
Availability,"e you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.py:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:1034,Error,Error,1034,,https://github.com/google/deepvariant/issues/879,1,['Error'],['Error']
Availability,"e/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test NO STATUS; //deepvariant:model_train_test NO STATUS; //deepvariant:modeling_test NO STATUS; //deepvariant:pileup_image_test NO STATUS; //deepvariant:postprocess_variants_lib_test NO STATUS; //deepvariant:postprocess_variants_test NO STATUS; //deepvariant:tf_utils_test NO STATUS; //deepvariant:utils_test NO STATUS; //deepvariant:variant_caller_test NO STATUS; //deepvariant:variant_calling_test NO STATUS; //deepvariant:variant_labeler_test NO STATUS; //deepvariant/core:cigar_test NO STATUS; //deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:3251,ERROR,ERROR,3251,,https://github.com/google/deepvariant/issues/6,3,['ERROR'],['ERROR']
Availability,"e/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.d '-frandom-seed=bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.o' -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/htslib -iquote bazel-out/k8-py3-opt/genfiles/external/htslib -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -iquote external/clif -iquote bazel-out/k8-py3-opt/genfiles/external/clif -iquote external/local_config_python -iquote bazel-out/k8-py3-opt/genfiles/external/local_config_python -iquote external/protobuf_archive -iquote bazel-out/k8-py3-opt/genfiles/external/protobuf_archive -isystem external/htslib/htslib/htslib_1_6 -isystem bazel-out/k8-py3-opt/genfiles/external/htslib/htslib/htslib_1_6 -isystem external/htslib -isystem bazel-out/k8-py3-opt/genfiles/external/htslib -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/local_config_python/python_include -isystem bazel-out/k8-py3-opt/genfiles/external/local_config_python/python_include -isystem external/protobuf_archive/src -isystem bazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-py3-opt/genfiles/deepvariant/core/python/hts_verbose.cc -o bazel-out/k8-py3-opt/bin/deepvariant/core/python/_objs/hts_verbose_cclib/deepvariant/core/python/hts_verbose.o); bazel-out/k8-py3-opt/genfiles/deepvariant/core/python/hts_verbose.cc: In function 'PyObject* deepvariant_core_python_hts__verbose_clifwrap::Init()':; bazel-out/k8-py3-opt/genfiles/deepvariant/core/python/hts_verbose.cc:134:143: error: 'Py_InitModule3' was not declared in this scope; PyObject* module = Py_InitModule3(""deepvariant.core.python.hts_verbose"", Methods, ""CLIF-generated module for deepvariant/core/hts_verbose.h"");; ^; (14:05:26) INFO: Elapsed time: 4.048s, Critical Path: 1.26s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/31:3016,error,error,3016,,https://github.com/google/deepvariant/issues/31,1,['error'],['error']
Availability,"e/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2083,mainten,maintenance,2083,,https://github.com/google/deepvariant/issues/833,1,['mainten'],['maintenance']
Availability,e2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/e,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14977,error,error,14977,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,e2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8218,ERROR,ERROR,8218,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"eSamReader; I0217 23:33:26.024591 140533724936000 make_examples_core.py:301] Task 15/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; 2024-02-17 23:33:25.886408: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933485 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.940178 139726133032768 make_examples_core.py:301] Task 4/16: Preparing inputs; I0217 23:33:25.967752 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; ...; 2024-02-17 23:33:25.888518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933323 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939591 140099871606592 make_examples_core.py:301] Task 0/16: Preparing inputs; I0217 23:33:25.967773 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024448 140099871606592 make_examples_core.py:301] Task 0/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; I0217 23:33:34.679437 140533724936000 make_examples_core.py:301] Task 15/16: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I0217 23:33:34.748554 14053372",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:9078,error,error,9078,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,e_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_cod,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18419,error,error,18419,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"earch/allele_frequency/pretrained_model_WGS;tab=objects?pli=1&prefix=&forceOnObjectsSortingFiltering=false) with `run_deepvariant` v.1.1 while including a PopVCF channel during `make_examples`. However, that model does not include a channel for `insert_size` as their work predates v1.4. . With the default extra channel for `'insert_size'` in v1.4, and `make_examples` having numerous options to include additional channels:; ```; --[no]use_allele_frequency: If True, add another channel for pileup images to represent allele frequency information gathered from population call sets.; (default: 'false'); --[no]add_hp_channel: If true, add another channel to represent HP tags per read.; (default: 'false'); --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. Are there `model-ckpt` files for these channel options available somewhere to provide `call_variants` via:; ```; --checkpoint: Required. Path to the TensorFlow model checkpoint to use to evaluate candidate variant calls.; ```. If so, do they include one additional channel or permutations of multiple channels?. If not, is there an alternative way to have `run_deepvariant` use different channels than what the default checkpoint contains during `call_variants`? For example, I am currently unable to include both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:1139,avail,available,1139,,https://github.com/google/deepvariant/issues/568,2,"['avail', 'checkpoint']","['available', 'checkpoint']"
Availability,"ecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:1520,error,error,1520,,https://github.com/google/deepvariant/issues/483,1,['error'],['error']
Availability,"ed DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to seeing what it can do with these data, so many thanks in advance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312:1461,down,downsampling,1461,,https://github.com/google/deepvariant/issues/312,1,['down'],['downsampling']
Availability,"eepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3888,error,error,3888,,https://github.com/google/deepvariant/issues/797,1,['error'],['error']
Availability,"eepvariant_runner.py"", line 472, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 464, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 350, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:4690,ERROR,ERROR,4690,,https://github.com/google/deepvariant/issues/129,1,['ERROR'],['ERROR']
Availability,"eference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:1559,Error,Error,1559,,https://github.com/google/deepvariant/issues/653,1,['Error'],['Error']
Availability,"els/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt""; ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```; docker run --rm --gpus '""device=1""' \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_eval \; --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --checkpoint_dir=""${TRAINING_DIR}"" \; --batch_size=512 \; --min_eval_interval_s=1 \; --eval_timeout=1000; ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:; ```; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta; output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta; output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta; output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:2282,checkpoint,checkpoints,2282,,https://github.com/google/deepvariant/issues/611,1,['checkpoint'],['checkpoints']
Availability,"em?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_0.1_percent.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@4.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@4.gz"" --norealign_reads --regions ""chr20:10,000,000-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354:1398,Error,Error,1398,,https://github.com/google/deepvariant/issues/354,1,['Error'],['Error']
Availability,"ensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7007,Error,Error,7007,,https://github.com/google/deepvariant/issues/608,1,['Error'],['Error']
Availability,"ential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/in",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416:1238,error,error,1238,,https://github.com/google/deepvariant/issues/416,1,['error'],['error']
Availability,"epper_hp/; [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0; [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10; [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec; [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14; [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec; [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s; user	1220m53.668s; sys	15m35.409s; [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_hp/; -------; [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; ; echo ""STARTING DEEPVARIANT""; ; time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:21509,echo,echo,21509,,https://github.com/google/deepvariant/issues/491,1,['echo'],['echo']
Availability,"epvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:1062,Error,Error,1062,,https://github.com/google/deepvariant/issues/485,1,['Error'],['Error']
Availability,error in make_examples extract_sample_name_from_reads,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:0,error,error,0,,https://github.com/google/deepvariant/issues/28,1,['error'],['error']
Availability,error in training DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:0,error,error,0,,https://github.com/google/deepvariant/issues/802,1,['error'],['error']
Availability,error running help command,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/362:0,error,error,0,,https://github.com/google/deepvariant/issues/362,1,['error'],['error']
Availability,error while running deepvariant with a bam file with phasing information,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/822:0,error,error,0,,https://github.com/google/deepvariant/issues/822,1,['error'],['error']
Availability,"es?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_varian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:1620,Error,Error,1620,,https://github.com/google/deepvariant/issues/833,2,['Error'],['Error']
Availability,"esolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1845,down,download,1845,,https://github.com/google/deepvariant/issues/902,1,['down'],['download']
Availability,esource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googles,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10117,error,error,10117,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"et_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:51:54.272911 127450974123840 make_examples_core.py:301] Task 17/20: Found 17451 candidate variants; I0822 07:51:54.272932 127450974123840 make_examples_core.py:301] Task 17/20: Created 18817 examples; I0822 07:52:09.283522 133276175411008 make_examples_core.py:301] Task 8/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00008-of-00020.gz.example_info.json; I0822 07:52:09.283617 133276175411008 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:3105,Error,Error,3105,,https://github.com/google/deepvariant/issues/869,1,['Error'],['Error']
Availability,"f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check fo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:3487,echo,echo,3487,,https://github.com/google/deepvariant/issues/739,1,['echo'],['echo']
Availability,"f_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:21339,checkpoint,checkpoint,21339,,https://github.com/google/deepvariant/issues/537,1,['checkpoint'],['checkpoint']
Availability,failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:38,Error,Error,38,,https://github.com/google/deepvariant/issues/70,1,['Error'],['Error']
Availability,"ference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/out_dir/test1.vcf.gz' ; --intermediate_results_dir='/out_dir/tmp/test1/' ; --num_shards='39' ; --make_examples_extra_args=""use_allele_frequency=true,population_vcfs=/popVCF_dir/UMAG1.POP.FREQ.vcf.gz"" ; --regions=/region_dir/regions_to_test.bed ; ```; - Error trace: (if applicable); ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/out_dir/tmp/test1/call_variants_output.tfrecord.gz"" --examples ""/out_dir/tmp/test1/make_examples.tfrecord@39.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/out_dir/tmp/test1/"". I0919 17:19:47.185331 46912500266816 call_variants.py:317] From /out_dir/tmp/test1/make_examples.tfrecord-00000-of-00039.gz.example_info.json: Shape of input examples: [100, 221, 8], Channels of input examples: [1, 2, 3, 4, 5, 6, 8, 19].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8. real 0m3.217s; user 0m4.066s; sys 0m4.174s. real 77m45.059s; user 2960m49.979s; sys 39m40.911s```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:3955,checkpoint,checkpoint,3955,,https://github.com/google/deepvariant/issues/568,3,['checkpoint'],['checkpoint']
Availability,"following: . ```; docker run \; -v ${HOME}:${HOME} \; google/deepvariant:1.6.1 \; train \; --config=${HOME}/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.num_epochs=10 \; --config.learning_rate=0.001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=tpu \; --config.batch_size=1024; ```. However, I am not an expert with TPUs, and this is entirely new to me. Below is the error I encountered. Do you have any suggestions or can you direct me to an updated tutorial to follow?. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0627 21:18:43.707066 139683296487232 train.py:92] Running with debug=False; I0627 21:18:43.707488 139683296487232 train.py:100] Use TPU at local; I0627 21:18:43.707705 139683296487232 train.py:103] experiment_dir: /home/gambardella/training_chk; INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.; I0627 21:18:43.707828 139683296487232 tpu_strategy_util.py:57] Deallocate tpu buffers before initializing tpu system.; INFO:tensorflow:Initializing the TPU system: local; I0627 21:18:43.846984 139683296487232 tpu_strategy_util.py:81] Initializing the TPU system: local; 2024-06-27 21:18:43.848149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-criti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841:1224,down,downstream,1224,,https://github.com/google/deepvariant/issues/841,1,['down'],['downstream']
Availability,gene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f5478ac499a: Pull complete ; b07098b67a6d: Pull complete ; 0accf0f55269: Pull complete ; ccc95462eb8f: Pull complete ; f1416983139e: Pull complete ; 2242c582e0cc: Pull complete ; 8f749be1be0b: Pull complete ; 03fdf02906f9: Pull complete ; ea2763a10d98: Pull complete ; fff529645086: Pull complete ; 42ad15be12fa: Pull complete ; 82830610edc8: Pull complete ; d1a85d710a45: Pull complete ; a7463a89d05f: Pull complete ; 966acfe9e7ff: Pull complete ; 987808dd3c93: Pull complete ; 079d9da9d9ee: Pull complete ; 875aa906c231: Pull complete ; 9463688d586c: Pull complete ; 2943d0ab0b4f: Pull complete ; 7a91c30c18a7: Pull complete ; 66996f762384: Pull complete ; 90237953ba0a: Pull complete ; Digest: sha256:b269b5b547bd8aa46f104f7bbdffaf6cdcf1df0143cfe85aa7f5a68767fa49bc; Status: Downloaded newer image for google/deepvariant:1.1.0; I0611 15:20:51.532788 140688071014144 run_deepvariant.py:317] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | p,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:2776,Down,Downloaded,2776,,https://github.com/google/deepvariant/issues/462,1,['Down'],['Downloaded']
Availability,"ges/tensorflow/python/training/monitored_session.py"", line 1464, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:8178,error,error,8178,,https://github.com/google/deepvariant/issues/679,1,['error'],['error']
Availability,"gets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4842,ERROR,ERROR,4842,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,getting error while running deepvariant: ERROR[12893] error waiting for container: unexpected EOF,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482:8,error,error,8,,https://github.com/google/deepvariant/issues/482,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,glesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/exte,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14397,error,error,14397,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,glibc causes segmentation fault,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/179:26,fault,fault,26,,https://github.com/google/deepvariant/issues/179,1,['fault'],['fault']
Availability,googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesou,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16989,error,error,16989,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"gs` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitialized -Wno-unused-function -Wno-sign-compare -Wno-write-strings -fno-inline -fno-canonical-system-headers -Wno-; builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/libssw/src/ssw.c -o bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw; .pic.o); external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory; compilation terminated.; Target //:binaries failed to build",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:2367,error,error,2367,,https://github.com/google/deepvariant/issues/123,1,['error'],['error']
Availability,"h `run_deepvariant` v.1.1 while including a PopVCF channel during `make_examples`. However, that model does not include a channel for `insert_size` as their work predates v1.4. . With the default extra channel for `'insert_size'` in v1.4, and `make_examples` having numerous options to include additional channels:; ```; --[no]use_allele_frequency: If True, add another channel for pileup images to represent allele frequency information gathered from population call sets.; (default: 'false'); --[no]add_hp_channel: If true, add another channel to represent HP tags per read.; (default: 'false'); --channels: Comma-delimited list of optional channels to add. Available Channels: read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size; ```. Are there `model-ckpt` files for these channel options available somewhere to provide `call_variants` via:; ```; --checkpoint: Required. Path to the TensorFlow model checkpoint to use to evaluate candidate variant calls.; ```. If so, do they include one additional channel or permutations of multiple channels?. If not, is there an alternative way to have `run_deepvariant` use different channels than what the default checkpoint contains during `call_variants`? For example, I am currently unable to include both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run_deepvariant ; --model_type=WGS; --ref='/ref_dir/reference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:1250,checkpoint,checkpoint,1250,,https://github.com/google/deepvariant/issues/568,1,['checkpoint'],['checkpoint']
Availability,"h_or_file, clear_devices, import_scope, **kwargs)[0]; 1675 ; 1676 ; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs); 1694 import_scope=import_scope,; 1695 return_elements=return_elements,; -> 1696 **kwargs)); 1697 ; 1698 saver = _create_saver_from_imported_meta_graph(; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements); 804 input_map=input_map,; 805 producer_op_list=producer_op_list,; --> 806 return_elements=return_elements); 807 ; 808 # Restores all the other collections.; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs); 486 'in a future version' if date is None else ('after %s' % date),; 487 instructions); --> 488 return func(*args, **kwargs); 489 return tf_decorator.make_decorator(func, new_func, 'deprecated',; 490 _add_deprecated_arg_notice_to_docstring(; >; >/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list); 420 except errors.InvalidArgumentError as e:; 421 # Convert to ValueError for backwards compatibility.; --> 422 raise ValueError(str(e)); 423 ; 424 # Create _DefinedFunctions for any imported functions.; >; >ValueError: NodeDef expected inputs 'float, int32' do not match 1 inputs specified; Op<name=CrossReplicaSum; signature=input:T, group_assignment:int32 -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT]>; NodeDef: {{node CrossReplicaSum}} = CrossReplicaSum[T=DT_FLOAT, _class=[""loc:@gradi...s_grad/mul""], _output_shapes=[[3,3,6,32]], _tpu_replicate=""cluster""](gradients/AddN_109). Best Regards,. Masaru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127:2462,error,errors,2462,,https://github.com/google/deepvariant/issues/127,1,['error'],['errors']
Availability,"he directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**; ```; INFO: Using cached SIF image; I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory; Traceback (most recent call last):; File ""./B",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:1940,error,error,1940,,https://github.com/google/deepvariant/issues/533,3,"['down', 'error']","['downloaded', 'error']"
Availability,"hecked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:1557,Error,Error,1557,,https://github.com/google/deepvariant/issues/550,1,['Error'],['Error']
Availability,"hello, ; I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s; user 112m32.542s; sys 0m37.407s; I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720:86,error,error,86,,https://github.com/google/deepvariant/issues/720,1,['error'],['error']
Availability,"hello,; I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening?; ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b); ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf); ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed); Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655:43,error,error,43,,https://github.com/google/deepvariant/issues/655,2,['error'],['error']
Availability,"hello,; When analyzing PacBio data, I encountered some problems.; I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error.; Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam; My cmd:; 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'; 2. /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref hs37d5.fasta \; --reads n1000.subreads_to_ccs_aligned.bam \; --output_vcf n1000.depv.vcf.gz \; --output_gvcf n1000.depv.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir intermediate_results_dir; Error:; ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem?; Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672:190,error,error,190,,https://github.com/google/deepvariant/issues/672,2,"['Error', 'error']","['Error', 'error']"
Availability,"help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1497,checkpoint,checkpoint,1497,,https://github.com/google/deepvariant/issues/733,1,['checkpoint'],['checkpoint']
Availability,"hen loading the weights of the model. **Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:1185,checkpoint,checkpoint,1185,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability,"hi ; i want to cast model in /opt/models/wgs/model.ckpt to fp16 to satisfy my poor machine，; but can not reload the model from model.ckpt.meta.; I try to get graph from source code ,so that i can restore and save a new model,but the model i saved did not math the model tensorname in the source code.; how can i convert trained model to fp16? or where i can download a fp16 model",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/389:358,down,download,358,,https://github.com/google/deepvariant/issues/389,1,['down'],['download']
Availability,"his mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1527,checkpoint,checkpoints,1527,,https://github.com/google/deepvariant/issues/845,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"his mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:2314,checkpoint,checkpoints,2314,,https://github.com/google/deepvariant/issues/857,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"his prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also noticed there are several deprecation warnings in the log file",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:4118,avail,avail,4118,,https://github.com/google/deepvariant/issues/269,1,['avail'],['avail']
Availability,"iants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; _______________________",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14022,mainten,maintenance,14022,,https://github.com/google/deepvariant/issues/774,1,['mainten'],['maintenance']
Availability,"ife in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2004,checkpoint,checkpoints,2004,,https://github.com/google/deepvariant/issues/722,1,['checkpoint'],['checkpoints']
Availability,"ilable Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:8507,Avail,Available,8507,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,import error for GLIBCXX_3.4.21,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/22:7,error,error,7,,https://github.com/google/deepvariant/issues/22,1,['error'],['error']
Availability,"in make_examples.py script when I pass to --regions parameter 'chr20' it produces error and if I use '20' it does produce for chr 20 only(it seems that the documentation is a bit ambiguous - in the description of ""exclude_regions"" it does pass only 20 to --regions flag). I used the file from the example. The error I get from passing ""chr20"" is:; ```; python input/bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""; ${BAM}"" --regions ""chr20"" --examples hexample.tfrecord.gz. [W::hts_idx_load2] The index file is older than the data file: /home/user78/project_examples_sources/input/data/HG002_NIST_150bp_50x.bam.bai; 2018-05-14 22:42:53.250882: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0514 22:42:53.251095 140678655026944 genomics_reader.py:174] Reading /home/user78/project_examples_sources/input/data/HG002_NIST_150bp_50x.bam with NativeSamReader; I0514 22:42:53.259068 140678655026944 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /home/user78/project_examples_sources/input/data/HG002_NIST_150bp_50x.bam.bai; 2018-05-14 22:42:53.286098: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0514 22:42:53.286269 140678655026944 genomics_reader.py:174] Reading /home/user78/project_examples_sources/input/data/HG002_NIST_150bp_50x.bam with NativeSamReader; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_OAibdX/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_OAibdX/runfiles/com_google_deepvaria",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/73:82,error,error,82,,https://github.com/google/deepvariant/issues/73,2,['error'],['error']
Availability,"in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 970, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1193, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1373, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1399, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; 	 [[node IteratorGetNext; (defined at /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py:60); ]]. Errors may have originated from an input operation.; Input Source operations connected to node IteratorGetNext:; In[0] IteratorV2 (defined at /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py:58). Operation defined at: (most recent call last); >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; >>> tf.compat.v1.app.run(); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; >>> _run_main(main, args); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; >>> sys.exit(main(argv)); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; >>> call_variants(; >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:13078,Error,Errors,13078,,https://github.com/google/deepvariant/issues/564,1,['Error'],['Errors']
Availability,"in; sys.exit(main(argv)); File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants; predictions = model.signatures['serving_default'](batch[1]); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__; return self._call_impl(args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl; return self._call_with_flat_signature(args, kwargs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature; return self._call_flat(args, self.captured_inputs, cancellation_manager); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat; return super(_WrapperFunction, self)._call_flat(args, captured_inputs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; return self._build_call_outputs(self._inference_function.call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call; outputs = execute.execute(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute; tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6; [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:9578,error,error,9578,,https://github.com/google/deepvariant/issues/797,1,['error'],['error']
Availability,"ine 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:3351,checkpoint,checkpoint,3351,,https://github.com/google/deepvariant/issues/273,1,['checkpoint'],['checkpoint']
Availability,"iners/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0710 12:09:25.874915 139944337696576 postprocess_variants.py:1318] Using 19 CPUs for parallelization of variant transformation.; I0710 12:09:45.096508 139944337696576 postprocess_variants.py:1211] Using sample name from call_var",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1843,avail,available,1843,,https://github.com/google/deepvariant/issues/849,1,['avail'],['available']
Availability,"ing system: ; NAME=Red Hat Enterprise Linux; VERSION=9.4 (Plow); - DeepVariant version: deepvariant:1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker (via podman); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1363,Error,Error,1363,,https://github.com/google/deepvariant/issues/849,1,['Error'],['Error']
Availability,"ion(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 444, in await_completion; six.reraise(t, v, tb); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 303, in finish_bundle; bundles = _read_values_to_bundles(reader); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/transform_evaluator.py"", line 293, in _read_values_to_bundles; read_result = [GlobalWindows.windowed_value(e) for e in reader]; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/concat_source.py"", line 83, in read; range_tracker.sub_range_tracker(source_ix)):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 175, in read_records; record = _TFRecordUtil.read_record(file_handle); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/tfrecordio.py"", line 131, in read_record; buf = file_handle.read(buf_length_expected); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 240, in read; self._fetch_to_internal_buffer(num_bytes); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/io/filesystem.py"", line 199, in _fetch_to_internal_buffer; self._read_buffer.write(decompressed); MemoryError: out of memory. real 471m5.950s; user 150m11.981s; sys 30m20.862s; ```; What reason are the cause of the error?; Can I train a new model without using shuffle_tfrecords_beam.py? If yes, can you tell me how to do that? ; Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:11317,error,error,11317,,https://github.com/google/deepvariant/issues/91,1,['error'],['error']
Availability,"ion_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2865,Checkpoint,Checkpoint,2865,,https://github.com/google/deepvariant/issues/722,10,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpoint']"
Availability,"iprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23423423423423443"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 4. Changing to 0.7.2rc gives following error: ; [12/12/2018 13:12:23 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 13:31:21 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] Job failed with error {...........cutout...; 13:33:48 Stopped running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 202, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:5079,checkpoint,checkpoint,5079,,https://github.com/google/deepvariant/issues/129,1,['checkpoint'],['checkpoint']
Availability,"irf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13351,mainten,maintenance,13351,,https://github.com/google/deepvariant/issues/761,1,['mainten'],['maintenance']
Availability,"isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc -o bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o); Execution platform: @bazel_tools//platforms:host_platform; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc: In function 'PyObject* third__party_nucleus_io_python_hts__verbose_clifwrap::Init()':; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc:134:22: error: 'Py_InitModule3' was not declared in this scope; PyObject* module = Py_InitModule3(""third_party.nucleus.io.python.hts_verbose"", Methods, ""CLIF-generated module for third_party/nucleus/io/hts_verbose.h"");; ^~~~~~~~~~~~~~; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc:134:22: note: suggested alternative: 'Py_Initialize'; PyObject* module = Py_InitModule3(""third_party.nucleus.io.python.hts_verbose"", Methods, ""CLIF-generated module for third_party/nucleus/io/hts_verbose.h"");; ^~~~~~~~~~~~~~; Py_Initialize; (13:01:20) INFO: Elapsed time: 180.151s, Critical Path: 34.76s; (13:01:20) INFO: 1380 processes: 1380 local.; (13:01:20) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:dv_vcf_constants_test NO STATUS; //deepvariant:exclude_contigs_test NO STATUS; //deepvariant:haplotypes_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:4324,error,error,4324,,https://github.com/google/deepvariant/issues/189,1,['error'],['error']
Availability,"ither '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5226,ERROR,ERROR,5226,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"ividualmodel/checkpoints/ckpt-14902"" \; > --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/Bactrocera_dorsalis_rearing_male_mt_chr_unpl.fasta"" \; > --reads ""${filesdir}_mapped/${sample}.bam"" \; > --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/modeltestout/2fullindividualmodeltest/${sample}.vcf.gz"". Here are the contents of the checkpoints folder for this training: . > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jun 29 01:06 ..; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 .; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 1399938809505",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:1469,checkpoint,checkpoint,1469,,https://github.com/google/deepvariant/issues/866,1,['checkpoint'],['checkpoint']
Availability,"k 0/16: Writing examples to /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz; I0217 23:33:34.811659 140099871606592 make_examples_core.py:301] Task 0/16: Overhead for preparing inputs: 8 seconds; I0217 23:33:34.827609 140099871606592 make_examples_core.py:301] Task 0/16: 0 candidates (0 examples) [0.02s elapsed]; ...; I0218 00:34:18.301548 140191938357056 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0218 00:34:18.301738 140191938357056 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0218 00:34:18.302148 140191938357056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants; I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s; user	928m53.495s; sys	2m16.403s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our Tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:13171,checkpoint,checkpoint,13171,,https://github.com/google/deepvariant/issues/774,1,['checkpoint'],['checkpoint']
Availability,"k: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:1868,Error,Error,1868,,https://github.com/google/deepvariant/issues/522,4,"['Error', 'error', 'ping']","['Error', 'error', 'pinging']"
Availability,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:2193,error,error,2193,,https://github.com/google/deepvariant/issues/243,2,['error'],['error']
Availability,"l', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:8232,ERROR,ERROR,8232,,https://github.com/google/deepvariant/issues/129,1,['ERROR'],['ERROR']
Availability,"l/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/W",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6666,ERROR,ERROR,6666,,https://github.com/google/deepvariant/issues/608,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:4656,down,download-archive,4656,,https://github.com/google/deepvariant/issues/844,1,['down'],['download-archive']
Availability,"lable Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); I0524 21:18:26.632611 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -2652458924365639691); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:8170,Avail,Available,8170,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"ld (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json ext",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:1602,error,error,1602,,https://github.com/google/deepvariant/issues/356,1,['error'],['error']
Availability,"ld_restore=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1333, in _build; build_save=build_save, build_restore=build_restore); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 775, in _build_internal; restore_sequentially, reshape); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 453, in _AddShardedRestoreOps; name=""restore_shard"")); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 422, in _AddRestoreOps; assign_ops.append(saveable.restore(saveable_tensors, shapes)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 113, in restore; self.op.get_shape().is_fully_defined()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 219, in assign; validate_shape=validate_shape); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 60, in assign; use_locking=use_locking, name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper; op_def=op_def); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op; op_def=op_def); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__; self._traceback = self._graph._extract_stack() # pylint: disable=protected-access. InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,6,32] rhs shape= [3,3,7,32]; 	 [[Node: save_1/Assign_3 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Conv2d_1a_3x3/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Conv2d_1a_3x3/weights, save_1/RestoreV2:3)]]. Could you please check and let us know about possible reason of this error ?. Thanks; Najeeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:10033,error,error,10033,,https://github.com/google/deepvariant/issues/117,1,['error'],['error']
Availability,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1900,checkpoint,checkpoint,1900,,https://github.com/google/deepvariant/issues/465,1,['checkpoint'],['checkpoint']
Availability,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:2619,checkpoint,checkpoint,2619,,https://github.com/google/deepvariant/issues/458,1,['checkpoint'],['checkpoint']
Availability,"les_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; IN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2798,mainten,maintenance,2798,,https://github.com/google/deepvariant/issues/876,1,['mainten'],['maintenance']
Availability,lesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_c,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18985,error,error,18985,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"lient/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:16347,error,errors,16347,,https://github.com/google/deepvariant/issues/358,1,['error'],['errors']
Availability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4203,down,download,4203,,https://github.com/google/deepvariant/issues/252,1,['down'],['download']
Availability,"ll_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); I082",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2883,down,downstream,2883,,https://github.com/google/deepvariant/issues/876,1,['down'],['downstream']
Availability,"llowing information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:7415,error,error,7415,,https://github.com/google/deepvariant/issues/489,1,['error'],['error']
Availability,"loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4458,ERROR,ERROR,4458,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:3393,checkpoint,checkpoint,3393,,https://github.com/google/deepvariant/issues/857,2,['checkpoint'],['checkpoint']
Availability,"log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ```. **Any additional context:**; DeepVariant's single command `run_deepvariant` works on my system via Singularity. DeepVariant `postprocess_variants` successfully produces a VCF file when omitting the `--output_gvcf` flag using the same data and singularity path bindings. Only DeepTrio seems to throw an error.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:5242,error,error,5242,,https://github.com/google/deepvariant/issues/429,1,['error'],['error']
Availability,"low binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restoring parameters from /tmp/deepvariant/model.ckpt-0; I0502 10:58:57.455770 139632719935232 tf_logging.py:82] Restoring parameters from /tmp/deepvariant/model.ckpt-0; INFO:tensorflow:Starting Session.; I0502 10:59:09.842276 139632719935232 tf_logging.py:82] Starting Session.; INFO:tensorflow:Saving checkpoint to path /tmp/deepvariant/model.ckpt; I0502 10:59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:2595,checkpoint,checkpoint,2595,,https://github.com/google/deepvariant/issues/69,1,['checkpoint'],['checkpoint']
Availability,"lps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; Traceback (most recent call last):; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>; File ""/pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:1159,error,error,1159,,https://github.com/google/deepvariant/issues/598,20,['error'],['error']
Availability,"m singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:1031,checkpoint,checkpoint,1031,,https://github.com/google/deepvariant/issues/679,1,['checkpoint'],['checkpoint']
Availability,m_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14108,error,error,14108,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,m_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_cod,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19267,error,error,19267,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"m_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.267024 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.306630 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; I0508 07:36:27.465119 140432686110464 make_examples.py:648] Task 8/25: Writing examples to /tmp/tmpd7h_gv7l/make_examples.tfrecord-00008-of-00025.gz; I0508 07:36:27.465305 140432686110464 make_examples.py:648] Task 8/25: Writing gvcf records to /tmp/tmpd7h_gv7l/gvcf.tfrecord-00008-of-00025.gz; I0508 07:36:27.465700 140432686110464 make_examples.py:648] Task 8/25: Overhead for preparing inputs: 74 seconds; I0508 07:36:27.529915 140432686110464 make_examples.py:648] Task 8/25: 0 candidates (0 examples) [0.06s elapsed]; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fq",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:7235,error,error,7235,,https://github.com/google/deepvariant/issues/455,1,['error'],['error']
Availability,make_examples (with --regions) failed due to parsing error from intervaltree 3.0.2,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:53,error,error,53,,https://github.com/google/deepvariant/issues/131,1,['error'],['error']
Availability,make_examples ERROR,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/24:14,ERROR,ERROR,14,,https://github.com/google/deepvariant/issues/24,1,['ERROR'],['ERROR']
Availability,"mber_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt""; ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```; docker run --rm --gpus '""device=1""' \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_eval \; --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --checkpoint_dir=""${TRAINING_DIR}"" \; --batch_size=512 \; --min_eval_interval_s=1 \; --eval_timeout=1000; ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:; ```; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta; output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta; output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta; output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-3400",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:2113,checkpoint,checkpoint,2113,,https://github.com/google/deepvariant/issues/611,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"med 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/__init__.py"", line 87, in <module>; from apache_beam import coders; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>; from apache_beam.coders.coders import *; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coders.py"", line 59, in <module>; from apache_beam.coders import coder_impl; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>; import numpy as np; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>; from . import core; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError: . IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.11 from ""/project/pbarc/haley.arnold/condaenvs/tensorflow/bin/python3""; * The NumPy version is: ""1.24.4"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: No module named 'numpy.core._multiarray_umath'`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:3998,error,error,3998,,https://github.com/google/deepvariant/issues/793,2,['error'],['error']
Availability,"mics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; I1029 09:19:03.708104 139762738964288 make_examples_core.py:243] Task 2/8: Writing examples to output/intermediate_results_dir/make_examples.tfrecord-00002-of-00008.gz; I1029 09:19:03.708200 139762738964288 make_examples_core.py:243] Task 2/8: Overhead for preparing inputs: 2 seconds; I1029 09:19:04.060117 140039545739072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; I1029 09:19:04.129533 140039545739072 make_examples_core.py:243] Task 6/8: Writing examples to output/intermediate_results_dir/make_examples.tfrecord-00006-of-00008.gz; I1029 09:19:04.129874 140039545739072 make_examples_core.py:243] Task 6/8: Overhead for preparing inputs: 2 seconds; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted. Current thread 0x00007f543a64b740 (most recent call first):; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:13940,error,error,13940,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,model_train.py has flag message format error (crashes on --help),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/78:39,error,error,39,,https://github.com/google/deepvariant/issues/78,1,['error'],['error']
Availability,"module>; tf.app.run(); File ""/home/chungtsai_su/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1000, in processing_regions_from_options; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; ```. Then I try to downgrade intervaltree from 3.0.2 to 2.1.0. . ```; chungtsai_su@seqslab:~/src/deepvariant$ pip show intervaltree; Name: intervaltree; Version: 3.0.2; Summary: Editable interval tree data structure for Python 2 and 3; Home-page: https://github.com/chaimleib/intervaltree; Author: Chaim Leib Halbert, Konstantin Tretyakov; Author-email: chaim.leib.halbert@gmail.com; License: Apache License, Version 2.0; Location: /home/chungtsai_su/.local/lib/python2.7/site-packages; Requires: sortedcontainers; Required-by:; chungtsai_su@seqslab:~/quickstart-output$ pip uninstall intervaltree; Uninstalling intervaltree-3.0.2:; Would remove:; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainer",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:2191,down,downgrade,2191,,https://github.com/google/deepvariant/issues/131,1,['down'],['downgrade']
Availability,"mpiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.dec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2447,Checkpoint,Checkpoint,2447,,https://github.com/google/deepvariant/issues/722,1,['Checkpoint'],['Checkpoint']
Availability,"mpiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detectin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1196,error,error,1196,,https://github.com/google/deepvariant/issues/722,1,['error'],['error']
Availability,"mples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0218 00:34:45.536543 140119155529536 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0218 00:34:45.537125 140119155529536 call_variants.py:510] Use saved model: False; Model: ""inceptionv3""; __________________________________________________________________________________________________; Layer (type) Output Shape Pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:14107,down,downstream,14107,,https://github.com/google/deepvariant/issues/774,1,['down'],['downstream']
Availability,"mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/modeltestset2_n.vcf.gz""`. **Warning/Error Code:** . ` warnings.warn(; I0327 22:12:06.039550 139725850806080 call_variants.py:471] Total 1 writing processes started.; I0327 22:12:06.051199 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0327 22:12:06.052814 139725850806080 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0327 22:12:06.053915 139725850806080 call_variants.py:510] Use saved model: True; I0327 22:12:15.247638 139725850806080 dv_utils.py:365] From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6].; I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match.; W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match.; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1483, in _call_impl; return self._call_with_structured_signature(args, kwargs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:5771,checkpoint,checkpoints,5771,,https://github.com/google/deepvariant/issues/797,1,['checkpoint'],['checkpoints']
Availability,"n3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Dec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1906,avail,available,1906,,https://github.com/google/deepvariant/issues/859,1,['avail'],['available']
Availability,"n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2003, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1472, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1534, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5. **Does the quick start test work on your system?**; it work. **Any additional context:**; I also run the mistaken command directly inside docker container, . [gosadmin@node3 trio]$ docker run -it -v ""${DIR}"":""/data"" google/deepvariant:${VERSION} /bin/bash. root@d2911291b750:/data# ls -alh; total 42G; drwxrwxr-x 2 1000 1000 4.0K May 8 03:39 .; drwxr-xr-x 1 root root 29 May 8 08:21 ..; -rw-rw-r-- 1 1000 1000 6.9M Apr 30 08:19 HG002.bai; -rw-rw-r-- 1 1000 1000 9.7G Apr 30 08:19 HG002.bam; -rw-rw-r-- 1 1000 1000 9.6M Apr 30 09:01 HG002_truth.bed; -rw-rw-r-- 1 1000 1000 147M Apr 30 09:00 HG002_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:00 HG002_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 6.8M Apr 30 08:33 HG003.bai; -rw-rw-r-- 1 1000 1000 8.4G Apr 30 08:33 HG003.bam; -rw-rw-r-- 1 1000 1000 9.2M Apr 30 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:2854,error,error,2854,,https://github.com/google/deepvariant/issues/455,1,['error'],['error']
Availability,nal/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_c,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13258,error,error,13258,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"nclude both `insert_size` and `allele_frequency` with v1.4. **Setup**; - Operating system:; - DeepVariant version: v1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS. **Steps to reproduce:**; - Command: ; ```; time singularity run -B '/usr/lib/locale/:/usr/lib/locale/,/path/to/region_files/:/region_dir/,/path/to/container/deep-variant/:/run_dir/,/path/to/output/:/path/to/reference_genome/:/ref_dir/,/path/to/bam_files/:/bam_dir/,/path/to/population_vcf/:/popVCF_dir/' . deepvariant_1.4.0.sif ; /opt/deepvariant/bin/run_deepvariant ; --model_type=WGS; --ref='/ref_dir/reference.fa' ; --reads='/bam_dir/id.bam' ; --output_vcf='/out_dir/test1.vcf.gz' ; --intermediate_results_dir='/out_dir/tmp/test1/' ; --num_shards='39' ; --make_examples_extra_args=""use_allele_frequency=true,population_vcfs=/popVCF_dir/UMAG1.POP.FREQ.vcf.gz"" ; --regions=/region_dir/regions_to_test.bed ; ```; - Error trace: (if applicable); ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/out_dir/tmp/test1/call_variants_output.tfrecord.gz"" --examples ""/out_dir/tmp/test1/make_examples.tfrecord@39.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/out_dir/tmp/test1/"". I0919 17:19:47.185331 46912500266816 call_variants.py:317] From /out_dir/tmp/test1/make_examples.tfrecord-00000-of-00039.gz.example_info.json: Shape of input examples: [100, 221, 8], Channels of input examples: [1, 2, 3, 4, 5, 6, 8, 19].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_l3__pco1/runfiles/absl_py/absl/ap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568:2513,Error,Error,2513,,https://github.com/google/deepvariant/issues/568,1,['Error'],['Error']
Availability,"nd Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2127,checkpoint,checkpoints,2127,,https://github.com/google/deepvariant/issues/722,1,['checkpoint'],['checkpoints']
Availability,"nes. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1940,error,error,1940,,https://github.com/google/deepvariant/issues/902,1,['error'],['error']
Availability,"nfo to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00013-of-00016.gz.example_info.json; I0203 17:23:09.199875 136895166957376 make_examples_core.py:2958] example_shape = None; I0203 17:23:09.200180 136895166957376 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:09.201941 136895166957376 make_examples_core.py:301] Task 13/16: Found 0 candidate variants; I0203 17:23:09.202048 136895166957376 make_examples_core.py:301] Task 13/16: Created 0 examples. real 112m20.375s; user 1760m59.767s; sys 11m47.541s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/call_variants_output.tfrecord.gz"" --examples ""/cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0203 17:23:14.218397 132068663560000 call_variants.py:471] Total 1 writing processes started.; W0203 17:23:14.224790 132068663560000 call_variants.py:482] Unable to read any records from /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord@16.gz. Output will contain zero records.; I0203 17:23:14.225926 132068663560000 call_variants.py:623] Complete: call_variants.; ```. And then the program hangs there for 10+ hours (UTC time when I'm reporting is Feb. 04, 04:05, and the program still appears running). . We've observed this for both ONT and HiFi data on multiple samples, further suggesting this isn't a data issue. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:3374,mainten,maintenance,3374,,https://github.com/google/deepvariant/issues/769,2,"['down', 'mainten']","['downstream', 'maintenance']"
Availability,"ng \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3926,checkpoint,checkpoint,3926,,https://github.com/google/deepvariant/issues/151,1,['checkpoint'],['checkpoint']
Availability,"ng/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/di",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:15616,error,error,15616,,https://github.com/google/deepvariant/issues/358,1,['error'],['error']
Availability,"nome from creating the BAM files. . **Steps to reproduce:** ; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/,""${REF_PATH}/"":/ref_dir/,""${BAM_PATH}/"":/in_dir/,""${RESULTS_DIR}/"":/out_dir/ \; deepvariant_deeptrio-${BIN_VERSION_DT}.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa \; --intermediate_results_dir=""/out_dir/${trioName}/"" \; --sample_name_child ""199713"" \; --sample_name_parent1 ""199710"" \; --sample_name_parent2 ""199718"" \; --reads_child=""/in_dir/199713.realigned.recalibrated.bam"" \; --reads_parent1=""/in_dir/199710.realigned.recalibrated.bam"" \; --reads_parent2=""/in_dir/199718.realigned.recalibrated.bam"" \; --output_vcf_child=""/out_dir/199713.output.vcf.gz"" \; --output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:1684,Error,Error,1684,,https://github.com/google/deepvariant/issues/429,1,['Error'],['Error']
Availability,"nt.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz; SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run each deepvariant over these samples; # We always run over proband. ## Proband ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:1402,echo,echo,1402,,https://github.com/google/deepvariant/issues/518,1,['echo'],['echo']
Availability,"nt/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:1811,Error,Error,1811,,https://github.com/google/deepvariant/issues/592,1,['Error'],['Error']
Availability,"nt/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN02990337.star.bam --examples /tmp/tmp6yy2ufd4/make_examples.tfrecord@16.gz --channels insert_size --regions enough.merge.bed --task 4. **Setup**; - Operating system: Google Docker container; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: RNASeq; ; **Steps to reproduce:**; - Command: /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN02990337.star.bam --examples /tmp/tmp6yy2ufd4/make_examples.tfrecord@16.gz --channels insert_size --regions enough.merge.bed --task 4; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/752:2568,Error,Error,2568,,https://github.com/google/deepvariant/issues/752,1,['Error'],['Error']
Availability,"ntime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; }; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:6748,echo,echo,6748,,https://github.com/google/deepvariant/issues/145,1,['echo'],['echo']
Availability,"nts (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****; ```; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:1429,error,error,1429,,https://github.com/google/deepvariant/issues/542,2,['error'],['error']
Availability,nvidia base image is no longer available,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/676:31,avail,available,31,,https://github.com/google/deepvariant/issues/676,1,['avail'],['available']
Availability,"ny.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:2160,error,error,2160,,https://github.com/google/deepvariant/issues/859,2,['error'],['error']
Availability,"ocker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:1044,error,error,1044,,https://github.com/google/deepvariant/issues/222,1,['error'],['error']
Availability,"ocker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:1325,avail,available,1325,,https://github.com/google/deepvariant/issues/217,1,['avail'],['available']
Availability,"ogle/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; At the call_variants.py step, running into error that tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; (A clear and concise description of what the issue is.). **Setup**; - Operating system:CentOS7 ; - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.):singularity run with SIF image pulled from docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:1243,Error,Error,1243,,https://github.com/google/deepvariant/issues/564,1,['Error'],['Error']
Availability,oglesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_goog,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9257,error,error,9257,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,oglesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_go,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10685,error,error,10685,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,oglesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10974,error,error,10974,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"om-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from a GCP VM instance.; 6. I have a standard setup with increased CPUs to 1025. . It is not clear to me what the issues are and what to make of the errors? ; I am sorry If this is not the correct forum to post in - please let me know where to seek help if not! ; Cheers, C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:9134,error,error,9134,,https://github.com/google/deepvariant/issues/129,2,['error'],"['error', 'errors']"
Availability,"ommand:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 474",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:1688,error,error,1688,,https://github.com/google/deepvariant/issues/833,1,['error'],['error']
Availability,"on()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the direc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:2468,checkpoint,checkpoint,2468,,https://github.com/google/deepvariant/issues/273,1,['checkpoint'],['checkpoint']
Availability,"on. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:1349,Error,Error,1349,,https://github.com/google/deepvariant/issues/356,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"on/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1290,checkpoint,checkpoint,1290,,https://github.com/google/deepvariant/issues/845,1,['checkpoint'],['checkpoint']
Availability,"on/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:2077,checkpoint,checkpoint,2077,,https://github.com/google/deepvariant/issues/857,1,['checkpoint'],['checkpoint']
Availability,"on/python_include -Ibazel-out/k8-opt/genfiles/external/local_config_python/python_include -Iexternal/protobuf_archive/src -Ibazel-out/k8-o; pt/genfiles/external/protobuf_archive/src '-f-Iexternal/protobuf_archive -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/hts; lib -Ibazel-out/k8-opt/genfiles/external/htslib -I. -Iexternal/bazel_tools -Ibazel-out/k8-opt/genfiles/external/bazel_tools -Iexternal/htslib/htslib/htslib_1_6 -Ibazel-out/; k8-opt/genfiles/external/htslib/htslib/htslib_1_6 -Iexternal/bazel_tools/tools/cpp/gcc3 -Iexternal/clif -Ibazel-out/k8-opt/genfiles/external/clif -Iexternal/local_config_py; thon -Ibazel-out/k8-opt/genfiles/external/protobuf_archive -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/genfiles/external/local_config_python/python_inc; lude -Iexternal/protobuf_archive/src -Ibazel-out/k8-opt/genfiles/external/protobuf_archive/src -std=c++11' deepvariant/core/python/hts_verbose.clif); _BackendError: Matcher failed with status 1; In file included from /dev/stdin:1:; In file included from /root/opt/clif/python/types.h:27:; In file included from bazel-out/k8-opt/genfiles/external/local_config_python/python_include/Python.h:19:; /usr/include/limits.h:123:16: fatal error: 'limits.h' file not found; `# include_next <limits.h>`. Before that, I adjust the ""/deepvariant/third_party/clif.bzl"" file to include a clif header which the same build command complained about. . `clif.bzl: ""--prepend"", ""/root/opt/clif/python/types.h""`. The system is a fresh ubuntu16.4; `Linux AnnoSpark 4.4.0-103-generic #126-Ubuntu SMP Mon Dec 4 16:23:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`. My gcc include appears to be so; `$gcc -xc -E -v -`; ; /usr/lib/gcc/x86_64-linux-gnu/4.8/include; /usr/local/include; /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed; /usr/include/x86_64-linux-gnu; /usr/include. clif build ; `commit 6c6d894a112d978bd5abfcab1052c60c5ee365a9`. Any help or direction is deeply appreciated.; Dan",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12:2607,error,error,2607,,https://github.com/google/deepvariant/issues/12,1,['error'],['error']
Availability,"on3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 444, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2865, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3697, in _create_op_internal; ret = Operation(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). real	41m45.880s; user	1063m44.358s; sys	25m21.900s; INFO: Cleaning up image...; ERROR: failed to delete container image tempDir /tmp/pbs.1173981.omics/rootfs-2853380811: unlinkat /tmp/pbs.1173981.omics/rootfs-2853380811/tmp-rootfs-1307439201/opt/traps/lib/libmodule64.so: permission denied; singularity/3.10.0 is unloaded . - `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No; **Any additional context:**; Input BAM file seems valid. Checked with samtools quickcheck -v command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:17167,ERROR,ERROR,17167,,https://github.com/google/deepvariant/issues/564,1,['ERROR'],['ERROR']
Availability,"on; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(mai",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:21529,checkpoint,checkpoint,21529,,https://github.com/google/deepvariant/issues/537,1,['checkpoint'],['checkpoint']
Availability,"onnection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-42:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes; self._send(buf); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; ```; There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens?. **Setup**; - Operating system: Ubuntu 22.04; - DeepVariant version: v1.6.1; - Installation method : singularity; - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:4589,error,error,4589,,https://github.com/google/deepvariant/issues/804,1,['error'],['error']
Availability,"oogle_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam --examples /tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz --gvcf /tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz --task 3. real	0m2.438s; user	0m2.289s; sys	0m3.833s; I0824 08:09:22.720529 140047104423680 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}' returned non-zero exit status 1.; Done...; `; this is my error when I run deepvariant script. - Operating system: CentOS; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Docker. how can I solve this problem",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/337:9770,error,error,9770,,https://github.com/google/deepvariant/issues/337,1,['error'],['error']
Availability,ooglesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_google,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17845,error,error,17845,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,ooglesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19556,error,error,19556,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,optimize_for_inference_lib' is not defined error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:43,error,error,43,,https://github.com/google/deepvariant/issues/597,1,['error'],['error']
Availability,"ord-00008-of-00020.gz.example_info.json; I0822 07:52:09.283617 133276175411008 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:52:09.283712 133276175411008 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0822 07:52:09.283882 133276175411008 make_examples_core.py:301] Task 8/20: Found 17371 candidate variants; I0822 07:52:09.283904 133276175411008 make_examples_core.py:301] Task 8/20: Created 18820 examples. real 34m15.728s; user 624m43.553s; sys 2m24.932s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@20.gz"" --checkpoint ""/output/checkpoints/ckpt-679"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0822 07:52:10.812179 127086447671104 call_variants.py:563] Total 1 writing processes started.; I0822 07:52:10.813103 127086447671104 dv_utils.py:370] From /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0822 07:52:10.813141 127086447671104 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0822 07:52:10.813338 127086447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:4866,mainten,maintenance,4866,,https://github.com/google/deepvariant/issues/869,1,['mainten'],['maintenance']
Availability,"ord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3290,error,errors,3290,,https://github.com/google/deepvariant/issues/429,1,['error'],['errors']
Availability,"ow is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:1879,error,error,1879,,https://github.com/google/deepvariant/issues/797,1,['error'],['error']
Availability,ow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.g,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3543,error,error,3543,,https://github.com/google/deepvariant/issues/89,1,['error'],['error']
Availability,"pVariant v 1.6.0 with Singularity. I tried running the following:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} ; ```; But I get the following errors:; ```; /var/spool/slurmd/job29242279/slurm_script: line 19: --customized_model=/scratch/cs/pan-autoimmune/utilities/deepvariant/references/model/model.ckpt: No such file or directory; /var/spool/slurmd/job29242279/slurm_script: line 25: --make_examples_extra_args=split_skip_reads=true,channels='': command not found; ```. I've downloaded the model.ckpt files:; ```; ls /scratch/cs/pan-autoimmune/utilities/deepvariant/references/model; model.ckpt.data-00000-of-00001 model.ckpt.example_info.json model.ckpt.index model.ckpt.meta; ```; I downloaded them with:; ```; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/780:1031,down,downloaded,1031,,https://github.com/google/deepvariant/issues/780,2,['down'],['downloaded']
Availability,"paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 312 in <module>; parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_sampl. real 1154m27.406s; user 287m31.460s; sys 2m20.121s; I0509 05:42:42.322269 47221860157248 run_deeptrio.py:674] None. Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""/paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta"" --reads_parent1 ""/paedyl01/disk1/yangyxt/wesplus/50_samples_; Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error; `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:4798,error,error,4798,,https://github.com/google/deepvariant/issues/646,1,['error'],['error']
Availability,"pile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:1023,ERROR,ERROR,1023,,https://github.com/google/deepvariant/issues/189,1,['ERROR'],['ERROR']
Availability,"ple_id, file]; }; .set { read_pairs }. // Process steps; /// Germline variant calling; deepvar(read_pairs, params.reference, params.bed_file); }; process deepvar {; tag ""Germline Variant on ${sample_id}""; publishDir ""${params.outdir}/6.variantM"", mode: 'copy'; cpus 16. input:; tuple val(sample_id), path(read_files); val(params.reference); val(params.bed_file); ; output:; tuple val(sample_id), path(""${sample_id}_raw.vcf.gz""), path(""${sample_id}_raw.gvcf.gz""), emit: raw_vcfs. script:; """"""; sudo docker run \; -v ""${PWD}"":""${PWD}"" \; google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${PWD}/${params.reference} \; --reads ${PWD}/output/4.markDuplicate/${sample_id}_sorted_md.bam \; --regions ${PWD}/${params.bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:1467,Error,Error,1467,,https://github.com/google/deepvariant/issues/883,1,['Error'],['Error']
Availability,postprocess_variants step is erroring out in quickstart example,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:29,error,erroring,29,,https://github.com/google/deepvariant/issues/901,1,['error'],['erroring']
Availability,"pply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was finalized.; I0524 21:18:33.525068 140032543119168 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0524 21:18:33.525994 140032543119168 saver.py:1298] Restoring parameters from /opt/models/wgs/model.ckpt; INFO:tensorflow:prediction_loop marked as finished; I0524 21:18:34.251420 140032543119168 error_handling.py:115] prediction_loop marked as finished; WARNING:tensorflow:Reraising captured error; W0524 21:18:34.251592 140032543119168 error_handling.py:149] Reraising captured error; Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1375, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tenso",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:11224,error,error,11224,,https://github.com/google/deepvariant/issues/537,1,['error'],['error']
Availability,protobuf error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94:9,error,error,9,,https://github.com/google/deepvariant/issues/94,1,['error'],['error']
Availability,"pvariant/issues/485), but the final fix is not provided. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity image built from docker image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**; - Command: ; ```; # Modified script; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=hs37d5_PhiX.fa \; --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \; --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \; --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \; --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \; --num_shards=15; ```; I have also tried postprocessing with `group_variants`, which also produces a similar error.; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/postprocess_variants \; --group_variants=false \; --ref=hs37d5_PhiX.fa \; --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \; --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz; ```; - Error trace: (if applicable); ```; I0217 17:00:21.108631 47945364948800 postprocess_variants.py:1115] Using sample name from call_variants output. Sample name: sample_; 2022-02-17 17:00:21.116319: I deepvariant/postprocess_variants.cc:88] Read from: ""...""/call_variants_output.tfrecord.gz; 2022-02-17 17:00:22.403255: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 228285; I0217 17:00:24.204934 47945364948800 postprocess_variants.py:1180] CVO sorting took 0.051486388842264814 minutes; I0217 17:00:24",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:1583,error,error,1583,,https://github.com/google/deepvariant/issues/517,1,['error'],['error']
Availability,"pvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```. While the run has not errored out, I do believe that there is an issue here and would appreciate if anyone has any insight. Regards; Erin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:1291,mainten,maintenance,1291,,https://github.com/google/deepvariant/issues/776,3,"['down', 'error', 'mainten']","['downstream', 'errored', 'maintenance']"
Availability,"quenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones. If indels were leftaligned before output, this would solve the issue I think and likely many multi-allelic will become single-allele. Any plan for this in the future?. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/487:1732,down,downstream,1732,,https://github.com/google/deepvariant/issues/487,1,['down'],['downstream']
Availability,"quencing instrument, reference genome, anything special that is unlike the case studies?); Illumina NovaSeq data, reference genome hg19. ; **Steps to reproduce:**; - Command:; - `/opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`; - Error trace: (if applicable); `Fatal Python error: Bus error. Current thread 0x00002af3a27fa740 (most recent call first):; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 58 in <dictcomp>; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 57 in get_candidates; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/variant_caller.py"", line 382 in calls_and_gvcfs; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1379 in candidates_in_region; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:1315,Error,Error,1315,,https://github.com/google/deepvariant/issues/646,3,"['Error', 'error']","['Error', 'error']"
Availability,"quirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.d '-frandom-seed=bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o' -fPIC -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/189:1204,ERROR,ERROR,1204,,https://github.com/google/deepvariant/issues/189,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"r exception occurred:. Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` int",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:2122,error,error,2122,,https://github.com/google/deepvariant/issues/610,1,['error'],['error']
Availability,"r run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The numbe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:1145,failure,failure,1145,,https://github.com/google/deepvariant/issues/392,3,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,"r while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1156,checkpoint,checkpoint,1156,,https://github.com/google/deepvariant/issues/845,1,['checkpoint'],['checkpoint']
Availability,"r) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:3663,echo,echo,3663,,https://github.com/google/deepvariant/issues/739,1,['echo'],['echo']
Availability,"r: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InR",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1687,down,download,1687,,https://github.com/google/deepvariant/issues/902,1,['down'],['download']
Availability,"rallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples; > real	18m21.465s; > user	893m16.250s; > sys	15m57.561s; > ; > ***** Running the command:*****; > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp""; > ; > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; > Traceback (most recent call last):; > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; > tf.compat.v1.app.run(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:4573,checkpoint,checkpoint,4573,,https://github.com/google/deepvariant/issues/602,1,['checkpoint'],['checkpoint']
Availability,rce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9836,error,error,9836,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,rce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googl,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10402,error,error,10402,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,rce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11547,error,error,11547,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"rchive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/6170",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7059,Error,Error,7059,,https://github.com/google/deepvariant/issues/608,2,"['Error', 'down']","['Error', 'downloading']"
Availability,re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesour,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11839,error,error,11839,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:3983,error,error,3983,,https://github.com/google/deepvariant/issues/446,1,['error'],['error']
Availability,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:4203,error,error,4203,,https://github.com/google/deepvariant/issues/374,1,['error'],['error']
Availability,"rently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4650,ERROR,ERROR,4650,,https://github.com/google/deepvariant/issues/19,1,['ERROR'],['ERROR']
Availability,"riant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, sinc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:2535,error,errors,2535,,https://github.com/google/deepvariant/issues/60,1,['error'],['errors']
Availability,"riant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>; 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>; I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>; I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs; 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:2409,Error,Error,2409,,https://github.com/google/deepvariant/issues/677,1,['Error'],['Error']
Availability,"riment_dir: /home/training_outs/epoch1/; WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; W0828 10:40:42.596594 140318776715072 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.; INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); I0828 10:40:42.658734 140318776715072 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',); /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; I0828 10:40:47.952382 140318776715072 keras_modeling.py:325] Number of l2 regularizers: 95.; I0828 10:40:48.007193 140318776715072 keras_modeling.py:362] inceptionv3: load_weights from checkpoint: /home/training_outs/epoch1//checkpoints/ckpt-5997; I0828 10:40:49.193293 140318776715072 train.py:191] Exponential Decay: initial_learning_rate=0.0001; decay_steps=45448; learning_rate_decay_rate=0.947; I0828 10:40:49.193522 140318776715072 train.py:203] Use LinearWarmup:; warmup_steps=10000; warmup_learning_rate=1e-05; I0828 10:40:49.401860 140318776715072 keras_modeling.py:472] Restored checkpoint ckpt-5997 at step=0. tune/f1_weighted=tf.Tensor(0.0, shape=(), dtype=float32); WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:4510,checkpoint,checkpoint,4510,,https://github.com/google/deepvariant/issues/876,2,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"rintf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512"": exit status 1: turn self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session; init_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 202, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6038,checkpoint,checkpoint,6038,,https://github.com/google/deepvariant/issues/129,1,['checkpoint'],['checkpoint']
Availability,"rkspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6451,down,downloader,6451,,https://github.com/google/deepvariant/issues/608,1,['down'],['downloader']
Availability,"rocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; make_examples.tfrecord-00005-of-00008.gz.example_info.json; make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2193,error,error,2193,,https://github.com/google/deepvariant/issues/818,1,['error'],['error']
Availability,"rra.bio/)`; - DeepVariant version:; `1.1.0`; - Installation method (Docker, built from source, etc.):; `google/deepvariant:1.1.0`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ```; Pacbio bam from GIAB ; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/NA12878/PacBio_SequelII_CCS_11kb/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam; reference from Broad GCP; gs://gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta; Though this is a slight ref mismatch, B37 vs hs37. I don't think that should cause that problem? The make_examples step finished successfully.; ```; **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=${REF_GENOME_FASTA} \; --reads=${input_read} \; --num_shards=${NUM_THREADS} \; --output_vcf=${basename}.vcf.gz; ```; - Error trace: (if applicable); ```parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 6; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --tas",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:1143,Error,Error,1143,,https://github.com/google/deepvariant/issues/446,1,['Error'],['Error']
Availability,"rs:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:1087,avail,available,1087,,https://github.com/google/deepvariant/issues/226,1,['avail'],['available']
Availability,"runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.559681: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 283943 end: 284101; Fatal Python error: Aborted. Current thread 0x00007f84f61df740 (most recent call first):; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:18659,error,error,18659,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.521271: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 276773 end: 276899; Fatal Python error: Aborted. Current thread 0x00007f1d0c68a740 (most recent call first):; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:23378,error,error,23378,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.553988: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 277024 end: 277165; Fatal Python error: Aborted. Current thread 0x00007f1c6e524740 (most recent call first):; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:21805,error,error,21805,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.554670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 275948 end: 276106; Fatal Python error: Aborted. Current thread 0x00007f9594dae740 (most recent call first):; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:20232,error,error,20232,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.544568: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 271187 end: 271287; Fatal Python error: Aborted. Current thread 0x00007f4836ae3740 (most recent call first):; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:17086,error,error,17086,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.668786: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 279152 end: 279350; Fatal Python error: Aborted. Current thread 0x00007f5d7f60d740 (most recent call first):; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:24951,error,error,24951,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.586897: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 277206 end: 277403; Fatal Python error: Aborted. Current thread 0x00007f6797491740 (most recent call first):; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:15513,error,error,15513,,https://github.com/google/deepvariant/issues/581,1,['error'],['error']
Availability,"runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2095,error,error,2095,,https://github.com/google/deepvariant/issues/818,1,['error'],['error']
Availability,"running call variants on ONT data using the command . `run_pepper_margin_deepvariant call_variant -b ./Sample/alignments/GRCh38/41195.minimap2.bam -f /data/Homo_sapiens_assembly38.fasta -o Sample -t 4 -s Sample --phased_output --ont_r9_guppy5_sup; `. no errors but it's stuck at Starting Candidate Finding and the CPU is at 0% for hours. Can I stop it and resume? why there is no error? will it ever finish?. ....; [12-24-2023 05:47:47] INFO: SUMMARY PROCESSED 7280/7280.; [12-24-2023 05:47:47] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: FINISHED PREDICTION; [12-24-2023 05:54:33] INFO: ELAPSED TIME: 867 Min 41 Sec; [12-24-2023 05:54:33] INFO: PREDICTION FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: TOTAL ELAPSED TIME FOR INFERENCE: 867 Min 44 Sec; [12-24-2023 05:54:33] INFO: STEP 3/3 FINDING CANDIDATES; [12-24-2023 05:54:33] INFO: OUTPUT: 41195/pepper/; [12-24-2023 05:55:25] INFO: STARTING CANDIDATE FINDING. - Operating system: Linux; - DeepVariant version: kishwars/pepper_deepvariant r0.8; - Installation method (Docker, built from source, etc.): Docker; ONT long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/758:254,error,errors,254,,https://github.com/google/deepvariant/issues/758,2,['error'],"['error', 'errors']"
Availability,"ry long time on `make_exmaples` and find a huge number of examples (~50X more than my previous data aligned to GRCh38). And finally, deepvariant fails. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: PCR-free WGS data from 1000 Genomes, CHM13-based pangenome graph. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B $inpath \; -B $outpath \; -B ${TMPDIR}:${TMPDIR} \; -B $(dirname $ref) \; ~/container/deepvariant.1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$ref \; --reads=${inpath}/${sample}.cram \; --output_vcf=${outpath}/${sample}.vcf.gz \; --output_gvcf=${outpath}/${sample}.g.vcf.gz \; --num_shards=${threads} \; --intermediate_results_dir ${TMPDIR} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"". ```; - Error trace: (if applicable); ```; I0706 12:06:34.396180 140036894541632 make_examples_core.py:301] Task 16/40: 20600183 candidates (20662757 examples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Deepvariant previously work well with data aligned to linear ref",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:1215,Error,Error,1215,,https://github.com/google/deepvariant/issues/847,1,['Error'],['Error']
Availability,"s about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/validation_set_channelsize_F1F2shuffled.pbtxt"" \; --config.init",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:2164,error,error,2164,,https://github.com/google/deepvariant/issues/840,1,['error'],['error']
Availability,"s has been made or that the program has even initialized. I am also asking my cluster resources about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Sample",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:2108,error,error,2108,,https://github.com/google/deepvariant/issues/840,1,['error'],['error']
Availability,"s same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 6: bazel-out/host/bin/external/clif/proto: from: not found; bazel-out/host/bin/external/clif/proto: 9: bazel-out/host/bin/external/clif/proto: Syntax error: ""("" unexpected (expecting ""then""). It would be great if I can get some help on this. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:2216,error,error,2216,,https://github.com/google/deepvariant/issues/122,1,['error'],['error']
Availability,"s using the WGS case study data of Deepvariant on chr20. I used the 0.6.0 version of released DeepVariant model as a started training model. **The make_examples script is:**; `python ../bin/make_examples.zip \; --mode training \; --ref ""file/ucsc.hg19.chr20.unittest.fasta.gz"" \; --reads ""file/NA12878_S1.chr20.10_10p1mb.bam"" \; --confident_regions ""file/test_nist.b37_chr20_100kbp_at_10mb.bed"" \; --truth_variants ""file/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz"" \; --examples ""output/examples.tfrecord.gz""; `; **my-training-dataset.pbtxt file:**; `name: ""my-training-dataset""; tfrecord_path: ""/home/suanfa/Documents/wangpeng/testmake_examples/output/examples.tfrecord.gz""; num_examples: 1`. **The model_train script is:**; `python ../bin/model_train.zip \; --dataset_config_pbtxt ""./my-training-dataset.pbtxt"" \; --start_from_checkpoint ""/my/path/of/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt""; `. **The following error have happened while the model_train.zip is invoked:**; > I0502 10:58:51.903573 139632719935232 model_train.py:182] Initializing model from checkpoint at /home/suanfa/Documents/source/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard/model.ckpt; 2018-05-02 10:58:56.347500: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA; 2018-05-02 10:58:57.263635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restori",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:1175,error,error,1175,,https://github.com/google/deepvariant/issues/69,1,['error'],['error']
Availability,"s); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main; > sys.exit(main(argv)); > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; > call_variants(; > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants; > init_op = tf.group(tf.compat.v1.global_variables_initializer(),; > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer; > return variables_initializer(global_variables()); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables; > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection; > return get_default_graph().get_collection(key, scope); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph; > return _default_graph_stack.get_default(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default; > self._global_default_graph = Graph(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__; > self._scoped_c_graph = c_api_util.ScopedTFGraph(); > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__; > self.graph = c_api.TF_NewGraph(); > RuntimeError: random_device::random_device(const std::string&): device not available; > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>; > Traceback (most recent call last):; > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__; > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:7373,avail,available,7373,,https://github.com/google/deepvariant/issues/602,1,['avail'],['available']
Availability,"s/tensorflow_estimator/python/estimator/estimator.py"", line 1175, in _train_model; return self._train_model_default(input_fn, hooks, saving_listeners); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1206, in _train_model_default; return self._train_with_estimator_spec(estimator_spec, worker_hooks,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1388, in _train_with_estimator_spec; tf.compat.v1.train.warm_start(*self._warm_start_settings); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 532, in warm_start; checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 311, in init_from_checkpoint; distribution_strategy_context.get_replica_context().merge_call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3048, in merge_call; return self._merge_call(merge_fn, args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 3055, in _merge_call; return merge_fn(self._strategy, *args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper; return func(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 306, in <lambda>; init_from_checkpoint_fn = lambda _: _init_from_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/checkpoint_utils.py"", line 347, in _init_from_checkpoint; raise ValueError(; ValueError: Shape of variable InceptionV3/Conv2d_1a_3x3/weights:0 ((3, 3, 6, 32)) doesn't match with shape of tensor InceptionV3/Conv2d_1a_3x3/weights ([3, 3, 9, 32]) from checkpoint reader.; ```. How does one specify the shape in `model_train`?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/500:4518,checkpoint,checkpoint,4518,,https://github.com/google/deepvariant/issues/500,1,['checkpoint'],['checkpoint']
Availability,"s://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xfe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:1615,avail,available,1615,,https://github.com/google/deepvariant/issues/231,1,['avail'],['available']
Availability,"s://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2017,ERROR,ERROR,2017,,https://github.com/google/deepvariant/issues/129,1,['ERROR'],['ERROR']
Availability,"s_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:1730,checkpoint,checkpoint,1730,,https://github.com/google/deepvariant/issues/625,3,['checkpoint'],['checkpoint']
Availability,"save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2639,Checkpoint,Checkpoint,2639,,https://github.com/google/deepvariant/issues/722,1,['Checkpoint'],['Checkpoint']
Availability,"se tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; Traceback (most recent call last):; F",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:1141,down,downstream,1141,,https://github.com/google/deepvariant/issues/725,1,['down'],['downstream']
Availability,"seos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. - DeepVariant version: **1.6.1**; - Installation method (Docker, built from source, etc.): **Docker**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**; - Command: ; ``` ; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. - Error trace: (if applicable). ```; I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds; 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner; region_processor.proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:1482,Error,Error,1482,,https://github.com/google/deepvariant/issues/812,1,['Error'],['Error']
Availability,"ses/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2201,echo,echo,2201,,https://github.com/google/deepvariant/issues/871,1,['echo'],['echo']
Availability,"set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. **Error 1**; ```; INFO: Using cached SIF image; I0403 10:34:56.987876 23171167450944 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp40dn43xh. ***** Intermediate results will be written to /tmp/tmp40dn43xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**; ```; INFO: Using cached SIF image; I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:1697,Error,Error,1697,,https://github.com/google/deepvariant/issues/533,1,['Error'],['Error']
Availability,"sing Docker on a Mac M1 and am encountering issues with the Dockerfile during the Bazel build process. I want to ensure compatibility with ARM64 architecture. **Docker version**: Docker version 27.1.1, build 6312585; **Bazel Version**: 7.3.1; **MacBook Model**: M1 chip (ARM64 architecture). **Error**: ; ![IMG_3267](https://github.com/user-attachments/assets/11e28824-b941-42cc-9d33-7e9155a03543); ![IMG_3268](https://github.com/user-attachments/assets/4e923de6-99d5-43ee-80c6-29b32504527d). **My Dockerfilee code**:. ```; # Base image suitable for ARM64 architecture; FROM arm64v8/ubuntu:latest AS base. # Prevent interactive prompts; ENV DEBIAN_FRONTEND=noninteractive. # Install necessary packages; RUN apt-get update && \; apt-get install -y \; git \; curl \; unzip \; wget \; openjdk-17-jdk \; build-essential \; bzip2 \; python3-pip \; parallel && \; apt-get clean && \; rm -rf /var/lib/apt/lists/*. # Install Bazel (adjust version as needed); RUN curl -LO ""https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64"" && \; chmod +x bazel-7.3.1-linux-arm64 && \; mv bazel-7.3.1-linux-arm64 /usr/local/bin/bazel. # Install Conda; RUN curl -LO ""https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"" && \; bash Miniconda3-latest-Linux-aarch64.sh -b -p /opt/miniconda && \; rm Miniconda3-latest-Linux-aarch64.sh. # Setup Conda environment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:1101,down,download,1101,,https://github.com/google/deepvariant/issues/871,1,['down'],['download']
Availability,singularity install errors,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:20,error,errors,20,,https://github.com/google/deepvariant/issues/668,1,['error'],['errors']
Availability,"singularity run -B /slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$reference \; --reads=$sample_dir2 \; --output_vcf=""${OUTPUT_DIR}""/${NameN}.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/${NameN}.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/${NameN}_intermediate_results_dir"" \; --num_shards=60. FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58057->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:57892->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:49924->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58178->114.114.114.114:53: i/o timeout",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/831:571,ping,pinging,571,,https://github.com/google/deepvariant/issues/831,4,['ping'],['pinging']
Availability,"sion(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1339, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfile",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:21550,error,error,21550,,https://github.com/google/deepvariant/issues/537,1,['error'],['error']
Availability,"sis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate in docker. ****. ***** Running the command:*****; ```; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna"" --reads ""Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/15M11163_L7_PE_markedduplicates.bam"" --examples ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/make_examples.tfrecord@1.gz"" --gvcf ""Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/15M11163_L7_PE_output_intermediate/gvcf.tfrecord@1.gz"" --regions ""Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed"" --task {}. perl: warning: Setting locale failed.; perl: warning: Pl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:1923,Error,Error,1923,,https://github.com/google/deepvariant/issues/542,1,['Error'],['Error']
Availability,source_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesou,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9547,error,error,9547,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,source_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14683,error,error,14683,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"sr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Baze",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2884,error,error,2884,,https://github.com/google/deepvariant/issues/833,1,['error'],['error']
Availability,"ss The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants.py:1081] Transforming call_variants_output to variants.; I0126 17:29:22.052276 140157300115200 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF.; I0126 17:29:22.509974 140157300115200 genomics_writer.py:172] Writing /output/MergedSplited.ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:1231,Error,Error,1231,,https://github.com/google/deepvariant/issues/413,1,['Error'],['Error']
Availability,"sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374, in call_variants; raise ValueError(f'Shape mismatch in {example_info_json} and '; ValueError: Shape mismatch in ./examples.tfrecord-00000-of-00032.gz.example_info.json and /opt/models/pacbio/model.ckpt.example_info.json.; ```. My command line looks like this:; `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --add_hp_channel --add_hp_channel --alt_aligned_pileup diff_channels --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/HG002.merged.bam --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --realign_reads --regions 20 --sample_name HG002 --split_skip_reads --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --batch_size 512 --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes ok. Do you have some input on this?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:3319,checkpoint,checkpoint,3319,,https://github.com/google/deepvariant/issues/628,1,['checkpoint'],['checkpoint']
Availability,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:🆑:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:2127,Error,Error,2127,,https://github.com/google/deepvariant/issues/380,1,['Error'],['Error']
Availability,"t_fn=self._scaffold.init_fn); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 288, in prepare_session; config=config); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 202, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6488,checkpoint,checkpoint,6488,,https://github.com/google/deepvariant/issues/129,1,['checkpoint'],['checkpoint']
Availability,"ta/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:02.136116 140440947410688 make_examples.py:1363] Task 63: 0 candidates (0 examples) [1.19s elapsed]; I0208 06:50:01.437930 140440947410688 make_examples.py:1363] Task 63: 101 candidates (101 examples) [10859.30s elapsed]; I0208 07:30:38.055526 140440947410688 make_examples.py:1380] Found 176 candidate variants; I0208 07:30:38.056374 140440947410688 make_examples.py:1381] Created 178 examples. real	346m1.860s; user	7558m17.436s; sys	11m12.192s; ```. looks like it starts making predictions; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized com",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:2749,checkpoint,checkpoint,2749,,https://github.com/google/deepvariant/issues/269,1,['checkpoint'],['checkpoint']
Availability,"tempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""project",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:8154,checkpoint,checkpoint,8154,,https://github.com/google/deepvariant/issues/129,1,['checkpoint'],['checkpoint']
Availability,"tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); I0524 21:18:26.632062 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3314463783741359823); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); I0524 21:18:26.632296 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -1873770143808342957); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); I0524 21:18:26.632360 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -3891821674854936774); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); I0524 21:18:26.632421 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6041584165456864718); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); I0524 21:18:26.632479 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -4899456949080638211); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); I0524 21:18:26.632545 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6180324062742322030); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:7157,Avail,Available,7157,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"terprise Linux 9; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES mapped to hg19. **My code:**; - Commands: ; ```; #!/bin/bash; #$ -l m_mem_free=200G; #$ -l os=rhel9; #$ -m bea; #$ -cwd; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:997,echo,echo,997,,https://github.com/google/deepvariant/issues/870,2,['echo'],['echo']
Availability,"the deep variant wrapper dv_call_variants.py crushing when installed using conda. **Setup**; - Ubuntu 20.04:; - DeepVariant version - 1.4.0:; - Installation method - Conda; - Type of data - sequencing, illumina. **Steps to reproduce:**; - Command:; dv_call_variants.py --outfile OUTFILE --examples EXAMPLES --sample SAMPLE. - Error trace: (if applicable) ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_zviaa5zy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 39, in <module>; import numpy as np; ModuleNotFoundError: No module named 'numpy'. numpy installed in enviroment",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/573:326,Error,Error,326,,https://github.com/google/deepvariant/issues/573,1,['Error'],['Error']
Availability,the error about training model,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:4,error,error,4,,https://github.com/google/deepvariant/issues/69,1,['error'],['error']
Availability,"the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1266,Error,Error,1266,,https://github.com/google/deepvariant/issues/733,1,['Error'],['Error']
Availability,"thonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 37, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tenso",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4561,error,error,4561,,https://github.com/google/deepvariant/issues/812,1,['error'],['error']
Availability,"ting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:1090,error,error,1090,,https://github.com/google/deepvariant/issues/166,1,['error'],['error']
Availability,"tion analysis has yielded a high number of false positive findings, essentially a product of low quality data. It is crucial that these sites are filtered out. . **Setup**; - Operating system: Linux; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Illumina WES. **Steps to reproduce:**; - Command: . DeepVariant:; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \. GLnexus:; glnexus_cli --config DeepVariantWES --bed ${regions} \; 2_gvcf/*.g.vcf.gz > 3_bcf/Exomes.bcf. **Any additional context:**. Hi there!; Apologies for bringing up another similar issue, but I would like some help with the correct filtering of my merged vcf file.; Essentially, I have identified a significant number of false positive sites in a downstream assoc. analysis, where MAF for these variants is widely different than the population average. This strongly suggests that these sites are of low quality and need to be filtered out. Here are some examples from the merged vcf file. For each variant I have only shown a handful of samples (total is over 5000):. False positive / bad site that needs filtering:. `1	1722625	1_1722625_A_T	A	T	48	.	AF=0.222894;AQ=48	GT:DP:AD:GQ:PL:RNC	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:7:0,7:42:44,47,0:..	0/0:0:0,0:1:0,0,0:..	1/1:6:0,6:36:38,38,0:..	0/1:12:3,9:0:19,2,0:..	./.:3:3,0:0:20,0,50:II	1/1:2:0,2:23:29,25,0:..	0/0:2:2,0:6:0,6,59:..	1/1:2:0,2:22:31,24,0:..	1/1:2:0,2:26:28,29,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	1/1:7:0,7:40:43,42,0:..	./.:3:3,0:0:20,0,50:II	1/1:4:1,3:1:28,3,0:..	0/0:0:0,0:1:0,0,0:..	0/0:33:33,0:50:0,123,1229:..	1/1:5:3,2:13:21,15,0:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:1172,down,downstream,1172,,https://github.com/google/deepvariant/issues/645,1,['down'],['downstream']
Availability,"tlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/validation_set_channelsize_F1F2shuffled.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/fullindividualmodel"" \; --strategy=mirrored \; --config.batch_size=32`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:3240,checkpoint,checkpoints,3240,,https://github.com/google/deepvariant/issues/840,1,['checkpoint'],['checkpoints']
Availability,"tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes; I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes; I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:18623,error,error,18623,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,"to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0502 14:40:56.961649 140501830911808 run_deepvariant.py:364] Re-using the directory for intermediate results in /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p. ***** Intermediate results will be written to /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1519,Error,Error,1519,,https://github.com/google/deepvariant/issues/640,1,['Error'],['Error']
Availability,"tor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_examples_core.py:301] Task 15/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; 2024-02-17 23:33:25.886408: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933485 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:7671,error,error,7671,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,train_model.zip error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10:16,error,error,16,,https://github.com/google/deepvariant/issues/10,1,['error'],['error']
Availability,"ttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); I0524 21:18:26.632669 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3158275143315040778); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); I0524 21:18:26.632792 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -4822366763137283978); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); I0524 21:18:26.632860 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2291186206241199287); INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); I0524 21:18:26.632941 140032543119168 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7884439564287565365); INFO:tensorflow:Calling model_fn.; I0524 21:18:26.633588 140032543119168 estimator.py:1162] Calling model_fn.; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0524 21:18:32.742463 140032543119168 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:TPU job name tpu_worker; I0524 21:18:33.019782 140032543119168 tpu_estimator.py:514] TPU job name tpu_worker; INFO:tensorflow:Graph was fin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:9532,Avail,Available,9532,,https://github.com/google/deepvariant/issues/537,2,['Avail'],['Available']
Availability,"tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; ```. I am new to Deep Learning and am struggling to decide whether something is wrong with my training approach/scripts or whether the model just needs more time / different hyperparams. Given the number of examples, I can only run 1 epoch at a time before I hit the 24hr cluster wall-time limit. So I have only trained for around 30,000 steps in total across 2 epochs so far (starting from last checkpoint after 1st epoch). . All advice much appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:14425,checkpoint,checkpoint,14425,,https://github.com/google/deepvariant/issues/876,1,['checkpoint'],['checkpoint']
Availability,"tup**; - Operating system: Cent; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**; - Command:; ```bash; #!/bin/bash; sample=$1; threads=$2. chr=$3; indir=""01.mapping""; outdir=""02.snps""; sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}""; rm -rf ${outdir}/${sample}_chr${chr}; ```; - Error trace: (if applicable); ```bash; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]; I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]; I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]; I0325 17:32:27.143459 47041007318848 make_examples_core.py:301] Task 44/48: 2558 candidates (2674 examples) [8.39s elapsed]; I0325 17:32:26.490880 46937528883008 make_examples_core.py:301] Task 32/48: 1393 candidates (1485 examples) [15.67s elapsed]; I0325 17:32:28.232726 4727600187",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:1263,Error,Error,1263,,https://github.com/google/deepvariant/issues/794,1,['Error'],['Error']
Availability,"tup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-10-31 20:36:34.101375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:1157,Error,Error,1157,,https://github.com/google/deepvariant/issues/901,1,['Error'],['Error']
Availability,"ty-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; Traceback (most recent call last):; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:1693,ERROR,ERROR,1693,,https://github.com/google/deepvariant/issues/870,1,['ERROR'],['ERROR']
Availability,"ty.d/libs; ...; 2024-02-17 23:33:25.887517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933275 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939588 140533724936000 make_examples_core.py:301] Task 15/16: Preparing inputs; I0217 23:33:25.967685 140533724936000 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024591 140533724936000 make_examples_core.py:301] Task 15/16: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; 2024-02-17 23:33:25.886408: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933485 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.940178 139726133032768 make_examples_core.py:301] Task 4/16: Preparing inputs; I0217 23:33:25.967752 139726133032768 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; ...; 2024-02-17 23:33:25.888518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:33:25.933323 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:25.939591 140099871606592 make_examples_core.py:301] Task 0/16: Preparing inputs; I0217 23:33:25.967773 140099871606592 genomics_reader.py:222] Reading input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam with NativeSamReader; I0217 23:33:26.024448 14009987160",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:8528,error,error,8528,,https://github.com/google/deepvariant/issues/774,1,['error'],['error']
Availability,"ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1855,error,error,1855,,https://github.com/google/deepvariant/issues/223,1,['error'],['error']
Availability,"uestion is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1497,down,downsampling,1497,,https://github.com/google/deepvariant/issues/876,1,['down'],['downsampling']
Availability,"ularly and skip over variants it for some reason cannot find?. ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf; I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter; [E::fai_retrieve] Failed to retrieve block: error reading file; Traceback (most recent call last):; File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main; header=header); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf; for variant in variant_generator:; File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants; nonvariant.end, fasta_reader); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template; ranges.make_range(retval.reference_name, start, start + 1)); File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 90, in query; return self._reader.bases(region); ValueError: Invalid argument: Couldn't fetch bases for reference_name: ""1"" start: 10147 end: 10148; + echo 'grep -v -E '\''RefCall[[:space:]]'\'' '\''UFC100105-Normal-SM-CUCI1.vcf'\'' > '\''UFC100105-Normal-SM-CUCI1.filtered.vcf'\'''; + grep -v -E 'RefCall[[:space:]]' UFC100105-Normal-SM-CUCI1.vcf; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164:2137,echo,echo,2137,,https://github.com/google/deepvariant/issues/164,1,['echo'],['echo']
Availability,"unction(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel; return hook(metadata_directory, config_settings); File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 157, in prepare_metadata_for_build_wheel; self.run_setup(); File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 248, in run_setup; super(_BuildMetaLegacyBackend,; File ""/tmp/pip-build-env-6gh6ol84/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 142, in run_setup; exec(compile(code, __file__, 'exec'), locals()); File ""setup.py"", line 499, in <module>; setup_package(); File ""setup.py"", line 479, in setup_package; generate_cython(); File ""setup.py"", line 274, in generate_cython; raise RuntimeError(""Running cythonize failed!""); RuntimeError: Running cythonize failed!; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:3795,error,error,3795,,https://github.com/google/deepvariant/issues/727,3,['error'],['error']
Availability,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s; user	0m3.036s; sys	0m0.866s; ```. **Setup**; - Operating system: CentOS Linux release 8.2.2004 (Core); - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity/Docker; - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**; The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:4779,ERROR,ERROR,4779,,https://github.com/google/deepvariant/issues/533,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"untu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9248,avail,available,9248,,https://github.com/google/deepvariant/issues/489,1,['avail'],['available']
Availability,"urce, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Operating system:; Redhat enterprise v7.9, x86_64; **Steps to reproduce:**; - Command:; BIN_VERSION=""1.5.0""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/HG38.fa \; --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=4 \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \; --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2""; I allocated 4 cores and 70GBs to run that program.; I added the VAF thresholds for SNPs and Indels because I read the reported issues:; https://github.com/google/deepvariant/issues/578; - Error trace: (if applicable); And here are some most recent results I got from stdout:; I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]; I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]; I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]; I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]; I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]; I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]; I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]; I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 candidates (8306995 examples) [16",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:1465,Error,Error,1465,,https://github.com/google/deepvariant/issues/683,1,['Error'],['Error']
Availability,urce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11261,error,error,11261,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,urce_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18133,error,error,18133,,https://github.com/google/deepvariant/issues/19,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3601,echo,echo,3601,,https://github.com/google/deepvariant/issues/151,1,['echo'],['echo']
Availability,"ut the model keeps generating this error while running the call_variants.py step. WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0626 13:39:06.145823 140632388314944 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:1145,checkpoint,checkpoint,1145,,https://github.com/google/deepvariant/issues/845,1,['checkpoint'],['checkpoint']
Availability,"ut.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 8 channels while the examples have 6.; ```. Running with the checkpoint for 0.9.0 works with this command. Is the model on the GS bucket correct?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:2249,checkpoint,checkpoint,2249,,https://github.com/google/deepvariant/issues/392,3,['checkpoint'],['checkpoint']
Availability,"v) [anovak@phoenix-01 trash]$ ls output/models/*meta; output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta; output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta; output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:3396,checkpoint,checkpoint,3396,,https://github.com/google/deepvariant/issues/611,1,['checkpoint'],['checkpoint']
Availability,"v2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt.; 2019",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:122494,Checkpoint,CheckpointSaverHook,122494,,https://github.com/google/deepvariant/issues/172,1,['Checkpoint'],['CheckpointSaverHook']
Availability,"variant and merged these with GLnexus as recommended. However, there seems to be an issue in the final merged VCF file. Many genotypes are called as 0/0 when they have very low or zero DP:; e.g. ; `; 1	1319056	1_1319056_A_G	A	G	51	.	AF=0.32848;AQ=51	GT:DP:AD:GQ:PL:RNC	0/0:0:0,0:1:0,0,0:..	1/1:2:0,2:3:29,6,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/1:2:0,2:1:12,2,0:..	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vcf header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GLnexusVersion=v1.4.1-0-g68e25e5; ##GLnexusConfigName=DeepVariant; ##GLnexusConfigCRC32C=2932316105; ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:1344,down,downstream,1344,,https://github.com/google/deepvariant/issues/633,1,['down'],['downstream']
Availability,"ver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2606,checkpoint,checkpoint,2606,,https://github.com/google/deepvariant/issues/845,2,['checkpoint'],['checkpoint']
Availability,"which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1426,avail,available,1426,,https://github.com/google/deepvariant/issues/761,1,['avail'],['available']
Availability,while installing im getting an bazel error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:37,error,error,37,,https://github.com/google/deepvariant/issues/231,1,['error'],['error']
Availability,"x/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Un",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:1461,error,error,1461,,https://github.com/google/deepvariant/issues/231,1,['error'],['error']
Availability,"xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**; ```; INFO: Using cached SIF image; I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory; Traceback (most recent call last):; File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:2088,Error,Error,2088,,https://github.com/google/deepvariant/issues/533,1,['Error'],['Error']
Availability,"y:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, some very similar samples keep running",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:11806,mainten,maintenance,11806,,https://github.com/google/deepvariant/issues/855,2,"['down', 'mainten']","['downstream', 'maintenance']"
Availability,"ys.exit(main(argv)); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 220, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 157, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 109, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 170, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/7361351.1.gpu.q/Bazel.runfiles_ii4x9mqm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 240, in __init__; self.header = self._reader.header; google.protobuf.message.DecodeError: Error parsing message. ```. I ran the WES example from you with no problem, but I experience issues with my own data (I have the same setup when running singularity). ; I checked the reference and input bam files, they don't seem to be corrupted... but just googling the error did not help much. Cannot think of anything else, maybe you have some suggestions where the problem could be coming from? Otherwise I will try to find the raw data for my .bam files and do remapping to hg38 and use the ref that worked previously. . Thanks again!!; Alisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:4083,Error,Error,4083,,https://github.com/google/deepvariant/issues/870,2,"['Error', 'error']","['Error', 'error']"
Availability,"zing model with random parameters; I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model; I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0; I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_tELT0A/runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:3756,checkpoint,checkpoint,3756,,https://github.com/google/deepvariant/issues/172,1,['checkpoint'],['checkpoint']
Deployability," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:1480,install,installed,1480,,https://github.com/google/deepvariant/issues/243,2,['install'],"['installation', 'installed']"
Deployability," /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.4; - DeepVariant version: pepper_deepvariant:r0.8-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command: ; ```; 	docker run --ipc=host \; 	--gpus all \; 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; 	-v ""${BASE}"":""${BASE}"" \; 	-v ""${REF}"":""${REF}"" \; 	-v ""${BAMPATH}"":""${BAMPATH}"" \; 	kishwars/pepper_deepvariant:r0.8-gpu \; 	run_pepper_margin_deepvariant call_variant \; 	-o ""${OUTPUT_DIR}"" \; 	-b ""${BAM}"" \; 	-f ""${REF}"" \; 	-p ""${OUTPUT_PREFIX}"" \; 	-t ${THREADS} \; 	-g \; 	--ont_r9_guppy5_sup; ```. - Error trace: (if applicable); ; [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:2058,Install,Installation,2058,,https://github.com/google/deepvariant/issues/539,1,['Install'],['Installation']
Deployability," /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17543,upgrade,upgrade,17543,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability," 1/2: Found 3672 candidate variants; I0105 15:55:21.263317 140329169033024 make_examples_core.py:301] Task 1/2: Created 3944 examples. real 1m56.796s; user 3m3.813s; sys 0m4.710s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --checkpoint ""/opt/models/wgs"". 2024-01-05 15:55:31.140705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13127,install,installed,13127,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability," 3.0.2; Summary: Editable interval tree data structure for Python 2 and 3; Home-page: https://github.com/chaimleib/intervaltree; Author: Chaim Leib Halbert, Konstantin Tretyakov; Author-email: chaim.leib.halbert@gmail.com; License: Apache License, Version 2.0; Location: /home/chungtsai_su/.local/lib/python2.7/site-packages; Requires: sortedcontainers; Required-by:; chungtsai_su@seqslab:~/quickstart-output$ pip uninstall intervaltree; Uninstalling intervaltree-3.0.2:; Would remove:; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:3294,Install,Installing,3294,,https://github.com/google/deepvariant/issues/131,2,"['Install', 'install']","['Installing', 'installed']"
Deployability," = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:5147,install,installed,5147,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability," _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process; sample_reads = self.region_reads_norealign(; File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign; reads = itertools.chain(reads, sam_reader.query(region)); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query; return self._reader.query(region); ValueError: FAILED_PRECONDITION: Cannot query without an index; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1; ; ```; ; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Quickstart works. This issue also happens when I try to run the pipeline with docker-only.; ; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:6953,pipeline,pipeline,6953,,https://github.com/google/deepvariant/issues/666,1,['pipeline'],['pipeline']
Deployability," already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17383,upgrade,upgrade,17383,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability, amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6055,install,install,6055,,https://github.com/google/deepvariant/issues/489,1,['install'],['install']
Deployability," anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:1746,release,release,1746,,https://github.com/google/deepvariant/issues/733,1,['release'],['release']
Deployability," have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1055,install,installed,1055,,https://github.com/google/deepvariant/issues/223,1,['install'],['installed']
Deployability," help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:1653,install,installed,1653,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability," latest DeepVaraint v1.6.1 for ONT data variant calling. *Make_example* and *Call_variants* works perfectly, but when it came to **postprocess_variant** things get out of control. In detail, it reported as below:; ```bash; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/inter/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{VCF.gz}"" --cpus ""120"". 2024-04-08 06:27:55.589078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-04-08 06:27:55.589111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; I0408 06:27:57.480687 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0408 06:55:56.836046 140282942986048 postprocess_variants.py:1313] CVO sorting took 27.989152932167052 minutes; I0408 06:55:56.837136 140282942986048 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0408 06:55:56.837199 140282942986048 postprocess_variants.py:1318] Using 120 CPUs for parallelization of variant transformation.; I0408 07:06:00.821415 140282942986048 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0408 07:29:46.200004 140282942986048 postprocess_variants.py:1365] Writing variants to VCF.; I0408 07:29:46.201339 140282942986048 postprocess_variants.py:973] Writing output to VCF file: /input/R9G4.vcf.gz; I0408 07:29:46.877771 140282942986048 genomics_writer.py:183] Writing /input/R9G4.vcf.gz with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:1219,install,installed,1219,,https://github.com/google/deepvariant/issues/804,1,['install'],['installed']
Deployability," libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[=============================================================================================================================================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2353,install,install,2353,,https://github.com/google/deepvariant/issues/489,1,['install'],['install']
Deployability," load input/weights-51-0.995354.ckpt* instead. ***** Intermediate results will be written to /tmp/tmpd74of138 in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_TIME = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_TIME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-02-17 23:32:31.107126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:5331,install,installed,5331,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability," settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:1032,Install,Install,1032,,https://github.com/google/deepvariant/issues/231,4,['Install'],"['Install', 'Installing']"
Deployability," single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/intel/openvino/`, while there is nothing like that in the deeptrio image. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416:2121,install,installed,2121,,https://github.com/google/deepvariant/issues/416,1,['install'],['installed']
Deployability," to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:8924,install,install-,8924,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-']
Deployability," version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:1011,Install,Installation,1011,,https://github.com/google/deepvariant/issues/342,1,['Install'],['Installation']
Deployability,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440:854,Install,Installation,854,,https://github.com/google/deepvariant/issues/440,1,['Install'],['Installation']
Deployability,""" \; --bam gs://deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --ref gs://deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta.gz \; --regions ""chr20:10,000,000-10,010,000""; ```. My `runner.sh` (changed `PROJECT_ID`,`OUTPUT_BUCKET`, and `STAGING_FOLDER_NAME`) is; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:1773,pipeline,pipeline,1773,,https://github.com/google/deepvariant/issues/60,1,['pipeline'],['pipeline']
Deployability,""", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:2865,install,install,2865,,https://github.com/google/deepvariant/issues/469,1,['install'],['install']
Deployability,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/210:1178,pipeline,pipeline,1178,,https://github.com/google/deepvariant/issues/210,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"### Issue; When running the build-prereq shell script, I'm getting an error when the Tensorflow install begins. ### Error message; ```; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; - [1 files][ 41.1 MiB/ 41.1 MiB] 1.0 MiB/s ; Operation completed over 1 objects/41.1 MiB. ; tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.; ```. ### Debugging efforts; After browsing around a bit, I discovered that this issue was solved for some through installing the .whl separately. So, I download the whl from the [GCloud bucket](https://console.cloud.google.com/storage/browser/deepvariant/packages/tensorflow/) and executed `sudo python2.7 pip install <name of .whl file>` through the terminal. It ran, just to tell me “.dist-info directory not found”. I think this might be due to some inconsistency in the packages installed through the build-prereq.sh script, because I can see that all the packages that it installed (e.g. numpy) are for Python 3.5, but the Tensorflow version it's trying to get is for cp27 (Python 2.7). Not sure about where to go from here, would love some assistance :). ### System details; OS: Ubuntu 16.04 LTS; Python interpreters: Default with Ubuntu (2.7 and 3.5.2); Deep Variant version: Installed it today from the main repo, so probably r0.4.1. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30:96,install,install,96,,https://github.com/google/deepvariant/issues/30,7,"['Install', 'install']","['Installed', 'Installing', 'install', 'installed', 'installing']"
Deployability,#include <optional> error after update from v0.8.0 to v0.9.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:32,update,update,32,,https://github.com/google/deepvariant/issues/236,1,['update'],['update']
Deployability,"#set -euo pipefail; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/deep_output; STAGING_FOLDER_NAME=stage; OUTPUT_FILE_NAME=deeptest_FB4_chr20.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1217,pipeline,pipelines,1217,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipelines']
Deployability,"${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3055,configurat,configuration,3055,,https://github.com/google/deepvariant/issues/60,4,"['configurat', 'install']","['configuration', 'installed']"
Deployability,(1) Update binary version to 0.7.1. Re-run all case studies and updat…,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/234:4,Update,Update,4,,https://github.com/google/deepvariant/pull/234,1,['Update'],['Update']
Deployability,(Note: We are not taking pull requests at this time. This is an update to the blog from within the DeepVariant team.). This adds a system for showing author images at the top of blog posts.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/218:64,update,update,64,,https://github.com/google/deepvariant/pull/218,1,['update'],['update']
Deployability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:3138,Install,Installation,3138,,https://github.com/google/deepvariant/issues/845,1,['Install'],['Installation']
Deployability,* DeepVariant Logo.; * DeepVariant RNA-seq case study.; * DeepVariant RNA-seq release.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/576:78,release,release,78,,https://github.com/google/deepvariant/pull/576,1,['release'],['release']
Deployability,"* Install OpenVINO by pip; * Update OpenVINO to latest 2021.3 version; * Use `enum34==1.1.8` to fix ""AttributeError: module 'enum' has no attribute 'IntFlag'"" (https://github.com/python-poetry/poetry/issues/1122#issuecomment-628037127). test run: https://github.com/dkurt/deepvariant/actions/runs/755874669",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442:2,Install,Install,2,,https://github.com/google/deepvariant/pull/442,2,"['Install', 'Update']","['Install', 'Update']"
Deployability,"***; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-10-31 20:36:34.101375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-10-31 20:36:35.010025: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.; I1031 20:36:35.011695 132485076334400 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: NA12878; I1031 20:36:35.013445 132485076334400 postprocess_variants.py:1313] CVO sorting took 1.1885166168212891e-05 minutes; I1031 20:36:35.013573 132485076334400 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I1031 20:36:35.014770 132485076334400 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: NA12878; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:2301,install,installed,2301,,https://github.com/google/deepvariant/issues/901,1,['install'],['installed']
Deployability,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:112,install,install,112,,https://github.com/google/deepvariant/issues/342,2,['install'],"['install', 'installing']"
Deployability,"**Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55; `; Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588; `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0; BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**; - Operating system:; - DeepVariant version: 1.3.0, latest ; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**; - Command: Call variants with run_deepvariant wrapper script. ; - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520:1126,Install,Installation,1126,,https://github.com/google/deepvariant/issues/520,1,['Install'],['Installation']
Deployability,"**Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode.; Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode.; The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed.; So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below).; Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you!. Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650:269,pipeline,pipeline,269,,https://github.com/google/deepvariant/issues/650,1,['pipeline'],['pipeline']
Deployability,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:🆑:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:202,install,install,202,,https://github.com/google/deepvariant/issues/380,4,"['INSTALL', 'install']","['INSTALL', 'install', 'installation']"
Deployability,"**Describe the issue:**. I apologize as this is a question rather than a problem.; So this ticket isn't using any predefined template. Here's my question:; given the newly released human T2T reference (v2.0), should DV be re-trained against that reference?; I must admit I don't understand DV deep enough to ponder with what the potential benefits would be, so am curious about your thoughts. Thanks!. Steve. p.s. the data modes most relevant for us are CCS, ONT.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534:172,release,released,172,,https://github.com/google/deepvariant/issues/534,1,['release'],['released']
Deployability,"**Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**; - Operating system: Ubuntu 22.04; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```; time docker run --rm --gpus 1 \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt""; ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```; docker run --rm --gpus '""device=1""' \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; -u $(id -u):$(id -g) \; google/deepvariant:1.4.0-gpu \; /opt/deepvariant/bin/model_eval \; --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --checkpoint_dir=""${TRAINING",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:491,Install,Installation,491,,https://github.com/google/deepvariant/issues/611,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:153,Install,Installation,153,,https://github.com/google/deepvariant/issues/367,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; /opt/deepvariant/bin/run_deepvariant crashes when start the GPU stage of call variants. **Setup**; google/deepvariant:0.10.0; Docker; subset of illumina resequencing data; $nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2017 NVIDIA Corporation; Built on Fri_Nov__3_21:07:56_CDT_2017; Cuda compilation tools, release 9.1, V9.1.85; $ nvidia-smi; NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:360,release,release,360,,https://github.com/google/deepvariant/issues/358,1,['release'],['release']
Deployability,"**Describe the issue:**; After running, no VCF is found, the logs however are available. **Setup**; - Operating system: ubuntu 22.04 (WSL2); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): docker; - Type of data: (I find variant only in chr17 for easier reading and faster speed); - input read: aligned HG004 to CHM13 (https://downloads.pacbcloud.com/public/revio/2022Q4/HG004-rep1/); - reference genome: CHM13 (https://github.com/marbl/CHM13). **Steps to reproduce:**; - Command:; `docker run --volume ""/root/deepvariant/input"":""/input"" --volume ""/root/deepvariant/output"":""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/chm13v2.0.fa --reads=/input/aligntest.bam --regions ""chr17"" --output_vcf=/output.vcf.gz --output_gvcf=/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results --logging_dir=/output`; ; **Any additional context:**; [make_examples.log](https://github.com/user-attachments/files/16189177/make_examples.log); [call_variants.log](https://github.com/user-attachments/files/16189180/call_variants.log); [postprocess_variants.log](https://github.com/user-attachments/files/16189182/postprocess_variants.log); [vcf_stats_report.log](https://github.com/user-attachments/files/16189186/vcf_stats_report.log)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/850:173,Install,Installation,173,,https://github.com/google/deepvariant/issues/850,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; After upgrading to v1.6, we noticed this strange behavior, where the program hangs on a sharded BAM that holds only alt-contig mapping reads. **Setup**; - Operating system: on GCE via Google Life Sciences API (through Cromwell); - DeepVariant version: v1.6; - Installation method (Docker, built from source, etc.): official v1.6 docker; - Type of data: Both PacBio HiFi and ONT (10.4), on GRCh38. . **Steps to reproduce:**. - Command. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --haploid_contigs chrX,chrY \; --par_regions_bed GRCh38.PAR.bed \; --reads=/cromwell_root/<sample_id>.alts.bam \; --output_vcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.vcf.gz \; --output_gvcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.g.vcf.gz \; --num_shards=16; ```. - Relevant log ; (note it says ""0 examples"", so I suspect it is when no examples are available, not just when there's only alt-mapping reads, e.g. if one simulates reads error-free from the reference itself, it probably will have the same issue). ```; /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00011-of-00016.gz.example_info.json; I0203 17:23:03.253894 135328978921280 make_examples_core.py:2958] example_shape = None; I0203 17:23:03.254237 135328978921280 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:03.255900 135328978921280 make_examples_core.py:301] Task 11/16: Found 0 candidate variants; I0203 17:23:03.256017 135328978921280 make_examples_core.py:301] Task 11/16: Created 0 examples; I0203 17:23:04.930985 137565708298048 make_examples_core.py:301] Task 7/16: Writing example info to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00007-of-00016.gz.example_info.json; I0203 17:23:04.931358 137565708298048 make_examples_core.py:2958] example_shape = None; I0203 17:23:04.931699 137565708298048 make_examples_core.py:2959] example_chann",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:285,Install,Installation,285,,https://github.com/google/deepvariant/issues/769,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Are the `--output_gvcf` flags with DeepTrio required arguments? . In the `--helpfull` page of DeepTrio, they are not indicated as required; however, when omitting just these flags, DeepTrio fails to generate the expected VCF outputs. ; ```; --output_gvcf_child ; --output_gvcf_parent1; --output_gvcf_parent2; ```. **Setup**; - Operating system: Linux; - DeepTrio version: 1.1.0; - Installation method (Docker, built from source, etc.): Singularity ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) I am running DeepTrio using cattle genomes on a local machine with 56 CPUs and 500GB RAM using the same cattle reference genome from creating the BAM files. . **Steps to reproduce:** ; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/,""${REF_PATH}/"":/ref_dir/,""${BAM_PATH}/"":/in_dir/,""${RESULTS_DIR}/"":/out_dir/ \; deepvariant_deeptrio-${BIN_VERSION_DT}.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa \; --intermediate_results_dir=""/out_dir/${trioName}/"" \; --sample_name_child ""199713"" \; --sample_name_parent1 ""199710"" \; --sample_name_parent2 ""199718"" \; --reads_child=""/in_dir/199713.realigned.recalibrated.bam"" \; --reads_parent1=""/in_dir/199710.realigned.recalibrated.bam"" \; --reads_parent2=""/in_dir/199718.realigned.recalibrated.bam"" \; --output_vcf_child=""/out_dir/199713.output.vcf.gz"" \; --output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:406,Install,Installation,406,,https://github.com/google/deepvariant/issues/429,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**; - Operating system: Amazon Linux 2023; - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda; - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**; - Command: `mamba install deepvariant -c bioconda`; - Error trace: ; ```; Pinned packages:; - python 3.10.*. Could not solve for environment specs; The following packages are incompatible; └─ deepvariant is installable with the potential options; ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.10.0|1.0.0] would require; │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; ├─ deepvariant [0.7.1|0.7.2] would require; │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);; └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel).; ```. **Does the quick start test work on your system?**; N/A. **Any additional context:**; My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736:39,install,install,39,,https://github.com/google/deepvariant/issues/736,7,"['Install', 'install', 'release']","['Installation', 'install', 'installable', 'installed', 'releases']"
Deployability,"**Describe the issue:**; Cannot install latest DeepVariant via Conda in my new environment. Error prompts, similar to that of #736. If having everything in default, I can have python=2.7 and deepvariant=0.7.0; but cannot update to 1.15 or latest. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (ssh to university, docker unavailable); - DeepVariant version: 0.7.0 installable, but cannot get 1.15 or latest; - Installation method (Docker, built from source, etc.): conda; - Type of data: N/A. **Steps to reproduce:**; - Command:; $ create -n deepvariant python=3.8 (current version 3.8.19); $ conda install deepvariant. - Error trace: (if applicable). > #warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE; > warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > failed; > ; > LibMambaUnsatisfiableError: Encountered problems while solving:; > - package deepvariant-0.4.1-np113py27_0 requires python 2.7*, but none of the providers can be installed; > ; > Could not solve for environment specs; > The following packages are incompatible; > ├─ deepvariant is installable with the potential options; > │ ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; > │ │ └─ tensorflow 1.12.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.10.0|1.0.0] would require; > │ │ └─ tensorflow 2.0.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; > │ │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; > │ ├─ deepvariant [0.7.1|0.7.2] would require; > │ │ └─ tensorflow 1.11.* , which conflicts with any installable versions previously reported;; > │ └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; > │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835:32,install,install,32,,https://github.com/google/deepvariant/issues/835,6,"['Install', 'install', 'release', 'update']","['Installation', 'install', 'installable', 'release', 'update']"
Deployability,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/334:807,pipeline,pipelines,807,,https://github.com/google/deepvariant/issues/334,3,"['Install', 'pipeline']","['Installation', 'pipeline', 'pipelines']"
Deployability,"**Describe the issue:**; Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:552,Install,Installation,552,,https://github.com/google/deepvariant/issues/677,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/340:346,Install,Installation,346,,https://github.com/google/deepvariant/issues/340,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Hello, i am trying to recreate the steps done in this post: https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/; I have a bam file with phasing information which a got by running whatshap on the data and now i want use deepvariant for a second time just like the steps in the post. ; I have a illumina bam file which i phased with nanopore data. I run this command: ; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads whatshap/HG003.GRCh38.chr20.haplotagged.bam \; --use_hp_information \; --output_vcf deepvariant2/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20. And get this error : ; NotImplementedError: The --use_hp_information flag has been deprecated. DeepVariant now phases internally for PacBio mode. I get what deprecated means but what is the thing i have to do to specify to deepvariant that my bam file is phased because i dont think it is using the haplotagged info in the bam file now?. **Setup**; - DeepVariant version: latest; - Installation method : source; - Type of data: bam file with phasing information by whatshap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/822:1169,Install,Installation,1169,,https://github.com/google/deepvariant/issues/822,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio?. **Setup**; - Operating system: Cent OS; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/532:352,Install,Installation,352,,https://github.com/google/deepvariant/issues/532,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**; - Operating system: CentOS Linux 7 (Core); - Singularity version: 3.5-8.el7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: WGS. **Steps to reproduce:**; - Command:; > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=${reference_genome} \; > --reads=${bam} \; > --regions=""chr1"" \; > --output_vcf=${vcf_dir}/${sample}.vcf.gz \; > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \; > --intermediate_results_dir ${tmp_dir} \; > --num_shards=${ncpu}. - Error trace: (if applicable); > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**; I also tried with `--no-home` flag which did not work at all. ; I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/601:486,Install,Installation,486,,https://github.com/google/deepvariant/issues/601,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:780,Install,Installation,780,,https://github.com/google/deepvariant/issues/339,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I am attempting to use DeepVariant 1.4 with a model trained on DeepVariant 1.3. I encounter the error:; ""ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 7."". **Setup**; - Operating system: Linux Ubuntu 20.04; - DeepVariant version: 1.4; - Installation method: Docker; Just regular bam files being called on the T2T reference fasta. **Steps to reproduce:**; /opt/deepvariant/bin/run_deepvariant \; --ref=hprc-jun1-mc-chm13-minaf.0.1.fasta \; --reads=HSB340-CHM13v2.chrY.sorted.deduped.cram \; --customized_model=model.ckpt-364300 \; --output_vcf=HSB340-CHM13v2.chrY.deepvariant.vcf.gz \; --output_gvcf=HSB340-CHM13v2.chrY.deepvariant.g.vcf.gz \; --model_type WGS \; --make_examples_extra_args phase_reads=true,channels=blank \; --regions CHM13v2.chrY \; --num_shards=24. parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""hprc-jun1-mc-chm13-minaf.0.1.fasta"" --reads ""HSB340-CHM13v2.chrY.sorted.deduped.cram"" -examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --channels ""blank"" --gvcf ""/tmp/tmpwn2kfxca/gvcf.tfrecord@24.gz"" --phase_reads --regions ""CHM13v2.chrY"" --task {}. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpwn2kfxca/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpwn2kfxca/make_examples.tfrecord@24.gz"" --checkpoint ""model.ckpt-364300"" --openvino_model_dir ""/tmp/tmpwn2kfxca""; I0901 22:59:14.275113 140554215814976 call_variants.py:317] From /tmp/tmpwn2kfxca/make_examples.tfrecord-00000-of-00024.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 18].; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2ucnuw5e/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/563:350,Install,Installation,350,,https://github.com/google/deepvariant/issues/563,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I am not obtaining any output files even though there are no major issues in the log file, (see attached); I ran it with the same data first, only for the chr20, and everything went fine. For all the genome now, I don't have the vcfs.; [deepvariant_run.log](https://github.com/user-attachments/files/16596005/deepvariant_run.log). **Setup**; - Operating system: Windows, WSL2 (5.15.146.1-microsoft-standard-WSL2); - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: NA12878, bam file. **Steps to reproduce:**; sudo docker run \; -v ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing:/input"" \; -v ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing/output:/output"" \; google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/genome.fa \; --reads=/input/sorted.bam \; --output_vcf=/output/outputdeepvar.vcf \; --output_gvcf=/output/outputdeepvar.g.vcf \; --num_shards=4 \; > ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing/output/deepvariant_run.log"" 2>&1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868:471,Install,Installation,471,,https://github.com/google/deepvariant/issues/868,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I am observing some weird genotypes calls, when I call variants from RNA-seq data. I've followed the nicely written tutorial, the only thing I changed was a minimum coverage of 5X (instead of 3X). Below I have some examples (GT, AD and PL). | GT | AD | PL | QUAL | GQ | ; | ------------- | ------------- | ------------- | ------------- | ------------- |; | 1/1 | 117,86 | 58,42,0 | 42 | 42; | 0/1 | 88,13 | 2,0,13 | 4 | 4. Why is the first SNP called as homozygous ALT, even if I have more reads for the REF compared to ALT (117 vs 86)?. From what I've read, the AD values is calculated by chunks. **Setup**; - Operating system: CentOS 8; - DeepVariant version: 1.5.0, with 1.4.0 RNA model; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) RNA-seq",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701:718,Install,Installation,718,,https://github.com/google/deepvariant/issues/701,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:173,Install,Installation,173,,https://github.com/google/deepvariant/issues/471,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam ✅; 2. non-parallel + cram ✅ ; 3. parallel + bam ✅ ; 4. non-parallel + cram 🔴 . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:733,Install,Installation,733,,https://github.com/google/deepvariant/issues/786,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; I'm doing a series of test of how can I run DV faster with my resources. I'm trying splitting the bam file - run make examples on separate - run Call Variants of both at the same time and call variants. Everything goes fine until the postprocess The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:742,Install,Installation,742,,https://github.com/google/deepvariant/issues/413,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411:239,Install,Installation,239,,https://github.com/google/deepvariant/issues/411,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Make_Example fail because of bed.file. **Setup**; - Operating system:ubuntu18.04; - DeepVariant version:v1.0.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: same as case study. **Steps to reproduce:**; - Command:; /usr/local/seqslab/deepvariant/bazel-bin/deepvariant/make_examples \; --mode calling \; --ref /opt/command/test_dir/ref.fa \; --reads /opt/command/test_dir/0-0.bam \; --regions /opt/command/test_dir/part_0.bed \; --examples /opt/command/test_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:139,Install,Installation,139,,https://github.com/google/deepvariant/issues/374,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; On a specific batch of samples, GQs and QUALs seem to be abnormal.; The GQ and QUAL distributions are bimodal and for variants they are much lower than I would expect. It doesn't seem like there is anything wrong with the calls themselves; I get an expected number of variants. I also can not find anything wrong with the input data. It has high base quality throughout the reads, they are 100bp paired end reads from a NovaSeq with the four value binned base quality scores. This is the visual report for one sample. <img width=""1307"" alt=""image"" src=""https://user-images.githubusercontent.com/8237552/202532045-0aa0f5fa-28bd-40d3-be4f-72a74e5ea072.png"">. Here is an example. I would expect this variant to have a much higher GQ and QUAL. I also have attached deepvariant's channels png for this variant. ; ```; chr1 169421916 . A G 18.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:17:58:29,29:0.5:18,0,22; ```; ![chr1_169421916_A-G](https://user-images.githubusercontent.com/8237552/202532164-d069b9c8-d1e8-4d19-9dba-f1b95f34fcd7.png). Is this expected or is something strange happening here, any insight you can provide would be very appreciated.; Thank you. **Setup**; - Operating system: Ubuntu 20.04; - DeepVariant version: 1.4 (but also 1.2); - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Novaseq, 100bp paired, HG38. **Steps to reproduce:**; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ -c --pwd $(pwd) -W $(pwd) -B $(pwd) docker://google/deepvariant:1.4.0 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref $REF --reads $CRAM --output_vcf $VCF --output_gvcf $GVCF --intermediate_results_dir ./int_results --regions $BED; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586:1263,Install,Installation,1263,,https://github.com/google/deepvariant/issues/586,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Running DeepVariant v1.1.0 on viral amplicon PacBio HiFi data, aligned with pbmm2, using the run_deepvariant script. Core dump during the make_examples step with exit status 252. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), singularity version 3.5.3-1.el7; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): singularity image pulled from docker://google/deepvariant:1.1.0; - Type of data: PacBio HiFi amplicons. **Steps to reproduce:**; - Command:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ref.fa \; --reads reads.bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:248,release,release,248,,https://github.com/google/deepvariant/issues/419,2,"['Install', 'release']","['Installation', 'release']"
Deployability,"**Describe the issue:**; Shuffle script for tfrecords (https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-training-case-study.md) runs out of memory when using a training set from multiple BAM files. This is what I followed:; - Run make_examples for each BAM file to obtain tfrecords; - Run shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on all the records from all the BAM files. This requires over 230 GB of CPU RAM, and the process is eventually killed. I do not know whether the memory requirement will keep growing beyond this point. Is there another way to deal with this situation? For example, it would be possible to run shuffling for data from each bam file independently. However, I am not sure what the flow would look like after that point. **Setup**; - Operating system: Ubuntu Bionic; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/360:906,Install,Installation,906,,https://github.com/google/deepvariant/issues/360,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; Thank you so much for the great tool. . I'm working on a heterozygous mouse long-read RNA-seq dataset from PacBio and would like to perform variant call + phasing at read-level. I'm wondering whether you have some recommendations regarding the points below:; - I'm currently using `--model_type=PACBIO` with the bam files processed with `gatk SplitNCigarReads`. Does this model consider RNA editing? Or should I use `--model_type=WES`? I saw some discussions mentioning WES model considers RNA-editing in https://github.com/google/deepvariant/issues/775; - Is there anyway that I could integrate the known variants from genomic data into the variant calling? Or should it be integrated after `DeepVariant` variant call at vcf-level?. **Setup**; - Operating system: Ubuntu 2.20; - DeepVariant version: v1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HiFi, mm10, long-read RNA-seq data. **Steps to reproduce:**; - Command:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GRCm38.primary_assembly.genome.fa \; --reads=SNCR.bam \; --output_vcf=output.vcf.gz \; --num_shards 16; ```. Thank you so much for your kind help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/890:611,integrat,integrate,611,,https://github.com/google/deepvariant/issues/890,3,"['Install', 'integrat']","['Installation', 'integrate', 'integrated']"
Deployability,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434:553,Install,Installation,553,,https://github.com/google/deepvariant/issues/434,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402:1200,Install,Installation,1200,,https://github.com/google/deepvariant/issues/402,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; When calling I get a low number of records in a vcf file (20-40k variants). In comparison with GATK there are at last 10 times less variants generated. Also, it is much less than from 1 thousand genomes deepvariant dataset truncated to the same exome region - 1.7 mln records. **Setup**; - Operating system:; CentoOS 7; - DeepVariant version:; 1.3.0; - Installation method (Docker, built from source, etc.):; podman/singularity; - Type of data:; The data are human's whole exome sequences from Illumina. As a reference I use hg38 with alt contigs. I use truseq v1.2 exome bed file. Besides using bwa-mem2, samtools merge and sort there is not much preprocessing. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=\$PWD/$idxbase \; --reads=\$PWD/${sample_id}.bam \; --regions=\$PWD/$bed_file \; --output_vcf=\$PWD/${sample_id}.vcf.gz \; --output_gvcf=\$PWD/${sample_id}.g.vcf.gz \; --num_shards=${task.cpus}. ```. I also used glnexus; ```; glnexus_cli ; --bed /in/truseq-dna-exome-targeted-regions-manifest-v1-2.bed ; --config DeepVariantWES /in/vcf_deepvariant/*.g.vcf.gz > merged.bcf; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/501:378,Install,Installation,378,,https://github.com/google/deepvariant/issues/501,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; When doing the variant calling with a VCF file of proposed variants (`variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}`) and specifying GVCF output, ref call sites end up with only 2 entries in the FORMAT field AD while the ALT field contain also 2 entries which should result in 3 AD entries. ; Example: `chr1 11391 . T A,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:24:0:0,0:0:0,26,27,990,990,990`. Here the AD field has 2 entries but should have 3. Thereafter, the merging with GLNexus fails with the following error `Failed to genotype: Invalid: genotyper: VCF allele depth FORMAT field is malformed (sample <0>:11391-11391 (AD))`. . **Setup**; - Operating system: Linux; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Singularity. **Steps to reproduce:**; - Command: ` singularity exec -B /usr/lib/locale/:/usr/lib/locale/ ${SINGULARITY_IMG} bash /opt/deepvariant/bin/run_deepvariant --model_type WGS --customized_model ${MODEL} --ref ${REF} --reads ${IN_BAM} --output_vcf ${OUT_VCF}.2.vcf.gz --output_gvcf ${OUT_VCF}.2.g.vcf.gz --num_shards 24 --make_examples_extra_args=\""realign_reads=false,min_mapping_quality=${MIN_MAPQ},min_base_quality=${MIN_QS},variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}\"" --sample_name ${IN_PN} ${BED_ARG};""; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/341:747,Install,Installation,747,,https://github.com/google/deepvariant/issues/341,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:187,Install,Installation,187,,https://github.com/google/deepvariant/issues/455,1,['Install'],['Installation']
Deployability,"**Describe the issue:**; while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**; - OS: CentOS Linux release 7.4.1708 (Core); - DeepVariant version:conda install bioconda/label/cf201901::deepvariant; - Installation method (Docker, built from source, etc.): Conda; - Type of data: NA. **Steps to reproduce:**; - Command: conda install bioconda/label/cf201901::deepvariant; - Error trace: ; '''conda install bioconda/label/cf201901::deepvariant -y; Collecting package metadata: done; Solving environment: '''. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**; NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806:41,install,install,41,,https://github.com/google/deepvariant/issues/806,7,"['Install', 'install', 'release']","['Installation', 'install', 'installed', 'release']"
Deployability,"**Have you checked the FAQ? [](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-wgs-case-study.md). **Describe the issue:**; Merging vcf files error.; **Setup**; - Operating system: working on cluster ; - DeepVariant version:latest; - Installation method (Docker):; - Type of data: (GIAB AshkenazimTrio [HG002,HG003,HG004] analysis.). **Steps to reproduce:**; - Command: ; ```; udocker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | udocker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```; - Error trace: (if applicable); ; > Num BCF records read 118736378 query hits 14552613; > [E::bgzf_read_block] Invalid BGZF header at offset 265038798; > [E::bgzf_read] Read block operation failed with error 2 after 0 of 32 bytes; > [E::bgzf_read] Read block operation failed with error 3 after 0 of 32 bytes; > Error: BCF read err. ![Screenshot from 2024-05-06 15-00-29](https://github.com/google/deepvariant/assets/45700858/2c6e5565-78aa-4e55-9488-82b2f5f04514)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815:250,Install,Installation,250,,https://github.com/google/deepvariant/issues/815,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**. We found a back-to-back call of two SNPs that we cannot explain as the BAM file suggests a deletion. IGV screenshot: https://www.dropbox.com/s/c0wfelxc1cca14b/igv_snapshot.png?dl=0. Happy to provide a BAM file etc. But maybe this is easy enough to explain; I just cannot figure out why this comes out as:; TC and GC and not as TC and G/-. chr19 15174241 rs1044006 T C 66 . AC=1;AF=0.5;AN=2;AQ=66;DP=78 GT:AD:DP:GQ:PL:RNC 0/1:0,78:78:10:20,0,9:.; chr19 15174242 chr19_15174242_G_C G C 53 . AC=1;AF=0.5;AN=2;AQ=53;DP=177 GT:AD:DP:GQ:PL:RNC 0/1:77,100:177:50:53,0,52:. **Setup**; - Operating system: Centos 7; - DeepVariant version: 1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Novaseq 6000, exomes, GRCh38. **Steps to reproduce:**; - Command: Does not apply.; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/453:754,Install,Installation,754,,https://github.com/google/deepvariant/issues/453,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459:232,Install,Installation,232,,https://github.com/google/deepvariant/issues/459,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470:1255,Install,Installation,1255,,https://github.com/google/deepvariant/issues/470,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; ```; Deepvariant failed on PACBIO data. ; ```. **Setup**; - Operating system:; `google cloud through [Terra](https://terra.bio/)`; - DeepVariant version:; `1.1.0`; - Installation method (Docker, built from source, etc.):; `google/deepvariant:1.1.0`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ```; Pacbio bam from GIAB ; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/NA12878/PacBio_SequelII_CCS_11kb/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam; reference from Broad GCP; gs://gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta; Though this is a slight ref mismatch, B37 vs hs37. I don't think that should cause that problem? The make_examples step finished successfully.; ```; **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=${REF_GENOME_FASTA} \; --reads=${input_read} \; --num_shards=${NUM_THREADS} \; --output_vcf=${basename}.vcf.gz; ```; - Error trace: (if applicable); ```parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 6; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.tri",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:283,Install,Installation,283,,https://github.com/google/deepvariant/issues/446,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:176,install,installed,176,,https://github.com/google/deepvariant/issues/462,2,"['Install', 'install']","['Installation', 'installed']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:828,Install,Installation,828,,https://github.com/google/deepvariant/issues/485,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476:251,Install,Install,251,,https://github.com/google/deepvariant/issues/476,3,"['Install', 'install']","['Install', 'Installation', 'installs']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I have tried to run in my personal computer the WES deepvariant case. However I get the following error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@16.gz --gvcf /output/inter_res/gvcf.tfrecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:612,install,installation,612,,https://github.com/google/deepvariant/issues/483,1,['install'],['installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:232,Install,Installation,232,,https://github.com/google/deepvariant/issues/480,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; Hello,. I would like to run deepvariant on a very depth datasets (up to 10K reads at the same locus). When I am looking to the VCF, I can see depth per allele at max 1.5K. Is it possible to modify the call_variant parameter to allow more reads per bp position ?; I am also wondering how I can see the full list of parameters for call_variant (the allowed flags to put in ""--call_variants_extra_args"" like min_fraction_snps, min_fraction_indels, ...). Thanks . **Setup**; - Operating system:CentOS; - DeepVariant version:1.2; - Installation method (Docker, built from source, etc.):docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)PacBio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/496:644,Install,Installation,644,,https://github.com/google/deepvariant/issues/496,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:440,Install,Installation,440,,https://github.com/google/deepvariant/issues/491,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/478:134,install,installed,134,,https://github.com/google/deepvariant/issues/478,3,"['Install', 'install', 'release']","['Installation', 'installed', 'release']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:367,Install,Installation,367,,https://github.com/google/deepvariant/issues/522,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity image built from docker image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**; - Command: ; ```; # Modified script; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=hs37d5_PhiX.fa \; --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \; --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \; --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \; --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \; --num_shards=15; ```; I have also tried postprocessing with `group_variants`, which also produces a similar error.; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/postprocess_variants \; --group_variants=false \; --ref=hs37d5_PhiX.fa \; --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \; --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz; ```; - Error t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:659,Install,Installation,659,,https://github.com/google/deepvariant/issues/517,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**; I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```; 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores; 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0); Fatal Python error: Aborted; ```; and the job eventually fails:. ```; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:164,install,installation,164,,https://github.com/google/deepvariant/issues/539,2,"['install', 'pipeline']","['installation', 'pipeline']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/525:232,Install,Installation,232,,https://github.com/google/deepvariant/issues/525,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; Deepvariant dies with protobuf error message when using Docker containers for version 1.2.0 and above. Works with 1.1.0 container. . **Setup**; - Operating system: Centos7; - DeepVariant version: 1.2.0, 1.3.0, latest; - Installation method (Docker, built from source, etc.): Docker container, executed with Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): short reads, Novaseq 6000. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --output_vcf=Indiv_I33975_Sample_I33975-L2.dedup.vcf.gz --output_gvcf=Indiv_I33975_Sample_I33975-L2.dedup.g.vcf.gz --regions=xgen-exome-research-panel-targets-v2.bed --num_shards=16; - ; - Error trace: (if applicable). Command output:; sys.exit(main(argv)); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main; proto_utils.uses_fast_cpp_protos_or_die(); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_egfjk32i/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die; raise ValueError('Expected to be using C++ protobuf implementation '; ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python; Traceback (most recent call last):; File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_24d7l2zv/runfiles/absl_py/absl/app.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:337,Install,Installation,337,,https://github.com/google/deepvariant/issues/499,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:; YES. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: ubuntu **16.04**; - DeepVariant version: **1.1.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/529:264,Install,Installation,264,,https://github.com/google/deepvariant/issues/529,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: **YES**. **Describe the issue:**. _Building release binaries of DV-1.4 on ubuntu16.04 failed.; I modified scripts to install packages that is need to build DV-1.4, which didn't work for me.; Updating operating system is not allowed for me, so I am eager to know whether there is any chance that I build DV-1.4 on ubuntu16.04 technically?_. **Setup**; - Operating system: **Ubuntu16.04**; - DeepVariant version: **1.4**; - Installation method (Docker, built from source, etc.): **built from source**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591:135,release,release,135,,https://github.com/google/deepvariant/issues/591,3,"['Install', 'install', 'release']","['Installation', 'install', 'release']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); WES data from the tutorial. **Steps to reproduce:**; - Command:; ; ```; #!/usr/bin/env bash. INPUT_DIR=""input""; OUTPUT_DIR=""output"". BIN_VERSION=1.4.0; export TMPDIR=""$PWD/tmp_dir"". singularity run \; --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \; --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \; --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=28; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas?. Thanks!; Dave",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546:399,Install,Installation,399,,https://github.com/google/deepvariant/issues/546,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**; - Operating system: CentOS 7 ; - DeepVariant version: 1.4.0 (google/deepvariant:latest); - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`; - Type of data: N/A. **Steps to reproduce:**; - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`; - Error trace:; ```; Traceback (most recent call last):; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportE",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:298,Install,Installation,298,,https://github.com/google/deepvariant/issues/610,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: yes. **Describe the issue:**; I am running DeepVariant on a custom genome assembly using a hybrid of pacbio hifi and illumina short reads and it's been running for 17days. I wonder if something is wrong and is there a way to speed thing up? I am already using 30 shards. **Setup**; - Operating system: centOS 7; - DeepVariant version: 1.4.0 and 1.1.0 (tried version 1.1.0 been running for 17 days then I'm trying the 1.4.0 and it's been running for 3 days now); - Installation method (Docker, built from source, etc.): converted docker image to singularity image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) I am using custom genome. **Steps to reproduce:**; - Command: ; ```; singularity exec ~/virtual_server/deepvariant.sif \; bash -c ""; /opt/deepvariant/bin/run_deepvariant \; --model_type=""HYBRID_PACBIO_ILLUMINA"" \; --ref=""${REF_DIR}""/scaffolds_FINAL.fasta \; --reads=""${INPUT_DIR}""/hybrid_hifi_Kapa_combined.bam \; --output_vcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.g.vcf.gz \; --num_shards=$SLURM_CPUS_PER_TASK \; --logging_dir=""${OUTPUT_DIR}""/logs \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp""; ```; - Error trace: (if applicable) attached r_deepvariant_hybrid_2.txt. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? I was able to run quick start. I was also able to run DeepVariant on the same singularity system with pacbio HiFi reads only, using human reference genome hg19. **Any additional context:**; [r_deepvariant_hybrid_2_662510.txt](https://github.com/google/deepvariant/files/9853335/r_deepvariant_hybrid_2_662510.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578:555,Install,Installation,555,,https://github.com/google/deepvariant/issues/578,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated.; (A clear and concise description of what the issue is.). **Setup**; - Operating system:Mac OS ; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true; - Error trace: No error.(if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561:366,Install,Installation,366,,https://github.com/google/deepvariant/issues/561,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**; - Operating system: HPC; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker --> Singularity; - Type of data: WGS data. **Steps to reproduce:**; - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`; - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41; /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz""; ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/599:346,Install,Installation,346,,https://github.com/google/deepvariant/issues/599,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:218,release,release,218,,https://github.com/google/deepvariant/issues/550,2,"['Install', 'release']","['Installation', 'release']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:552,Install,Installation,552,,https://github.com/google/deepvariant/issues/545,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; According to the running log: the setlocale failed when trying to change LC_ALL to 'en_US.UTF-8'. **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker pull ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) DNA seq. **Steps to reproduce:**; - Command: ; - singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale"" \; --env LANG=""en_US.UTF-8"" \; --env LC_ALL=""C"" \; --env LANGUAGE=""en_US.UTF-8"" \; --env LC_CTYPE=""UTF-8"" \; ...... - Error trace: (if applicable); ![image](https://user-images.githubusercontent.com/40780228/190950415-84faaa5d-7371-42a7-9e13-f6caf53a3dea.png). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566:287,Install,Installation,287,,https://github.com/google/deepvariant/issues/566,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; Running singularity on the test data I get the following:; ```. OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ...; ...; Try --helpfull to get a list of all flags.; deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; deepvariant.sing.sh: line 18: make_examples: command not found; deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```; I have checked and these paths and files exist and can be opened used the above links. . **Setup**; - Operating system: linux; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/558:1466,Install,Installation,1466,,https://github.com/google/deepvariant/issues/558,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; The same script runs successfully on Chr5 but not on the other 4 chromosomes. **Setup**; - Operating system: Debian GNU/Linux 9; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.):; - Type of data: hybrid of Illumina and HiFi data, the reference is the assembly based on the hifi reads. **Steps to reproduce:**; - Command: singularity run --bind ${PWD} \; /software/deepvariant/deepvariant.img \; /opt/deepvariant/bin/run_deepvariant \; --model_type HYBRID_PACBIO_ILLUMINA \; --ref ragtag.fasta \; --reads hifi_illu.bam \; --intermediate_results_dir ./tmp \; --output_vcf rep1.hifi-illu.Chr1.vcf.gz \; --num_shards 4 \; --regions Chr1_RagTag. - Error trace: Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1380, in _do_call; return fn(*args); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1363, in _run_fn; return self._call_tf_sessionrun(options, feed_dict, fetch_list,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1456, in _call_tf_sessionrun; return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,; tensorflow.python.framework.errors_impl.DataLossError: inflate() failed with error -3: invalid literal/length code; [[{{node IteratorGetNext}}]]",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/548:278,Install,Installation,278,,https://github.com/google/deepvariant/issues/548,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:276,install,installing,276,,https://github.com/google/deepvariant/issues/580,3,"['Install', 'install']","['Installation', 'install', 'installing']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants.; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - 150bp paired-end Illumina data. **Steps to reproduce:**; - Command: ; `/opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; --make_examples_extra_args=""normalize_reads=true"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \; --num_shards=${threads}`; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):; ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/612:529,Install,Installation,529,,https://github.com/google/deepvariant/issues/612,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; At the call_variants.py step, running into error that tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; (A clear and concise description of what the issue is.). **Setup**; - Operating system:CentOS7 ; - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.):singularity run with SIF image pulled from docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:417,Install,Installation,417,,https://github.com/google/deepvariant/issues/564,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:646,Install,Installation,646,,https://github.com/google/deepvariant/issues/640,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity image build from dockerhub; - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --version; - Error trace: (if applicable); ```; tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.4.0; ```; The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** ; The GPU is NVIDIA GeForce 3090; The GPU Driver Version: 520.61.05; The CUDA version in the host is V11.8.89 as followings:; ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png); It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. ; ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619:217,release,release,217,,https://github.com/google/deepvariant/issues/619,3,"['Install', 'install', 'release']","['Installation', 'installed', 'release']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Downstream association analysis has yielded a high number of false positive findings, essentially a product of low quality data. It is crucial that these sites are filtered out. . **Setup**; - Operating system: Linux; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Illumina WES. **Steps to reproduce:**; - Command: . DeepVariant:; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \. GLnexus:; glnexus_cli --config DeepVariantWES --bed ${regions} \; 2_gvcf/*.g.vcf.gz > 3_bcf/Exomes.bcf. **Any additional context:**. Hi there!; Apologies for bringing up another similar issue, but I would like some help with the correct filtering of my merged vcf file.; Essentially, I have identified a significant number of false positive sites in a downstream assoc. analysis, where MAF for these variants is widely different than the population average. This strongly suggests that these sites are of low quality and need to be filtered out. Here are some examples from the merged vcf file. For each variant I have only shown a handful of samples (total is over 5000):. False positive / bad site that needs filtering:. `1	1722625	1_1722625_A_T	A	T	48	.	AF=0.222894;AQ=48	GT:DP:AD:GQ:PL:RNC	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:7:0,7:42:44,47,0:..	0/0:0:0,0:1:0,0,0:..	1/1:6:0,6:36:38,38,0:..	0/1:12:3,9:0:19,2,0:..	./.:3:3,0:0:20,0,50:II	1/1:2:0,2:23:29,25,0:..	0/0:2:2,0:6:0,6,59:..	1/1:2:0,2:22:31,24,0:..	1/1:2:0,2:26:28,29,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	1/1:7:0,7:40:43,42,0:..	./.:3:3,0:0:20,0,50:II	1/1:4:1,3:1:28,3,0:..	0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:372,Install,Installation,372,,https://github.com/google/deepvariant/issues/645,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Having problem running deeptrio examples [https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md) . **Setup**; - Operating system: Ubuntu 22.04, Docker 23+; - DeepVariant version: deeptrio-1.5.0-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). It always give ```Error: The directory ""/reference/GRCh38_no_alt_analysis_set.sdf"" already exists. Please remove it first or choose a different directory.``` even after I ensure that there are no GRCh38_no_alt_analysis_set.sdf exist in said directory; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And that being said, this command also raises another error showing ```Error: An IO problem occurred: ""Not in GZIP format""```; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ``` . **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? **Quick start on single variant analysis is optimal**. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632:429,Install,Installation,429,,https://github.com/google/deepvariant/issues/632,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Hello,. Using WES model, deepvariant calls the following variant in the vcf file:; ```; NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44; ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. ; What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample?. Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/618:1024,Install,Installation,1024,,https://github.com/google/deepvariant/issues/618,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; I do not understand how Deepvariant decides if there is a call at sites with low/zero GQ/DP... below is an example of a merged VCF with GLNexus and the same sites in the individual gVCFs. . **Setup**; - Operating system: Linux; - DeepVariant version: 1.5; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES Data (Illumina). **Steps to reproduce:**; - Command:; - ; DeepVariant:. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${cram_in} \; --regions ${regions} \; --output_gvcf ${sample}.g.vcf.gz \; --output_vcf ${sample}.vcf.gz \; --num_shards 8 \; ```. GLNexus:. ```; glnexus_cli --config DeepVariantWES --bed /work_beegfs/***/exomes/hg38_exomregions_withoutCHR.bed /work_beegfs/***/exomes/2_gvcf/*.g.vcf.gz > /work_beegfs/***/exomes/3_bcf/Exomes.bcf; bcftools view ../3_bcf/Exomes.bcf | bgzip -@ 4 -c > Exomes.vcf.gz; bcftools index Exomes.vcf.gz; ```. Merged VCF with GLNexus:. `1	69897	1_69897_T_C	T	C	48	.	AF=0.076465;AQ=48	GT:DP:AD:GQ:PL:RNC	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,3,29:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	1/1:3:0,3:15:31,19,0:..	0/0:0:0,0:1:0,3,29:..	./.:6:2,4:8:0,8,13:II	./.:1:1,0:0:29,3,0:II	1/1:2:0,2:7:13,11,0:..	./.:1:1,0:0:29,3,0:II`. Same site/samples gVCF (each line is one sample in same order as merged file above):; ```. 1	69792	.	T	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69638	.	C	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0; 1	69850	.	C	<*>	0	.	END=69929	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69897	.	T	<*>	0	.	END=69897	GT:GQ:MIN_DP:PL	./.:0:1:29,3,0; 1	69683	.	T	<*>	0	.	END=69911	GT:GQ:MIN_DP:PL	0/0:1:0:0,3,29; 1	69037	.	G	<*>	0	.	END=70036	GT:GQ:MIN_DP:PL	0/0:1:0:0,0,0; 1	69897	.	T	C,<*>	31.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:19:3:0,3,0:1,0:31,19,0,990,990,990; 1	69848	.	G	<*>	0	.	END=69920	GT:GQ:M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/639:379,Install,Installation,379,,https://github.com/google/deepvariant/issues/639,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651:232,Install,Installation,232,,https://github.com/google/deepvariant/issues/651,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:249,Install,Installation,249,,https://github.com/google/deepvariant/issues/717,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**; - Operating system: linux（Centos）; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**; - Error trace: (if applicable); <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:203,install,install,203,,https://github.com/google/deepvariant/issues/668,2,"['Install', 'install']","['Installation', 'install']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Hi developers,; I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:839,Install,Installation,839,,https://github.com/google/deepvariant/issues/653,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**; - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. ; 2. Run deepvariant with default parameters to call the file.; 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y; 4. ; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/695:611,Install,Installation,611,,https://github.com/google/deepvariant/issues/695,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yep. **Describe the issue:**; The issue is actually with GLnexus, however noone is replying there (people with same issues), and since it is recommended here in the deepvariant pipeline, I thought I would ask here. . I have produced a large set of gVCF files using deepvariant and merged these with GLnexus as recommended. However, there seems to be an issue in the final merged VCF file. Many genotypes are called as 0/0 when they have very low or zero DP:; e.g. ; `; 1	1319056	1_1319056_A_G	A	G	51	.	AF=0.32848;AQ=51	GT:DP:AD:GQ:PL:RNC	0/0:0:0,0:1:0,0,0:..	1/1:2:0,2:3:29,6,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/1:2:0,2:1:12,2,0:..	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:269,pipeline,pipeline,269,,https://github.com/google/deepvariant/issues/633,1,['pipeline'],['pipeline']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. **Describe the issue:**; Run into Fatal python Bus error repeatedly. **Setup**; - Operating system: CentOS 7 ; - DeepVariant version: 1.4 (DeepTrio); - Installation method (Docker, built from source, etc.): singularity ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Illumina NovaSeq data, reference genome hg19. ; **Steps to reproduce:**; - Command:; - `/opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`; - Error trace: (if applicable); `Fatal Python error: Bus error. Current thread 0x00002af3a27fa740 (most recent call first):; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 58 in <dictcomp>; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 57 in get_candidates; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:249,Install,Installation,249,,https://github.com/google/deepvariant/issues/646,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:459,Install,Installation,459,,https://github.com/google/deepvariant/issues/678,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes.; **Describe the issue:**; (A clear and concise description of what the issue is.); I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Operating system:; Redhat enterprise v7.9, x86_64; **Steps to reproduce:**; - Command:; BIN_VERSION=""1.5.0""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/HG38.fa \; --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=4 \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \; --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2""; I allocated 4 cores and 70GBs to run that program.; I added the VAF thresholds for SNPs and Indels because I read the reported issues:; https://github.com/google/deepvariant/issues/578; - Error trace: (if applicable); And here are some most recent results I got from stdout:; I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]; I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]; I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]; I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:456,Install,Installation,456,,https://github.com/google/deepvariant/issues/683,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: YES. **Describe the issue:** ; 1. When the input sequence (fastq) matches the reference sequence, the program will keep running.; 2. Sequence obtain from data generation tools. (dwgsim) . **Setup**; - Operating system: Red Hat Enterprise Linux release 8.6 (Ootpa); - DeepVariant version: deepvariant1.6.0.sif; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); GRCh38| (Sequence obtain from data generation tools. (dwgsim) ，length,8-9kB, 150bp, PE, sequence may the same as reference sequence) . **Steps to reproduce:**; - Command: ; time singularity run ~/singularity/deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref ${ref} \; --reads ${bamSavePath}/${name}.sorted.bam \; --output_vcf ${vcf} \; --output_gvcf ${outputPath}/vcf/${name}/${name}.g.vcf.gz \; --num_shards $(nproc) \; --regions ${BED} \; --sample_name ${name} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" . - Error trace: (if applicable); I0729 14:44:37.339473 140223721211712 make_examples_core.py:301] Task 0/4: Preparing inputs; I0729 14:44:37.339473 140478861559616 make_examples_core.py:301] Task 3/4: Preparing inputs; I0729 14:44:37.350302 140710547908416 make_examples_core.py:301] Task 1/4: Preparing inputs; I0729 14:44:37.339477 139779121772352 make_examples_core.py:301] Task 2/4: Preparing inputs; I0729 14:44:37.476220 140223721211712 genomics_reader.py:222] Reading result/simulate_A/bam/chr6_CYP21A2_CYP21A1P__A_Q_8845_R_8585/chr6_CYP21A2_CYP21A1P__A_Q_8845_R_8585.sorted.bam with NativeSamReader; I0729 14:44:37.485832 140223721211712 make_examples_core.py:301] Task 0/4: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:337,release,release,337,,https://github.com/google/deepvariant/issues/855,2,"['Install', 'release']","['Installation', 'release']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**; - Operating system: Cent; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**; - Command:; ```bash; #!/bin/bash; sample=$1; threads=$2. chr=$3; indir=""01.mapping""; outdir=""02.snps""; sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}""; rm -rf ${outdir}/${sample}_chr${chr}; ```; - Error trace: (if applicable); ```bash; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]; I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]; I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]; I0325 17:32:27.143459 47041007318848 mak",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:308,Install,Installation,308,,https://github.com/google/deepvariant/issues/794,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:628,Install,Installation,628,,https://github.com/google/deepvariant/issues/856,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; After running deepvariant in a docker container twice, the output dir in which I expect the output.g.vcf.gz and output.vcf.gz files, is empty. The /tmp/ folder doesn't contain any intermediate files neither. **Setup**; - Operating system: Ubuntu 22.04 LTS; - DeepVariant version:; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS HiFi PacBio. **Steps to reproduce:**; - Command: sudo docker run -v /media/USER/Expansion/DATA/hifi_reads:/input -v /home/st/Applications/deepvariant:/reference -v $(pwd)/output:/output google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa --reads=/input/DATA_s1.hifi_reads_sorted.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=$(nproc); - Error trace: No errors. **Any additional context:** Previously, I used pbmm2 to align and sort my raw BAM file.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810:402,Install,Installation,402,,https://github.com/google/deepvariant/issues/810,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**; - Operating system: Ubuntu 22.04.2 LTS ; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**; - Command: sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/RILWLs1.fasta \; --reads=/input/Out.fastq \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Yes. Test data works fine. ; ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807:269,Install,Installation,269,,https://github.com/google/deepvariant/issues/807,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am having some errors while fetching variants from chrX, Could you please have a look I added a error.txt file for reference. ; Thank you. > INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chrX"" start: 14000 end: 15000. **Setup**; - Operating system:linux; - DeepVariant version:latest; - Installation method (Docker, built from source, etc.):udocker; - Type of data: test file for checking Calling variants in non-autosomal contigs [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-xy-calling-case-study.md). **Steps to reproduce:**; - Command:; BIN_VERSION=""1.6.1"". ```; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". udocker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"" ; ```. - Error trace: ; ; [error.txt](https://github.com/user-attachments/files/16281125/error.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853:422,Install,Installation,422,,https://github.com/google/deepvariant/issues/853,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:404,Install,Installation,404,,https://github.com/google/deepvariant/issues/879,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; Python encountered a MemoryError, no matter how much we increase the memory allocation the error exists. **Setup**; - Operating system:; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): apptainer pull docker://google/deepvariant:""1.6.1""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) bam files, ONT. **Steps to reproduce:**; - Command: ; apptainer exec --bind /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint /work/XXXX/ls6/deepvariant/deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant --model_type ONT_R104 --ref /work/XXXX/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/XXXX/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --output_vcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.vcf.gz --output_gvcf /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/FL9-1_chr10.output.g.vcf.gz --num_shards 64 --logging_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/ --intermediate_results_dir /scratch/XXXX/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results. - Error trace: (if applicable); ; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2p_bcqtz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:288,Install,Installation,288,,https://github.com/google/deepvariant/issues/854,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; The postprocess_variants step fails with following error message:; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Setup**; - Operating system: CentOS Linux 7 (Core); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio Sequencing. **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:382,Install,Installation,382,,https://github.com/google/deepvariant/issues/818,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): singularity image built form Docker Hub; - Type of data: bacteria whole genome. **Steps to reproduce:**; - Command:; smakemake pipeline; rule run_deepvariant:; output:; vcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.vcf.gz"",; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; input:; reference_fasta = ""/project/databases/bacteroides_genome/reference_genomic.fna"",; reads = rules.sam2bam.output.sorted_bam; params:; inter_dir = ""../../results/deepVariant/{dataset}/{sample}/intermediate"",; log_dir = ""../../results/deepVariant/{dataset}/{sample}/log"",; work_dir = ""/project/"",; deepvariant = ""/project/software/deepVariant.sif""; shell:; """"""; module load singularity/3.7.0; singularity exec -B {params.work_dir} {params.deepvariant} /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref={input.reference_fasta} \; --reads={input.reads} \; --output_vcf={output.vcf} \; --output_gvcf={output.vcf} \; --make_examples_extra_args --channels=insert_size \; --intermediate_results_dir {params.inter_dir} \; --num_shards=6 \; --logging_dir={params.log_dir}; """"""; - Error trace: ; ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --outfile_base ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant"". I0626 19:01:30.369722 139699125458752 genomics_reader.py:222] Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_report.py"", line 103, in <module>; tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:205,Install,Installation,205,,https://github.com/google/deepvariant/issues/839,2,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); `run_deepvariant` is erroring out in the `postprocess_variants` step. **Setup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:378,Install,Installation,378,,https://github.com/google/deepvariant/issues/901,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I can't pull the container using singularity. **Setup**; - Operating system: Ubuntu; - Installation method (Docker, built from source, etc.): tried with singularity; My system has singularity installed, I tried getting the container but it failed:. I used this code; ```; BIN_VERSION=""1.6.1""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; ```. and It got several warning messages:. WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; /usr/bin/env: ‘python’: No such file or directory; Cleaning up...; ERROR: pulling container failed!. I was thinking of installing DeepVariant with conda, but the version in conda is 1.5.0, and I was told to always run the most up-to-date one.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/886:211,Install,Installation,211,,https://github.com/google/deepvariant/issues/886,3,"['Install', 'install']","['Installation', 'installed', 'installing']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I have processed around 30 samples albeit having some issues with GPU, possibly due to nvidia driver / cuda version. However, recently postprocess has started stalling with the same error. Any help troubleshooting this would be greatly appreciated!. **Setup**; - Operating system: ; NAME=Red Hat Enterprise Linux; VERSION=9.4 (Plow); - DeepVariant version: deepvariant:1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker (via podman); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:506,Install,Installation,506,,https://github.com/google/deepvariant/issues/849,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I mapped the WGS data to pangenome graph with `vg giraffe`. When calling variant with deepvariant, it takes a very long time on `make_exmaples` and find a huge number of examples (~50X more than my previous data aligned to GRCh38). And finally, deepvariant fails. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: PCR-free WGS data from 1000 Genomes, CHM13-based pangenome graph. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B $inpath \; -B $outpath \; -B ${TMPDIR}:${TMPDIR} \; -B $(dirname $ref) \; ~/container/deepvariant.1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$ref \; --reads=${inpath}/${sample}.cram \; --output_vcf=${outpath}/${sample}.vcf.gz \; --output_gvcf=${outpath}/${sample}.g.vcf.gz \; --num_shards=${threads} \; --intermediate_results_dir ${TMPDIR} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"". ```; - Error trace: (if applicable); ```; I0706 12:06:34.396180 140036894541632 make_examples_core.py:301] Task 16/40: 20600183 candidates (20662757 examples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:461,Install,Installation,461,,https://github.com/google/deepvariant/issues/847,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; google/deepvariant:1.5.0-gpu google/deepvariant:1.6.1-gpu docker images run as CPU-only because they are using ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:503,Install,Installation,503,,https://github.com/google/deepvariant/issues/844,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; Although some variants are clearly heterozygous in IGV, deepvariant GT shows a homozygous genotype. **Setup**; - Operating system: linux; - DeepVariant version: 1.6.1; - Installation method : Docker; - Type of data: illumina, WES, hg38. **Steps to reproduce:**; ```; docker run --rm -i \; -v ${ref_dir}:/opt/ref \; -v ${kit_dir}:/opt/kit \; -v ${input_dir}:/opt/sample \; ${deepvariant_docker} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/opt/ref/${ref_fasta}"" \; --reads=""/opt/sample/${input_bam_file}"" \; --regions=""/opt/kit/${kit_bed_file}"" \; --output_vcf=""/opt/sample/${input_bam_file/.bam/.dv.vcf}"" \; --num_shards=""${threads}""; ```. Here are 5 selected variants called by 5 different versions of deepvariant:; ```; v0.10.0 chr12 11353713 . T C 57.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:54:43:27,15:0.348837:57,0,55; v1.1.0 chr12 11353713 . T C 36.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:34:44:28,15:0.340909:36,0,38; v1.4.0 chr12 11353713 . T C 23.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:28,15:0.340909:23,0,25; v1.5.0 chr12 11353713 . T C 24.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:17:44:28,15:0.340909:24,0,17; v1.6.1 chr12 11353713 . T C 24.6 PASS . GT:GQ:DP:AD:VAF:PL 1/1:5:44:28,15:0.340909:22,3,0; -----------------------------; v0.10.0 chr3 195779035 . G A 3.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:33:24,8:0.242424:1,0,38; v1.1.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:41:33:24,8:0.242424:0,43,45; v1.4.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:23:33:24,8:0.242424:0,32,22; v1.5.0 chr3 195779035 . G A 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:16:33:24,8:0.242424:0,26,16; v1.6.1 chr3 195779035 . G A 13.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:14:33:24,8:0.242424:13,28,0; -----------------------------; v0.10.0 chr6 159711482 . C T 46.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:36:70:38,32:0.457143:46,0,36; v1.1.0 chr6 159711482 . C T 6.9 PASS . GT:GQ:DP:A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/824:294,Install,Installation,294,,https://github.com/google/deepvariant/issues/824,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:239,integrat,integrate,239,,https://github.com/google/deepvariant/issues/865,4,"['Install', 'install', 'integrat', 'update']","['Installation', 'install', 'integrate', 'update']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:YES. **Describe the issue:**; When Running deep variant wes mode, there arised an assetion error when loading the weights of the model. **Setup**; - Operating system:Linux ; - DeepVariant version:1.6.1; - Installation method (Docker, built from source, etc.):Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; ```; DV=""singularity run /autofs/bal34/xyu/softwares/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant ""; ${DV} \; --model_type=WES \; --customized_model=/autofs/bal34/xyu/run_software/dv_illu/model/model.ckpt \; --ref ${REF_FILE_PATH} \; --reads {1} \; --output_vcf ${BASE_DIR}/{2}/output.vcf.gz \; --num_shards 30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir ${BASE_DIR}/{2}/intermediate_results_dir; ```; - Error trace: (if applicable); ```; WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; W0731 11:52:32.961261 140355267913536 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:297,Install,Installation,297,,https://github.com/google/deepvariant/issues/857,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** ; (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:612,Install,Installation,612,,https://github.com/google/deepvariant/issues/733,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:232,Install,Installation,232,,https://github.com/google/deepvariant/issues/743,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; I am struggling to get DeepTrio to run to completion on a small dataset. It completes at the end of call_variants.py but my system just collapses when at postprocess_variants.; Through using --dry_run=true, I'm able to keep going only after being sufficiently confident the last step has completed without error.; So in short, is it possible to re-run the wrapper command and have the analysis pipeline pick up where it left off? . **Setup**; - Operating system: Rocky Linux 8; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): through Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) illumina, 151bp, same reference as case studies; - RAM 64 GB; - CPUs 32 (c6i.8xlarge). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes they do. they complete because they are small. ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Unfortunately, i cant run it on g4dn.8xlarge available to me since that EC2 running Amazon Linux 2, and GPU DeepVariant seems to need Ubuntu.; In short, a ""step_x_completed"" sentinel file at end of each step would be great IMO. . Thanks,; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749:511,pipeline,pipeline,511,,https://github.com/google/deepvariant/issues/749,2,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; The step of postprocess_variants cannot find the VCF file. - Operating system:Centos; - DeepVariant version:1.6.0; - Installation method :singularity; - Type of data: (NGS sequence). **Steps to reproduce:**; - Command:; mkdir /public1/home/yinhang/data/tmp/${SAMPLE}/; singularity exec /public/software/apps/deepvariant-1.6.0-cpu_singularity/deepvariant-1.6.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${ref_genome} \; --reads=""${SORTED_BAM_DIR}/${SAMPLE}/${SAMPLE}.sorted.bam"" \; --output_vcf=""${VCF_DIR}/${SAMPLE}/${SAMPLE}.vcf.gz"" \; --output_gvcf=""${VCF_DIR}/${SAMPLE}/${SAMPLE}.g.vcf.gz"" \; --intermediate_results_dir=/public1/home/yinhang/data/tmp/${SAMPLE}/ \; --num_shards=60 \; --sample_name ${SAMPLE}. - Error trace:. I0217 17:31:41.680527 139931052529472 call_variants.py:623] Complete: call_variants. real	339m11.258s; user	13508m55.048s; sys	183m10.091s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public1/home/yinhang/projects/two_genomes/04_T2T/04_ragtag/03_syri/01_data/SH_N0.fasta"" --infile ""/public1/home/yinhang/data/tmp/SRR1572254/call_variants_output.tfrecord.gz"" --outfile ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz"" --cpus ""60"" --gvcf_outfile ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public1/home/yinhang/data/tmp/SRR1572254/gvcf.tfrecord@60.gz"" --sample_name ""SRR1572254"". /bin/bash: warning: setlocale: LC_ALL: cannot change locale (zh_CN.UTF-8); /bin/bash: warning: setlocale: LC_ALL: cannot change locale (zh_CN.UTF-8); I0217 17:31:47.293663 139808123168576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: SRR1572254; I0217 17:31:47.294324 139808123168576 postprocess_variants.py:1216] --sample_name ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/773:234,Install,Installation,234,,https://github.com/google/deepvariant/issues/773,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; When variant is not detected, the program will freeze in the last step；. **Setup**; - Operating system:Centos7.6; - DeepVariant version: 1.6 ; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO-SMART；A reference sequence for a normal person；. **Steps to reproduce:**; - Command: /bin/singularity run -B /work/:/work/ /work/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=human_geneA_reference.fa --reads=reference.align.bam --output_vcf=out.vcf --output_gvcf=out.gvcf --num_shards=32; - Error trace: Last line： I0119 11:43:53.450599 47012502976320 call_variants.py:623] Complete: call_variants（Stuck at this step）. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. [deepvariant_1.6.pdf](https://github.com/google/deepvariant/files/13986125/deepvariant_1.6.pdf)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/764:262,Install,Installation,262,,https://github.com/google/deepvariant/issues/764,1,['Install'],['Installation']
Deployability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; Yes. **Describe the issue:**; DeepTrio v1.6 crashes reproducibly with a segmentation fault. **Setup**; - Operating system:; Linux 3.10.0-1160.81.1.el7.x86_64; - DeepVariant version:; 1.6; - Installation method (Docker, built from source, etc.):; Docker image converted to apptainer image which can be downloaded [here](https://downloads.molgeniscloud.org/downloads/vip/images/deepvariant_deeptrio-1.6.0.sif); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Nanopore data derived from [GIAB](https://github.com/genome-in-a-bottle/giab_data_indexes) HG002 mapped to GRCh38. The data subsampled resulting in a 80MB .bam file. **Steps to reproduce:**; - Command:; ```; local args=(); args+=(""--model_type"" ""ONT""); args+=(""--ref"" ""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""); args+=(""--reads_child"" ""i_am_my_father_HG002_validated.bam""); args+=(""--reads_parent1"" ""i_am_my_father_HG002_copy_validated.bam""); args+=(""--sample_name_child"" ""HG002""); args+=(""--sample_name_parent1"" ""HG002_copy""); args+=(""--output_gvcf_child"" ""i_am_my_father_HG002_chunk_8_snv.g.vcf.gz""); args+=(""--output_gvcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.g.vcf.gz""); args+=(""--num_shards"" ""6""); args+=(""--regions"" ""regions_chunk_8.bed""); args+=(""--intermediate_results_dir"" ""intermediate_results""); args+=(""--output_vcf_child"" ""i_am_my_father_HG002_chunk_8_snv.vcf.gz""); args+=(""--output_vcf_parent1"" ""i_am_my_father_HG002_copy_chunk_8_snv.vcf.gz""). ${CMD_DEEPVARIANT_DEEPTRIO} ""${args[@]}""; ```; content of .bed file:; ```; $ cat regions_chunk_8.bed; chr9 0 138394717; ```. stats of .bam file:; ```; chr1 248956422 1319 0; chr2 242193529 929 0; chr3 198295559 749 0; chr4 190214555 1042 0; chr5 181538259 649 0; chr6 170805979 667 0; chr7 159345973 613 0; chr8 145138636 622 0; chr9 138394717 586 0; chr10 133797422 622 0; chr11 135086622 538 0; chr12 133275309 4",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724:282,Install,Installation,282,,https://github.com/google/deepvariant/issues/724,1,['Install'],['Installation']
Deployability,"**ISSUE**; First of all, I found DeepVariant to be a very good and innovative tool. I'm considering including it in my exome analysis pipeline. I followed the tutorial (DeepVariant worked correctly with the Complete Genomics model), and I created my own model using Genome in a Bottle samples. To do this, I sequenced the same reference sample three times to use each BAM file for training, validation, and testing. I didn't encounter any errors during the model creation process, but when I tried to test it, the process got stuck at the call_variants step. **Setup**; - Operating system: Ubuntu 22.04.4 LTS; - DeepVariant version:1.6.1; - Installation method:docker; - Type of data: MGI DNBSEQ 400, exome sequencing. **Steps to reproduce:**; - Command:; _Create examples for trainning set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/26_r_groups.bam"" --examples ""/output/training_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:134,pipeline,pipeline,134,,https://github.com/google/deepvariant/issues/869,2,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/359:1029,Install,Installation,1029,,https://github.com/google/deepvariant/issues/359,1,['Install'],['Installation']
Deployability,"**_Describe the issue:_**; I was wondering if the authors have recommended method for generating a set of high-confidence de novo germline mutations from trio-deepvariant vcf files. I have used DeepVariant 0.9.0 version for calling the germline mutations and I have merged HG002, HG003 and HG004 .g.vcf.gz files using GLnexus. If there isn't a recommendation calling de novo germline mutations from trio-deepvariant vcf files, I am hoping to implement the method described in DeNovoGear: de novo indel and point mutation discovery and phasing. _**Miscellaneous**_; - I was hoping to use GATK GenotypeGVCFs for merging the HG002, HG003 and HG004 .g.vcf.gz files, but deepvariant .g.vcf.gz file is not compatible with GATK GenotypeGVCFs. ; - I was hoping to use the HG002-HG003-HG004 trio vcf file as input towards strelka2 de novo mutation caller, but the vcf file was again not compatible. **_Setup_**. Operating system: Ubuntu; DeepVariant version: 0.9.0; Installation method (Docker, built from source, etc.): Docker; Type of data: sample: HG002, HG003 and HG004 PacBio CCS fastq. **_Steps to reproduce:_**. **_Any additional context:_**. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/377:957,Install,Installation,957,,https://github.com/google/deepvariant/issues/377,1,['Install'],['Installation']
Deployability,", I can have python=2.7 and deepvariant=0.7.0; but cannot update to 1.15 or latest. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (ssh to university, docker unavailable); - DeepVariant version: 0.7.0 installable, but cannot get 1.15 or latest; - Installation method (Docker, built from source, etc.): conda; - Type of data: N/A. **Steps to reproduce:**; - Command:; $ create -n deepvariant python=3.8 (current version 3.8.19); $ conda install deepvariant. - Error trace: (if applicable). > #warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > warning libmamba Added empty dependency for problem type SOLVER_RULE_UPDATE; > warning libmamba Problem type not implemented SOLVER_RULE_STRICT_REPO_PRIORITY _- many times_; > failed; > ; > LibMambaUnsatisfiableError: Encountered problems while solving:; > - package deepvariant-0.4.1-np113py27_0 requires python 2.7*, but none of the providers can be installed; > ; > Could not solve for environment specs; > The following packages are incompatible; > ├─ deepvariant is installable with the potential options; > │ ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; > │ │ └─ tensorflow 1.12.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.10.0|1.0.0] would require; > │ │ └─ tensorflow 2.0.* , which conflicts with any installable versions previously reported;; > │ ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; > │ │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; > │ ├─ deepvariant [0.7.1|0.7.2] would require; > │ │ └─ tensorflow 1.11.* , which conflicts with any installable versions previously reported;; > │ └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; > │ └─ tensorflow-estimator 2.0.* , which conflicts with any installable versions previously reported;; > └─ pin-1 is not installable because it requires; > └─ python 3.8.* , which conflicts with any installable versions previously reported. Thanks guys.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835:1118,install,installed,1118,,https://github.com/google/deepvariant/issues/835,9,['install'],"['installable', 'installed']"
Deployability,", done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C com",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:3068,INSTALL,INSTALL,3068,,https://github.com/google/deepvariant/issues/739,1,['INSTALL'],['INSTALL']
Deployability,",0:II	0/1:2:0,2:1:12,2,0:..	0/0:0:0,0:1:0,3,29:..	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,0,0:..	1/1:5:0,5:9:40,12,0:..	1/1:3:0,3:7:36,10,0:..	0/0:0:0,0:1:0,3,29:.`. This is messing with downstream analysis, and overall just looks like poor QC. Additionally, the annotation/filter field is missing. In the gVCFs there was still a ""PASS"" label. This is also required for downstream analysis. ; So I am wondering where I went wrong, or whether there is a more suitable software to merge gVCFs. Thank you!. **Setup**; - Operating system: linux/cluster; - DeepVariant version: latest (1.5); - Installation method: Docker; - Type of data: ; Illumina WES data (.cram to .gvcf). **Steps to reproduce:**; - Command:; ```; glnexus_cli --config DeepVariant --bed ${regions} \; folder/*.g.vcf.gz > output.bcf; ```. - Error trace: no errors. This is the vcf header:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GLnexusVersion=v1.4.1-0-g68e25e5; ##GLnexusConfigName=DeepVariant; ##GLnexusConfigCRC32C=2932316105; ##GLnexusConfig={unifier_config: {drop_filtered: false, min_allele_copy_number: 1, min_AQ1: 10, min_AQ2: 10, min_GQ: 0, max_alleles_per_site: 32, monoallelic_sites_for_lost_alleles: true, preference: common}, genotyper_config: {revise_genotypes: true, min_assumed_allele_frequency: 9.99999975e-05, snv_prior_calibration: 0.600000024, indel_prior_calibration: 0.449999988, required_dp: 0, allow_partial_data: true, allele_dp_format: AD, ref_dp_format: MIN_DP, output_residuals: false, more_PL: true, squeeze: false, trim_uncalled_alleles: true, top_two_half_calls: false, output_form",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:1746,Install,Installation,1746,,https://github.com/google/deepvariant/issues/633,1,['Install'],['Installation']
Deployability,"----------------------------------; ; _mt19937.pyx:138:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_mt19937.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-jpvzz1fb/numpy_99473a9ff1d94f3fae4c587acb96b3c1/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/opt/miniconda3/lib/python3.9/subprocess.py"", line 373, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/opt/miniconda3/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_mt19937.c', '_mt19937.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/root/.local/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_build_wheel; return hook(metadata_directory, config_settings); Fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727:2028,install,install-,2028,,https://github.com/google/deepvariant/issues/727,1,['install'],['install-']
Deployability,"----; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 235, in <module>; main(); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 231, in main; find_process_files(root_dir); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 222, in find_process_files; process(root_dir, fromfile, tofile, function, hash_db); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 188, in process; processor_function(fromfile, tofile); File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e9fd7d7ef0c2/tools/cythonize.py"", line 77, in process_pyx; subprocess.check_call(; File ""/public/home/zhanghl3/miniconda3/envs/deepvariant/lib/python3.10/subprocess.py"", line 369, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['/public/home/zhanghl3/miniconda3/envs/deepvariant/bin/python3', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.; Cythonizing sources; Traceback (most recent call last):; File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>; main(); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main; json_out['return_val'] = hook(**hook_input['kwargs']); File ""/usr/local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 149, in prepare_metadata_for_b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:9398,install,install-,9398,,https://github.com/google/deepvariant/issues/859,1,['install'],['install-']
Deployability,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:103,Install,Install,103,,https://github.com/google/deepvariant/issues/231,4,"['Install', 'Update']","['Install', 'Update']"
Deployability,".0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build_release_binaries.sh; [sudo] password for solokopi: ; build_release_binaries.sh: line 39: bazel: command not found; build_release_binaries.sh: line 43: baze",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18351,upgrade,upgrade,18351,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,".3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build_release_binaries.sh; [sudo] password for solokopi: ; build_release_binaries.sh: line 39: bazel: command not found; build_release_binaries.sh: line 43: bazel: command not found; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18540,Install,Install,18540,,https://github.com/google/deepvariant/issues/89,6,"['Install', 'install']","['Install', 'Installing', 'installed']"
Deployability,".4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [24",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:2610,update,update,2610,,https://github.com/google/deepvariant/issues/489,2,"['install', 'update']","['install', 'update']"
Deployability,".; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [78 lines of output]; Running ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1307,install,installed,1307,,https://github.com/google/deepvariant/issues/859,1,['install'],['installed']
Deployability,"/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/runtime_by_region_vis.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/multisample_make_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_somatic.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/train.zip .; COPY --from=builder /opt/deepvariant/scripts/run_deepvariant.py .; COPY --from=builder /opt/deepvariant/scripts/run_deepsomatic.py . RUN ./run-prereq.sh. RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \; 	update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0. # Create shell wrappers for python zip files for easier use.; RUN \; BASH_HEADER='#!/bin/bash' && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/make_examples.zip ""$@""' > \; 	/opt/deepvariant/bin/make_examples && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/call_variants.zip ""$@""' > \; 	/opt/deepvariant/bin/call_variants && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/call_variants_slim.zip ""$@""' > \; 	/opt/deepvariant/bin/call_variants_slim && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/postprocess_variants.zip ""$@""' > \; 	/opt/deepvariant/bin/postprocess_variants && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /opt/deepvariant/bin/vcf_stats_report.zip ""$@""' > \; 	/opt/deepvariant/bin/vcf_stats_report && \; printf ""%s\n%s\n"" \; 	""${BASH_HEADER}"" \; 	'python3 /",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:4930,update,update-alternatives,4930,,https://github.com/google/deepvariant/issues/871,4,"['install', 'update']","['install', 'update-alternatives']"
Deployability,"0 run_deepvariant.py:519] Re-using the directory for intermediate results in /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9. ***** Intermediate results will be written to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9 in docker. ****; ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:4196,install,installed,4196,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:17844,upgrade,upgrade,17844,,https://github.com/google/deepvariant/issues/89,1,['upgrade'],['upgrade']
Deployability,"0.10_10p1mb.bam \; --ref gs://deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta.gz \; --regions ""chr20:10,000,000-10,010,000""; ```. My `runner.sh` (changed `PROJECT_ID`,`OUTPUT_BUCKET`, and `STAGING_FOLDER_NAME`) is; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - cod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:1806,pipeline,pipelines,1806,,https://github.com/google/deepvariant/issues/60,2,['pipeline'],"['pipeline-file', 'pipelines']"
Deployability,"01 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>> && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel; 52 |; --------------------; ERROR: failed to solve: process ""/bin/sh -c ./build-prereq.sh && PATH=\""${HOME}/bin:${PATH}\"" ./build_release_binaries.sh # PATH for bazel"" did not complete successfully: exit code: 100; ```. Looks like the repositories are either old or the sign has expired. How to fix this error?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:2344,update,updates,2344,,https://github.com/google/deepvariant/issues/902,1,['update'],['updates']
Deployability,1.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#;,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4413,update,updates,4413,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8855,release,release,8855,,https://github.com/google/deepvariant/issues/292,1,['release'],['release']
Deployability,"22bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-out/ppc-opt/bin/external/org_tensorflow/tensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:-}'); Execution platform: @bazel_tools//platforms:host_platform; Traceback (most recent call last):; File ""bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source"", line 252, in <module>; Main(); File ""bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source"", line 242, in Main; os.execv(args[0], args); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3.6'; (15:44:57) INFO: Elapsed time: 34.327s, Critical Path: 15.32s; (15:44:57) INFO: 910 processes: 910 local.; (15:44:57) FAILED: Build did NOT complete successfully; ```. I am running on a CentOS 7 docker container. I am trying to build DeepVariant 1.0 (the current github release).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:3410,release,release,3410,,https://github.com/google/deepvariant/issues/356,1,['release'],['release']
Deployability,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:1809,Install,Install,1809,,https://github.com/google/deepvariant/issues/231,2,['Install'],['Install']
Deployability,"6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:2953,install,install,2953,,https://github.com/google/deepvariant/issues/252,2,['install'],['install']
Deployability,"7056 make_examples_core.py:301] Task 3/16: Found 9819 candidate variants; I0218 00:34:18.302218 140191938357056 make_examples_core.py:301] Task 3/16: Created 10372 examples. real	62m19.124s; user	928m53.495s; sys	2m16.403s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpd74of138/make_examples.tfrecord@16.gz"" --checkpoint ""input/weights-51-0.995354.ckpt"". 2024-02-18 00:34:28.767569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:34:28.768358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; 2024-02-18 00:34:45.482939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:34:45.513278 140119155529536 call_variants.py:471] Total 1 writing processes started.; I0218 00:34:45.536368 140119155529536 dv_utils.py:365] From /tmp/tmpd74of138/make_examples.tfrecord-00000-of-00016.gz.example_info.json: Shape of input examples: [100,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:13797,install,installed,13797,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,8 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the s,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5463,update,update,5463,,https://github.com/google/deepvariant/issues/489,1,['update'],['update']
Deployability,"9.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:3082,install,installed,3082,,https://github.com/google/deepvariant/issues/678,1,['install'],['installed']
Deployability,":..	0/1:3:0,3:2:27,4,0:..	0/0:0:0,0:1:0,3,29:..	0/1:2:0,2:3:19,3,0:..	./.:1:1,0:0:29,3,0:II	./.:1:1,0:0:29,3,0:II	1/1:4:0,4:2:30,8,0:..	./.:1:1,0:0:29,3,0:II	0/0:0:0,0:1:0,3,29:..	1/1:8:0,8:12:41,18,0:..	0/1:2:0,2:2:22,4,0:..	1/1:6:0,6:6:30,12,0:..	0/1:2:0,2:2:16,4,0:..	0/1:2:0,2:3:15,3,0:..	./.:1:1,0:0:29,3,0:II`. The overall coverage of both these sites is really quite low... compared to a site such as this, which has good coverage and no ./. or gVCF generated 0/0:. `1	6633042	1_6633042_C_T	C	T	57	.	AF=0.025366;AQ=57	GT:DP:AD:GQ:PL:RNC	0/0:62:62,0:50:0,258,2579:..	0/0:50:50,0:50:0,180,1799:..	0/0:60:60,0:50:0,195,1949:..	0/0:57:57,0:50:0,252,2519:..	0/0:57:57,0:50:0,228,2279:..	0/0:39:39,0:50:0,159,1589:..	0/0:46:46,0:50:0,189,1889:..	0/0:23:23,0:48:0,69,689:..	0/0:25:25,0:50:0,75,749:..	0/0:23:23,0:50:0,69,689:..	0/0:40:40,0:50:0,189,1889:..	0/0:63:63,0:50:0,171,1949:..	0/0:56:56,0:50:0,213,2129:..	0/0:53:53,0:50:0,159,1589:..	0/0:60:60,0:50:0,213,2129:..	0/0:24:24,0:50:0,72,719:..	0/0:49:49,0:50:0,147,1469:..	0/0:27:27,0:50:0,51,749:..	0/0:40:40,0:50:0,156,1559:..	0/0:19:19,0:50:0,57,569:..	0/0:43:43,0:50:0,180,1799:..	0/0:21:21,0:50:0,66,659:..	0/0:69:69,0:50:0,207,2069:..	0/0:16:16,0:48:0,48,479:..	0/0:69:69,0:50:0,207,2069:..	0/0:46:46,0:50:0,138,1379:..	0/1:78:36,39:47:48,0,52:..	0/0:49:49,0:50:0,159,1589:..	0/0:25:25,0:50:0,81,809:..	0/0:101:101,0:50:0,300,2999:..	0/1:77:40,36:50:51,0,55:..	0/0:65:65,0:50:0,240,2399:..	0/0:22:22,0:48:0,66,659:..	0/0:55:55,0:50:0,219,2189:..	0/0:38:38,0:50:0,210,2099:..	0/0:56:56,0:50:0,171,1709:..	0/1:78:32,46:46:49,0,48:..`. It would be good to know the most appropriate way to remove the bad variants. As I understand the GQ field is the best place to start. If I am correct, the GLnexus command which I used filters by GQ > 20 ( see: https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration ). Perhaps upping this GQ to > 30 or slightly higher would remove this sites? I am very appreciative of every input, thank you so much!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:6660,Configurat,Configuration,6660,,https://github.com/google/deepvariant/issues/645,1,['Configurat'],['Configuration']
Deployability,"; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2164,pipeline,pipeline,2164,,https://github.com/google/deepvariant/issues/129,1,['pipeline'],['pipeline']
Deployability,"; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>; from deepvariant import modeling; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>; from tensorflow.python.tpu import tpu_config ; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>; from tensorflow_estimator.python.estimator.tpu.tpu_config import *; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1 import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1.estimator import experimental; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>; from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder; File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>; from tensorflow_estimator.python.estimator import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>; from tensorflow.python.saved_model import path_helpers; ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode; l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4.; - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much!; Best,; CW",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/673:2234,install,installed,2234,,https://github.com/google/deepvariant/issues/673,1,['install'],['installed']
Deployability,"======== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successful",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1224,Install,Installing,1224,,https://github.com/google/deepvariant/issues/859,1,['Install'],['Installing']
Deployability,===========================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 P,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4321,update,updates,4321,,https://github.com/google/deepvariant/issues/489,1,['update'],['updates']
Deployability,"After creating the directories and downloading the required files in their respective folders, I used the single command run_deeppvariant script. But it not's working. I'm attaching the screenshot. . - Operating system: MacOS; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina seq, short reads. <img width=""1510"" alt=""Screenshot 2022-12-05 at 12 09 45"" src=""https://user-images.githubusercontent.com/75676816/205648651-e8ad6b73-7139-4fa6-9b5d-b496cdcf7bc2.png"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/596:259,Install,Installation,259,,https://github.com/google/deepvariant/issues/596,1,['Install'],['Installation']
Deployability,Binaries for v1.1 necessary to update bioconda package,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/436:31,update,update,31,,https://github.com/google/deepvariant/issues/436,1,['update'],['update']
Deployability,Building DockerFile fails at Stage 'Install CLIF binary'',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476:36,Install,Install,36,,https://github.com/google/deepvariant/issues/476,1,['Install'],['Install']
Deployability,"Building deepvariant seems very hard, if you can put deepvariant onto anaconda cloud, then it'll be very easy for us to install.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9:120,install,install,120,,https://github.com/google/deepvariant/issues/9,1,['install'],['install']
Deployability,Cannot install DeepVariant,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17:7,install,install,7,,https://github.com/google/deepvariant/issues/17,1,['install'],['install']
Deployability,Cannot install latest DeepVariant via Conda,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/835:7,install,install,7,,https://github.com/google/deepvariant/issues/835,1,['install'],['install']
Deployability,Clang 11 installation not working,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:9,install,installation,9,,https://github.com/google/deepvariant/issues/489,1,['install'],['installation']
Deployability,"Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 24日 星期五 19:42:05 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:8490,Update,Update,8490,,https://github.com/google/deepvariant/issues/89,1,['Update'],['Update']
Deployability,Conda installation fails,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736:6,install,installation,6,,https://github.com/google/deepvariant/issues/736,1,['install'],['installation']
Deployability,"Congratulations on the new release, and glad to see that Intel MKL was utilized for optimized processing. When you have a chance, could you please document the following and provide use-cases for new users:. https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml. https://github.com/google/deepvariant/blob/r0.7/cloudbuild_CBI.yaml. Thanks,; ~p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87:27,release,release,27,,https://github.com/google/deepvariant/issues/87,1,['release'],['release']
Deployability,"Context: issue #116 . Htslib integration with GCS doesn't load app default credential from worker, and thus is only able to read from public bucket. The workaround is to localize BED file into VM worker prior running make_examples.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/119:29,integrat,integration,29,,https://github.com/google/deepvariant/issues/119,1,['integrat'],['integration']
Deployability,Create DV 0.5.2 release for gVCF creation fix.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/56:16,release,release,16,,https://github.com/google/deepvariant/pull/56,1,['release'],['release']
Deployability,Create DV 0.5.2 release to fix gVCF creation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/55:16,release,release,16,,https://github.com/google/deepvariant/pull/55,1,['release'],['release']
Deployability,"D=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out wa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1323,pipeline,pipeline,1323,,https://github.com/google/deepvariant/issues/214,1,['pipeline'],['pipeline']
Deployability,"DER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. Changing `--shards` to `128` changes the output in the VCF file. Running a diff between the two outputs shows that they are not the same. Why would that happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1727,pipeline,pipeline,1727,,https://github.com/google/deepvariant/issues/112,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,DV 0.5.2 bugfix release: fix gVCF reference base issue,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/57:16,release,release,16,,https://github.com/google/deepvariant/pull/57,1,['release'],['release']
Deployability,DV blog updates,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/215:8,update,updates,8,,https://github.com/google/deepvariant/pull/215,1,['update'],['updates']
Deployability,"D_OPENCL=0; export TF_NEED_OPENCL_SYCL=0; export TF_NEED_S3=1; export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1; export TF_CUDA_VERSION=""10.0""; export CUDA_TOOLKIT_PATH=""/usr/local/cuda""; export TF_CUDNN_VERSION=""7""; export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4085,release,release,4085,,https://github.com/google/deepvariant/issues/145,1,['release'],['release']
Deployability,"Dear Authors,; I just install deepvariant using conda and prepare to test the software with HiFi data. I noticed in the recently published DipAsm [package](https://github.com/shilpagarg/DipAsm) the parameters 'model_type' was set as 'PACBIO' in 'dv.sh' file, but there are only two options supplied in conda-deepvariant(wgs & wes). Since the alleged HiFidata base accuracy is very close to Illumina data, I wonder can I use wgs model for now? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/395:22,install,install,22,,https://github.com/google/deepvariant/issues/395,1,['install'],['install']
Deployability,"Dear Deepvariant team,. I was attempting to run Deepvariant on GCP by following the sample scripts from the tutorials, but it failed. I have checked the configuration regarding the Compute Engine quota and it should meet the requirements (i.e. CPU, Persistent Disk and In-use IP addresses). The error message from the log is like:; ""RuntimeError: Job failed with error ""run"": operation ""projects/deepvariant-phh/operations/7761698599878123803"" failed: executing pipeline: Execution failed: action 2: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION)"". I have read some of the related discussed issues but still can't solve my problem. The log files and my script file are attached. Your help is appreciated. . [staging_temp%2Frunner_logs_20181118_014355.log](https://github.com/google/deepvariant/files/2592663/staging_temp.2Frunner_logs_20181118_014355.log); [log.txt](https://github.com/google/deepvariant/files/2592666/log.txt). [script.txt](https://github.com/google/deepvariant/files/2592665/script.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:153,configurat,configuration,153,,https://github.com/google/deepvariant/issues/120,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Dear Developer,. I am going to use DeepVariant for variant calling. I have followed the instruction provided [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), to install the tool. After downloading the all needed sample data I run the command:; ```; sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Comman",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/217:201,install,install,201,,https://github.com/google/deepvariant/issues/217,1,['install'],['install']
Deployability,"Dear Developers, this project sounds awesome and thanks for opening up to the whole community. I have a de-novo assembly of a bacterial strain against whom i would like to align another set of an assembly representing different strains and then hopefully be able to use your pipeline. Wondering what recommendations you have to getting those fai and bam files, tools, settings etc... ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/3:275,pipeline,pipeline,275,,https://github.com/google/deepvariant/issues/3,1,['pipeline'],['pipeline']
Deployability,"Dear Developers,. Thank you very much for the amazing tool! I recently encountered an issue while running DeepVariant on a GIAB sample. I managed to run DeepVariant open another sample successfully so I believe there may be something wrong in the current sample input. It would be most helpful and appreciated if you could kindly take a look at the error messages. **Setup**; - Operating system: CentOS 7 x86_64; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Singularity (v3.10.0); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:445,Install,Installation,445,,https://github.com/google/deepvariant/issues/833,1,['Install'],['Installation']
Deployability,"Dear all. I am new to deepvariant. We are trying to use deepvariant on a HPC cluster with singularity.; We installed nvidia and cuda drivers through conda, and tested it with other python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:107,install,installed,107,,https://github.com/google/deepvariant/issues/761,1,['install'],['installed']
Deployability,"Dear developers,. When trying to train my own data with the latest 1.6.0, there are some error messages popped up:. It seems like some necessary libraries are missing. W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:721,install,installed,721,,https://github.com/google/deepvariant/issues/722,2,"['install', 'release']","['installed', 'release']"
Deployability,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/405:177,pipeline,pipeline,177,,https://github.com/google/deepvariant/issues/405,1,['pipeline'],['pipeline']
Deployability,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:134,install,installation,134,,https://github.com/google/deepvariant/issues/325,1,['install'],['installation']
Deployability,"DeepVariant fails to run with test data, giving error:; ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**; running from HPC; OS info:; `cat /etc/os-release`; output:. ```; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. - DeepVariant version: **1.6.1**; - Installation method (Docker, built from source, etc.): **Docker**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**; - Command: ; ``` ; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. - Error trace: (if applicable). ```; I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds; 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:187,release,release,187,,https://github.com/google/deepvariant/issues/812,2,"['Install', 'release']","['Installation', 'release']"
Deployability,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/352:116,pipeline,pipeline,116,,https://github.com/google/deepvariant/issues/352,2,"['Install', 'pipeline']","['Installation', 'pipeline']"
Deployability,"Describe the issue:; Hello, i am trying to recreate the steps done in this post: https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/; I have a bam file with phasing information which a got by running whatshap on the data and now i want use deepvariant for a second time just like the steps in the post.; I have a illumina bam file which i phased with nanopore data. I run this command:; singularity exec --bind /usr/lib/locale/; docker://google/deepvariant:${BIN_VERSION}; /opt/deepvariant/bin/run_deepvariant; --model_type PACBIO; --ref reference/GRCh38_no_alt_analysis_set.fasta; --reads whatshap/HG003.GRCh38.chr20.haplotagged.bam; --use_hp_information; --output_vcf deepvariant2/output.vcf.gz; --num_shards $(nproc); --regions chr20. And get this error :; NotImplementedError: The --use_hp_information flag has been deprecated. DeepVariant now phases internally for PacBio mode. I get what deprecated means but what is the thing i have to do to specify to deepvariant that my bam file is phased because i dont think it is using the haplotagged info in the bam file now?. Setup. DeepVariant version: latest; Installation method : source; Type of data: bam file with phasing information by whatshap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/823:1136,Install,Installation,1136,,https://github.com/google/deepvariant/issues/823,1,['Install'],['Installation']
Deployability,"Describe the issue:; I can't get vcf output after bind mount a root directory. Setup; - Operating system: Windows 11, but mount an Ubuntu VM through multipass; - Type of data: fasta, bam and vcf file. Steps to reproduce:; - Command:; #Configure the DeepVariant environment variables (missing input directory,...); BIN_VERSION=""1.5.0"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; PWD=/mountpoint/fastQ; INPUT_DIR=""${PWD}/testdata_input""; mkdir -p ${INPUT_DIR}; OUTPUT_DIR=""${PWD}/04.deepvariant_out""; mkdir -p ""${OUTPUT_DIR}"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; - Error trace: ; ; It displays: Task reading input the .bam file but it ends up with 0 candidates.; I suppose it can read the input files. Does the quick start test work on your system?; This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675:347,update,update,347,,https://github.com/google/deepvariant/issues/675,2,"['install', 'update']","['install', 'update']"
Deployability,"Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:7092,install,installed,7092,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"Elementary OS 5.1; Docker version 18.09.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR06",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/250:311,release,release-,311,,https://github.com/google/deepvariant/issues/250,1,['release'],['release-']
Deployability,Error running bioconda installed version,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664:23,install,installed,23,,https://github.com/google/deepvariant/issues/664,1,['install'],['installed']
Deployability,Error: validating pipeline: zones and regions cannot be specified together,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:18,pipeline,pipeline,18,,https://github.com/google/deepvariant/issues/96,1,['pipeline'],['pipeline']
Deployability,"File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write; self._prewrite_check(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check; compat.as_bytes(self.__name), compat.as_bytes(self.__mode)); tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system; ```. I guess since the model resides in the container, it can't be updated as singularity is read-only without sudo. Would there be some way of passing the `--checkpoint ""/opt/models/pacbio/model.ckpt""` parameter, but allowing it to be saved elsewhere?. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404:2417,update,updated,2417,,https://github.com/google/deepvariant/issues/404,1,['update'],['updated']
Deployability,Fresh AWS install failing on quick start data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:10,install,install,10,,https://github.com/google/deepvariant/issues/462,1,['install'],['install']
Deployability,"GATK on my local computer (with 8 GB of RAM and 4 cores), so I’m not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didn’t match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and I’m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:3894,pipeline,pipelines,3894,,https://github.com/google/deepvariant/issues/171,1,['pipeline'],['pipelines']
Deployability,"GE = ""en_US:en"",; 	LC_ALL = (unset),; 	LC_TIME = ""en_US.UTF-8"",; 	LC_MONETARY = ""en_US.UTF-8"",; 	LC_CTYPE = ""C.UTF-8"",; 	LC_ADDRESS = ""en_US.UTF-8"",; 	LC_TELEPHONE = ""en_US.UTF-8"",; 	LC_NAME = ""en_US.UTF-8"",; 	LC_MEASUREMENT = ""en_US.UTF-8"",; 	LC_IDENTIFICATION = ""en_US.UTF-8"",; 	LC_NUMERIC = ""en_US.UTF-8"",; 	LC_PAPER = ""en_US.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-02-17 23:32:31.107126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.108506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.006781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:32:31.007601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:32:31.110201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:6565,install,installed,6565,,https://github.com/google/deepvariant/issues/774,1,['install'],['installed']
Deployability,"GTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE XA:Z:X,+140993784,79M,2; PG:Z:MarkDuplicates AS:i:74 XS:i:69 MD:Z:17C61 NM:i:1 RG:Z:DM_23_2198; NB501857:464:HH7FWBGXV:2:23210:26812:14806 147 X 140994064 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA XA:Z:X,-140994589,50M3D29M,4; PG:Z:MarkDuplicates AS:i:79 XS:i:67 MD:Z:79 NM:i:0 RG:Z:DM_23_2198; Local realignment; X:140993145-140994144/ X_140993145_140994144realigned_reads.bam X_140993145_140994144realigned_reads.bam.bai; frmascla@frt:DeepV-TEST$ samtools view Local/X_140993145_140994144realigned_reads.bam | grep NB501857:464:HH7FWBGXV:2:23210:26812:14806; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993784 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE; NB501857:464:HH7FWBGXV:2:23210:26812:14806 19 X 140993854 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA. Original alignment, bam file; https://www.dropbox.com/scl/fi/c9tc01sdtf2sroxj3u3bj/original_alignment.bam?rlkey=jgxnyhyse2ekcu6t1s3l3lnnl&dl=0; Local realignment, bam file; https://www.dropbox.com/scl/fi/oqhny0s7h9hu3zcyrprig/X_140993145_140994144realigned_reads.bam?rlkey=zmbon72t19vjlcdht1zt6m5xg&dl=0. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; Yes. **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity container; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina 2x79bp paired-end sequencing, WES, reference genome=hg19",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763:2991,Install,Installation,2991,,https://github.com/google/deepvariant/issues/763,1,['Install'],['Installation']
Deployability,Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~202110,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:6192,install,installed,6192,,https://github.com/google/deepvariant/issues/489,1,['install'],['installed']
Deployability,"Getting ""ValueError: NOT_FOUND: Could not open"" when running the pipeline",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:65,pipeline,pipeline,65,,https://github.com/google/deepvariant/issues/588,1,['pipeline'],['pipeline']
Deployability,"Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. ; Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)?. You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts.""; from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657:6,Install,Installing,6,,https://github.com/google/deepvariant/issues/657,3,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,"Hello DV team, and thanks for creating such a great tool! . I am currently trying to retrain the wgs model for a new species (a fish) however, during training, I see no evaluation statistics (precision, recall, f1) for either het or homalt. Or more specifically they are all 0.0. Eval stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:926,Install,Installation,926,,https://github.com/google/deepvariant/issues/876,1,['Install'],['Installation']
Deployability,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394:65,upgrade,upgrade,65,,https://github.com/google/deepvariant/issues/394,9,"['install', 'release', 'update', 'upgrade']","['installation', 'installing', 'release', 'released', 'update', 'upgrade']"
Deployability,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:217,release,release,217,,https://github.com/google/deepvariant/issues/236,3,"['install', 'release']","['install', 'release', 'releases']"
Deployability,"Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error?. My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/139:435,update,update,435,,https://github.com/google/deepvariant/issues/139,1,['update'],['update']
Deployability,"Hello! I'm experiencing an issue when trying to run make_examples. Instead of Docker we're using Singularity, and deepvariant has run before with just calling the run_deepvariant.py. . For example, this is what has worked for us in the past in our environment: . > module load singularity; > source activate $condapath/DeepVariant. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/run_deepvariant.py [...]. When trying to run make_examples, this code:. > singularity exec $softwarepath/Singularity_files/deepvariant_1.5.0.sif python3 $softwarepath/deepvariant/deepvariant/make_examples.py [...] . is now throwing this error code: . > Traceback (most recent call last):; > File ""/$softwarepath/deepvariant/deepvariant/make_examples.py"", line 35, in <module>; > from deepvariant import dv_constants; > ModuleNotFoundError: No module named 'deepvariant'. Does this mean there is a problem with our install? Any ideas or suggestions? . Thank you very much for any light you can shed on this issue!. Best, ; Haley",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/771:953,install,install,953,,https://github.com/google/deepvariant/issues/771,1,['install'],['install']
Deployability,Hello!. Should deepvariant work with a newer version of `bazel`?; It would be great to update it from `0.21.0` to fresh `2.0.0`:; https://github.com/google/deepvariant/blob/master/settings.sh#L42,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/259:87,update,update,87,,https://github.com/google/deepvariant/issues/259,1,['update'],['update']
