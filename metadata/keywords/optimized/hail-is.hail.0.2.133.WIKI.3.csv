quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance," = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2565,perform,performance,2565,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['perform'],['performance']
Performance," = drop(va, vep)'). # subset VEP annotations if needed; subset = ','.join([x.rsplit('.')[-1] for x in annotations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global conc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40712,cache,cache,40712,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['cache'],"['cache', 'cached']"
Performance," Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90205,perform,performance,90205,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89286,perform,performance,89286,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79208,perform,performance,79208,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," NDArray of positional quotients.; """"""; return self._bin_op_numeric(""/"", other, self._div_ret_type_f). def __rtruediv__(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by a ndarray or a scalar using floor division. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). def __rmatmul__(self, other):; if not isinstance(other, NDArrayNumericExpression):; other = hl.nd.array(other); return other.__matmul__(self). [docs] def __matmul__(self, other):; """"""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression` or :class:`.NumericExpres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:107657,perform,performs,107657,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['perform'],['performs']
Performance," Notes; The key type of the table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the table to keys not present in another table.; To restrict to keys present in other, use semi_join().; Examples; >>> table_result = table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_locali",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:15064,cache,cache,15064,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['cache'],['cache']
Performance," Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83805,perform,performance,83805,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This fu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1360,concurren,concurrent,1360,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,1,['concurren'],['concurrent']
Performance," Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; value",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43220,perform,performance,43220,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:56960,cache,cache,56960,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['cache'],['cache']
Performance," VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45462,load,load,45462,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance," Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:4803,scalab,scalable,4803,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8072,optimiz,optimizer,8072,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['optimiz'],['optimizer']
Performance," [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1719,load,loadings,1719,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance," a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mitochondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7450,load,loaded,7450,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance," a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered entry); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset and the second index is; the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:16804,perform,performs,16804,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['performs']
Performance," annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87177,perform,performance,87177,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns:; MatrixTable – Dataset with columns from both datasets. union_rows(*, _check_cols=True)[source]; Take the union of dataset rows.; Examples; Union the rows of two datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be ide",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:66735,perform,perform,66735,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,['perform'],['perform']
Performance," compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#637",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90875,optimiz,optimization,90875,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance," disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86062,perform,performance,86062,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:1806,load,load,1806,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['load'],['load']
Performance," genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; acce",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:10158,perform,performance,10158,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['performance']
Performance," hl.eval(locus.in_autosome_or_par()); True. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isAutosomalOrPseudoAutosomal"", tbool). [docs] def in_mito(self):; """"""Returns ``True`` if the locus is on mitochondrial DNA. Examples; --------. >>> hl.eval(locus.in_mito()); False. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isMitochondrial"", tbool). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def sequence_context(self, before=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89118,load,load,89118,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['load'],['load']
Performance," hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:12103,perform,performs,12103,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performs']
Performance," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7709,optimiz,optimized,7709,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,4,"['load', 'optimiz']","['load', 'optimized']"
Performance," in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:; (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the min_gq parameter, if the proband allele balance is; lower than the min_child_ab parameter, if the depth ratio between the; proband and parents is smaller than the min_depth_ratio parameter, if; the allele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:58312,throughput,throughput,58312,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance," interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:21314,perform,perform,21314,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['perform'],['perform']
Performance," key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:67976,perform,performance,67976,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," less than \(2^{31}\). Parameters:. ndarray (numpy.ndarray) – ndarray with two dimensions, each of non-zero size.; block_size (int, optional) – Block size. Default given by default_block_size(). Returns:; BlockMatrix. classmethod fromfile(uri, n_rows, n_cols, block_size=None, *, _assert_type=None)[source]; Creates a block matrix from a binary file.; Examples; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') . To create a block matrix of the same dimensions:; >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) . Notes; This method, analogous to numpy.fromfile,; reads a binary file of float64 values in row-major order, such as that; produced by numpy.tofile; or BlockMatrix.tofile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; A NumPy ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:24370,load,load,24370,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['load']
Performance," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:118718,load,load,118718,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86149,perform,performance,86149,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25405,cache,cache,25405,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance," ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:59754,perform,performance,59754,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18240,load,loadings,18240,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance," of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:30115,perform,performance,30115,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value \(1/m\) for variants in Hardy-Weinberg equilibrium and is further motivated in the paper cited above. (The r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138885,load,loadings,138885,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance," partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110418,cache,cache,110418,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['cache'],['cache']
Performance," redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it.; """"""; self._jkt.unpersist(). [docs] @handle_py4j; @typecheck_method(cols=tupleof(oneof(strlike, Ascending, Descending))); def order_by(self, *cols):; """"""Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. :param cols: Columns to sort by.; :type: str or asc(str) or desc(str). :return: Key table sorted by ``cols``.; :rtype: :class:`.KeyTable`; """""". jsort_columns = [asc(col)._jrep if isinstance(col, str) else col._jrep for col in cols]; return KeyTable(self.hc,; self._jkt.orderBy(jarray(Env.hail().keytable.SortColumn, jsort_columns))). [docs] @handle_py4j; def num_partitions(self):; """"""Returns the number of partitions in the key table.; ; :rtype: int; """"""; return self._jkt.nPartitions(). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike); de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:24890,perform,performed,24890,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['perform'],['performed']
Performance," reference genome (``'GRCh37'`` by default).; With an argument, sets the default reference genome to the argument. Returns; -------; :class:`.ReferenceGenome`; """"""; if new_default_reference is not None:; Env.hc().default_reference = new_default_reference; return None; return Env.hc().default_reference. [docs]def get_reference(name) -> ReferenceGenome:; """"""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:26173,load,loaded,26173,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['load'],['loaded']
Performance," require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16497,cache,cache,16497,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['cache'],['cache']
Performance," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31997,perform,perform,31997,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['perform'],['perform']
Performance," samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, hl.maximal_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:97100,optimiz,optimize,97100,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimize']
Performance," specified by the left-hand side (must; begin with ``g``). This is analogous to :py:meth:`~hail.VariantDataset.annotate_variants_expr` and; :py:meth:`~hail.VariantDataset.annotate_samples_expr` where the annotation paths are ``va`` and ``sa`` respectively. ``expr`` is in genotype context so the following symbols are in scope:. - ``g``: genotype annotation; - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations. For more information, see the documentation on writing `expressions <overview.html#expressions>`__; and using the `Hail Expression Language <exprlang.html>`__. .. warning::. - If the resulting genotype schema is not :py:class:`~hail.expr.TGenotype`,; subsequent function calls on the annotated variant dataset may not work such as; :py:meth:`~hail.VariantDataset.pca` and :py:meth:`~hail.VariantDataset.linreg`. - Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to :py:class:`~hail.expr.TGenotype`. - Genotypes are immutable. For example, if ``g`` is initially of type ``Genotype``, the expression; ``g.gt = g.gt + 1`` will return a ``Struct`` with one field ``gt`` of type ``Int`` and **NOT** a ``Genotype``; with the ``gt`` incremented by 1. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:10603,perform,performance,10603,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance," tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23245,cache,cache,23245,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['cache'],['cache']
Performance," transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23439,concurren,concurrently,23439,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['concurren'],['concurrently']
Performance," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:18601,perform,perform,18601,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['perform'],['perform']
Performance," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:109365,perform,performs,109365,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance," value of 50. [4]:. p = hl.plot.histogram(mt.DP, range=(0, 30), bins=30); show(p). [Stage 4:> (0 + 1) / 1]. Cumulative Histogram; The cumulative_histogram() method works in a similar way to histogram(). [5]:. p = hl.plot.cumulative_histogram(mt.DP, range=(0,30), bins=30); show(p). Scatter; The scatter() method can also take in either Python types or Hail fields as arguments for x and y. [6]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 7:> (0 + 1) / 1]. We can also pass in a Hail field as a label argument, which determines how to color the data points. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:5237,cache,cache,5237,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['cache'],['cache']
Performance," vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-alle",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54414,latency,latency,54414,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['latency'],['latency']
Performance," want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1623,scalab,scalable,1623,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['scalab'],['scalable']
Performance," while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument --cores. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:2566,perform,performing,2566,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['performing']
Performance," will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:140971,load,loadings,140971,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance," will never be missing, even if the `missing` string appears; in the column IDs. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; row_fields: :obj:`dict` of :class:`str` to :class:`.HailType`; Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key: :class:`str` or :obj:`list` of :obj:`str`; Key fields(s). If empty, creates an index `row_id` to use as key.; entry_type: :class:`.HailType`; Type of entries in matrix table. Must be one of: :py:data:`.tint32`,; :py:data:`.tint64`, :py:data:`.tfloat32`, :py:data:`.tfloat64`, or; :py:data:`.tstr`. Default: :py:data:`.tint32`.; missing: :class:`str`; Identifier to be treated as missing. Default: NA; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header: :obj:`bool`; If ``True``, assume the file has no header and name the row fields `f0`,; `f1`, ... `fK` (0-indexed) and the column keys 0, 1, ... N.; force_bgz : :obj:`bool`; If ``True``, load **.gz** files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep : :class:`str`; This parameter is a deprecated name for `delimiter`, please use that; instead.; delimiter : :class:`str`; A single character string which separates values in the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns; -------; :class:`.MatrixTable`; MatrixTable constructed from imported data.; """"""; row_key = wrap_to_list(row_key); comment = wrap_to_list(comment); paths = [hl.current_backend().fs.canonicalize_path(p) for p in wrap_to_list(paths)]; missing_list = wrap_to_list(missing). def comment_filter(table):; return (; hl.rbind(; hl.array(comment),; lambda hl_comment: hl_comment.any(; lambda com: hl.if_else(hl.len(com) == 1, tab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:72287,load,load,72287,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance," will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. Ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88869,perform,performance,88869,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144280,cache,cache,144280,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"(*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from VCF file(s); :rtype: :py:class:`.VariantDataset`. """""". if generic:; jvds = self._jhc.importVCFsGeneric(jindexed_seq_args(path), force, force_bgz, joption(header_file),; joption(min_partitions), drop_samples, jset_args(call_fields)); else:; jvds = self._jhc.importVCFs(jindexed_seq_args(path",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:25280,load,load,25280,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5140,perform,performance,5140,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['perform'],['performance']
Performance,") – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogram; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Compute call rate; >>> call_rate = vds.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144439,cache,cache,144439,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:161652,load,loadings,161652,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"); .or_error(""Found non-left-aligned variant in split_multi""); ). return hl.bind(error_on_moved, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). return split_rows(hl.sorted(kept_alleles.map(make_struct)), permit_shuffle); else:. def make_struct(i, cond):; def struct_or_empty(v):; return hl.case().when(cond(v.locus), hl.array([new_struct(v, i)])).or_missing(). return hl.bind(struct_or_empty, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). def make_array(cond):; return hl.sorted(kept_alleles.flatmap(lambda i: make_struct(i, cond))). left = split_rows(make_array(lambda locus: locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split tho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117351,throughput,throughput,117351,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['throughput'],['throughput']
Performance,"++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowled",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1888,scalab,scalable,1888,index.html,https://hail.is,https://hail.is/index.html,1,['scalab'],['scalable']
Performance,"+------------+------+---------+----------+--------------+; <BLANKLINE>; +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+; <BLANKLINE>; +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; -----. Hail is designed to be maximally compatible with files in the `VCF v4.2; spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :func:`.import_vcf` takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); cont",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:98365,load,load,98365,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,", Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:74562,latency,latency,74562,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['latency'],['latency']
Performance,", \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:5643,perform,perform,5643,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['perform']
Performance,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1821,perform,performs,1821,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['performs']
Performance,", n_rows, n_cols, block_size). [docs] @classmethod; @typecheck_method(; entry_expr=expr_float64,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14308,perform,performance,14308,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['perform'],['performance']
Performance,", pair adenine (A) with uracil (U) instead of thymine (T). Returns; -------; :class:`.StringExpression`; """"""; s = s.reverse(). if rna:; pairs = [('A', 'U'), ('U', 'A'), ('T', 'A'), ('G', 'C'), ('C', 'G')]; else:; pairs = [('A', 'T'), ('T', 'A'), ('G', 'C'), ('C', 'G')]. d = {}; for b1, b2 in pairs:; d[b1] = b2; d[b1.lower()] = b2.lower(). return s.translate(d). [docs]@typecheck(; contig=expr_str, position=expr_int32, before=expr_int32, after=expr_int32, reference_genome=reference_genome_type; ); def get_sequence(contig, position, before=0, after=0, reference_genome='default') -> StringExpression:; """"""Return the reference sequence at a given locus. Examples; --------. Return the reference allele for ``'GRCh37'`` at the locus ``'1:45323'``:. >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) # doctest: +SKIP; ""T"". Notes; -----; This function requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:160651,load,load,160651,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['load']
Performance,",5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28766,load,loadings,28766,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,",; gvcf_paths: Optional[List[str]] = None,; vds_paths: Optional[List[str]] = None,; vds_sample_counts: Optional[List[int]] = None,; intervals: Optional[List[Interval]] = None,; import_interval_size: Optional[int] = None,; use_genome_default_intervals: bool = False,; use_exome_default_intervals: bool = False,; gvcf_external_header: Optional[str] = None,; gvcf_sample_names: Optional[List[str]] = None,; gvcf_info_to_keep: Optional[Collection[str]] = None,; gvcf_reference_entry_fields_to_keep: Optional[Collection[str]] = None,; call_fields: Collection[str] = ['PGT'],; branch_factor: int = VariantDatasetCombiner._default_branch_factor,; target_records: int = VariantDatasetCombiner._default_target_records,; gvcf_batch_size: Optional[int] = None,; batch_size: Optional[int] = None,; reference_genome: Union[str, ReferenceGenome] = 'default',; contig_recoding: Optional[Dict[str, str]] = None,; force: bool = False,; ) -> VariantDatasetCombiner:; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; if not (gvcf_paths or vds_paths):; raise ValueError(""at least one of 'gvcf_paths' or 'vds_paths' must be nonempty""); if gvcf_paths is None:; gvcf_paths = []; if len(gvcf_paths) > 0:; if len(set(gvcf_paths)) != len(gvcf_paths):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_paths).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(f'gvcf paths should be unique, the following paths are repeated:{duplicates}'); if gvcf_sample_names is not None and len(set(gvcf_sample_names)) != len(gvcf_sample_names):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_sample_names).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(; ""provided sample names ('gvcf_sample_names') should be unique, ""; f'the following names are repeated:{duplicates}'; ). if vds_paths is None:; vds_paths = []; if vds_sample_counts is not None and len(vds_paths) != len(vds_sample_counts):; raise ValueError(; ""'vds_pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:23667,load,load,23667,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['load'],['load']
Performance,",; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; -----; This method assumes the GATK high-throughput sequencing fields exist:; `GT`, `AD`, `DP`, `GQ`, `PL`. This method replicates the functionality of `Kaitlin Samocha's de novo; caller <https://github.com/ksamocha/de_novo_scripts>`__. The version; corresponding to git commit ``bde3e40`` is implemented in Hail with her; permission and assistance. This method produces a :class:`.Table` with the following fields:. - `locus` (``locus``) -- Variant locus.; - `alleles` (``array<str>``) -- Variant alleles.; - `id` (``str``) -- Proband sample ID.; - `prior` (``float64``) -- Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; `pop_frequency_prior` parameter, and the global prior; ``1 / 3e7``. If the `ignore_in_sample_allele_frequency` parameter is ``True``,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Fath",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:20555,throughput,throughput,20555,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['throughput'],['throughput']
Performance,", *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3768,scalab,scalable,3768,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"-+----------+--------------+; | 1:123456 | [""A"",""C""] | NA | NA | NA | [1,NA] |; +---------------+------------+------+---------+----------+--------------+. +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+. +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; Hail is designed to be maximally compatible with files in the VCF v4.2; spec.; import_vcf() takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:41921,load,load,41921,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"-- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int variant_block_size: Number of variant regressions to perform simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(kinshipMatrix=KinshipMatrix,; y=strlike,; covariates=listof(strlike),; global_root=strlike,; va_root=strlike,; run_assoc=bool,; use_ml=bool,; delta=nullable(numeric),; sparsity_threshold=numeric,; use_dosages=bool,; n_eigs=nullable(integral),; dropped_variance_fraction=(nullable(float))); def lmmreg(self, kinshipMatrix, y, covariates=[], global_root=""global.lmmreg"", va_root=""va.lmmreg"",; run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False,; n_eigs=None, dropped_variance_fraction=None):; """"""Use a kinship-based line",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:114002,perform,perform,114002,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"---+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42454,optimiz,optimization,42454,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['optimiz'],['optimization']
Performance,"-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | A | B |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 65 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 72 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 70 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 60 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+. In addition to the Table.join() method, Hail provides another; join syntax using Python’s bracket indexing syntax. The syntax looks like; right_table[left_table.key], which will return an Expression; instead of a Table. This expression is a dictionary mapping the; keys in the left table to the rows in the right table.; We can annotate the left table with this expression to perform a left join:; left_table.annotate(x = right_table[left_table.key].x]. For example, below; we add the field ‘B’ from ht2 to ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:7297,perform,perform,7297,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['perform'],['perform']
Performance,"---------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_interv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:26855,load,load,26855,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:59825,perform,performance,59825,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:10033,optimiz,optimized,10033,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88735,perform,performance,88735,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:686,concurren,concurrent,686,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['concurren'],['concurrent']
Performance,". Notes; The resulting ndarray will have the same shape as the block matrix. Returns:; numpy.ndarray. to_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray of tfloat64) – Entries for the row. Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters:. n_partitions (int or None) – Number of partitions of the table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by function",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:42269,cache,cache,42269,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111007,cache,cache,111007,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,".T @ G2); Q1, R1 = hl.nd.qr(G2)._persist(); fact2 = _krylov_factorization(A, Q1, p, compute_U=False); moments_and_stdevs = fact2.spectral_moments(num_moments, R1); # Add back exact moments; moments = moments_and_stdevs.moments + hl.nd.array([; fact.S.map(lambda x: x ** (2 * i)).sum() for i in range(1, num_moments + 1); ]); moments_and_stdevs = hl.eval(hl.struct(moments=moments, stdevs=moments_and_stdevs.stdevs)); moments = moments_and_stdevs.moments; stdevs = moments_and_stdevs.stdevs. scores = V * S; eigens = hl.eval(S * S); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). hail_array_scores = scores._data_array(); cols_and_scores = hl.zip(A.source_table.index_globals().cols, hail_array_scores).map(; lambda tup: tup[0].annotate(scores=tup[1]); ); st = hl.Table.parallelize(cols_and_scores, key=A.col_key). if compute_loadings:; lt = A.source_table.select(); lt = lt.annotate_globals(U=U); idx_name = '_tmp_pca_loading_index'; lt = lt.add_index(idx_name); lt = lt.annotate(loadings=hl.array(lt.U[lt[idx_name], :])).select_globals(); lt = lt.drop(lt[idx_name]); else:; lt = None. return eigens, st, lt, moments, stdevs. @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix),; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; compute_scores=bool,; transpose=bool,; ); def _blanczos_pca(; A,; k=10,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; compute_scores=True,; transpose=False,; ):; r""""""Run randomized principal component analysis approximation (PCA); on numeric columns derived from a matrix table. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl._blanczos_pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:17702,load,loadings,17702,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21357,latency,latency,21357,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['latency'],['latency']
Performance,".filter_cols(hl.is_missing(cols_to_remove.index(ds['s']))). See also; semi_join_cols(), filter_cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:17941,cache,cache,17941,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['cache'],['cache']
Performance,".pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1492,load,loadings,1492,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"/en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: arr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:47076,cache,cache,47076,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['cache'],['cache']
Performance,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/annotationdb.html:1004,load,load,1004,docs/0.1/annotationdb.html,https://hail.is,https://hail.is/docs/0.1/annotationdb.html,1,['load'],['load']
Performance,"0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantgq_means, bins = np.arange(0, 84, 2)); plt.xlabel(""Variant Mean GQ""); pl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:18668,cache,cache,18668,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['cache'],['cache']
Performance,"0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34886,perform,perform,34886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['perform'],['perform']
Performance,"0].p_value,; 'p_value_excess_het': hwe[1].p_value,; }),; ),; ). return mt.annotate_rows(**{name: result}). [docs]@typecheck(left=MatrixTable, right=MatrixTable, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14032,perform,performs,14032,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['perform'],['performs']
Performance,"3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genotype probabilities.; Examples; >>> hl.eval(hl.gp_dosage([0.0, 0.5, 0.5])); 1.5. Notes; This function is only defined for bi-allelic variants. The gp argument; must be length 3. The value is gp[1] + 2 * gp[2]. Parameters:; gp (Expression of type tarray of tfloat64) – Length 3 array of bi-allelic genotype probabilities. Returns:; Expression of type tfloat64. hail.expr.functions.get_sequence(contig, position, before=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_cod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:19045,load,load,19045,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['load']
Performance,"30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_indepen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101956,optimiz,optimizer,101956,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93363,optimiz,optimized,93363,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimized']
Performance,"4) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr (CollectionExpression) – Expression of type tarray or tset. Returns:; Expression – Aggregation express",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:16703,perform,perform,16703,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['perform'],['perform']
Performance,"4,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:12494,perform,performed,12494,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performed']
Performance,"4`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32627,perform,perform,32627,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['perform'],['perform']
Performance,"8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78956,perform,performance,78956,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72481,perform,performance,72481,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79680,perform,performance,79680,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"95) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88098,perform,performance,88098,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,": hl.tstr} solves this problem. Parameters:. paths (str or list of str) – Files to import.; key (str or list of str) – Key fields(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True`, assume the file has no header and name the N fields f0,; f1, … fN (0-indexed).; impute (bool) – If True, Impute field types from the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:35780,load,load,35780,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,": str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:27141,load,load,27141,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,":. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:. >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). .. note::. For optimal performance, be sure to explicitly give the alternative (``NA``) the same type as the consequent (``sa.pheno.bloodPressure``). **Notes**. ``expr`` is in sample context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateSamplesExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; root=nullable(strlike),; expr=nullable(strlike),; vds_key=nullable(strlike),; product=bool); def annotate_samples_table(self, table, root=None, expr=None, vds_key=None, product=False):; """"""Annotate samples with a key table. **Examples**. To annotates samples using `samples1.tsv` with type imputation::. >>> table = hc.import_t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:14718,perform,performance,14718,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,":; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:15490,perform,performs,15490,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['perform'],['performs']
Performance,":`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30485,cache,cache,30485,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['cache'],['cache']
Performance,"; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1128,load,loadings,1128,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:108486,optimiz,optimized,108486,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['optimiz'],['optimized']
Performance,"; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7844,cache,cache,7844,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"; Returns True if the locus is in a pseudoautosomal region; of chromosome Y.; Examples; >>> hl.eval(locus.in_y_par()); False. Note; Many variant callers only generate variants on chromosome X for the; pseudoautosomal region. In this case, all loci mapped to chromosome; Y are non-pseudoautosomal. Returns:; BooleanExpression. property position; Returns the position along the chromosome.; Examples; >>> hl.eval(locus.position); 1034245. Returns:; Expression of type tint32 – This locus’s position along its chromosome. sequence_context(before=0, after=0)[source]; Return the reference genome sequence at the locus.; Examples; Get the reference allele at a locus:; >>> hl.eval(locus.sequence_context()) ; ""G"". Get the reference sequence at a locus including the previous 5 bases:; >>> hl.eval(locus.sequence_context(before=5)) ; ""ACTCGG"". Notes; This function requires that this locus’ reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome. Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:9165,load,load,9165,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['load'],['load']
Performance,"; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46737,perform,performance,46737,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matrix with 1 row, 10 columns,; and elements from row 2 of bm.; bm[:3, -1] is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of bm.; bm[::2, ::2] is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of bm. Use fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3811,cache,cache,3811,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23973,load,loadings,23973,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"<https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:222465,cache,cache,222465,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"= {row.ID : row.SEX for row in kt1.collect()}. **Notes**. This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-dep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23159,cache,cache,23159,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,['cache'],"['cache', 'cached']"
Performance,"=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:59690,perform,performs,59690,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_json_in_file=True, use_string_key_as_file_name=False)[source]; Export entries of the mt by column as separate text files.; Examples; >>> range_mt = hl.utils.range_matrix_table(10, 10); >>> range_mt = range_mt.annotate_entries(x = hl.rand_unif(0, 1)); >>> hl.experimental.export_entries_by_col(range_mt, 'output/cols_files'). Notes; This function writes a directory with one file per column in mt. The; files contain one tab-separated field (with header) for each row field; and entry field in mt. The column fields of mt are written as JSON; in the first line of each file, prefixed with a #.; The above will produce a directory at output/cols_files with the; following files:; $ ls -l output/cols_files; total 80; -rw-r--r-- 1 hail-dev wheel 71",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:28430,load,load,28430,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1728,load,load,1728,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['load'],['load']
Performance,"ASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:25017,load,load,25017,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"A` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162233,load,loadings,162233,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"BS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The minimum allele frequency filter is applied per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, ki",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:174389,perform,perform,174389,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23378,perform,performance,23378,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['perform'],['performance']
Performance,"Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77673,perform,performance,77673,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"CF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partitioning(path)[source]¶; Write partitioning.json.gz file for legacy VDS file. Parameters:path (str) – path to VDS file. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:25071,load,load,25071,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"DArrayExpression`; """"""; ir = BlockMatrixCollect(self._bmir); return construct_expr(ir, hl.tndarray(hl.tfloat64, 2)). @property; def is_sparse(self):; """"""Returns ``True`` if block-sparse. Notes; -----; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns; -------; :obj:`bool`; """"""; return Env.backend()._to_java_blockmatrix_ir(self._bmir).typ().isSparse(). @property; def T(self):; """"""Matrix transpose. Returns; -------; :class:`.BlockMatrix`; """"""; if self.n_rows == 1 and self.n_cols == 1:; return self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:41819,cache,cache,41819,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['cache'],['cache']
Performance,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:103172,cache,cache,103172,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"K behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns:; Table. hail.methods.import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:17075,load,load,17075,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23746,cache,cache,23746,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"PUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:84723,perform,performance,84723,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['perform'],['performance']
Performance,"Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_level)). [docs] def unpersist(self):; """"""; Unpersists this VDS from memory/disk.; ; **Notes**; This function will have no effect on a VDS that was not previously persisted.; ; There's nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it.; ; """"""; self._jvds.unpersist(). @property; @handle_py4j; def global_schema(self):; """"""; Returns the signature of the global annotations contained in this VDS. **Examples**. >>> print(vds.global_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.global_schema). :rtype: :class:`.Type`; """""". if self._global_schema is None:; self._global_schema = Type._from_java(self._jvds.globalSignature()); return self._global_schema. @property; @handle_py4j; def colkey_schema(self):; """"""; Returns the signature of the column key (sample) contained in this VDS. **Examples**. >>> print(vds.colkey_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.colkey_schema). :rtype: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:179613,perform,performed,179613,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performed']
Performance,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1234,perform,performing,1234,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,"['perform', 'scalab']","['performing', 'scalable']"
Performance,"Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86370,perform,performance,86370,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:3800,load,load,3800,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['load'],['load']
Performance,"Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1124,load,load,1124,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['load'],['load']
Performance,"Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is Fals",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25117,cache,cache,25117,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['cache'],['cache']
Performance,"The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is locate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28361,cache,cache,28361,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['cache'],['cache']
Performance,"\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter ``n_eigs`` to use only the top ``n_eigs`` eigenvectors. Alternatively, specify ``dropped_variance_fraction`` to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, ``dropped_variance_fraction=0.01`` will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two. **Further background**. For the history and mathematics of linear mixed models in genetics, including `FastLMM <https://www.microsoft.com/en-us/research/project/fas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:136237,perform,performance,136237,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,"^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pair",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:18061,perform,perform,18061,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['perform'],['perform']
Performance,"_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following cod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:86800,throughput,throughput,86800,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48475,perform,performs,48475,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['performs']
Performance,"_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:46359,perform,performance,46359,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"_right_row_fields=bool); def union_cols(; self, other: 'MatrixTable', row_join_type: str = 'inner', drop_right_row_fields: bool = True; ) -> 'MatrixTable':; """"""Take the union of dataset columns. Warning; -------. This method does not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120046,perform,performed,120046,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['perform'],['performed']
Performance,"` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:166328,load,loadings,166328,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167531,load,loadings,167531,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"`, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167986,load,loadings,167986,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102323,load,load,102323,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.pval`` | Double | :math:`p`-value |; +-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:122434,perform,performed,122434,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performed']
Performance,"a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-val",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94405,perform,perform,94405,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['perform']
Performance,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:6666,optimiz,optimized,6666,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,3,"['optimiz', 'perform', 'scalab']","['optimized', 'performance', 'scalability']"
Performance,"ability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:40490,perform,performance,40490,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['perform'],['performance']
Performance,"able) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[allele_index - 1] > 0). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:20774,latency,latency,20774,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['latency'],['latency']
Performance,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:2095,optimiz,optimizer,2095,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,1,['optimiz'],['optimizer']
Performance,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54073,cache,cache,54073,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125277,perform,performant,125277,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['perform'],"['performance', 'performant']"
Performance,"ail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9894,load,loaded,9894,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['load'],['loaded']
Performance,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49723,cache,cache,49723,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"amples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:1860,load,load,1860,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['load'],['load']
Performance,"an join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row values of another table: table[expr] will refer to the row of table that has a key value of expr. If the row is not unique, one such row is chosen arbitrarily.; Here’s a subtle bit: if expr is an expression indexed by a row of table2, then table[expr] is also an expression indexed by a row of table2.; Also note that while they look similar, table['field'] and table1[table2.key] are doing very different things!; table['field'] selects a field from the table, while table1[table2.key] creates a mapping between the keys of table2 and the rows of table1. [9]:. t1['a'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5bee73d130>; Index:; ['row']",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:5299,perform,performs,5299,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['perform'],['performs']
Performance,"an; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For eac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138389,load,loadings,138389,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"ance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*St",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:6373,load,load,6373,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"and aggregate to produce a new table:; >>> table3 = (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:3639,cache,cache,3639,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['cache'],['cache']
Performance,"ange this behavior. Annotations; The below annotations can be accessed with sa.imputesex. isFemale (Boolean) – True if the imputed sex is female, false if male, missing if undetermined; Fstat (Double) – Inbreeding coefficient; nTotal (Long) – Total number of variants considered; nCalled (Long) – Number of variants with a genotype call; expectedHoms (Double) – Expected number of homozygotes; observedHoms (Long) – Observed number of homozygotes. Parameters:; maf_threshold (float) – Minimum minor allele frequency threshold.; include_par (bool) – Include pseudoautosomal regions.; female_threshold (float) – Samples are called females if F < femaleThreshold; male_threshold (float) – Samples are called males if F > maleThreshold; pop_freq (str) – Variant annotation for estimate of MAF.; If None, MAF will be computed. Returns:Annotated dataset. Return type:VariantDataset. join(right)[source]¶; Join two variant datasets.; Notes; This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self).; The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:73696,perform,performs,73696,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance,"annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:2110,load,loadings,2110,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"ap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\); are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:30712,load,loadings,30712,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27985,perform,performs,27985,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['performs']
Performance,"are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45649,load,loaded,45649,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"arget** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). t = import_table(; path,; comment=""@"",; impute=False,; no_header=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; **kwargs,; ). if t.row.dtype == tstruct(f0=tstr):; if reference_genome:; t = t.select(interval=hl.parse_locus_interval(t['f0'], reference_genome)); else:; interval_regex = r""([^:]*):(\d+)\-(\d+)"". def checked_match",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:25602,load,loaded,25602,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['loaded']
Performance,"arly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:6289,throughput,throughput,6289,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['throughput'],['throughput']
Performance,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:10754,optimiz,optimized,10754,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"ass:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True); ; Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; ; >>> kt = hc.import_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:76379,latency,latency,76379,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['latency'],['latency']
Performance,"ated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102413,load,load,102413,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"ated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3159,throughput,throughput,3159,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"ath}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; interval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:12926,load,load,12926,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['load'],['load']
Performance,"ation is a ``Set`` and can be queried for filter membership with expressions ; like ``va.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:24750,load,load,24750,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"ation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12569,perform,perform,12569,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['perform']
Performance,"ations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40810,cache,cache,40810,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"bal.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:95097,perform,performed,95097,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performed']
Performance,"before converting to; DataFrame.; flatten (bool) – If true, flatten before converting to; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Return type:pyspark.sql.DataFrame. to_pandas(expand=True, flatten=True)[source]¶; Converts this key table into a Pandas DataFrame. Parameters:; expand (bool) – If true, expand_types before converting to; Pandas DataFrame.; flatten (bool) – If true, flatten before converting to Pandas; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Returns:Pandas DataFrame constructed from the key table. Return type:pandas.DataFrame. union(*kts)[source]¶; Union the rows of multiple tables.; Examples; Take the union of rows from two tables:; >>> other = hc.import_table('data/kt_example1.tsv', impute=True); >>> union_kt = kt1.union(other). Notes; If a row appears in both tables identically, it is duplicated in; the result. The left and right tables must have the same schema; and key. Parameters:kts (args of type KeyTable) – Tables to merge. Returns:A table with all rows from the left and right tables. Return type:KeyTable. unpersist()[source]¶; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously persisted.; There’s nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it. write(output, overwrite=False)[source]¶; Write as KT file.; *Examples*; >>> kt1.write('output/kt1.kt'). Note; The write path must end in “.kt”. Parameters:; output (str) – Path of KT file to write.; overwrite (bool) – If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:29871,perform,performed,29871,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['perform'],['performed']
Performance,"ble Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:988,load,load,988,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['load'],['load']
Performance,"ble, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or filtered entry); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var. The first index is the state in the left dataset and the secon",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14193,perform,performs,14193,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['perform'],['performs']
Performance,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80028,cache,cache,80028,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain info",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3926,scalab,scalable,3926,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"brium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[str, ReferenceGenome] = 'default') -> LocusExpression:; """"""Construct a locus expression from a chromosome and position. Examples; --------. >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37). Parameters; ----------; contig : str or :class:`.StringExpression`; Chromosome.; pos : int or :class:`.Expression` of type :py:data:`.tint32`; Base position along the chromosome.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:35469,perform,perform,35469,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['perform'],['perform']
Performance,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:8231,load,loaded,8231,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['load'],['loaded']
Performance,bute). aggregate_by_key() (hail.KeyTable method). (hail.VariantDataset method). allele() (hail.representation.Variant method). alt (hail.representation.AltAllele attribute). alt() (hail.representation.Variant method). alt_allele() (hail.representation.Variant method). alt_alleles (hail.representation.Variant attribute). AltAllele (class in hail.representation). annotate() (hail.KeyTable method). annotate_alleles_expr() (hail.VariantDataset method). annotate_genotypes_expr() (hail.VariantDataset method). annotate_global() (hail.VariantDataset method). annotate_global_expr() (hail.VariantDataset method). annotate_samples_expr() (hail.VariantDataset method). annotate_samples_table() (hail.VariantDataset method). annotate_variants_db() (hail.VariantDataset method). annotate_variants_expr() (hail.VariantDataset method). annotate_variants_table() (hail.VariantDataset method). annotate_variants_vds() (hail.VariantDataset method). B. balding_nichols_model() (hail.HailContext method). C. cache() (hail.KeyTable method). (hail.VariantDataset method). Call (class in hail.representation). category() (hail.representation.AltAllele method). colkey_schema (hail.VariantDataset attribute). collect() (hail.KeyTable method). columns (hail.KeyTable attribute). complete_trios() (hail.representation.Pedigree method). concordance() (hail.VariantDataset method). contains() (hail.representation.Interval method). contig (hail.representation.Locus attribute). (hail.representation.Variant attribute). count() (hail.KeyTable method). (hail.VariantDataset method). count_variants() (hail.VariantDataset method). D. deduplicate() (hail.VariantDataset method). delete_va_attribute() (hail.VariantDataset method). dosage() (hail.representation.Genotype method). dp (hail.representation.Genotype attribute). drop() (hail.KeyTable method). drop_samples() (hail.VariantDataset method). drop_variants() (hail.VariantDataset method). E. end (hail.representation.Interval attribute). eval_expr() (hail.HailContext me,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/genindex.html:1411,cache,cache,1411,docs/0.1/genindex.html,https://hail.is,https://hail.is/docs/0.1/genindex.html,1,['cache'],['cache']
Performance,"by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:2169,perform,perform,2169,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,1,['perform'],['perform']
Performance,"c, vds._jvdf.toVDS()); return func(coerced_vds, *args, **kwargs); else:; raise TypeError(""genotype signature must be Genotype, but found '%s'"" % type(vds.genotype_schema)). return func(vds, *args, **kwargs). @decorator; def convertVDS(func, vds, *args, **kwargs):; if vds._is_generic_genotype:; if isinstance(vds.genotype_schema, TGenotype):; vds = VariantDataset(vds.hc, vds._jvdf.toVDS()). return func(vds, *args, **kwargs). vds_type = lazy(). [docs]class VariantDataset(object):; """"""Hail's primary representation of genomic data, a matrix keyed by sample and variant. Variant datasets may be generated from other formats using the :py:class:`.HailContext` import methods,; constructed from a variant-keyed :py:class:`KeyTable` using :py:meth:`.VariantDataset.from_table`,; and simulated using :py:meth:`~hail.HailContext.balding_nichols_model`. Once a variant dataset has been written to disk with :py:meth:`~hail.VariantDataset.write`,; use :py:meth:`~hail.HailContext.read` to load the variant dataset into the environment. >>> vds = hc.read(""data/example.vds""). :ivar hc: Hail Context.; :vartype hc: :class:`.HailContext`; """""". def __init__(self, hc, jvds):; self.hc = hc; self._jvds = jvds. self._globals = None; self._sample_annotations = None; self._colkey_schema = None; self._sa_schema = None; self._rowkey_schema = None; self._va_schema = None; self._global_schema = None; self._genotype_schema = None; self._sample_ids = None; self._num_samples = None; self._jvdf_cache = None. [docs] @staticmethod; @handle_py4j; @typecheck(table=KeyTable); def from_table(table):; """"""Construct a sites-only variant dataset from a key table. **Examples**. Import a text table and construct a sites-only VDS:. >>> table = hc.import_table('data/variant-lof.tsv', types={'v': TVariant()}).key_by('v'); >>> sites_vds = VariantDataset.from_table(table). **Notes**. The key table must be keyed by one column of type :py:class:`.TVariant`. All columns in the key table become variant annotations in the result.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:2004,load,load,2004,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['load']
Performance,"cache_memory_in_bytes=nullable(int)); def to_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; - **row_idx** (:py:data.`tint64`, key field) -- Row index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55455,cache,cache,55455,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['cache'],['cache']
Performance,"ce]; Calculate call concordance with another dataset. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:16650,perform,performs,16650,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['performs']
Performance,"cf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45542,load,load,45542,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ch should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101742,optimiz,optimization,101742,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95754,perform,performance,95754,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,ckMatrix from_numpy correctness bug; Bug fixes; Versioning. Version 0.2.89; Version 0.2.88; Version 0.2.87; Bug fixes. Version 0.2.86; Bug fixes; Performance improvements. Version 0.2.85; Bug fixes; New features. Version 0.2.84; Bug fixes; New features. Version 0.2.83; Bug fixes; New features; hailctl dataproc. Version 0.2.82; Bug fixes; New features; Performance Improvements; Python and Java Support; File Format. Version 0.2.81; hailctl dataproc. Version 0.2.80; New features; hailctl dataproc. Version 0.2.79; Bug fixes; New features. Version 0.2.78; Bug fixes; New features; Performance Improvements. Version 0.2.77; Bug fixes. Version 0.2.76; Bug fixes. Version 0.2.75; Bug fixes; New features; Performance improvements. Version 0.2.74; Bug fixes. Version 0.2.73; Bug fixes. Version 0.2.72; New Features; Bug fixes. Version 0.2.71; New Features; Bug fixes; hailctl dataproc. Version 0.2.70; Version 0.2.69; New Features; Bug fixes; hailctl dataproc. Version 0.2.68; Version 0.2.67; Critical performance fix. Version 0.2.66; New features. Version 0.2.65; Default Spark Version Change; New features; Performance improvements; Bug fixes. Version 0.2.64; New features; Bug fixes. Version 0.2.63; Bug fixes; Performance Improvements. Version 0.2.62; New features; Bug fixes; Performance improvements. Version 0.2.61; New features; Bug fixes. Version 0.2.60; New features; Bug fixes; hailctl dataproc. Version 0.2.59; Datasets / Annotation DB; hailctl dataproc. Version 0.2.58; New features; Bug fixes; Performance improvements; hailctl dataproc; Deprecations. Version 0.2.57; New features. Version 0.2.56; New features; Performance; Bug fixes; hailctl dataproc. Version 0.2.55; Performance; Bug fixes; File Format. Version 0.2.54; VCF Combiner; New features; Bug fixes. Version 0.2.53; Bug fixes. Version 0.2.52; Bug fixes. Version 0.2.51; Bug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:3693,perform,performance,3693,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ckpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:1224,perform,perform,1224,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['perform'],['perform']
Performance,"cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:18055,cache,cache,18055,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['cache'],['cache']
Performance,"combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13104,load,load,13104,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['load'],['load']
Performance,"compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138545,load,loadings,138545,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail q",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33234,perform,performs,33234,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performs']
Performance,"contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to :func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167518,queue,queue,167518,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['queue'],['queue']
Performance,"contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:3581,load,loaded,3581,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance,"count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88146,perform,performance,88146,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"cs for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:; >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). Note; For optimal performance, be sure to explicitly give the alternative (NA) the same type as the consequent (sa.pheno.bloodPressure). Notes; expr is in sample context so the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_table(table, root=None, expr=None, vds_key=None, product=False)[source]¶; Annotate samples with a key table.; Examples; To annotates samples using samples1.tsv with type imputation:; >>> table = hc.import_table('data/samples1.tsv', impute=True).key_by('Sample'); >>> vds_result = vds.annotate_samples_table(table, root='sa.pheno'). Given this file; $ cat data/samples1.tsv; Sample Height Status Age; PT-1234 154.1 ADHD 24; PT-1236 160.9 Control 19; PT-1238 NA ADHD 89; PT-1239 170.3 Control 55. the three new sample annotations are sa.pheno.Height: Double, sa.pheno.Sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:14261,perform,performance,14261,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"ctors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the curre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143324,load,loadings,143324,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"d VEP annotations to your VDS, make sure to add the initialization action ; :code:`gs://hail-common/vep/vep/vep85-init.sh` when starting your cluster. :param annotations: List of annotations to import from the database.; :type annotations: str or list of str . :param gene_key: Existing variant annotation used to map variants to gene symbols if importing gene-level ; annotations. If not provided, the method will add VEP annotations and parse them as described in the ; database documentation to obtain one gene symbol per variant.; :type gene_key: str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". # import modules needed by this function; import sqlite3. # collect user-supplied annotations, converting str -> list if necessary and dropping duplicates; annotations = list(set(wrap_to_list(annotations))). # open connection to in-memory SQLite database; conn = sqlite3.connect(':memory:'). # load database with annotation metadata, print error if not on Google Cloud Platform; try:; f = hadoop_read('gs://annotationdb/ADMIN/annotationdb.sql'); except FatalError:; raise EnvironmentError('Cannot read from Google Storage. Must be running on Google Cloud Platform to use annotation database.'); else:; curs = conn.executescript(f.read()); f.close(). # parameter substitution string to put in SQL query; like = ' OR '.join('a.annotation LIKE ?' for i in xrange(2*len(annotations))). # query to extract path of all needed database files and their respective annotation exprs ; qry = """"""SELECT file_path, annotation, file_type, file_element, f.file_id; FROM files AS f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE {}"""""".format(like). # run query and collect results in a file_path: expr dictionary; results = curs.execute(qry, [x + '.%' for x in annotations] + annotations).fetchall(). # all file_ids to be used; file_ids = list(set([x[4] for x in results])). # parameter substitution string; sub = ','.join('?' for x in file_ids). # query to fetch coun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:35207,load,load,35207,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['load']
Performance,"d a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78681,perform,perform,78681,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['perform']
Performance,"d as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:5727,perform,performance,5727,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['performance']
Performance,"d as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102547,load,loaded,102547,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['loaded']
Performance,"d format (with a “target” column) produces a table with two; fields:. interval (tinterval) - Row key. Same schema as above.; target (tstr). If reference_genome is defined AND the file has one field, intervals; are parsed with parse_locus_interval(). See the documentation for; valid inputs.; If reference_genome is NOT defined and the file has one field,; intervals are parsed with the regex `""([^:]*):(\d+)\-(\d+)""; where contig, start, and end match each of the three capture groups.; start and end match positions inclusively, e.g.; start <= position <= end.; For files with three or five fields, start and end match positions; inclusively, e.g. start <= position <= end. Parameters:. path (str) – Path to file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding (dict of (str, str)) – Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; **kwargs – Additional optional arguments to import_table() are valid; arguments here except: no_header, comment, impute, and; types, as these are used by import_locus_intervals(). Returns:; Table – Interval-keyed table. hail.methods.import_matrix_table(paths, row_fields={}, row_key=[], entry_type=dtype('int32'), missing='NA', min_partitions=None, no_header=False, force_bgz=False, sep=None, delimiter=None, comment=())[source]; Import tab-delimited file(s) as a MatrixTable.; Examples; Consider the following file containing counts from a RNA sequencing; dataset:; $ cat data/matrix1.tsv; Barcode Tissue Days GENE1 GENE2 GENE3 GENE4; TTAGCCA brain 1.0 0 0 1 0; ATCACTT kidney 5.5 3 0 2 0; CTCTTCT kidney 2.5 0 0 0 1; CTATATA brain 7.0 0 0 3 0. The field Days contains floating-point numbers and each of the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:21496,load,loaded,21496,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"d in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""; if self.entry.dtype != other.entry.dtype:; raise ValueError(; f'entry types differ:\n' f' left: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".j",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120899,perform,perform,120899,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['perform'],['perform']
Performance,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:38972,perform,performance,38972,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"d support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66643,perform,performance,66643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"dCalls()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(maf=nullable(strlike),; bounded=bool,; min=nullable(numeric),; max=nullable(numeric)); def ibd(self, maf=None, bounded=True, min=None, max=None):; """"""Compute matrix of identity-by-descent estimations. .. include:: requireTGenotype.rst. **Examples**. To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:. >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in [0.2, 0.9], using minor allele frequencies stored in; ``va.panel_maf``:. >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). **Notes**. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :py:meth:`~hail.VariantDataset.ibd` requires the dataset to be; bi-allelic (otherwise run :py:meth:`~hail.VariantDataset.split_multi` or otherwise run :py:meth:`~hail.VariantDataset.filter_multi`); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first. The resulting :py:class:`.KeyTable` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... :param maf: Expression for the minor allele frequency.; :type maf: str or None. :param bool bounded: Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]). :param min: Sample pairs with a PI_HAT below this value will; no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:82590,perform,perform,82590,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"d_prune(). If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:44461,queue,queue,44461,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"darray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 rows and 10 columns:. - ``bm[0, 0]`` is the element in row 0 and column 0 of ``bm``. - ``bm[0:1, 0]`` is a block matrix with 1 row, 1 column,; and element ``bm[0, 0]``. - ``bm[2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5460,cache,cache,5460,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['cache'],['cache']
Performance,"darray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:2924,perform,performed,2924,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['perform'],['performed']
Performance,"data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:142223,load,loadings,142223,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: array<str>,; medGenIds: array<s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60464,cache,cache,60464,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value di",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85962,perform,performance,85962,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46644,cache,cache,46644,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['cache'],['cache']
Performance,"declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45625,load,load,45625,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"dia.org/wiki/List_of_tz_database_time_zones; Currently, the parser implicitly uses the “en_US” locale.; This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39341,load,loadings,39341,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"doop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailct",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:81229,optimiz,optimizer,81229,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"ds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). Notes. In order to combine two datasets, these requirements must be met:. the samples must match; the variant annotation schemas must match (field order within structs matters).; the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with va.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of samples with called genotypes. AF; Double; Calculated alternate allele frequency (q). AC; Int; Count of alternate alleles. rHeterozygosity; Double; Proportion of heterozygotes. rHetHomVar; Double; Ratio of heterozygotes to homozygous alternates. rExpectedHetFrequency; Double; Expected rHeterozygosity based on HWE. pHWE; Do",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:171567,perform,performed,171567,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performed']
Performance,"e Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele wa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:169258,perform,performing,169258,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performing']
Performance,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90133,load,load,90133,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['load'],['load']
Performance,"e Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40153,load,loadings,40153,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"e called homozygous variant on the right. Parameters:. left (MatrixTable) – First dataset to compare.; right (MatrixTable) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:20696,load,loaded,20696,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loaded']
Performance,"e chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the fun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:9069,perform,performance,9069,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['performance']
Performance,"e eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167437,load,loadings,167437,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"e of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143512,load,loadings,143512,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['load'],['loadings']
Performance,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:11469,perform,performs,11469,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['performs']
Performance,"e pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23989,optimiz,optimization,23989,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['optimiz'],['optimization']
Performance,"e same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:99295,perform,perform,99295,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['perform'],['perform']
Performance,"e variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/ldpruned.variants"", ""v"")). Notes; Variants are pruned in each contig from smallest to largest start position. The LD pruning algorithm is as follows:; pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:75149,perform,performance,75149,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"e(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by ‘locus’ and ‘alleles’ from a BGEN file:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; Hail supports importing data from v1.2 of the BGEN file format.; Genotypes must be unphased and diploid, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:9516,load,load,9516,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"e, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir.StringTableReader(paths, min_partitions, force_bgz, force, file_per_partition); table_type = hl.ttable(global_type=hl.tstruct(), row_type=hl.tstruct(file=hl.tstr, text=hl.tstr), row_key=[]); string_table = Table(ir.TableRead(st_reader, _assert_type=table_type)); return s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:66158,load,load,66158,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"e.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the resu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40074,load,loadings,40074,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"e:; struct {; id: int32; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5beee034f0>; Index:; ['row']; --------------------------------------------------------. Keys need not be unique or non-missing, although in many applications they will be both.; When tables are joined in Hail, they are joined based on their keys. In order to join two tables, they must share the same number of keys, same key types (i.e. string vs integer), and the same order of keys.; Let’s look at a simple example of a join. We’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:3969,load,load,3969,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['load'],['load']
Performance,"e` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:76155,perform,performs,76155,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ean of an array of numbers with .mean(), find their max with; .max(), and so on.; But what if we wanted to compute the mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:10939,scalab,scalable,10939,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['scalab'],['scalable']
Performance,"eature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167772,load,loadings,167772,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"efault: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:176356,cache,cache,176356,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"efining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """"""; if len(delimiter) < 1:; raise ValueError('import_table: empty delimiter is not supported'). paths = wrap_to_list(paths); comment = wrap_to_list(comment); missing = wrap_to_list(missing). ht = hl.import_lines(paths, min_partitions, force_bgz, force). should_remove_line_expr = should_remove_line(; ht.text, filter=filter, comment=comment, skip_blank_lines=skip_blank_lines; ); if should_remove_line_expr is not None:; ht = ht.filter(should_remove_line_expr, keep=False). try:; if len(paths) <= 1:; # With zero or one files and no filters, the first row, if it exists must be in the first; # partiti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:60415,load,load,60415,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"egate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31278,perform,performs,31278,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['perform'],['performs']
Performance,"einberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the signifi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12076,perform,perform,12076,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['perform']
Performance,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:11968,perform,performing,11968,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performing']
Performance,"elding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18856,load,loadings,18856,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['load'],['loadings']
Performance,"ely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=Non",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:37410,load,load,37410,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45135,load,load,45135,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ence. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:1110,load,load,1110,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['load'],['load']
Performance,"ent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1672,perform,perform,1672,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['perform']
Performance,"ent; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100937,perform,performance,100937,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4335,load,load,4335,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"er for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2456,load,load,2456,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['load'],['load']
Performance,"erage age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:2072,load,load,2072,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['load'],['load']
Performance,"ero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103176,scalab,scalable,103176,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['scalab'],['scalable']
Performance,"ers:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitutio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45861,load,loaded,45861,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"ersampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:22613,load,loadings,22613,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1776,load,loaded,1776,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['load'],['loaded']
Performance,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2801,optimiz,optimized,2801,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"es a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the; filters row field. This annotation is a set<str> and can be; queried for filter membership with expressions like; ds.filters.contains(""VQSRTranche99.5...""). Variants that are flagged; as “PASS” will have no filters applied; for these variants,; hl.len(ds.filters) is 0. Thus, filtering to PASS variants; can be done with MatrixTable.filter_rows() as follows:; >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). Column Fields. s (tstr) – Column key. This is the sample ID. Row Fields. locus (tlocus or tstruct) – Row key. The; chromosome (CHROM field) and position (POS field). If reference_genome; is defined, the type will be tlocus parameterized by; reference_genome. Otherwise, the type will be a tstruct with; two fields: contig with type tstr and position with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:42798,perform,perform,42798,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['perform']
Performance,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:2636,queue,queueing,2636,index.html,https://hail.is,https://hail.is/index.html,2,['queue'],"['queue', 'queueing']"
Performance,"es the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178243,cache,cache,178243,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:2731,concurren,concurrent,2731,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['concurren'],['concurrent']
Performance,"ethod(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110279,cache,cache,110279,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['cache'],['cache']
Performance,"ethod(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_bad_ad=bool,; generic=bool,; call_fields=oneof(strlike, listof(strlike))); def import_vcf(self, path, force=False, force_bgz=False, header_file=None, min_partitions=None,; drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False,; call_fields=[]):; """"""Import VCF file(s) as variant dataset. **Examples**. >>> vds = hc.import_vcf('data/example2.vcf.bgz'). **Notes**. Hail is designed to be maximally compatible with files in the `VCF v4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :py:meth:`~hail.HailContext.import_vcf` takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as :ref:`Hadoop glob patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (*.vcf*) or block compressed; (*.vcf.bgz*). If you have a large compressed VCF that ends in *.vcf.gz*, it is likely that the file is actually block-compressed,; and you should rename the file to "".vcf.bgz"" accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the ``force`` optional parameter. However, this is not recommended -- all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:20729,load,load,20729,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"eturned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:75168,perform,perform,75168,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['perform']
Performance,"eturns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21870,perform,performance,21870,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"eturns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make inter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2081,load,load,2081,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['load'],['load']
Performance,"experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopSc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60068,cache,cache,60068,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['cache'],['cache']
Performance,"ey of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); retur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21526,load,loadings,21526,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"f projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:3331,load,loadings,3331,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"f terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4049,load,loadings,4049,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24778,optimiz,optimization,24778,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"f type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39883,load,loadings,39883,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"ference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102511,load,load,102511,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23926,cache,cache,23926,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['cache'],['cache']
Performance,"file={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the BashJob.declare_resource_group(); method. We then pass g.ofile as the output file root to run_gwas.py as that represents the temporary file; root given to all files in the resource group ({root} when declaring the resource group). Clumping By Chromosome; The second function performs clumping for a given chromosome. The input arguments are the Batch; for which to create a new BashJob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory becaus",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:7847,perform,performs,7847,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['performs']
Performance,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76341,perform,performance,76341,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['perform'],['performance']
Performance,"first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd',",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:19217,load,loadings,19217,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._convert_to_json(x). def _convert_to_json(self, x):; return x. def _from_json(self, s):; x = json.loads(s); return self._convert_from_json_na(x). def _convert_from_json_na(self, x, _should_freeze: bool = False):; if x is None:; return x; else:; return self._convert_from_json(x, _should_freeze). def _convert_from_json(self, x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:5224,load,loads,5224,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['load'],['loads']
Performance,"func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167581,queue,queue,167581,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['queue'],['queue']
Performance,"g``. :param str bed: PLINK BED file. :param str bim: PLINK BIM file. :param str fam: PLINK FAM file. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param str missing: The string used to denote missing values **only** for the phenotype field. This is in addition to ""-9"", ""0"", and ""N/A"" for case-control phenotypes. :param str delimiter: FAM file field delimiter regex. :param bool quantpheno: If True, FAM phenotype is interpreted as quantitative. :return: Variant dataset imported from PLINK binary file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importPlink(bed, bim, fam, joption(min_partitions), delimiter, missing, quantpheno). return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; drop_samples=bool,; drop_variants=bool); def read(self, path, drop_samples=False, drop_variants=False):; """"""Read .vds files as variant dataset. When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. :param path: VDS files to read.; :type path: str or list of str. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations; or gneotypes. :param bool drop_variants: If True, create samples-only variant; dataset (no variants or genotypes). :return: Variant dataset read from disk.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(; self,; self._jhc.readAll(jindexed_seq_args(path), drop_samples, drop_variants)). [docs] @handle_py4j; @typecheck_method(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:19081,load,loading,19081,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['loading']
Performance,"generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86449,perform,performance,86449,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70008,perform,performance,70008,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3523,perform,performance,3523,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"gin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223848,cache,cache,223848,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"gions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77382,perform,performance,77382,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:96885,perform,performant,96885,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['perform'],"['performance', 'performant']"
Performance,"h minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:165002,load,loadings,165002,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"hail.experimental.datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1007,load,load,1007,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['load'],['load']
Performance,"he FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23056,load,load,23056,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"he original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (mt._af < 1)). gt_norm = (mt._call.n_alt_alleles() - 2 * mt._af) / hl.sqrt(n_variants * 2 * mt._af * (1 - mt._af)). return mt.select_cols(scores=hl.agg.array_sum(mt._loadings * gt_norm)).cols(). def _get_expr_or_join(expr, source, other_source, loc):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1970,load,loadings,1970,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"he99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23247,load,load,23247,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"heck(; ds=oneof(MatrixTable, lambda: hl.vds.VariantDataset),; min_af=numeric,; max_af=numeric,; min_dp=int,; max_dp=int,; min_gq=int,; ref_AF=nullable(expr_float64),; ); def compute_charr(; ds: Union[MatrixTable, 'hl.vds.VariantDataset'],; min_af: float = 0.05,; max_af: float = 0.95,; min_dp: int = 10,; max_dp: int = 100,; min_gq: int = 20,; ref_AF: Optional[Float64Expression] = None,; ):; """"""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """""". # Determine whether the input data is in the VDS format; if not, convert matrixtable to VDS and extract only the variant call information; if isinstance(ds, hl.vds.VariantDataset):; mt = ds.variant_data; else:; mt = ds. if all(x in mt.entry for x in ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:60443,load,loaded,60443,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['load'],"['load', 'loaded']"
Performance,"hema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotations from Python objects. annotate_global_expr; Annotate global with expression. annotate_samples_expr; Annotate samples with expression. annotate_samples_table; Annotate samples with a key table. annotate_variants_db; Annotate variants using the Hail annotation database. annotate_variants_expr; Annotate variants with expression. annotate_variants_table; Annotate variants with a key table. annotate_variants_vds; Annotate variants with variant annotations from .vds file. cache; Mark this variant dataset to be cached in memory. concordance; Calculate call concordance with another variant dataset. count; Returns number of samples and variants in the dataset. count_variants; Count number of variants in variant dataset. deduplicate; Remove duplicate variants. delete_va_attribute; Removes an attribute from a variant annotation field. drop_samples; Removes all samples from variant dataset. drop_variants; Discard all variants, variant annotations and genotypes. export_gen; Export variant dataset as GEN and SAMPLE file. export_genotypes; Export genotype-level information to delimited text file. export_plink; Export variant dataset as PLINK2 BED, BIM and FAM. export_samples; Export sample information to delimited text file. export_variants; Export variant information to delimited text file. export_vcf; Export variant dataset as a .vcf or .vcf.bgz file. file_version; File version of variant dataset. filter_alleles; Filter a user-defined set of alternate alleles for each variant. filter_gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:2476,cache,cache,2476,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['cache'],"['cache', 'cached']"
Performance,"hile this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4204,load,loadings,4204,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"hose versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; iden",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90498,perform,performance,90498,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"i, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__,; reads a binary file of float64 values in row-major order, such as that; produced by `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__; or :meth:`BlockMatrix.tofile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. A NumPy ndarray must have type float64 for the output of; func:`numpy.tofile` to be a valid binary input to :meth:`.fromfile`.; This is not checked. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary input file.; n_rows: :obj:`int`; Number of rows.; n_cols: :obj:`int`; Number of columns.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`default_block_size`. See Also; --------; :meth:`.from_numpy`; """""". if not block_size:; block_size = BlockMatrix.default_block_size(). return cls(; BlockMatrixRead(BlockMatrixBinaryReader(uri, [n_rows, n_cols], block_size), _assert_type=_assert_type); ). [docs] @classmethod; @typecheck_method(ndarray=np.ndarray, block_size=nullable(int)); def from_numpy(cls, ndarray, block_size=None):; """"""Distributes a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__; as a b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:11121,load,load,11121,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['load'],['load']
Performance,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178055,cache,cache,178055,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2082,concurren,concurrent,2082,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,2,['concurren'],['concurrent']
Performance,"ime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79611,perform,performance,79611,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"imental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:82728,perform,performance,82728,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. [5]:. users.aggregate(hl.agg.stats(users.age)). [5]:. Struct(mean=34.05196182396607, stdev=12.186273150937211, min=7.0, max=73.0, n=943, sum=32111.0). counter; What about non-numeric data, like the occupation field?; counter is modeled on the Python Counter object: it",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:2684,load,load,2684,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['load'],['load']
Performance,"index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:79484,cache,cache,79484,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['cache'],['cache']
Performance,"info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:65858,load,load,65858,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"ing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1284,load,load,1284,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['load'],['load']
Performance,"ing the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143337,load,loadings,143337,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"ing, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list); if is_chained and any(len(lst) ==",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12210,perform,performance,12210,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['performance']
Performance,"ingular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t = None; return hl.eval(g.eigenvalues), scores, None if t is None else t.drop('eigenvalues', 'scores'). class TallSkinnyMatrix:; def __init__(self, blo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7574,load,loadings,7574,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"ion on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str or list of str. Returns:Key table with dropped columns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:5138,cache,cache,5138,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['cache'],['cache']
Performance,"ions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:22550,load,load,22550,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['load']
Performance,"its, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; The number of entries must be less than \(2^{31}\). Parameters:; uri (str, optional) – URI of binary output file. See also; to_numpy(). tree_matmul(b, *, splits, path_prefix=None)[source]; Matrix multiplication in situations with large inner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:43533,load,load,43533,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['load']
Performance,"ive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:2294,load,load,2294,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['load'],['load']
Performance,"ix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). classmethod from_ndarray(ndarray_expression, block_size=4096)[source]; Create a BlockMatrix from an ndarray. classmet",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:22125,concurren,concurrently,22125,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['concurren'],['concurrently']
Performance,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7018,load,loadings,7018,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20717,load,loadings,20717,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"ks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86009,perform,performance,86009,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"l an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the max_size parameter to skip; groups larger than max_size. Warning; skat() considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which all covariates are defined.; For each row, missing values of x are mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:79389,scalab,scalable,79389,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['scalab'],['scalable']
Performance,"l pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:39300,perform,performance,39300,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72572,perform,performing,72572,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performing']
Performance,"le, Table, Table, Table). hail.methods.de_novo(mt, pedigree, pop_frequency_prior, *, min_gq=20, min_p=0.05, max_parent_ab=0.05, min_child_ab=0.2, min_dp_ratio=0.1, ignore_in_sample_allele_frequency=False)[source]; Call putative de novo events from trio data. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Call de novo events:; >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; This method assumes the GATK high-throughput sequencing fields exist:; GT, AD, DP, GQ, PL.; This method replicates the functionality of Kaitlin Samocha’s de novo; caller. The version; corresponding to git commit bde3e40 is implemented in Hail with her; permission and assistance.; This method produces a Table with the following fields:. locus (locus) – Variant locus.; alleles (array<str>) – Variant alleles.; id (str) – Proband sample ID.; prior (float64) – Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; pop_frequency_prior parameter, and the global prior; 1 / 3e7. If the ignore_in_sample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:52807,throughput,throughput,52807,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"le. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; Table – Repartitioned table. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; The data type of the row struct:; >>> table1.row.dtype; d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:55761,perform,performance,55761,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['perform'],['performance']
Performance,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5285,optimiz,optimizer,5285,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['optimiz'],['optimizer']
Performance,"line can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datase",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:65925,perform,performed,65925,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['perform'],['performed']
Performance,"list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:117955,load,load,117955,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"lit before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:44522,queue,queue,44522,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143241,load,loadings,143241,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"loat`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.Table`; Table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter(hl.rand_bool(p, seed)). [docs] @typecheck_method(n=int, shuffle=bool); def repartition(self, n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:92697,perform,performance,92697,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['perform'],['performance']
Performance,"ls of genes or transcripts. export_entries_by_col(mt, path[, ...]); Export entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4270,load,load,4270,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90370,perform,performance,90370,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ls_by()'""; ); Env.hc()._warn_cols_order = False. return Table(ir.MatrixColsTable(self._mir)). [docs] def entries(self) -> Table:; """"""Returns a matrix in coordinate table form. Examples; --------; Extract the entry table:. >>> entries_table = dataset.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:88772,perform,performance,88772,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['perform'],['performance']
Performance,"lters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cann",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23328,load,load,23328,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"m old to new sample IDs. :return: Dataset with remapped sample IDs.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.renameSamples(mapping); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(num_partitions=integral,; shuffle=bool); def repartition(self, num_partitions, shuffle=True):; """"""Increase or decrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:192087,perform,performance,192087,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBack",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6686,load,loadings,6686,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; ---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20385,load,loadings,20385,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"mal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18553,load,loadings,18553,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['load'],['loadings']
Performance,"many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101798,perform,performance,101798,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"minator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:30598,load,loadings,30598,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,"ming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162164,load,loadings,162164,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"mmended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:21062,perform,performance,21062,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['perform'],['performance']
Performance,"model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only the top n_eigs eigenvectors. Alternatively, specify dropped_variance_fraction to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, dropped_variance_fraction=0.01 will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two.; Further background; For the history and mathematics of linear mixed models in genetics, including FastLMM, see Christoph Lippert’s PhD thesis. For an investigation of various approaches to defining kinship, see Comparison of Methods to Account for Relatedness ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:106649,perform,performance,106649,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"mple_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:48011,load,load,48011,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"mples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include after the locus. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89697,load,load,89697,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['load'],['load']
Performance,"mported from a text file or Spark DataFrame with import_table(); or from_dataframe(), generated from a variant dataset; with aggregate_by_key(), make_table(),; samples_table(), or variants_table().; In the examples below, we have imported two key tables from text files (kt1 and kt2).; >>> kt1 = hc.import_table('data/kt_example1.tsv', impute=True). ID; HT; SEX; X; Z; C1; C2; C3. 1; 65; M; 5; 4; 2; 50; 5. 2; 72; M; 6; 3; 2; 61; 1. 3; 70; F; 7; 3; 10; 81; -5. 4; 60; F; 8; 2; 11; 90; -10. >>> kt2 = hc.import_table('data/kt_example2.tsv', impute=True). ID; A; B. 1; 65; cat. 2; 72; dog. 3; 70; mouse. 4; 60; rabbit. Variables:hc (HailContext) – Hail Context. Attributes. columns; Names of all columns. key; List of key columns. num_columns; Number of columns. schema; Table schema. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate columns programmatically. annotate; Add new columns computed from existing columns. cache; Mark this key table to be cached in memory. collect; Collect table to a local list. count; Count the number of rows. drop; Drop columns. exists; Evaluate whether a boolean expression is true for at least one row. expand_types; Expand types Locus, Interval, AltAllele, Variant, Genotype, Char, Set and Dict. explode; Explode columns of this key table. export; Export to a TSV file. export_cassandra; Export to Cassandra. export_elasticsearch; Export to Elasticsearch. export_mongodb; Export to MongoDB. export_solr; Export to Solr. filter; Filter rows. flatten; Flatten nested Structs. forall; Evaluate whether a boolean expression is true for all rows. from_dataframe; Convert Spark SQL DataFrame to key table. from_pandas; Convert Pandas DataFrame to key table. from_py. import_bed; Import a UCSC .bed file as a key table. import_fam; Import PLINK .fam file into a key table. import_interval_list; Import an interval list file in the GATK standard format. indexed; Add the numerical index of each row as a new column.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:1504,cache,cache,1504,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,['cache'],"['cache', 'cached']"
Performance,"n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:101280,optimiz,optimization,101280,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['optimiz'],['optimization']
Performance,"n requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.name; ); ). return _func(""getReferenceSequence"", tstr, contig, position, before, after, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_genome_type); def is_valid_contig(contig, reference_genome='default') -> BooleanExpression:; """"""Returns ``True`` if `contig` is a valid contig name in `reference_genome`. Examples; --------. >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :class:`.BooleanExpression`; """"""; return _func(""isValidContig"", tbool, contig, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:161576,load,loaded,161576,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['loaded']
Performance,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42316,cache,cache,42316,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79351,optimiz,optimizer,79351,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"n_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; path = new_temp_file(); cls.write_from_entry_e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14571,concurren,concurrently,14571,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['concurren'],['concurrently']
Performance,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:18573,cache,cache,18573,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,2,"['cache', 'optimiz']","['cache', 'optimize']"
Performance,"ner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apach",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:79358,cache,cache,79358,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['cache'],['cache']
Performance,"nfinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr :",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6238,load,loadings,6238,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6021,load,load,6021,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['load'],['load']
Performance,"ng glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=None, _create_row_uids=False, _create_col_uids=False)[source]; Import VCF file(s) as a MatrixTable.; Examples; Import a standard bgzipped VCF with GRCh37 as the reference genome.; >>> ds = hl.import_vcf('data/example2.vcf.bgz', reference_genome='GRCh37'). Impo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:37696,load,load,37696,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ng time; Estimating cost. Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. Howe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:1221,load,load,1221,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['load'],['load']
Performance,"nment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - Str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:5620,cache,cache,5620,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7917,perform,performs,7917,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['performs']
Performance,"ns. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175075,cache,cache,175075,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"nt dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry grou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:130027,perform,performing,130027,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performing']
Performance,"nt_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:19453,cache,cache,19453,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['cache'],['cache']
Performance,"nterval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid arguments here except:; `no_header`, `delimiter`, `impute`, `skip_blank_lines`, `types`, and `comment` as these; are used by import_bed. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". # UCSC BED spec defined here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1. t = import_table(; path,; no_header=True,; delimiter=r""\s+"",; impute=False,; skip_blank_lines=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; comment=[""""""^browser.*"""""", """"""^track.*"""""", r""""""^\w+=(""[\w\d ]+""|\d+).*""""""],; **kwargs,; ). if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:30662,load,loaded,30662,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['loaded']
Performance,"nterval:; >>> vds_result = vds.filter_intervals(Interval.parse('17:38449840-38530994')). Another way of writing this same query:; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove varian",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54206,perform,performs,54206,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ntly on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute ro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6365,load,loadings,6365,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"nts considered; - **nCalled** (*Long*) -- Number of variants with a genotype call; - **expectedHoms** (*Double*) -- Expected number of homozygotes; - **observedHoms** (*Long*) -- Observed number of homozygotes. :param float maf_threshold: Minimum minor allele frequency threshold. :param bool include_par: Include pseudoautosomal regions. :param float female_threshold: Samples are called females if F < femaleThreshold. :param float male_threshold: Samples are called males if F > maleThreshold. :param str pop_freq: Variant annotation for estimate of MAF.; If None, MAF will be computed. :return: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop_freq)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(right=vds_type); def join(self, right):; """"""Join two variant datasets. **Notes**. This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self). The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). :param right: right-hand variant dataset; :type right: :py:class:`.VariantDataset`. :return: Joined variant dataset; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.join(right._jvds)). [docs] @handle_py4j; @typecheck(datasets=tupleof(vds_type)); def union(*datasets):; """"""Take the union of datasets vertically (include all variants). **Examples**. .. testsetup::. vds_autosomal = vds; vds_chromX = vds; vds_chromY = vds. Union two datasets:. >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:. >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:. >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = Va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:90574,perform,performs,90574,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performs']
Performance,"o partition the GVCF files. The same partitioning is used; for all GVCF files. Finer partitioning yields more parallelism but less work per task.; gvcf_info_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:5329,load,load,5329,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['load'],['load']
Performance,"oadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34155,perform,performs,34155,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['perform'],['performs']
Performance,"ocally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the inf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:4060,scalab,scalable,4060,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"ocally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regard to memory and Hadoop replication errors. Parameters:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:45976,queue,queue,45976,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"ocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; N",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46795,concurren,concurrently,46795,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['concurren'],['concurrently']
Performance,"ocus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. GT (tcall) – The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; dosage (tfloat64) – The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See also; index_bgen(). Parameters:. path (str or list of str) – BGEN file(s) to read.; entry_fields (list of str) – List of entry fields to cre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:11018,perform,performance,11018,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['performance']
Performance,"od on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85489,perform,perform,85489,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['perform']
Performance,"og; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93519,optimiz,optimizer,93519,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"ogle cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; ); def from_fasta_file(cls, name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X ch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10975,load,loaded,10975,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['load'],['loaded']
Performance,"oject(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1807,load,loadings,1807,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"ollowing four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using linear regression on the maximum genotype per gene. Here va.genes is a variant; annotation of type Set[String] giving the set of genes containing the variant (see Extended example below; for a deep dive):; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:83908,perform,perform,83908,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['perform']
Performance,"om the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97129,perform,performance,97129,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performance']"
Performance,"ome, min_match=0.95, include_strand=False):; """"""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:170350,load,load,170350,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['load']
Performance,"ome=reference_genome_type,; min_match=builtins.float,; include_strand=builtins.bool,; ); def liftover(x, dest_reference_genome, min_match=0.95, include_strand=False):; """"""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:170267,load,loaded,170267,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['loaded']
Performance,"ompute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; MatrixTable – Matrix table where each entry corresponds to an entry in the block matrix. to_ndarray()[source]; Collects a BlockMatrix into a local hail ndarray expression on driver. This should not; be done for large matrices. Returns:; NDArrayExpression. to_numpy(_force_blocking=False)[source]; Collects the block matrix into a NumPy ndarray.; Examples; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; The resulting ndarray will have the same shape as the block matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:40343,cache,cache,40343,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"on Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1173,queue,queued,1173,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['queue'],['queued']
Performance,"on used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is passed, a list is returned.; The namespace of the expressions includes one aggregable for each column; of the key table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22343,cache,cache,22343,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['cache'],['cache']
Performance,"on. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configurati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83628,optimiz,optimizer,83628,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"onger. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Genotype>`. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped. If ``generic`` equals True, the genotype schema is a :py:class:`~hail.type.TStruct` with field names equal to the IDs of the FORMAT fields.; The ``GT`` field is automatically read in as a :py:class:`~hail.type.TCall` type. To specify additional fields to import as a; :py:class:`~hail.type.TCall` type, use the ``call_fields`` parameter. All other fields are imported as the type specified in the FORMAT header field. An example genotype schema after importing a VCF with ``generic=True`` is. .. code-block:: text. Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. .. warning::. - The variant dataset generated with ``generic=True`` will have significantly slower performance. - Not all :py:class:`.VariantDataset` methods will work with a generic genotype schema. - The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. :py:meth:`~hail.HailContext.import_vcf` does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant. Since Hail's genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If ``generic=False``, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS ""genotype schema. Below are two example haploid genotypes and diploid equivalents that Hail sees. .. code-block:: text. Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:22593,perform,performance,22593,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['perform'],['performance']
Performance,"ons.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. No Data (missing variant); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_right_homvar = summary[2][4]; >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:30627,perform,performs,30627,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance,"ool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1453,concurren,concurrent,1453,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,1,['concurren'],['concurrent']
Performance,"or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39943,load,loadings,39943,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27164,perform,performance,27164,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:130282,optimiz,optimization,130282,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['optimiz'],['optimization']
Performance,"org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Reparti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107658,perform,performance,107658,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['perform'],['performance']
Performance,ormance regression fix; Performance; Bug fixes; New features; Cheat sheets. Version 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail com,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:6478,perform,performance,6478,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ort Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_reg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1655,load,load,1655,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['load'],['load']
Performance,"ory where rectangles were written.; binary: :obj:`bool`; If true, reads the files as binary, otherwise as text delimited. Returns; -------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72330,load,loadtxt,72330,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['load'],['loadtxt']
Performance,"otate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v in partial.items():; if full[k] != v:; return False; return True. if point_type == k_type[0]:; needs_wrapper = True; k_name = k_type.f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:12580,latency,latency,12580,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,1,['latency'],['latency']
Performance,"otations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a wast",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23528,load,load,23528,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"ow(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Doubl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:24438,load,loadings,24438,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['load'],['loadings']
Performance,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:104041,optimiz,optimizer,104041,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['optimiz'],['optimizer']
Performance,"pectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more effi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:168458,load,loadings,168458,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"pects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:29856,cache,cache,29856,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8191,perform,perform,8191,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['perform'],['perform']
Performance,"ple, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive functi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39567,load,loadings,39567,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"plied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:24944,load,load,24944,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"porated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18125,load,loadings,18125,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"port_vcf('data/example3.vcf.bgz', generic=True, call_fields=['GTA']); ... .annotate_genotypes_expr('g = g.GTA.toGenotype()')). Notes; annotate_genotypes_expr() evaluates the expression given by expr and assigns; the result of the right hand side to the annotation path specified by the left-hand side (must; begin with g). This is analogous to annotate_variants_expr() and; annotate_samples_expr() where the annotation paths are va and sa respectively.; expr is in genotype context so the following symbols are in scope:. g: genotype annotation; v (Variant): Variant; va: variant annotations; s (Sample): sample; sa: sample annotations; global: global annotations. For more information, see the documentation on writing expressions; and using the Hail Expression Language. Warning. If the resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:11292,perform,performance,11292,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"pr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function; \(f(x) = 0 + 1 + \dots + x\):; >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40290,load,loadings,40290,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"pr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1552,load,loadings,1552,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"put KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:5075,cache,cache,5075,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,['cache'],"['cache', 'cached']"
Performance,"r (CallExpression); mother (CallExpression); child (CallExpression). Returns:; Int32Expression. hail.expr.functions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_refer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:22476,load,loaded,22476,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['loaded']
Performance,"r Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:2358,load,load,2358,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['load'],['load']
Performance,"r some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5433,load,loaded,5433,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance,"r) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.; Examples; >>> vds = hc.import_vcf('data/example2.vcf.bgz'). Notes; Hail is designed to be maximally compatible with files in the VCF v4.2 spec.; import_vcf() takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as Hadoop glob patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (.vcf) or block compressed; (.vcf.bgz). If you have a large compressed VCF that ends in .vcf.gz, it is likely that the file is actually block-compressed,; and you should rename the file to “.vcf.bgz” accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the force optional parameter. However, this is not recommended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:19392,load,load,19392,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"r); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __mul__(other)[source]; Positionally multiply by a ndarray or a scalar. Parameters:; other (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:3016,perform,performs,3016,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['perform'],['performs']
Performance,r.aggregators). array_windows() (in module hail.linalg.utils). ArrayExpression (class in hail.expr). ArrayNumericExpression (class in hail.expr). asc() (in module hail). ascertainment_bias() (in module hail.experimental.ldscsim). available_datasets (hail.experimental.DB property). B. balding_nichols_model() (in module hail.methods). binarize() (in module hail.experimental.ldscsim). binary_search() (in module hail.expr.functions). bind() (in module hail.expr.functions). binom_test() (in module hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method).,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:12758,cache,cache,12758,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['cache'],['cache']
Performance,"r; to the specified annotation path.; The expr argument expects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:29787,cache,cache,29787,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['cache'],"['cache', 'cached']"
Performance,"rabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56341,optimiz,optimized,56341,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimized']
Performance,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22593,perform,performance,22593,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['perform'],['performance']
Performance,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45993,perform,performance,45993,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"re checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22343,load,loaded,22343,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['load'],['loaded']
Performance,"re=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expressio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89667,load,loaded,89667,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['load'],['loaded']
Performance,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95220,perform,performance,95220,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27476,optimiz,optimization,27476,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9195,load,loadings,9195,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,1,['load'],['loadings']
Performance,"ri):; """"""Collects and writes data to a binary file. Examples; --------; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') # doctest: +SKIP. To create a :class:`numpy.ndarray` of the same dimensions:. >>> a = np.fromfile('/local/file').reshape((10, 20)) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__,; produces a binary file of float64 values in row-major order, which can; be read by functions such as `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__; (if a local file) and :meth:`BlockMatrix.fromfile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary output file. See Also; --------; :meth:`.to_numpy`; """"""; hl.current_backend().validate_file(uri). _check_entries_size(self.n_rows, self.n_cols). writer = BlockMatrixBinaryWriter(uri); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(_force_blocking=bool); def to_numpy(self, _force_blocking=False):; """"""Collects the block matrix into a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__. Examples; --------; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; -----; The resulting ndarray will have the same shape as the block matrix. Returns; -------; :class:`numpy.ndarray`; """"""; from hail.backend.service_backend import ServiceBackend. if self.n_rows * self.n_cols > 1 << 31 or _force_blocking:; path = new_temp_file(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:39142,load,load,39142,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['load'],['load']
Performance,"riant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Remove all variants which overlap an interval in a UCSC BED file:; >>> kt = KeyTable.import_bed('data/file2.bed'); >>> filtered_vds = vds.filter_variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:59906,latency,latency,59906,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['latency'],['latency']
Performance,"rite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; perfor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45748,load,loaded,45748,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['loaded']
Performance,"rough files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40978,cache,cache,40978,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"rove; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89690,perform,performance,89690,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"rs (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:165440,load,loadings,165440,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"rs or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:36365,load,load,36365,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ruct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:168051,load,loadings,168051,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"rval.parse('17:38449840-38530994')); ; Another way of writing this same query:; ; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.In",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:74346,perform,performs,74346,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6844,latency,latency,6844,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['latency'],['latency']
Performance,"s : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:59652,load,load,59652,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"s :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167544,load,loadings,167544,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"s and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; 'temp_path': self._temp_path,; 'reference_genome': str(self._reference_genome),; 'dataset_type': self._dataset_type,; 'gvcf_type': self._gv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13238,load,loaded,13238,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['load'],['loaded']
Performance,"s at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:9224,load,load,9224,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"s interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval converted to `dest_reference_genome`.; """""". if not 0.0 <= min_match <= 1.0:; raise TypeError(""'liftover' requires 'min_match' is in the range [0, 1]. Got {}"".format(min_match)). if isinstance(x.dtype, tlocus):; rg = x.dtype.reference_genome; method_name = ""liftoverLocus""; rtype = tstruct(result=tlocus(dest_reference_genome), is_negative_strand=tbool); else:; rg = x.dtype.point_type.reference_genome; method_name = ""liftoverLocusInterval""; rtype = tstruct(result=tinterval(tlocus(dest_reference_genome)), is_negative_strand=tbool). if not rg.has_liftover(dest_reference_genome.name):; raise TypeError(; """"""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file."""""".format(rg.name, dest_reference_genome.name); ). expr = _func(method_name, rtype, x, to_expr(min_match, tfloat64)); if not include_strand:; expr = expr.result; return expr. [docs]@typecheck(; f=func_spec(1, expr_float64),; min=expr_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expressio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:172050,load,load,172050,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['load']
Performance,"s method. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. Returns:Variant dataset with no genotype metadata. Return type:VariantDataset. ibd(maf=None, bounded=True, min=None, max=None)[source]¶; Compute matrix of identity-by-descent estimations. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:; >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in [0.2, 0.9], using minor allele frequencies stored in; va.panel_maf:; >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). Notes; The implementation is based on the IBD algorithm described in the PLINK; paper.; ibd() requires the dataset to be; bi-allelic (otherwise run split_multi() or otherwise run filter_multi()); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first.; The resulting KeyTable entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:; maf (str or None) – Expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]).; min (float or None) – Sample pairs with a PI_HAT below this v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:66954,perform,perform,66954,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['perform']
Performance,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7419,load,loadings,7419,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21118,load,loadings,21118,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"s of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and transcript_ids is None:; raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids'); ht = _load_gencode_gtf(gtf_file, reference_genome); criteria = []; if gene_symbols:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:7850,load,load,7850,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['load'],['load']
Performance,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77577,perform,performance,77577,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"s().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_itera",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23359,load,loadings,23359,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5153,perform,performance,5153,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['perform'],['performance']
Performance,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1996,scalab,scalable,1996,index.html,https://hail.is,https://hail.is/index.html,1,['scalab'],['scalable']
Performance,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5172,optimiz,optimizer,5172,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['optimiz'],['optimizer']
Performance,"s; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:data:`.tfloat32` or :py:data:`.tfloat64`. Default:; :py:data:`.tfloat64`.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102797,load,loaded,102797,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['loaded']
Performance,"s=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is computed as half the mean of the non-missing entries of column :math:`j`. Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} = \\frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value :math:`1/m` for variants in Hardy-Weinberg e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162760,load,loadings,162760,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"s` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101817,load,load,101817,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"scent estimates. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:; >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in \([0.2, 0.9]\), using minor allele frequencies stored in; the row field panel_maf:; >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; The dataset must have a column field named s which is a StringExpression; and which uniquely identifies a column.; The implementation is based on the IBD algorithm described in the PLINK; paper.; identity_by_descent() requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first.; The resulting Table entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:. dataset (MatrixTable) – Variant-keyed and sample-keyed MatrixTable containing genotype information.; maf (Float64Expression, optional) – Row-indexed expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2, and PI_HAT to take; on biologically meani",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:4003,perform,perform,4003,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['perform'],['perform']
Performance,"self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self):; return await Env.backend()._async_execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). @typecheck_method(caller=str, row=expr_struct()); def _select(self, caller, row) -> 'Table':; analyze(caller, row, self._row_indices); base, cleanup = self._process_joins(row); return cleanup(Table(ir.TableMapRows(base._tir, row._ir))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'Table':; base",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:13155,load,loaded,13155,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['load'],['loaded']
Performance,"set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is compu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138452,load,loadings,138452,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"sible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in additi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:54795,perform,performance,54795,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"sing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100848,perform,performance,100848,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"sion 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:6579,perform,performance,6579,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,sions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77518,perform,performance,77518,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"spect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143715,load,loadings,143715,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['load'],['loadings']
Performance,"ssage from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl datapr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:62530,perform,performance,62530,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ssary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; - `dosage` (:py:data:`.tfloat64`) -- The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See Also; --------; :func:`.index_bgen`. Parameters; ----------; path : :class:`s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:41496,perform,performance,41496,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['perform'],['performance']
Performance,ssion method). king() (in module hail.methods). L. lambda_gc() (in module hail.methods). last() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). ld_matrix() (in module hail.methods). ld_prune() (in module hail.methods). ld_score() (in module hail.experimental). ld_score_regression() (in module hail.experimental). len() (in module hail.expr.functions). length() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). lengths (hail.genetics.ReferenceGenome property). lgt_to_gt() (in module hail.vds). liftover() (in module hail.expr.functions). linear_mixed_model() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). ma,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:32872,load,load,32872,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['load'],['load']
Performance,"ssion model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chain",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12126,perform,perform,12126,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['perform'],['perform']
Performance,"symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. 0. No Data (missing variant); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var; ; The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; ; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_righ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:41428,perform,performs,41428,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performs']
Performance,"t can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162332,load,loadings,162332,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"t change the schema of the table; it is a method of; filtering the table to keys not present in another table.; To restrict to keys present in other, use semi_join().; Examples; >>> table_result = table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Exa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:15171,cache,cache,15171,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['cache'],['cache']
Performance,"t for a Hail Table or MatrixTable. Parameters; ----------; t_path : str; Path to the Hail Table or MatrixTable files. Returns; -------; :class:`bokeh.plotting.figure` or :class:`bokeh.models.layouts.Column`; """""". def get_rows_data(rows_files):; file_sizes = []; partition_bounds = []; parts_file = [x['path'] for x in rows_files if x['path'].endswith('parts')]; if parts_file:; parts = hadoop_ls(parts_file[0]); for i, x in enumerate(parts):; index = x['path'].split(f'{parts_file[0]}/part-')[1].split('-')[0]; if i < len(parts) - 1:; test_index = parts[i + 1]['path'].split(f'{parts_file[0]}/part-')[1].split('-')[0]; if test_index == index:; continue; file_sizes.append(x['size_bytes']); metadata_file = [x['path'] for x in rows_files if x['path'].endswith('metadata.json.gz')]; if metadata_file:; with hadoop_open(metadata_file[0], 'rb') as f:; rows_meta = json.load(f); try:; partition_bounds = [; (; x['start']['locus']['contig'],; x['start']['locus']['position'],; x['end']['locus']['contig'],; x['end']['locus']['position'],; ); for x in rows_meta['jRangeBounds']; ]; except KeyError:; pass; return partition_bounds, file_sizes. def scale_file_sizes(file_sizes):; min_file_size = min(file_sizes) * 1.1; total_file_size = sum(file_sizes); all_scales = [('T', 1e12), ('G', 1e9), ('M', 1e6), ('K', 1e3), ('', 1e0)]; for overall_scale, overall_factor in all_scales:; if total_file_size > overall_factor:; total_file_size /= overall_factor; break; for scale, factor in all_scales:; if min_file_size > factor:; file_sizes = [x / factor for x in file_sizes]; break; total_file_size = f'{total_file_size:.1f} {overall_scale}B'; return total_file_size, file_sizes, scale. files = hadoop_ls(t_path). rows_file = [x['path'] for x in files if x['path'].endswith('rows')]; entries_file = [x['path'] for x in files if x['path'].endswith('entries')]; success_file = [x['modification_time'] for x in files if x['path'].endswith('SUCCESS')]. metadata_file = [x['path'] for x in files if x['path'].endswith('met",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:4615,load,load,4615,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,1,['load'],['load']
Performance,"t import *. import plotly. Loading BokehJS ... The Hail team has implemented a plotting module for hail based on the very popular ggplot2 package from R’s tidyverse. That library is very fully featured and we will never be quite as flexible as it, but with just a subset of its functionality we can make highly customizable plots. The Grammar of Graphics; The key idea here is that there’s not one magic function to make the plot you want. Plots are built up from a set of core primitives that allow for extensive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:1782,load,load,1782,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['load'],['load']
Performance,"t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:21071,load,loadings,21071,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['load'],['loadings']
Performance,"t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struct{x: int32, y: str}> |; +--------+---------------------------------+; | ""bar"" | [(2,""B""),(-3,""C"")] |; | ""foo"" | [(4,""A"")] |; | ""quam"" | [(0,""D"")] |; +--------+---------------------------------+. Notes; The order of the values array is not guaranteed. Parameters:; name (str) – Field name for all values per key. Returns:; Table. count()[source]; Count the number of rows in the table.; Examples; Count the number of rows in a table loaded from ‘data/kt_example1.tsv’. Each line of the TSV; becomes one row in the Hail Table.; >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns:; int – The number of rows in the table. describe(handler=<built-in function print>, *, widget=False)[source]; Print information about the fields in the table. Note; The widget argument is experimental. Parameters:. handler (Callable[[str], None]) – Handler function for returned string.; widget (bool) – Create an interactive IPython widget. distinct()[source]; Deduplicate keys, keeping exactly one row for each unique key. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'a': 'foo', 'b': 1},; ... {'a': 'bar', 'b': 5},; ... {'a': 'bar', 'b': 2}],; ... hl.tstruct(a=hl.tstr, b=hl.tint32),; ... key='a'). >>> t1.show(); +-------+-------+; | a | b |; +-------+-------+; | str | int32 |; +-------+-------+; | ""bar"" | 5 |; | ""bar"" | 2 |; | ""foo"" | 1 |; +-------+-------+. >>> t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:17839,load,loaded,17839,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['load'],['loaded']
Performance,"t-wise square root. sum; Sums array elements over one or both axes. svd; Computes the reduced singular value decomposition. to_matrix_table_row_major; Returns a matrix table with row key of row_idx and col key col_idx, whose entries are structs of a single field element. to_ndarray; Collects a BlockMatrix into a local hail ndarray expression on driver. to_numpy; Collects the block matrix into a NumPy ndarray. to_table_row_major; Returns a table where each row represents a row in the block matrix. tofile; Collects and writes data to a binary file. tree_matmul; Matrix multiplication in situations with large inner dimension. unpersist; Unpersists this block matrix from memory/disk. write; Writes the block matrix. write_from_entry_expr; Writes a block matrix from a matrix table entry expression. property T; Matrix transpose. Returns:; BlockMatrix. abs()[source]; Element-wise absolute value. Returns:; BlockMatrix. property block_size; Block size. Returns:; int. cache()[source]; Persist this block matrix in memory.; Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; BlockMatrix – Cached block matrix. ceil()[source]; Element-wise ceiling. Returns:; BlockMatrix. checkpoint(path, overwrite=False, force_row_major=False, stage_locally=False)[source]; Checkpoint the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before checkpointing.; If False, checkpoint blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static default_block_size()[source]; Default block side length. densify()[source]; Restore all dropped blocks as explicit blocks of zeros. Returns:; Bl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:10509,cache,cache,10509,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:139765,perform,performs,139765,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performs']
Performance,"t_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = key_and_value[0]; value = key_and_value[1]; return (key, value.replace('""|;\\$', '')). return hl.dict(unparsed_attributes.split('; ').map(parse_attribute)). ht = ht.annotate(attribute=parse_attributes(ht['attribute'])). ht = ht.checkpoint(new_temp_file()). attributes = ht.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:4100,load,load,4100,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['load'],['load']
Performance,"t_table('data/sample_mapping.txt'); >>> mapping_dict = {row.old_id: row.new_id for row in mapping_table.collect()}; >>> vds_result = vds.rename_samples(mapping_dict). Parameters:mapping (dict) – Mapping from old to new sample IDs. Returns:Dataset with remapped sample IDs. Return type:VariantDataset. repartition(num_partitions, shuffle=True)[source]¶; Increase or decrease the number of variant dataset partitions.; Examples; Repartition the variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_partitions (int) – Desired number of partitions, must be less than the current number if shuffle=False; shuffle (bool) – If true, use full shuffle to repartition. Returns:Variant dataset with the number of partitions equal to at most num_partitions. Return type:VariantDataset. rowkey_schema¶; Returns the signatu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:152192,perform,performance,152192,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"table1.add_index(); >>> table_result.show() ; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; This method returns a table with a new field whose name is given by; the name parameter, with type tint64. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like Table.take() or Table.export()) will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:7590,perform,performance,7590,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['perform'],['performance']
Performance,"tal_file_size = sum(file_sizes); all_scales = [('T', 1e12), ('G', 1e9), ('M', 1e6), ('K', 1e3), ('', 1e0)]; for overall_scale, overall_factor in all_scales:; if total_file_size > overall_factor:; total_file_size /= overall_factor; break; for scale, factor in all_scales:; if min_file_size > factor:; file_sizes = [x / factor for x in file_sizes]; break; total_file_size = f'{total_file_size:.1f} {overall_scale}B'; return total_file_size, file_sizes, scale. files = hadoop_ls(t_path). rows_file = [x['path'] for x in files if x['path'].endswith('rows')]; entries_file = [x['path'] for x in files if x['path'].endswith('entries')]; success_file = [x['modification_time'] for x in files if x['path'].endswith('SUCCESS')]. metadata_file = [x['path'] for x in files if x['path'].endswith('metadata.json.gz')]; if not metadata_file:; raise FileNotFoundError('No metadata.json.gz file found.'). with hadoop_open(metadata_file[0], 'rb') as f:; overall_meta = json.load(f); rows_per_partition = overall_meta['components']['partition_counts']['counts']. if not rows_file:; raise FileNotFoundError('No rows directory found.'); rows_files = hadoop_ls(rows_file[0]). data_type = 'Table'; if entries_file:; data_type = 'MatrixTable'; rows_file = [x['path'] for x in rows_files if x['path'].endswith('rows')]; rows_files = hadoop_ls(rows_file[0]); row_partition_bounds, row_file_sizes = get_rows_data(rows_files). total_file_size, row_file_sizes, row_scale = scale_file_sizes(row_file_sizes). panel_size = 480; subpanel_size = 120. if not row_partition_bounds:; warning('Table is not partitioned. Only plotting file sizes'); row_file_sizes_hist, row_file_sizes_edges = np.histogram(row_file_sizes, bins=50); p_file_size = figure(width=panel_size, height=panel_size); p_file_size.quad(; right=row_file_sizes_hist,; left=0,; bottom=row_file_sizes_edges[:-1],; top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); p_file_size.yaxis.axis_label = f'File size ({row_scale}B)'; return p_file_siz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:5919,load,load,5919,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,1,['load'],['load']
Performance,"tarfile.open('tutorial_data.tar').extractall(); if not (os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt')):; raise RuntimeError('Something went wrong!'); else:; sys.stderr.write('Done!\n'). Downloading data (~50M) from Google Storage...; Download finished!; Extracting...; Done!. We will read a dataset from disk, and print some summary statistics; about it to re-familiarize ourselves. In [4]:. vds = hc.read('data/1kg.vds'); vds.summarize().report(). Samples: 1000; Variants: 10961; Call Rate: 0.983163; Contigs: ['X', '12', '8', '19', '4', '15', '11', '9', '22', '13', '16', '5', '10', '21', '6', '1', '17', '14', '20', '2', '18', '7', '3']; Multiallelics: 0; SNPs: 10961; MNPs: 0; Insertions: 0; Deletions: 0; Complex Alleles: 0; Star Alleles: 0; Max Alleles: 2. Types in action¶; We’ll produce some sample annotations with the; sample_qc; method, then use these annotations to demonstrate some of the expression; language features. In [5]:. vds = vds.variant_qc().cache().sample_qc(). In [6]:. pprint(vds.sample_schema). Struct{; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; }; }. Filtering with expressions¶; The schema printed above is the type of the sample annotations, which; are given the variable name ‘sa’ wherever they appear. Here, we use the; filter_samples_expr method to filter samples based on these; annotations. If we want to filter on the “dpMean” above, we need to; select the ‘qc’ field from the ‘sa’ struct, then select the ‘dpMean’; field from the ‘qc’ struct. These selections are done with dots.; There are four Hail methods that use the expression language to filter a; dataset: -; filter_variants_expr; -; filter_samples_expr;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:3419,cache,cache,3419,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['cache'],['cache']
Performance,"tarray(tint32)),; ),; # DP unchanged; GQ=hl.gq_from_pl(newPL),; PL=newPL,; ).drop('__old_to_new_no_na'). @typecheck(mt=MatrixTable, call_field=str, r2=numeric, bp_window_size=int, memory_per_core=int); def _local_ld_prune(mt, call_field, r2=0.2, bp_window_size=1000000, memory_per_core=256):; bytes_per_core = memory_per_core * 1024 * 1024; fraction_memory_to_use = 0.25; variant_byte_overhead = 50; genotypes_per_pack = 32; n_samples = mt.count_cols(); min_bytes_per_core = math.ceil((1 / fraction_memory_to_use) * 8 * n_samples + variant_byte_overhead); if bytes_per_core < min_bytes_per_core:; raise ValueError(""memory_per_core must be greater than {} MB"".format(min_bytes_per_core // (1024 * 1024))); bytes_per_variant = math.ceil(8 * n_samples / genotypes_per_pack) + variant_byte_overhead; bytes_available_per_core = bytes_per_core * fraction_memory_to_use; max_queue_size = int(max(1.0, math.ceil(bytes_available_per_core / bytes_per_variant))). info(f'ld_prune: running local pruning stage with max queue size of {max_queue_size} variants'). return Table(; ir.MatrixToTableApply(; mt._mir,; {; 'name': 'LocalLDPrune',; 'callField': call_field,; 'r2Threshold': float(r2),; 'windowSize': bp_window_size,; 'maxQueueSize': max_queue_size,; },; ); ).persist(). [docs]@typecheck(; call_expr=expr_call,; r2=numeric,; bp_window_size=int,; memory_per_core=int,; keep_higher_maf=bool,; block_size=nullable(int),; ); def ld_prune(call_expr, r2=0.2, bp_window_size=1000000, memory_per_core=256, keep_higher_maf=True, block_size=None):; """"""Returns a maximal subset of variants that are nearly uncorrelated within each window. .. include:: ../_templates/req_diploid_gt.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Prune variants in linkage disequilibrium by filtering a dataset to those variants returned; by :func:`.ld_prune`. If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split befo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:165589,queue,queue,165589,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['queue'],['queue']
Performance,"taset representation. to_merged_sparse_mt(vds, *[, ...]); Creates a single, merged sparse MatrixTable from the split VariantDataset representation. truncate_reference_blocks(ds, *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference informa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3569,load,load,3569,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['load'],['load']
Performance,"taset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:16141,perform,perform,16141,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['perform'],['perform']
Performance,"te NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88920,perform,performance,88920,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"te missing values **only** for the phenotype field. This is in addition to ""-9"", ""0"", and ""N/A"" for case-control phenotypes. :param str delimiter: FAM file field delimiter regex. :param bool quantpheno: If True, FAM phenotype is interpreted as quantitative. :return: Variant dataset imported from PLINK binary file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importPlink(bed, bim, fam, joption(min_partitions), delimiter, missing, quantpheno). return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; drop_samples=bool,; drop_variants=bool); def read(self, path, drop_samples=False, drop_variants=False):; """"""Read .vds files as variant dataset. When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. :param path: VDS files to read.; :type path: str or list of str. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations; or gneotypes. :param bool drop_variants: If True, create samples-only variant; dataset (no variants or genotypes). :return: Variant dataset read from disk.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(; self,; self._jhc.readAll(jindexed_seq_args(path), drop_samples, drop_variants)). [docs] @handle_py4j; @typecheck_method(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_bad_ad=bool,; generic=bool,; call_fields=oneof(strlike, listof(strlike))); def import_vcf(self, path, force=False, force_bgz=False, header_file=None, min_partitions=None,; drop_samples=False, store_gq=False, pp_as_pl=False, skip",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:19340,load,load,19340,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"ter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; _fses[requester_pays_config].copy(src, dest). [docs]def exists(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[requester_pays_config].exists(path). [docs]def is_file(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return _fses[r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:3877,load,load,3877,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,1,['load'],['load']
Performance,"th higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) . If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:; >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters:. ds (MatrixTable or VariantDataset) – Dataset.; min_af – Minimum reference allele frequency to filter variants.; max_af – Maximum reference allele frequency to filter variants.; min_dp – Minimum sequencing depth to filter variants.; max_dp – Maximum sequencing depth to filter variants.; min_gq – Minimum genotype quality to filter variants; ref_AF – Reference AF expression. Necessary when the sample size is below 10,000. Returns:; Table. hail.methods.mendel_errors(call, pedigree)[source]; Find Mendel errors; count per variant, individual and nuclear family. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multialleli",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:47045,load,loaded,47045,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['load'],"['load', 'loaded']"
Performance,"th:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.name; ); ). return _func(""getReferenceSequence"", tstr, contig, position, before, after, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_genome_type); def is_valid_contig(contig, reference_genome='default') -> BooleanExpression:; """"""Returns ``True`` if `contig` is a valid contig name in `reference_genome`. Examples; --------. >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :class:`.BooleanExpression`; """"""; return _func(""isValidContig"", tbool, contig, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_genome_type); def contig_length(contig, reference_genome='default') -> Int32E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:161606,load,load,161606,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['load'],['load']
Performance,"the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. interval (tinterval) - Row key. Genomic interval. Same schema as above.; target (tstr) - Fourth column of .bed file. UCSC bed files can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; Intervals in UCSC BED files are 0-indexed and half open.; The line “5 100 105” correpsonds to the interval [5:101-5:106) in Hail’s; 1-indexed notation. Details; here. Parameters:. path (str) – Path to .bed file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding (dict of (str, str)) – Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; **kwargs – Additional optional arguments to import_table() are valid arguments here except:; no_header, delimiter, impute, skip_blank_lines, types, and comment as these; are used by import_bed. Returns:; Table – Interval-keyed table. hail.methods.import_bgen(path, entry_fields, sample_file=None, n_partitions=None, block_size=None, index_file_map=None, variants=None, _row_fields=['varid', 'rsid'])[source]; Import BGEN file(s) as a MatrixTable.; Examples; Import a BGEN file as a matrix table with GT and GP entry fields:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:7188,load,loaded,7188,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23391,cache,cache,23391,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['cache'],['cache']
Performance,"the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:165127,load,loadings,165127,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:44992,load,load,44992,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"tics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:29924,load,loadings,29924,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,"til; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:1594,optimiz,optimized,1594,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:19001,load,loadings,19001,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"ting an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl dataproc. (#9609) Fixed bug; where hailctl dataproc modify did not correctly print; corresponding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc desc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:63831,cache,cache,63831,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['cache'],['cache']
Performance,"tion methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Rel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90750,optimiz,optimization,90750,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"titions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57721,perform,performance,57721,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"tive of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixT",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7167,load,loadings,7167,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"tive of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_it",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20866,load,loadings,20866,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['load'],['loadings']
Performance,"tl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88339,optimiz,optimizations,88339,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizations']
Performance,"to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:24820,load,loading,24820,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['loading']
Performance,"tomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. These can contain information about the rows, or summary data calculated per row.; Column fields are stored once per column. These can contain information about the columns, or summary data calculated per column.; Entry fields are the piece that makes this structure a matrix – there is an entry for each (row, column) pair. Importing and Reading; Like tables, matrix tables can be imported from a variety of formats: VCF, (B)GEN, PLINK, TSV, etc. Matrix tables can also be read from a “native” matrix table format. Let’s read a sample of prepared 1KG data. [1]:. import hail as hl; from bokeh.io import output_notebook, show; output_notebook(). hl.utils.get_1kg('data/'). Loading BokehJS ... Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2011-0.2.133-4c60fddb171a.log; 2024-10-04 20:11:52.232 Hail: INFO: 1KG files found. [2]:. mt = hl.read_matrix_table('data/1kg.mt'); mt.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingR",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:2776,load,load,2776,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['load'],['load']
Performance,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22186,cache,cache,22186,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"tries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[""C"",""A""]; showing top 10 rows. [7]:. mt.s.show(). sstr; ""HG00096""; ""HG00099""; ""HG00105""; ""HG00118""; ""HG00129""; ""HG00148""; ""HG00177""; ""HG00182""; ""HG00242""; ""HG00254""; showing top 10 rows. Let’s investigate the genotypes and the call rate. Let’s look at the first few genotypes:. [8]:. mt.GT.show(). 'HG00096''HG00099''HG00105''HG00118'locusallelesGTGTGTGTlocus<GRCh37>array<str>callcallcallcall; 1:904165[""G"",""A""]0/00/00/00/0; 1:909917[""G"",""A""]0/00/00/00/0; 1:986963[""C"",""T""]0/00/00/00/0; 1:1563691[""T"",""G""]NA0/00/00/0; 1:1707740[""T"",""G""]0/10/10/10/0; 1:2252970[""C"",""T""]0/0NA0/00/0; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6655,load,load,6655,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['load'],['load']
Performance,"ts, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result = dataset.rename({'s': 'info', 'info': 'vcf_info'}). Parameters:; fields (dict from str to str) – Mapping from old field names to new field names. Returns:; MatrixTable – Matrix table with renamed fields. repartition(n_partitions, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n_partitions (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; MatrixTable – Repartitioned dataset. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; Get the first five row field names:; >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:51667,perform,performance,51667,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['perform'],['performance']
Performance,"ts; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the key.; A table has an ordered list of fields known as the key. Our users table has one key, the id field. We can see all the fields, as well as the keys, of a table by calling describe(). [2]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:1546,load,load,1546,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['load'],['load']
Performance,"ts. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89645,perform,performance,89645,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"tudy (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; The Key to Understanding Joins; Joining Tables; Exercises. MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Table Joins Tutorial. View page source. Table Joins Tutorial; This tutorial walks through some ways to join Hail tables. We’ll use a simple movie dataset to illustrate. The movie dataset comes in multiple parts. Here are a few questions we might naturally ask about the dataset:. What is the mean rating per genre?; What is the favorite movie for each occupation?; What genres are most preferred by women vs men?. We’ll use joins to combine datasets in order to answer these questions.; Let’s initialize Hail, fetch the tutorial data, and load three tables: users, movies, and ratings. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'). users = hl.read_table('data/users.ht'); movies = hl.read_table('data/movies.ht'); ratings = hl.read_table('data/ratings.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2010-0.2.133-4c60fddb171a.log; 2024-10-04 20:10:22.038 Hail: INFO: Movie Lens files found!. The Key to Understanding Joins; To understand joins in Hail, we need to revisit one of the crucial properties of tables: the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:1231,load,load,1231,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['load'],['load']
Performance,"type.TStruct` with field names equal to the IDs of the FORMAT fields.; The ``GT`` field is automatically read in as a :py:class:`~hail.type.TCall` type. To specify additional fields to import as a; :py:class:`~hail.type.TCall` type, use the ``call_fields`` parameter. All other fields are imported as the type specified in the FORMAT header field. An example genotype schema after importing a VCF with ``generic=True`` is. .. code-block:: text. Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. .. warning::. - The variant dataset generated with ``generic=True`` will have significantly slower performance. - Not all :py:class:`.VariantDataset` methods will work with a generic genotype schema. - The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. :py:meth:`~hail.HailContext.import_vcf` does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant. Since Hail's genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If ``generic=False``, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS ""genotype schema. Below are two example haploid genotypes and diploid equivalents that Hail sees. .. code-block:: text. Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. .. note::; ; Using the **FILTER** field:; ; The information in the FILTER field of a VCF is contained in the ``va.filters`` annotation.; This annotation is a ``Set`` and can be queried for filter membership with expressions ; like ``va.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:22887,perform,perform,22887,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['perform'],['perform']
Performance,"ue between variants i and j, defined as; `Pearson's correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; :math:`\\rho_{x_i,x_j}` between the two genotype vectors :math:`x_i` and :math:`x_j`. .. math::. \\rho_{x_i,x_j} = \\frac{\\mathrm{Cov}(X_i,X_j)}{\\sigma_{X_i} \\sigma_{X_j}}. Also note that variants with zero variance (:math:`\\sigma = 0`) will be dropped from the matrix. .. caution::. The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you'll want to reduce the number of variants with methods like; :py:meth:`.sample_variants`, :py:meth:`.filter_variants_expr`, or :py:meth:`.ld_prune` before; calling this unless your dataset is very small. :param bool force_local: If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds :math:`5000^2` and locally otherwise. :return: Matrix of r values between pairs of variants.; :rtype: :py:class:`LDMatrix`; """""". jldm = self._jvdf.ldMatrix(force_local); return LDMatrix(jldm). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool,; min_ac=integral,; min_af=numeric); def linreg(self, y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0):; r""""""Test each variant for association using linear regression. .. include:: requireTGenotype.rst. **Examples**. Run linear regression per variant using a phenotype and two covariates stored in sample annotations:. >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`.linreg` method computes, for each variant, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:97028,perform,performance,97028,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,"uester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; If you are copying a file just to then load it into Python, you can use; open() instead. For example:; >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: ; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hailtop.fs.exists(path, *, requester_pays_config=None)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hailtop.fs.is_dir(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hailtop.fs.is_file(path, *, requester_pays_config=None)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hailtop.fs.ls(path, *, requester_pays_config=None)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one elem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:2082,load,load,2082,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['load'],['load']
Performance,ug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug fixes. Version 0.2.45; Bug fixes; hailctl dataproc. Version 0.2.44; New Features; Bug fixes; Performance. Version 0.2.43; Bug fixes. Version 0.2.42; New Features; Bug fixes. Version 0.2.41; Bug fixes; hailctl dataproc. Version 0.2.40; VCF Combiner; Bug fixes. Version 0.2.39; Bug fixes; New features; Performance Improvements; Documentation. Version 0.2.38; Critical Linreg Aggregator Correctness Bug; Performance improvements. Version 0.2.37; Bug fixes; New features. Version 0.2.36; Critical Memory Management Bug Fix; Bug fixes. Version 0.2.35; Critical Memory Management Bug Fix; New features; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.34; New features; Bug fixes; hailctl dataproc; File Format. Version 0.2.33; New features; Bug fixes; hailctl dataproc. Version 0.2.32; Critical performance regression fix; Performance; Bug fixes; New features; Cheat sheets. Version 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; ha,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:5505,perform,performance,5505,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:82136,optimiz,optimizer,82136,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"umn-indexed fields. globals; Returns a struct expression including all global fields. row; Returns a struct expression of all row-indexed fields, including keys. row_key; Row key struct. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_col_index; Add the integer index of each column as a new column field. add_row_index; Add the integer index of each row as a new row field. aggregate_cols; Aggregate over columns to a local value. aggregate_entries; Aggregate over entries to a local value. aggregate_rows; Aggregate over rows to a local value. annotate_cols; Create new column-indexed fields by name. annotate_entries; Create new row-and-column-indexed fields by name. annotate_globals; Create new global fields by name. annotate_rows; Create new row-indexed fields by name. anti_join_cols; Filters the table to columns whose key does not appear in other. anti_join_rows; Filters the table to rows whose key does not appear in other. cache; Persist the dataset in memory. checkpoint; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. choose_cols; Choose a new set of columns from a list of old column indices. collect_cols_by_key; Collect values for each unique column key into arrays. cols; Returns a table with all column fields in the matrix. compute_entry_filter_stats; Compute statistics about the number and fraction of filtered entries. count; Count the number of rows and columns in the matrix. count_cols; Count the number of columns in the matrix. count_rows; Count the number of rows in the matrix. describe; Print information about the fields in the matrix table. distinct_by_col; Remove columns with a duplicate row key, keeping exactly one column for each unique key. distinct_by_row; Remove rows with a duplicate row key, keeping exactly one row for each unique key. drop; Drop fields. entries; Returns a matrix in coordinate table form. explode_cols; Explodes a column field of typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:3430,cache,cache,3430,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['cache'],['cache']
Performance,"ur cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77338,perform,performance,77338,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"urce. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 66. Annotate; You can add new fields to a table with annotate. As an example, let’s create a new column called cleaned_occupation that replaces missing entries in the occupation field labeled as ‘other’ with ‘none.’. [5]:. missing_occupations = hl.set(['other', 'none']). t = users.annotate(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexoccupationzipcodecleaned_occupationint32int32strstrstrstr; 124""M""""technician""""85711""""technician""; 253""F""""other""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1759,load,load,1759,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['load'],['load']
Performance,"ures. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79075,perform,performance,79075,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritabilit",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125572,perform,performance,125572,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['load', 'perform']","['loaded', 'performance']"
Performance,"ve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83701,perform,performance,83701,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:141389,load,loadings,141389,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['load'],['loadings']
Performance,"ves each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:141084,load,loadings,141084,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88668,perform,performance,88668,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89214,perform,performance,89214,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few ent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77872,optimiz,optimizer,77872,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:19331,load,loadings,19331,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"x.); The header information for row fields is allowed to be missing, if the; column IDs are present, but the header must then consist only of tab-delimited; column IDs (no row field names).; The column IDs will never be missing, even if the missing string appears; in the column IDs. Parameters:. paths (str or list of str) – Files to import.; row_fields (dict of str to HailType) – Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key (str or list of str) – Key fields(s). If empty, creates an index row_id to use as key.; entry_type (HailType) – Type of entries in matrix table. Must be one of: tint32,; tint64, tfloat32, tfloat64, or; tstr. Default: tint32.; missing (str) – Identifier to be treated as missing. Default: NA; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True, assume the file has no header and name the row fields f0,; f1, … fK (0-indexed) and the column keys 0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:26219,load,load,26219,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"xporting missing genotypes without trailing; fields. Bug fixes. (#5306) Fix; ReferenceGenome.add_sequence causing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; opti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100776,perform,performance,100776,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"xpression`; Value or ndarray to divide by. Returns; -------; :class:`.NDArrayNumericExpression`; NDArray of positional quotients.; """"""; return self._bin_op_numeric(""/"", other, self._div_ret_type_f). def __rtruediv__(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by a ndarray or a scalar using floor division. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). def __rmatmul__(self, other):; if not isinstance(other, NDArrayNumericExpression):; other = hl.nd.array(other); return other.__matmul__(self). [docs] def __matmul__(self, other):; """"""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:107559,perform,performed,107559,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['perform'],['performed']
Performance,"xprs (varargs of str or Expression) – Names of fields to drop or field reference expressions. Returns:; MatrixTable – Matrix table without specified fields. entries()[source]; Returns a matrix in coordinate table form.; Examples; Extract the entry table:; >>> entries_table = dataset.entries(). Notes; The coordinate table representation of the source matrix table contains; one row for each non-filtered entry of the matrix – if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; M * N rows, which can be a very large number.; This representation can be useful for aggregating over both axes of a matrix table; at the same time – it is not possible to aggregate over a matrix table using; group_rows_by() and group_cols_by() at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with entries(),; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:27685,perform,performance,27685,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['perform'],['performance']
Performance,"y('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:39784,load,load,39784,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['load'],['load']
Performance,"y_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""; if block_size is None:; block_size = BlockMatrix.default_block_size(). if not 0.0 <= r2 <= 1:; raise ValueError(f'r2 must be in the range [0.0, 1.0], found {r2}'). if bp_window_size < 0:; raise ValueError(f'bp_window_size must be non-negative, found {bp_window_size}'). raise_unless_entry_indexed('ld_prune/call_expr', call_expr); mt = matrix_table_source('ld_prune/call_expr', call_expr). require_row_key_variant(mt, 'ld_prune'). # FIXME: remove once select_entries on a field is free; if call_expr in mt._fields_inverse:; field = mt._fields_inverse[call_expr]; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:169100,queue,queue,169100,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['queue'],['queue']
Performance,"ype=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; intervals_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); intervals = intervals_type._convert_from_json(obj['gvcf_import_inter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:33111,load,load,33111,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['load'],['load']
Performance,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:39125,cache,cache,39125,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['cache'],['cache']
Performance,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56660,throughput,throughput,56660,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8084,load,loadings,8084,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['load'],['loadings']
Performance,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24709,throughput,throughput,24709,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['throughput'],['throughput']
Performance,"},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. `name` must be unique and not overlap with Hail's pre-instantiated; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``.; The contig names in `xContigs`, `yContigs`, and `mtContigs` must be; present in `contigs`. The intervals listed in `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig length given in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ...",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:8543,load,load,8543,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['load'],['load']
Performance,"}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Thi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:17955,load,loadings,17955,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,2,['load'],['loadings']
Performance,"}\widehat{k^{(0)}_{ij}} :=; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2 + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}\end{split}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} := 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Details; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference; implementation in a couple key ways:. the principal components analysis does not use an unrelated set of; individuals; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Notes; The block_size controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation’s time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; block_size larger than 512 tends to cause memory exhaustion errors.; The minimum allele frequency filter is applied per-pair: if either of; the two individual’s individual-specific minor allele frequency is below; the threshold, then the variant’s contribution to relatedness estimates; is zero.; Under the PC-Relate model, kinship, [ phi_{ij} ], ranges from 0 to; 0.5, and is precisely half of the; fracti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:134889,perform,perform,134889,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['perform']
Performance,"~', x._ir), x.dtype, x._indices, x._aggregations). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_count(x):; """"""Count the number of 1s in the in the `two's complement <https://en.wikipedia.org/wiki/Two%27s_complement>`__ binary representation of `x`. Examples; --------; The binary representation of `7` is `111`, so:. >>> hl.eval(hl.bit_count(7)); 3. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; ----------; :class:`.Int32Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('BitCount', x._ir), tint32, x._indices, x._aggregations). [docs]@typecheck(array=expr_array(expr_numeric), elem=expr_numeric); def binary_search(array, elem) -> Int32Expression:; """"""Binary search `array` for the insertion point of `elem`. Parameters; ----------; array : :class:`.Expression` of type :class:`.tarray`; elem : :class:`.Expression`. Returns; -------; :class:`.Int32Expression`. Notes; -----; This function assumes that `array` is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last. The returned index is the lower bound on the insertion point of `elem` into; the ordered array, or the index of the first element in `array` not smaller; than `elem`. This is a value between 0 and the length of `array`, inclusive; (if all elements in `array` are smaller than `elem`, the returned value is; the length of `array` or the index of the first missing value, if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""; c = coercer_from_dtype(array.dtype.element_type); if not c.can_coerce(elem.dtype):; raise TypeError(; f""'binary_search': cannot search an array of type {array.dtype} for a value of type {elem.dtype}""; ); elem = c.coerce(elem); return hl.switch(elem).when_missing(hl.missing(hl.tint32",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:183705,perform,perform,183705,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['perform'],['perform']
Performance,"– The minimum individual-specific allele frequency for; an allele used to measure relatedness.; block_size (int) – the side length of the blocks of the block-; distributed matrices; this should be set such; that at least three of these matrices fit in; memory (in addition to all other objects; necessary for Spark and Hail).; min_kinship (float) – Pairs of samples with kinship lower than; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:137945,load,loadings,137945,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"﻿. . VariantDataset — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; VariantDataset. View page source. VariantDataset¶. class hail.VariantDataset(hc, jvds)[source]¶; Hail’s primary representation of genomic data, a matrix keyed by sample and variant.; Variant datasets may be generated from other formats using the HailContext import methods,; constructed from a variant-keyed KeyTable using VariantDataset.from_table(),; and simulated using balding_nichols_model().; Once a variant dataset has been written to disk with write(),; use read() to load the variant dataset into the environment.; >>> vds = hc.read(""data/example.vds""). Variables:hc (HailContext) – Hail Context. Attributes. colkey_schema; Returns the signature of the column key (sample) contained in this VDS. genotype_schema; Returns the signature of the genotypes contained in this VDS. global_schema; Returns the signature of the global annotations contained in this VDS. globals; Return global annotations as a Python object. num_samples; Number of samples. rowkey_schema; Returns the signature of the row key (variant) contained in this VDS. sample_annotations; Return a dict of sample annotations. sample_ids; Return sampleIDs. sample_schema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:841,load,load,841,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['load']
Performance,"﻿. Clumping GWAS Results — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Introduction; Hail GWAS Script; Docker Image; Batch Script; Functions; Control Code. Synopsis. Random Forest. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:488,perform,performing,488,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['perform'],['performing']
Performance,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:943,load,load,943,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,1,['load'],['load']
Performance,"﻿. Hail | ; Hail 0.2. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail 0.2. View page source. Hail 0.2; Hail is an open-source library for scalable data exploration and analysis, with; a particular emphasis on genomics. See the overview for; a high-level walkthrough of the library, the GWAS tutorial for a simple; example of conducting a genome-wide association study, and the installation page to get started; using Hail. Contents. Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Supported Configuration Variables. Overview; Expressions; Tables; MatrixTables. How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Schemas. Annotation Database; Database Query. Libraries; gnomad (Hail Utilities for gnomAD). For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Hadoop Glob Patterns. Change Log And Version Policy; Python Version Compatibility Policy; Frequently Asked Questions; Version 0.2.133; Version 0.2.132; Version 0.2.131; Version 0.2.130; Version 0.2.129; Version 0.2.128; Version 0.2.127; Version 0.2.126; Version 0.2.125; Version 0.2.124; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/index.html:478,scalab,scalable,478,docs/0.2/index.html,https://hail.is,https://hail.is/docs/0.2/index.html,1,['scalab'],['scalable']
Performance,"﻿. Hail | ; Import / Export. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Import / Export. View page source. Import / Export. This page describes functionality for moving data in and out of Hail.; Hail has a suite of functionality for importing and exporting data to and from; general-purpose, genetics-specific, and high-performance native file formats. Native file formats; When saving data to disk with the intent to later use Hail, we highly recommend; that you use the native file formats to store Table and; MatrixTable objects. These binary formats not only smaller than other formats; (especially textual ones) in most cases, but also are significantly faster to; read into Hail later.; These files can be created with methods on the Table and; MatrixTable objects:. Table.write(); MatrixTable.write(). These files can be read into a Hail session later using the following methods:. read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Import. General purpose; The import_table() function is widely-used to import textual data; into a Hail Table. import_matrix_table() is used to import; two-dimensional matrix data in textual representations into a Hail; MatrixTable. Finally, it is possible to create a Hail Table; from a pandas DataFrame with Table.from_pandas(). import",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:905,perform,performance,905,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['performance']
Performance,"﻿. Hail | ; MatrixTable Tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; MatrixTable Anatomy; Importing and Reading; MatrixTable operations; Exercise: GQ vs DP. Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; MatrixTable Tutorial. View page source. MatrixTable Tutorial; If you’ve gotten this far, you’re probably thinking:. “Can’t I do all of this in pandas or R?”; “What does this have to do with biology?”. The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You’ve learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.; In the last example of the Table Joins Tutorial, the ratings table had a compound key: movie_id and user_id. The ratings were secretly a movie-by-user matrix!; However, since this matrix is very sparse, it is reasonably represented in a so-called “coordinate form” Table, where each row of the table is an entry of the sparse matrix. For large and dense matrices (like sequencing data), the per-row overhead of coordinate reresentations is untenable. That’s why we built MatrixTable, a 2-dimensional generalization of Table. MatrixTable Anatomy; Recall that Table has two kinds of fields:. global fields; row fields. MatrixTable has four kinds of fields:. global fields; row fields; column fields; entry fields. Row fields are fields that are stored once per row. Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:966,scalab,scalability,966,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['scalab'],['scalability']
Performance,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:757,perform,perform,757,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['perform'],['perform']
Performance,"﻿. Hail | ; hail.experimental.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:892,load,loadings,892,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['load'],['loadings']
Performance,"﻿. Hail | ; hail.vds.combiner.new_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.combiner.new_combiner. View page source. hail.vds.combiner.new_combiner. hail.vds.combiner.new_combiner(*, output_path, temp_path, save_path=None, gvcf_paths=None, vds_paths=None, vds_sample_counts=None, intervals=None, import_interval_size=None, use_genome_default_intervals=False, use_exome_default_intervals=False, gvcf_external_header=None, gvcf_sample_names=None, gvcf_info_to_keep=None, gvcf_reference_entry_fields_to_keep=None, call_fields=['PGT'], branch_factor=100, target_records=24000, gvcf_batch_size=None, batch_size=None, reference_genome='default', contig_recoding=None, force=False)[source]; Create a new VariantDatasetCombiner or load one from save_path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html:1337,load,load,1337,docs/0.2/vds/hail.vds.combiner.new_combiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.new_combiner.html,1,['load'],['load']
Performance,"﻿. Hail | Get Help . 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Get Help!; Let us assist you on your journey to efficient genomic analysis. Cheatsheets; Cheatsheets are two-page PDFs loaded with short Hail Query examples and even shorter explanations. They push you over all the little roadblocks. Query Docs; When you need to find detailed information on how to get started with Hail Query, examples of Hail Query use, and how a function works: the reference document is your go to. To do a quick search of a Hail Query function, try out the search bar in the documentation. Batch Docs; For all your massively scalable compute needs, check out the Hail Batch reference documentation. Ask a question; When you reach a blocking issue with your analysis using Hail, and you think you are unable to find an answer to your question via the documentation, search through or ask a question on our Forum! It is highly recommended -- your question may be able to serve another person in our ever growing Hail community. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/gethelp.html:230,load,loaded,230,gethelp.html,https://hail.is,https://hail.is/gethelp.html,2,"['load', 'scalab']","['loaded', 'scalable']"
Safety," . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:1098,safe,safer,1098,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['safe'],['safer']
Safety," 1) / 1]. Often, these metrics are correlated. [30]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 30:> (0 + 1) / 1]. Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:. [31]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); print('After filter, %d/284 samples remain.' % mt.count_cols()). [Stage 32:> (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a variety of useful statistics, plot them, and filter. [33]:. mt = hl.variant_qc(mt). [34]:. mt.row.describe(). --------------------------------------------------------; Type:; struct {; locus: locus<GRCh37>,; alleles: array<str>,; rsid: str,; qual: float",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:15969,detect,detect,15969,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['detect'],['detect']
Safety," 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/10.1186/s12864-023-09869-2. 	 Chen, S., Francioli, L.C., Goodrich, J.K. et al. A genomic mutational constraint map; 	 using variation in 76,156 human genomes. Nature 625, 92–100; 	 (2024). https://doi.org/10.1038/s41586-023-06045-0 https://www.nature.com/articles/s41586-023-06045-0. 	 Mosca, M.J., Cho, H. Reconstruction of private genomes through reference-based genotype; 	 imputation. Genome Biol 24, 271; 	 (2023). https://doi.org/10.1186/s13059-023-03105-6 https://link.springer.com/article/10.1186/s13059-023-03105-6. 	 Stöberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntington’s disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699–708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:2989,risk,risk,2989,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety," = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regula",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:4011,recover,recover,4011,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['recover'],['recover']
Safety," Science. Blog. Hail-Powered Science; . An incomplete list of scientific work enabled by Hail.; . If you use Hail for published work, please cite the software. You can get a citation for the version of Hail you installed by executing:; import hail as hl; print(hl.citation()); Or you could include the following line in your bibliography:; Hail Team. Hail 0.2. https://github.com/hail-is/hail; Otherwise, we welcome you to add additional examples by editing this page directly, after which we will review the pull request to confirm the addition is valid. Please adhere to the existing formatting conventions.; Last updated on February 22, 2024; 2024. 	 Kwak, S.H., Srinivasan, S., Chen, L. et al. Genetic architecture and biology of; 	 youth-onset type 2 diabetes. Nat Metab 6, 226–237; 	 (2024). https://doi.org/10.1038/s42255-023-00970-0; https://www.nature.com/articles/s42255-023-00970-0. 	 Zhao, S., Crouse, W., Qian, S. et al. Adjusting for genetic confounders in; 	 transcriptome-wide association studies improves discovery of risk genes of complex; 	 traits. Nat Genet 56, 336–347; 	 (2024). https://doi.org/10.1038/s41588-023-01648-9; https://www.nature.com/articles/s41588-023-01648-9. 2023. 	 Lee, S., Kim, J. & Ohn, J.H. Exploring quantitative traits-associated copy number; 	 deletions through reanalysis of UK10K consortium whole genome sequencing cohorts. BMC; 	 Genomics 24, 787 (2023). https://doi.org/10.1186/s12864-023-09903-3 https://link.springer.com/article/10.1186/s12864-023-09903-3. 	 Langlieb, J., Sachdev, N.S., Balderrama, K.S. et al. The molecular cytoarchitecture of; 	 the adult mouse brain. Nature 624, 333–342; 	 (2023). https://doi.org/10.1038/s41586-023-06818-7; https://www.nature.com/articles/s41586-023-06818-7. 	 Leońska-Duniec, A., Borczyk, M., Korostyński, M. et al. Genetic variants in myostatin; 	 and its receptors promote elite athlete status. BMC Genomics 24, 761; 	 (2023). https://doi.org/10.1186/s12864-023-09869-2 https://link.springer.com/article/1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:1132,risk,risk,1132,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety," any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None]) – Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Opt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3496,timeout,timeout,3496,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['timeout'],['timeout']
Safety," isinstance(hl.current_backend(), ServiceBackend):; path = hl.TemporaryFilename().name; hl.current_backend().fs.open(path, mode='wb').write(nd.tobytes()); uri = path; else:; path = new_local_temp_file(); nd.tofile(path); uri = local_path_uri(path); return cls.fromfile(uri, n_rows, n_cols, block_size). [docs] @classmethod; @typecheck_method(; entry_expr=expr_float64,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before cen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14054,avoid,avoid,14054,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['avoid'],['avoid']
Safety," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25490,avoid,avoid,25490,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22233,detect,detected,22233,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['detect'],['detected']
Safety," syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a generator which will produce each result in the; same order as the iterables, only blocking if the result is not yet; ready. You can convert the generator to a list with list.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:4106,timeout,timeout,4106,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['timeout'],['timeout']
Safety," to remap these; values. Here is an example:; >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') . The info field AC in data/export.vcf will have Number=1.; New Fields; split_multi_hts() adds the following fields:. was_split (bool) – True if this variant was originally; multiallelic, otherwise False.; a_index (int) – The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with a_index = 1 and 1:100:A:C; with a_index = 2. See also; split_multi(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root (str) – Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table – A biallelic variant dataset. hail.methods.summarize_variants(mt, show=True, *, handler=None)[source]; Summarize the variants present in a dataset and print the results.; Examples; >>> hl.summarize_variants(dataset) ; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:91542,avoid,avoids,91542,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['avoid'],['avoids']
Safety," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:109434,predict,predicting,109434,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['predict'],['predicting']
Safety," v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.select('x')\n""; f"" Correct: ht = ht.select(ht.x)\n""; f"" Correct: ht = ht.select(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.select(ht.x.replace(' ', '_'))""; ); analyze(caller, e, indices, broadcast=False). name = e._ir.name; check_keys(caller, name, protected_key); final_fields.append(name); if is_top_level_field(e):; select_fields.append(name); else:; insertions[name] = e; for k, e in named_exprs.items():; check_keys(caller, k, protected_key); final_fields.append(k); insertions[k] = e. check_collisions(caller, final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:13915,redund,redundant,13915,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,1,['redund'],['redundant']
Safety," with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Double; }; }. Now that we’ve got principal components per sample, we may as well plot; them! Human history exerts a strong effect in genetic datasets. Even; with a 50MB sequencing dataset, we can recover the major human; populations. In [51]:. pca_table = pca.samples_table().to_pandas(); colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}; plt.scatter(pca_table[""sa.pca.PC1""], pca_table[""sa.pca.PC2""],; c = pca_table[""sa.SuperPopulation""].map(colors),; alpha = .5); plt.xlim(-0.6, 0.6); plt.xlabel(""PC1""); plt.ylabel(""PC2""); legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]; plt.legend(handles=legend_entries, loc=2); plt.show(). Now we can rerun our linear regression, controlling for the first few; principal components and sample sex. In [52]:. pvals = (common_vds; .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca'); .linreg('sa.CaffeineConsumption', covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale']); .query_variants('variants.map(v => va.linreg.pval).collect()')). 2018-10-18 01:27:07 Hail: INFO: Running linear regression on 843 samples with 5 covariates including inter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:25605,recover,recover,25605,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['recover'],['recover']
Safety," with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; )",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:93257,avoid,avoid,93257,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['avoid'],['avoid']
Safety,"""Key columns by a new set of fields. See :meth:`.Table.key_by` for more information on defining a key. Parameters; ----------; keys : varargs of :class:`str` or :class:`.Expression`.; Column fields to key by.; named_keys : keyword args of :class:`.Expression`.; Column fields to key by.; Returns; -------; :class:`.MatrixTable`; """"""; key_fields, computed_keys = get_key_by_exprs(""MatrixTable.key_cols_by"", keys, named_keys, self._col_indices). if not computed_keys:; return MatrixTable(ir.MatrixMapCols(self._mir, self._col._ir, key_fields)); else:; new_col = self.col.annotate(**computed_keys); base, cleanup = self._process_joins(new_col). return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, new_col._ir, key_fields))). @typecheck_method(new_key=str); def _key_rows_by_assert_sorted(self, *new_key):; rk_names = list(self.row_key); i = 0; while i < min(len(new_key), len(rk_names)):; if new_key[i] != rk_names[i]:; break; i += 1. if i < 1:; raise ValueError(; f'cannot implement an unsafe sort with no shared key:\n new key: {new_key}\n old key: {rk_names}'; ). return MatrixTable(ir.MatrixKeyRowsBy(self._mir, list(new_key), is_sorted=True)). [docs] @typecheck_method(keys=oneof(str, Expression), named_keys=expr_any); def key_rows_by(self, *keys, **named_keys) -> 'MatrixTable':; """"""Key rows by a new set of fields. Examples; --------. >>> dataset_result = dataset.key_rows_by('locus'); >>> dataset_result = dataset.key_rows_by(dataset['locus']); >>> dataset_result = dataset.key_rows_by(**dataset.row_key.drop('alleles')). All of these expressions key the dataset by the 'locus' field, dropping; the 'alleles' field from the row key. >>> dataset_result = dataset.key_rows_by(contig=dataset['locus'].contig,; ... position=dataset['locus'].position,; ... alleles=dataset['alleles']). This keys the dataset by the newly defined fields, 'contig' and 'position',; and the 'alleles' field. The old row key field, 'locus', is preserved as; a non-key field. Notes; -----; See :meth:`.Table.key_by` fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:28681,unsafe,unsafe,28681,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['unsafe'],['unsafe']
Safety,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144361,avoid,avoid,144361,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"',; 'mean_dp = g.map(g => g.dp).stats().mean'])). In [59]:. kt2.to_dataframe().show(). +-------+-----------+------------------+-----------------+; |maf_bin|purple_hair| mean_gq| mean_dp|; +-------+-----------+------------------+-----------------+; | > 5%| true| 36.09305651197578|7.407450459057423|; | < 1%| true| 22.68197887434976|7.374254453728496|; | < 1%| false|22.986128698357074|7.492131714314245|; | > 5%| false|36.341259980753755|7.533399982371768|; | 1%-5%| true|24.093123033233528|7.269552536649012|; | 1%-5%| false| 24.3519587208908|7.405582424428774|; +-------+-----------+------------------+-----------------+. We’ve shown that it’s easy to aggregate by a couple of arbitrary; statistics. This specific examples may not provide especially useful; pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Family-based tests and utilities; Distributed file system utilities; Interoperability with Python and Spark machine learning libraries; More!. For reference, here’s the full workflow to all tutorial endpoints; combined into one",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:29724,detect,detect,29724,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['detect'],['detect']
Safety,"(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:86270,avoid,avoids,86270,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['avoid'],['avoids']
Safety,"(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The functi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5431,timeout,timeout,5431,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['timeout'],['timeout']
Safety,"(one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True)[source]; Export a Table to Elasticsearch.; By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. Warning; export_elasticsearch() is EXPERIMENTAL. Note; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a config with the; es.mapping.id; option set to a field that contains a unique value for each row. hail.methods.export_bgen(mt, output, gp=None, varid=None, rsid=None, parallel=None, compression_codec='zlib')[source]; Export MatrixTable as MatrixTable as BGEN 1.2 file with 8; bits of per probability. Also writes SAMPLE file.; If parallel is None, the BGEN file is written to output + '.bgen'. Otherwise, output; + '.bgen' will be a directory containing many BGEN files. In either case, the SAMPLE file is; written to output + '.sample'. For example,; >>> hl.export_bgen(mt, '/path/to/dataset') . Will write two files: /path/to/dataset.bgen and /path/to/dataset.sample. In contrast,; >>> hl.export_bgen(mt, '/path/to/dataset', parallel='header_per_shard') . Will create /path/to/dataset.sample and will create mt.n_partitions() files into the; directory /path/to/dataset.bgen/.; Notes; The export_bgen() function requires genotype probabilities, either as an entry; field of mt (of t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:52147,avoid,avoid,52147,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['avoid'],['avoid']
Safety,", output_path). b.run(wait=False); backend.close(). run_rf_checkpoint_batching.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). indices = local_df_y.index.to_list(); results = [None] * len(indices). inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). for inputs in grouped(10, inputs):; j = b.new_python_job(); for wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:16378,predict,predict,16378,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,", statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing “linkage disequilibrium” between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra.; To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a block-sparse matrix. Otherwise, we say the matrix; is block-dense. The property is_sparse() encodes this state.; Dropped blocks are not stored in memory or on write(). In fact,; blocks that are dropped prior to an action like export() or; to_numpy() are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes.; Block-sparse matrices may be created with; sparsify_band(),; sparsify_rectangles(),; sparsify_row_intervals(),; and sparsify_triangle().; The following methods naturally propagate block-sparsity:. Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify()",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:6292,avoid,avoiding,6292,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['avoid'],['avoiding']
Safety,"-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al. “Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al. “Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al. “Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al. “Current clinical use of polygenic scores will risk exacerbating health disparities.” bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al. “Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals.” American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al. “The mutational constraint spectrum quantified from variation in 141,456 humans.” bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al. “Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease.” bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al. “Transcript expression-aware annotation improves rare variant discovery and interpretation.” bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al. “Landscape of multi-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:14256,risk,risk,14256,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,". Random Forest Model — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:588,predict,predict,588,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,". The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:3133,avoid,avoid,3133,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['avoid'],['avoid']
Safety,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111086,avoid,avoid,111086,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:10946,safe,safeguards,10946,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['safe'],['safeguards']
Safety,".” bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al. “Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channels.” bioRxiv (2018): 246850. https://www.biorxiv.org/content/10.1101/246850v1.abstract; Haas, Mary E., et al. “Genetic association of albuminuria with cardiometabolic disease and blood pressure.” American Journal of Human Genetics 103.4 (2018): 461-473. https://www.cell.com/ajhg/pdf/S0002-9297(18)30270-2.pdf; Abel, Haley J., et al. “Mapping and characterization of structural variation in 17,795 deeply sequenced human genomes.” bioRxiv (2018): 508515. https://www.biorxiv.org/content/10.1101/508515v1.abstract; Lane, Jacqueline M., et al. “Biological and clinical insights from genetics of insomnia symptoms.” bioRxiv (2018): 257956. https://www.biorxiv.org/content/10.1101/257956v1.abstract; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood onset and adult onset asthma.” bioRxiv (2018): 427427. https://www.biorxiv.org/content/10.1101/427427v1.abstract. 2017. Lessard, Samuel, et al. “Human genetic variation alters CRISPR-Cas9 on-and off-targeting specificity at therapeutically implicated loci.” Proceedings of the National Academy of Sciences 114.52 (2017): E11257-E11266. https://www.pnas.org/content/114/52/E11257.long. 2016. Ganna, Andrea, et al. “Ultra-rare disruptive and damaging mutations influence educational attainment in the general population.” Nature Neuroscience 19.12 (2016): 1563. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127781/. Footnote In addition to software development, the Hail team engages in theoretical, algorithmic, and empirical research inspired by scientific collaboration. Examples include Loss landscapes of regularized linear autoencoders, Secure multi-party linear regression at plaintext speed, and A synthetic-diploid benchmark for accurate variant-calling evaluation. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:21571,risk,risk,21571,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"= mt.filter_rows(mt.csq == 'LOF'); . # run sample QC and save into matrix table; mt = hl.sample_qc(mt). # filter for samples that are > 95% call rate; mt = mt.filter_cols(mt.sample_qc.call_rate >= 0.95) . # run variant QC and save into matrix table; mt = hl.variant_qc(mt). # filter for variants that are >95% call rate and >1% frequency; mt = mt.filter_rows(mt.variant_qc.call_rate > 0.95); mt = mt.filter_rows(mt.variant_qc_.AF[1] > 0.01). Simplified Analysis; Hail makes it easy to analyze your data. Let's start by filtering a dataset by variant and sample; quality metrics, like call rate and allele frequency. Quality Control Procedures; Quality control procedures, like sex check, are made easy using Hail's declarative syntax. imputed_sex = hl.impute_sex(mt.GT); mt = mt.annotate_cols(; sex_check = imputed_sex[mt.s].is_female == mt.reported_female; ). # must use Google cloud platform for this to work ; # annotation with vep; mt = hl.vep(mt). Variant Effect Predictor; Annotating variants with Variant effect predictor has never been easier. Rare-Variant Association Testing; Perform Gene Burden Tests on sequencing data with just a few lines of Python. gene_intervals = hl.read_table(""gs://my_bucket/gene_intervals.t""); mt = mt.annotate_rows(; gene = gene_intervals.index(mt.locus, all_matches=True).gene_name; ). mt = mt.explode_rows(mt.gene); mt = (mt.group_rows_by(mt.gene); .aggregate(burden = hl.agg.count_where(mt.GT.is_non_ref()))). result = hl.linear_regression_rows(y=mt.phenotype, x=mt.burden). # generate and save PC scores; eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(mt.GT, k=4). # run linear regression for the first 4 PCs; mt = mt.annotate_cols(scores = pca_scores[mt.sample_id].scores); results = hl.linear_regression_rows(; y=mt.phenotype,; x=mt.GT.n_alt_alleles(),; covariates=[; 1, mt.scores[0], mt.scores[1], mt.scores[2], mt.scores[3]]; ). Principal Component Analysis (PCA); Adjusting GWAS models with principal components as covariates has never been easier. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/tutorial.html:2272,predict,predictor,2272,tutorial.html,https://hail.is,https://hail.is/tutorial.html,1,['predict'],['predictor']
Safety,"G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:. >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:. >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; -------; Rows with a constant value (i.e., zero variance) will result `nan`; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of :func:`.aggregators.stats`). Notes; -----; In this method, each row of entries is regarded as a vector with elements; defined by `entry_expr` and missing values mean-imputed per row.; The ``(i, j)`` element of the resulting block matrix is the correlation; between rows ``i`` and ``j`` (as 0-indexed by order in the matrix table;; see :meth:`~hail.MatrixTable.add_row_index`). The correlation of two vectors is defined as the; `Pearson correlation coeffecient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / bl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:133308,avoid,avoid,133308,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['avoid'],['avoid']
Safety,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23824,avoid,avoid,23824,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"Parameters:; populations (int) – Number of populations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:7535,recover,recovery,7535,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['recover'],['recovery']
Safety,"Repartition to 500 partitions:; >>> dataset_result = dataset.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n_partitions (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; MatrixTable – Repartitioned dataset. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; Get the first five row field names:; >>> list(dataset.row)[:5]; ['locus', 'alleles', 'rsid', 'qual', 'filters']. Returns:; StructExpression – Struct of all row fields. property row_key; Row key struct.; Examples; Get the row key field names:; >>> list(dataset.row_key); ['locus', 'alleles']. Returns:; StructExpression. property row_value; Returns a struct expression including all non-key row-indexed fields.; Examples; Get the first five non-key row field names:; >>> list(dataset.row_value)[:5]; ['",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:52118,avoid,avoid,52118,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['avoid'],['avoid']
Safety,"Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/article/doi/10.1093/jamia/ocaa068/5876972; Karaca M, Atceken N, Karaca Ş, Civelek E, Şekerel BE, Polimanti R. “Phenotypic and Molecular Characterization of Risk Loci Associated With Asthma and Lung Function” Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:10104,risk,risk,10104,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"True, delete temporary directories with intermediate files.; backend_kwargs (Any) – See Backend._run() for backend-specific arguments. Return type:; Optional[Batch]. select_jobs(pattern); Select all jobs in the batch whose name matches pattern.; Examples; Select jobs in batch matching qc:; >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters:; pattern (str) – Regex pattern matching job names. Return type:; List[Job]. write_output(resource, dest); Write resource file or resource file group to an output destination.; Examples; Write a single job intermediate to a local file:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:; b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. Warning; To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; All JobResourceFile are temporary files and must be written; to a permanent location using write_output() if the output needs; to be saved. Parameters:. resource (Resource) – Resource to be written to a file.; dest (str) – Destination file path. For a single ResourceFile, this will; simply be dest. For a ResourceGroup, dest is the file; root and each resource file will be written to {root}.identifier; where identifier is the identifier of the file in the; ResourceGroup map. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:11956,avoid,avoid,11956,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['avoid'],['avoid']
Safety,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48545,predict,predicting,48545,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['predict'],['predicting']
Safety,"ackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10164,avoid,avoid,10164,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avoid'],['avoid']
Safety,"ads/2019/02/Cloud_19.pdf; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al. “Genetic contributions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/eaat6576.full; Molnos, Sophie Claudia. Metabolites: implications in type 2 diabetes and the effect of epigenome-wide interaction with genetic variation. Diss. Technische Universität München, 2018. https://mediatum.ub.tum.de/1372795f; Bis, Joshua C., et al. “Whole exome sequencing study identifies novel rare and common Alzheimer’s-associated variants involved in immune response and transcriptional regulation.” Molecular Psychiatry (2018): 1. https://www.nature.com/articles/s41380-018-0112-7; Gormley, Padhraig, et al. “Common variant burden contributes to the familial aggregation of migraine in 1,589 families.” Neuron 98.4 (2018): 743-753. https://www.ncbi.nlm.nih.gov/pubmed/30189203; Rivas, Manuel A., et al. “Insights into the genetic epidemiology of Crohn’s and rare diseases in the Ashkenazi Jewish population.” PLoS Genetics 14.5 (2018): e1007329. https://journals.plos.org/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:17815,risk,risk,17815,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54150,avoid,avoid,54150,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49803,avoid,avoid,49803,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28055,predict,predicting,28055,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['predict'],['predicting']
Safety,"aset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: Matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18672,avoid,avoids,18672,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['avoid'],['avoids']
Safety,at64; 'HIPred_score': float64; 'HIPred': str; 'GHIS': float64; 'P(rec)': float64; 'Known_rec_info': str; 'RVIS_EVS': float64; 'RVIS_percentile_EVS': float64; 'LoF-FDR_ExAC': float64; 'RVIS_ExAC': float64; 'RVIS_percentile_ExAC': float64; 'ExAC_pLI': float64; 'ExAC_pRec': float64; 'ExAC_pNull': float64; 'ExAC_nonTCGA_pLI': float64; 'ExAC_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:10772,predict,prediction,10772,docs/0.2/datasets/schemas/dbNSFP_genes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html,10,['predict'],['prediction']
Safety,"ations.; samples (int) – Number of samples.; variants (int) – Number of variants.; num_partitions (int) – Number of partitions.; pop_dist (array of float or None) – Unnormalized population distribution; fst (array of float or None) – \(F_{ST}\) values; af_dist (UniformDist or BetaDist or TruncatedBetaDist) – Ancestral allele frequency distribution; seed (int) – Random seed. Returns:Variant dataset simulated using the Balding-Nichols model. Return type:VariantDataset. eval_expr(expr)[source]¶; Evaluate an expression. Parameters:expr (str) – Expression to evaluate. Return type:annotation. eval_expr_typed(expr)[source]¶; Evaluate an expression and return the result as well as its type. Parameters:expr (str) – Expression to evaluate. Return type:(annotation, Type). static get_running()[source]¶; Return the running Hail context in this Python session.; Example; >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. Returns:Current Hail context. Return type:HailContext. grep(regex, path, max_count=100)[source]¶; Grep big files, like, really fast.; Examples; Print all lines containing the string hello in file.txt:; >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). Background; grep() mimics the basic functionality of Unix grep in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous text files like VCFs. Find background on regular expressions at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:7555,recover,recover,7555,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['recover'],['recover']
Safety,"biorxiv.org/content/10.1101/543504v1; Lacaze, Paul, et al. “The Medical Genome Reference Bank: a whole-genome data resource of 4000 healthy elderly individuals. Rationale and cohort design.” European Journal of Human Genetics 27.2 (2019): 308. https://www.nature.com/articles/s41431-018-0279-z; Cirulli, Elizabeth T., et al. “Genome-wide rare variant analysis for thousands of phenotypes in 54,000 exomes.” bioRxiv (2019): 692368. https://www.biorxiv.org/content/10.1101/692368v1.abstract; Kerminen, Sini, et al. “Geographic Variation and Bias in the Polygenic Scores of Complex Diseases and Traits in Finland.” American Journal of Human Genetics (2019). https://www.biorxiv.org/content/10.1101/485441v1.abstract; Jiang, Fan, Kyle Ferriter, and Claris Castillo. “PIVOT: Cost-Aware Scheduling of Data-Intensive Applications in a Cloud-Agnostic System.” https://renci.org/wp-content/uploads/2019/02/Cloud_19.pdf; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Cox, Samantha L., et al. “Genetic contributions to variation in human stature in prehistoric Europe.” bioRxiv (2019): 690545. https://www.biorxiv.org/content/10.1101/690545v1.abstract; Abrar, Faheem. A Modular Parallel Pipeline Architecture for GWAS Applications in a Cluster Environment. Diss. University of Saskatchewan, 2019. https://harvest.usask.ca/handle/10388/12087; Khera, Amit V., et al. “Whole-genome sequencing to characterize monogenic and polygenic contributions in patients hospitalized with early-onset myocardial infarction.” Circulation 139.13 (2019): 1593-1602. https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.035658. 2018. An, Joon-Yong, et al. “Genome-wide de novo risk score implicates promoter variation in autism spectrum disorder.” Science (2018): 1. https://science.sciencemag.org/content/362/6420/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:16918,risk,risk,16918,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80104,avoid,avoid,80104,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"ces of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_cols(cols_to_keep)[source]; Filters matrix columns. Parameters:; cols_to_keep (list of int) – Indices of columns to keep. Must be non-empty and increasing. Returns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21638,avoid,avoid,21638,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['avoid'],['avoid']
Safety,"ciated With Asthma and Lung Function” Allergy Asthma Immunol Res. (2020) 12(5):806-820. https://doi.org/10.4168/aair.2020.12.5.806 https://e-aair.org/DOIx.php?id=10.4168/aair.2020.12.5.806; Muniz Carvalho, C., Wendt, F.R., Maihofer, A.X. et al. Dissecting the genetic association of C-reactive protein with PTSD, traumatic events, and social support. Neuropsychopharmacol. (2020). https://doi.org/10.1038/s41386-020-0655-6 https://www.nature.com/articles/s41386-020-0655-6#citeas. 2019. Farhan, Sali MK, et al. “Exome sequencing in amyotrophic lateral sclerosis implicates a novel gene, DNAJC7, encoding a heat-shock protein” Nature Neuroscience (2019): 307835. https://www.nature.com/articles/s41593-019-0530-0; Gay, Nicole R. et al. “Impact of admixture and ancestry on eQTL analysis and GWAS colocalization in GTEx” bioRxiv (2019) 836825; https://www.biorxiv.org/content/10.1101/836825v1; Sakaue, Saori et al. “Trans-biobank analysis with 676,000 individuals elucidates the association of polygenic risk scores of complex traits with human lifespan” bioRxiv (2019): 856351 https://www.biorxiv.org/content/10.1101/856351v1; Polimanti, Renato et al. “Leveraging genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum G",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:11975,risk,risk,11975,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:8201,avoid,avoiding,8201,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['avoid'],['avoiding']
Safety,"d in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71067,abort,aborting,71067,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['abort'],['aborting']
Safety,"d to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map; Call fn on cloud machines with arguments from iterables. shutdown; Allow temporary resources to be cleaned up. submit; Call fn on a cloud machine with all remaining arguments and keyword arguments. async async_map(fn, iterables, timeout=None, chunksize=1); Aysncio compatible version of map(). Return type:; AsyncGenerator[int, None]. async async_submit(unapplied, *args, **kwargs); Aysncio compatible version of BatchPoolExecutor.submit(). Return type:; BatchPoolFuture. map(fn, *iterables, timeout=None, chunksize=1); Call fn on cloud machines with arguments from iterables.; This function returns a generator which will produce each result in the; same order as the iterables, only blocking if the result is not yet; ready. You can convert the generator to a list with list.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.ran",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:3841,timeout,timeout,3841,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['timeout'],['timeout']
Safety,"e the mean GQ?. [51]:. entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, ""< 1%"",; hl.if_else(entries.info.AF[0]<0.05, ""1%-5%"", "">5%""))). results2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.pheno.PurpleHair); .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,; mean_dp = hl.agg.stats(entries.DP).mean)). [52]:. results2.show(). [Stage 193:> (0 + 1) / 1]. af_binpurple_hairmean_gqmean_dpstrboolfloat64float64; ""1%-5%""False2.48e+017.43e+00; ""1%-5%""True2.46e+017.47e+00; ""< 1%""False2.35e+017.55e+00; ""< 1%""True2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:25602,detect,detect,25602,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['detect'],['detect']
Safety,"eBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers don’t match the file extensions:; >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). rg.foo and rg.bar will not have the .txt file extension and; instead will be {root}.foo and {root}.bar where {root} is a random; identifier.; Notes; The identifier is used to refer to a specific resource file. For example,; given the resource group r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8620,avoid,avoid,8620,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['avoid'],['avoid']
Safety,"eft: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".join(self.row_key.dtype.values())}\n'; f' right: {"", "".join(other.row_key.dtype.values())}'; ). if drop_right_row_fields:; other = other.select_rows(); else:; left_fields = set(self.row_value); other_fields = set(other.row_value) - set(other.row_key); renames, _ = deduplicate(other_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); other = other.rename(renames); info(; 'Table.union_cols: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return MatrixTable(ir.MatrixUnionCols(self._mir, other._mir, row_join_type)). [docs] @typecheck_method(n_rows=nullable(int), n_cols=nullable(int), n=nullable(int)); def head(self, n_rows: Optional[int], n_cols: Optional[int] = None, *, n: Optional[int] = None) -> 'MatrixTable':; """"""Subset matrix to first `n_rows` rows and `n_cols` cols. Examples; --------; >>> mt_range = hl.utils.range_matrix_table(100, 100). Passing only one argument will take the first `n_rows` rows:. >>> mt_range.head(10).count(); (10, 100). Passing two arguments refers to rows and columns, respectively:. >>> mt_range.head(10, 20).count(); (10, 20). Either argument may be ``None`` to indicate no filter. First 10 rows, all columns:. >>> mt_range.head(10, None).count(); (10, 100). All rows, first 10 columns:. >>> mt_range.head(None, 10).count(); (100, 10). Notes; -----; The number of partitions in the new matrix is eq",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:122398,avoid,avoid,122398,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['avoid'],['avoid']
Safety,"ersion 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ when using the; LocalBackend; Fixed executing a PythonJob when using the LocalBackend. Version 0.2.65. Added PythonJob; Added new Job.memory inputs lowmem, standard, and highmem corresponding to ~1Gi/core, ~4Gi/core, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:5012,timeout,timeout,5012,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['timeout'],['timeout']
Safety,"es; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; Table – Repartitioned table. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; The data type of the row struct:; >>> table1.row.dtype; dtype('struct{ID: int32, HT: int32, SEX: str, X: int32, Z: int32, C1: int32, C2: int32, C3: int32}'). The number of row fields:; >>> len(table1.row); 8. Returns:; StructExpression – Struct of all row fields, including key fields. property row_value; Returns a struct expression including all non-key row-indexed fields.; Examples; The data type of the row struct:; >>> table1.row_value.dtype; dtype('struct{HT: int32, SEX: str, X: int32, Z: int32, C1: int32, C2: i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:56212,avoid,avoid,56212,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['avoid'],['avoid']
Safety,"esulting table can be larger than; the number of records on the left or right if duplicate keys are; present. Parameters; ----------; right : :class:`.Table`; Table to join.; how : :class:`str`; Join type. One of ""inner"", ""left"", ""right"", ""outer"". Returns; -------; :class:`.Table`; Joined table. """"""; if _join_key is None:; _join_key = max(len(self.key), len(right.key)). left_key_types = list(self.key.dtype.values())[:_join_key]; right_key_types = list(right.key.dtype.values())[:_join_key]; if not left_key_types == right_key_types:; raise ValueError(; f""'join': key mismatch:\n ""; f"" left: [{', '.join(str(t) for t in left_key_types)}]\n ""; f"" right: [{', '.join(str(t) for t in right_key_types)}]""; ); left_fields = set(self._fields); right_fields = set(right._fields) - set(right.key). renames, _ = deduplicate(right_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); right = right.rename(renames); info(; 'Table.join: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return Table(ir.TableJoin(self._tir, right._tir, how, _join_key)). [docs] @typecheck_method(expr=BooleanExpression); def all(self, expr):; """"""Evaluate whether a boolean expression is true for all rows. Examples; --------; Test whether `C1` is greater than 5 in all rows of the table:. >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Expression to test. Returns; -------; :obj:`bool`; """"""; return self.aggregate(hl.agg.all(expr)). [docs] @typecheck_method(expr=BooleanExpression); def any(self, expr):; """"""Evaluate whether a Boolean expression is true for at least one row. Examples; --------. Test whether `C1` is equal to 5 any row in any row of the table:. >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Boolean exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:103081,avoid,avoid,103081,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['avoid'],['avoid']
Safety,"first_row_ht = ht.head(1). if find_replace is not None:; ht = ht.annotate(text=ht['text'].replace(*find_replace)). first_rows = first_row_ht.annotate(; header=first_row_ht.text._split_line(; delimiter, missing=hl.empty_array(hl.tstr), quote=quote, regex=len(delimiter) > 1; ); ).collect(); except FatalError as err:; if '_filter_partitions: no partition with index 0' in err.args[0]:; first_rows = []; else:; raise. if len(first_rows) == 0:; raise ValueError(f""Invalid file: no lines remaining after filters\n Files provided: {', '.join(paths)}""); first_row = first_rows[0]. if no_header:; fields = [f'f{index}' for index in range(0, len(first_row.header))]; else:; maybe_duplicated_fields = first_row.header; renamings, fields = deduplicate(maybe_duplicated_fields); ht = ht.filter(; ht.text == first_row.text, keep=False; ) # FIXME: seems wrong. Could easily fix with partition index and row_within_partition_index.; if renamings:; hl.utils.warning(; f'import_table: renamed the following {plural(""field"", len(renamings))} to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renamings); ). ht = ht.annotate(; split_text=(; hl.case(); .when(hl.len(ht.text) > 0, split_lines(ht, fields, delimiter=delimiter, missing=missing, quote=quote)); .or_error(hl.str(""Blank line found in file "") + ht.file); ); ); ht = ht.drop('text'). fields_to_value = {}; strs = []; if impute:; fields_to_impute_idx = []; fields_to_guess = []; for idx, field in enumerate(fields):; if types.get(field) is None:; fields_to_impute_idx.append(idx); fields_to_guess.append(field). hl.utils.info('Reading table to impute column types'); guessed = ht.aggregate(; hl.agg.array_agg(lambda x: hl.agg._impute_type(x), [ht.split_text[i] for i in fields_to_impute_idx]); ). reasons = {f: 'user-supplied type' for f in types}; imputed_types = dict(); for field, s in zip(fields_to_guess, guessed):; if not s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s[",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:62544,avoid,avoid,62544,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['avoid'],['avoid']
Safety,"g only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the project argument in Batch() unless you are creating a PythonJob; Set the default for Job.memory to be ‘standard’; Added the cancel_after_n_failures option to Batch(); Fixed executing a job with Job.memory set to ‘lowmem’, ‘standard’, and ‘highmem’ ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4800,avoid,avoid,4800,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['avoid'],['avoid']
Safety,"genome-wide data to investigate differences between opioid use vs. opioid dependence in 41,176 individuals from the Psychiatric Genomics Consortium” bioRxiv (2019): 765065 https://www.biorxiv.org/content/10.1101/765065v1; Lescai, Francesco et al. “Meta-analysis of Scandinavian Schizophrenia Exomes” bioRxiv (2019): 836957; https://www.biorxiv.org/content/10.1101/836957v2; Bolze, Alexandre, et al. “Selective constraints and pathogenicity of mitochondrial DNA variants inferred from a novel database of 196,554 unrelated individuals” bioRxiv (2019): 798264;https://www.biorxiv.org/content/10.1101/798264v1; De Lillo, A., De Angelis, F., Di Girolamo, M. et al. “Phenome-wide association study of TTR and RBP4 genes in 361,194 individuals reveals novel insights in the genetics of hereditary and wildtype transthyretin amyloidoses.” Hum Genet 138, 1331–1340 (2019). https://www.ncbi.nlm.nih.gov/pubmed/31659433; Pividori, Milton, et al. “Shared and distinct genetic risk factors for childhood-onset and adult-onset asthma: genome-wide and transcriptome-wide studies.” The Lancet Respiratory Medicine 7.6 (2019): 509-522. https://www.biorxiv.org/content/10.1101/427427v2; Werling, Donna, et al. “Whole-genome and RNA sequencing reveal variation and transcriptomic coordination in the developing human prefrontal cortex.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/585430v1; Satterstrom, Kyle F., et al. “Large-scale exome sequencing study implicates both developmental and functional changes in the neurobiology of autism.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/484113v3; Huang, Qin, et al. “Delivering genes across the blood-brain barrier: LY6A, a novel cellular receptor for AAV-PHP. B capsids.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:13101,risk,risk,13101,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"iant). In this representation, it is easy to aggregate over any fields we like, which is often the first step of rare variant analysis.; What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?. [51]:. entries = entries.annotate(maf_bin = hl.if_else(entries.info.AF[0]<0.01, ""< 1%"",; hl.if_else(entries.info.AF[0]<0.05, ""1%-5%"", "">5%""))). results2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.pheno.PurpleHair); .aggregate(mean_gq = hl.agg.stats(entries.GQ).mean,; mean_dp = hl.agg.stats(entries.DP).mean)). [52]:. results2.show(). [Stage 193:> (0 + 1) / 1]. af_binpurple_hairmean_gqmean_dpstrboolfloat64float64; ""1%-5%""False2.48e+017.43e+00; ""1%-5%""True2.46e+017.47e+00; ""< 1%""False2.35e+017.55e+00; ""< 1%""True2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:25313,detect,detect,25313,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['detect'],['detect']
Safety,"iants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114112,avoid,avoids,114112,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['avoid'],['avoids']
Safety,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178135,avoid,avoid,178135,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"ies; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16012,timeout,timeout,16012,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['timeout'],['timeout']
Safety,"igenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-02,1.75e-02,-1.98e-02]; ""HG00105""[1.09e-01,2.79e-01,-9.95e-02,-1.06e-01,8.79e-02,1.44e-02,2.80e-02,-3.38e-02,-1.08e-03,2.25e-02]; ""HG00118""[1.26e-01,2.95e-01,-7.58e-02,-1.08e-01,1.76e-02,7.91e-03,-5.25e-02,3.05e-02,2.00e-02,-7.78e-02]; ""HG00129""[1.06e-01,2.86e-01,-9.69e-02,-1.15e-01,1.03e-02,2.65e-02,-8.51e-02,2.49e-02,5.67e-02,-8.31e-03]; showing top 5 rows. Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations. [44]:. mt = mt.annotate_cols(scores = pcs[mt.s].scores). [45]:. p = hl.plot.scatter(mt.scores[0],; mt.scores[1],; label=mt.pheno.SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2'); show(p). [Stage 161:> (0 + 1) / 1]. Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field. [46]:. gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 166:> (0 + 1) / 1]. We’ll first make a Q-Q plot to assess inflation…. [47]:. p = hl.plot.qq(gwas.p_value); show(p). That’s more like it! This shape is indicative of a well-controlled (but not especially well-powered) study. And now for the Manhattan plot:. [48]:. p = hl.plot.manhattan",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:22289,recover,recover,22289,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['recover'],['recover']
Safety,"iguration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1243,timeout,timeout,1243,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,1,['timeout'],['timeout']
Safety,"ilarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:6927,avoid,avoid,6927,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['avoid'],['avoid']
Safety,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1499,timeout,timeout,1499,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,6,['timeout'],['timeout']
Safety,"ill be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-proj",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8570,avoid,avoid,8570,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avoid'],['avoid']
Safety,"iltop.batch.job). JobResourceFile (class in hailtop.batch.resource). L. LocalBackend (class in hailtop.batch.backend). M. map() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). memory() (hailtop.batch.job.Job method). N. new_bash_job() (hailtop.batch.batch.Batch method). new_job() (hailtop.batch.batch.Batch method). new_python_job() (hailtop.batch.batch.Batch method). P. plink_merge() (in module hailtop.batch.utils). PythonJob (class in hailtop.batch.job). PythonResult (class in hailtop.batch.resource). R. read_input() (hailtop.batch.batch.Batch method). read_input_group() (hailtop.batch.batch.Batch method). regions() (hailtop.batch.job.Job method). requester_pays_fs() (hailtop.batch.backend.Backend method). Resource (class in hailtop.batch.resource). ResourceFile (class in hailtop.batch.resource). ResourceGroup (class in hailtop.batch.resource). result() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). run() (hailtop.batch.batch.Batch method). running() (hailtop.batch.batch_pool_executor.BatchPoolFuture method). RunningBatchType (class in hailtop.batch.backend). S. select_jobs() (hailtop.batch.batch.Batch method). ServiceBackend (class in hailtop.batch.backend). shutdown() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). source() (hailtop.batch.resource.InputResourceFile method). (hailtop.batch.resource.JobResourceFile method). (hailtop.batch.resource.PythonResult method). (hailtop.batch.resource.Resource method). (hailtop.batch.resource.ResourceGroup method). spot() (hailtop.batch.job.Job method). storage() (hailtop.batch.job.Job method). submit() (hailtop.batch.batch_pool_executor.BatchPoolExecutor method). supported_regions() (hailtop.batch.backend.ServiceBackend static method). T. timeout() (hailtop.batch.job.Job method). V. validate_file() (hailtop.batch.backend.Backend method). W. write_output() (hailtop.batch.batch.Batch method). © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/genindex.html:4391,timeout,timeout,4391,docs/batch/genindex.html,https://hail.is,https://hail.is/docs/batch/genindex.html,1,['timeout'],['timeout']
Safety,"ino Populations. Clin Pharmacol; 	 Ther, 113:; 	 680-691. https://doi.org/10.1002/cpt.2787 https://ascpt.onlinelibrary.wiley.com/doi/full/10.1002/cpt.2787. 2022. 	 Huang, J., Tao, Q., Ang, T.F.A. et al. The impact of increasing levels of blood; 	 C-reactive protein on the inflammatory loci SPI1 and CD33 in Alzheimer’s disease. Transl; 	 Psychiatry 12, 523; 	 (2022). https://doi.org/10.1038/s41398-022-02281-6 https://www.nature.com/articles/s41398-022-02281-6. Wadon, M.E., Fenner, E., Kendall, K.M. et al. Clinical and genotypic analysis in; 	 determining dystonia non-motor phenotypic heterogeneity: a UK Biobank study. J Neurol; 	 269, 6436–6451 (2022). https://doi.org/10.1007/s00415-022-11307-4 https://link.springer.com/article/10.1007/s00415-022-11307-4. 	 Andi Madihah Manggabarani, Takuyu Hashiguchi, Masatsugu Hashiguchi, Atsushi Hayashi,; 	 Masataka Kikuchi, Yusdar Mustamin, Masaru Bamba, Kunihiro Kodama, Takanari Tanabata,; 	 Sachiko Isobe, Hidenori Tanaka, Ryo Akashi, Akihiro Nakaya, Shusei Sato, Construction of; 	 prediction models for growth traits of soybean cultivars based on phenotyping in diverse; 	 genotype and environment combinations, DNA Research, Volume 29, Issue 4, August 2022,; 	 dsac024, https://doi.org/10.1093/dnares/dsac024 https://academic.oup.com/dnaresearch/article/29/4/dsac024/6653298?login=false. 	 Chaffin, M., Papangeli, I., Simonson, B. et al. Single-nucleus profiling of human; 	 dilated and hypertrophic cardiomyopathy. Nature 608, 174–180; 	 (2022). https://doi.org/10.1038/s41586-022-04817-8 https://www.nature.com/articles/s41586-022-04817-8. 	 Lee, J., Lee, J., Jeon, S. et al. A database of 5305 healthy Korean individuals reveals; 	 genetic and clinical implications for an East Asian population. Exp Mol Med 54,; 	 1862–1871; 	 (2022). https://doi.org/10.1038/s12276-022-00871-4 https://www.nature.com/articles/s12276-022-00871-4. 	 Akingbuwa, W.A., Hammerschlag, A.R., Bartels, M. et al. Ultra-rare and common genetic; 	 variant analysis conv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:7525,predict,prediction,7525,references.html,https://hail.is,https://hail.is/references.html,1,['predict'],['prediction']
Safety,"intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose index is in position 1 of the LA list.; The index at position 2 of the LA list is 2, and the allele with absolute; index 2 is T. Local alleles make it possible to keep the data small to; match its inherent information content. Component tables; The VariantDataset is made up of two component matrix tables – the; reference_data and the variant_data.; The reference_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:5119,avoid,avoid,5119,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['avoid'],['avoid']
Safety,"ional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16093,timeout,timeout,16093,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['timeout'],['timeout']
Safety,"ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:111993,avoid,avoid,111993,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['avoid'],['avoid']
Safety,"ith :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset with the number of partitions equal to at most ``num_partitions``; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.coalesce(num_partitions, shuffle); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(max_partitions=integral); def naive_coalesce(self, max_partitions):; """"""Naively descrease the number of partitions. .. warning ::. :py:meth:`~hail.VariantDataset.naive_coalesce` simply combines adjacent partitions to achieve the desired number. It does not attempt to rebalance, unlike :py:meth:`~hail.VariantDatase",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:192625,avoid,avoid,192625,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['avoid'],['avoid']
Safety,"ks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file conta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:1158,predict,predict,1158,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,"lease; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#1381",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18530,detect,detected,18530,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['detect'],['detected']
Safety,"ly compute linkage disequilibrium between nearby; variants. Use :meth:`row_correlation` directly to calculate correlation; without windowing. More precisely, variants are 0-indexed by their order in the matrix table; (see :meth:`~hail.MatrixTable.add_row_index`). Each variant is regarded as a vector of; elements defined by `entry_expr`, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant. The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:139327,avoid,avoid,139327,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['avoid'],['avoid']
Safety,"m forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; j = b.new_python_job(); result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). output = hb.concatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:12479,predict,predict,12479,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,"message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; scrollable. Legend entries for such plots can be clicked to show/hide; traces on the plot, but this does not work and is a known issue that; will only be addressed if hail.ggplot is migrated off of plotly.; (#12584) Fixed bug; which arose as an assertion error about type mismatches. This was; usually triggered when working with tuples.; (#12583) Fixed bug; which showed an empty table for ht.col_key.show().; (#12582) Fixed bug; where matrix tables with duplicate col keys do not show properly.; Also fixed bug where tables and matrix tables with HTML unsafe column; headers are rendered wrong in Jupyter.; (#12574) Fixed a; memory leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:40796,unsafe,unsafe,40796,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['unsafe'],['unsafe']
Safety,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42400,avoid,avoid,42400,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:3655,interlock,interlocking,3655,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['interlock'],['interlocking']
Safety,"ncatenate(b, results); b.write_output(output, output_path). b.run(wait=False); backend.close(). run_rf_checkpoint.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. def main(df_x_path, df_y_path, output_path, python_image):; backend = hb.ServiceBackend(); b = hb.Batch(name='rf-loo', default_python_image=python_image). with hfs.open(df_y_path) as f:; local_df_y = pd.read_table(f, header=0, index_col=0). df_x_input = b.read_input(df_x_path); df_y_input = b.read_input(df_y_path). results = []. for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:14261,predict,predict,14261,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7987,predict,predicting,7987,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['predict'],['predicting']
Safety,"notate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49458,timeout,timeout,49458,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['timeout'],['timeout']
Safety,"ob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:8927,detect,detection,8927,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['detect'],['detection']
Safety,"om/article/10.1186/s13059-023-03105-6. 	 Stöberl, N., Donaldson, J., Binda, C.S. et al. Mutant huntingtin confers cell-autonomous; 	 phenotypes on Huntington’s disease iPSC-derived microglia. Sci Rep 13, 20477; 	 (2023). https://doi.org/10.1038/s41598-023-46852-z https://www.nature.com/articles/s41598-023-46852-z. 	 Tamman, A.J.F., Koller, D., Nagamatsu, S. et al. Psychosocial moderators of polygenic; 	 risk scores of inflammatory biomarkers in relation to GrimAge. Neuropsychopharmacol. 49,; 	 699–708; 	 (2024). https://doi.org/10.1038/s41386-023-01747-5 https://www.nature.com/articles/s41386-023-01747-5. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Al-Jumaan, M., Chu, H., Alsulaiman, A. et al. Interplay of Mendelian and polygenic risk; 	 factors in Arab breast cancer patients. Genome Med 15, 65; 	 (2023). https://doi.org/10.1186/s13073-023-01220-4 https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01220-4. 	 Ilves N, Pajusalu S, Kahre T, et al. High Prevalence of Collagenopathies in Preterm- and; 	 Term-Born Children With Periventricular Venous Hemorrhagic Infarction. Journal of Child; 	 Neurology. 2023;38(6-7):373-388. doi:10.1177/08830738231186233. https://journals.sagepub.com/doi/full/10.1177/08830738231186233. 	 Mignogna, G., Carey, C.E., Wedow, R. et al. Patterns of item nonresponse behaviour to; 	 survey questionnaires are systematic and associated with genetic loci. Nat Hum Behav 7,; 	 1371–1387; 	 (2023). https://doi.org/10.1038/s41562-023-01632-7 https://www.nature.com/articles/s41562-023-01632-7. 	 Josefine U Melchiorsen, Kimmie V Sørensen, Jette Bork-Jensen, Hüsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:3578,risk,risk,3578,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"ompatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:1263,timeout,timeout,1263,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['timeout'],['timeout']
Safety,"ot explained by the covariates alone. - s2 : :obj:`.tfloat64`, the variance of the residuals, :math:`\sigma^2` in the paper. """"""; mt = matrix_table_source('skat/x', x); k = len(covariates); if k == 0:; raise ValueError('_linear_skat: at least one covariate is required.'); _warn_if_no_intercept('_linear_skat', covariates); mt = mt._select_all(; row_exprs=dict(group=group, weight=weight), col_exprs=dict(y=y, covariates=covariates), entry_exprs=dict(x=x); ); mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])); yvec, covmat, n = mt.aggregate_cols(; (hl.agg.collect(hl.float(mt.y)), hl.agg.collect(mt.covariates.map(hl.float)), hl.agg.count()), _localize=False; ); mt = mt.annotate_globals(yvec=hl.nd.array(yvec), covmat=hl.nd.array(covmat), n_complete_samples=n); # Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #; covmat_Q, _ = hl.nd.qr(mt.covmat); mt = mt.annotate_globals(covmat_Q=covmat_Q); null_mu = mt.covmat_Q @ (mt.covmat_Q.T @ mt.yvec); y_residual = mt.yvec - null_mu; mt = mt.annotate_globals(y_residual=y_residual, s2=y_residual @ y_residual.T / (n - k)); mt = mt.annotate_rows(G_row_mean=hl.agg.mean(mt.x)); mt = mt.annotate_rows(G_row=hl.agg.collect(hl.coalesce(mt.x, mt.G_row_mean))); ht = mt.rows(); ht = ht.filter(hl.all(hl.is_defined(ht.group), hl.is_defined(ht.weight))); ht = ht.group_by('group').aggregate(; weight_take=hl.agg.take(ht.weight, n=max_size + 1),; G_take=hl.agg.take(ht.G_row, n=max_size + 1),; size=hl.agg.count(),; ); ht = ht.annotate(; weight=hl.nd.array(hl.or_missing(hl.len(ht.weight_take) <= max_size, ht.weight_take)),; G=hl.nd.array(hl.or_missing(hl.len(ht.G_take) <= ma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:81642,predict,predicted,81642,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['predict'],['predicted']
Safety,"otypes_table(); .aggregate_by_key(key_expr=['''maf_bin = if (va.qc.AF < 0.01) ""< 1%""; else if (va.qc.AF < 0.05) ""1%-5%""; else ""> 5%"" ''',; 'purple_hair = sa.PurpleHair'],; agg_expr=['mean_gq = g.map(g => g.gq).stats().mean',; 'mean_dp = g.map(g => g.dp).stats().mean'])). In [59]:. kt2.to_dataframe().show(). +-------+-----------+------------------+-----------------+; |maf_bin|purple_hair| mean_gq| mean_dp|; +-------+-----------+------------------+-----------------+; | > 5%| true| 36.09305651197578|7.407450459057423|; | < 1%| true| 22.68197887434976|7.374254453728496|; | < 1%| false|22.986128698357074|7.492131714314245|; | > 5%| false|36.341259980753755|7.533399982371768|; | 1%-5%| true|24.093123033233528|7.269552536649012|; | 1%-5%| false| 24.3519587208908|7.405582424428774|; +-------+-----------+------------------+-----------------+. We’ve shown that it’s easy to aggregate by a couple of arbitrary; statistics. This specific examples may not provide especially useful; pieces of information, but this same pattern can be used to detect; effects of rare variation:. Count the number of heterozygous genotypes per gene by functional; category (synonymous, missense, or loss-of-function) to estimate; per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in; cases and controls to detect genes involved in disease. Eplilogue¶; Congrats! If you’ve made it this far, you’re perfectly primed to read; the Overview, look through the; Hail objects representing many; core concepts in genetics, and check out the many Hail functions defined; in the Python API. If you use Hail; for your own science, we’d love to hear from you on Gitter; chat or the discussion; forum.; There’s also a lot of functionality inside Hail that we didn’t get to in; this broad overview. Things like:. Flexible import and export to a variety of data and annotation; formats (VCF, BGEN, PLINK, JSON, TSV, …); Simulation; Burden tests; Kinship and pruning (IBD, GRM, RRM); Fami",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:29431,detect,detect,29431,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['detect'],['detect']
Safety,"ournals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007329; Satterstrom, F. Kyle, et al. “ASD and ADHD have a similar burden of rare protein-truncating variants.” bioRxiv (2018): 277707. https://www.biorxiv.org/content/10.1101/277707v1; Zekavat, Seyedeh M., et al. “Deep coverage whole genome sequences and plasma lipoprotein (a) in individuals of European and African ancestries.” Nature Communications 9.1 (2018): 2606. https://www.nature.com/articles/s41467-018-04668-w; Natarajan, Pradeep, et al. “Deep-coverage whole genome sequences and blood lipids among 16,324 individuals.” Nature Communications 9.1 (2018): 3391. https://www.nature.com/articles/s41467-018-04668-w; Ganna, Andrea, et al. “Quantifying the impact of rare and ultra-rare coding variation across the phenotypic spectrum.” American Journal of Human Genetics 102.6 (2018): 1204-1211. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992130/; Khera, Amit V., et al. “Genome-wide polygenic scores for common diseases identify individuals with risk equivalent to monogenic mutations.” Nature Genetics 50.9 (2018): 1219. https://www.nature.com/articles/s41588-018-0183-z?_ga=2.263293700.980063710.1543017600-1151073636.1543017600; Roselli, Carolina, et al. “Multi-ethnic genome-wide association study for atrial fibrillation.” Nature Genetics 50.9 (2018): 1225. https://www.nature.com/articles/s41588-018-0133-9; Arachchi, Harindra, et al. “matchbox: An open‐source tool for patient matching via the Matchmaker Exchange.” Human Mutation 39.12 (2018): 1827-1834. https://onlinelibrary.wiley.com/doi/abs/10.1002/humu.23655; Laisk, Triin, et al. “GWAS meta-analysis highlights the hypothalamic-pituitary-gonadal axis (HPG axis) in the genetic regulation of menstrual cycle length.” bioRxiv (2018): 333708. https://www.biorxiv.org/content/10.1101/333708v1.abstract; Rees, Elliott, et al. “Association between schizophrenia and both loss of function and missense mutations in paralog conserved sites of voltage-gated sodium channel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:19841,risk,risk,19841,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Safety,"ows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122411,avoid,avoids,122411,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['avoid'],['avoids']
Safety,"pot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The storage expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, jobs requesting one or more cores receive; 5 GiB of storage for the root file system /. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at /io. Batch automatically writes all ResourceFile to; /io.; The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters:; storage (Union[str, int, None]) – Units are in bytes if storage is an int. If None, use the; default storage size for the ServiceBackend (0 Gi). Return type:; Self. Returns:; Same job object with storage set. timeout(timeout); Set the maximum amount of time this job can run for in seconds.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters:; timeout (Union[int, float, None]) – Maximum amount of time in seconds for a job to run before being killed.; If None, there is no timeout. Return type:; Self. Returns:; Same job object set with a timeout in seconds. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:9300,timeout,timeout,9300,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,6,['timeout'],['timeout']
Safety,"r, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version {}\n'.format(self.sc.version)); if self._jsc.uiWebUrl().isDefined():; sys.stderr.write('SparkUI available at {}\n'.format(self._jsc.uiWebUrl().get())). if not quiet:; connect_logger('localhost', 12888). sys.stderr.write(; 'Welcome to\n'; ' __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\_,_/_/_/ version {}\n'.format(self.version)). [docs] @staticmethod; def get_running():; """"""Return the running Hail context in this Python session. **Example**. .. doctest::; :options: +SKIP. >>> HailContext() # oops! Forgot to bind to 'hc'; >>> hc = HailContext.get_running() # recovery. Useful to recover a Hail context that has been created but is unbound. :return: Current Hail context.; :rtype: :class:`.HailContext`; """""". return Env.hc(). @property; def version(self):; """"""Return the version of Hail associated with this HailContext. :rtype: str; """"""; return self._jhc.version(). [docs] @handle_py4j; @typecheck_method(regex=strlike,; path=oneof(strlike, listof(strlike)),; max_count=integral); def grep(self, regex, path, max_count=100):; """"""Grep big files, like, really fast. **Examples**. Print all lines containing the string ``hello`` in *file.txt*:. >>> hc.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hc.grep('\d', ['data/file1.txt','data/file2.txt']). **Background**. :py:meth:`~hail.HailContext.grep` mimics the basic functionality of Unix ``grep`` in parallel, printing results to screen. This command is provided as a convenience to those in the statistical genetics community who often search enormous",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:3991,recover,recovery,3991,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['recover'],['recovery']
Safety,"regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#142",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16425,avoid,avoid,16425,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['avoid'],['avoid']
Safety,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27494,avoid,avoids,27494,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['avoid'],['avoids']
Safety,"result of row_correlation() using; linalg.utils.locus_windows() and; BlockMatrix.sparsify_row_intervals(); in order to only compute linkage disequilibrium between nearby; variants. Use row_correlation() directly to calculate correlation; without windowing.; More precisely, variants are 0-indexed by their order in the matrix table; (see add_row_index()). Each variant is regarded as a vector of; elements defined by entry_expr, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant.; The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants \(i\) and \(j\) are on the; same contig and within radius base pairs (inclusive) then the; \((i, j)\) element is their; Pearson correlation coefficient.; Otherwise, the \((i, j)\) element is 0.0.; Rows with a constant value (i.e., zero variance) will result in nan; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; aggregators.stats()).; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that’s; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; See the warnings in row_correlation(). In particular, for large; matrices it may be preferable to run its stages separately.; entry_expr and locus_expr are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:40964,avoid,avoid,40964,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['avoid'],['avoid']
Safety,"rite a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the cor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:3414,predict,predict,3414,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['predict'],['predict']
Safety,"rmatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; Unlike the BashJob, a PythonJob returns a new; PythonResult for every invocation of PythonJob.call(). A; PythonResult can be used as an argument in subsequent invocations of; PythonJob.call(), as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:2783,detect,detects,2783,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['detect'],['detects']
Safety,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20662,abort,aborting,20662,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['abort'],['aborting']
Safety,"rray(hl.tint32), hl.tarray(hl.tint32)); return _func(""locus_windows_per_contig"", rt, coords, radius). [docs]@typecheck(a=expr_array(), seed=nullable(builtins.int)); def shuffle(a, seed: Optional[builtins.int] = None) -> ArrayExpression:; """"""Randomly permute an array. Example; -------. >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters; ----------; a : :class:`.ArrayExpression`; Array to permute.; seed : :obj:`int`, optional; Random seed. Returns; -------; :class:`.ArrayExpression`; """"""; return sorted(a, key=lambda _: hl.rand_unif(0.0, 1.0)). [docs]@typecheck(path=builtins.str, point_or_interval=expr_any); def query_table(path, point_or_interval):; """"""Query records from a table corresponding to a given point or range of keys. Notes; -----; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in :meth:`.Table.annotate`. Warning; -------; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters; ----------; path : :class:`str`; Table path.; point_or_interval; Point or interval to query. Returns; -------; :class:`.ArrayExpression`; """"""; table = hl.read_table(path); row_typ = table.row.dtype. key_typ = table.key.dtype; key_names = list(key_typ); len = builtins.len; if len(key_typ) == 0:; raise ValueError(""query_table: cannot query unkeyed table""). def coerce_endpoint(point):; if point.dtype == key_typ[0]:; point = hl.struct(**{key_names[0]: point}); ts = point.dtype; if isinstance(ts, tstruct):; i = 0; while i < len(ts):; if i >= len(key_typ):; raise ValueError(; f""query_table: queried with {len(ts)} key field(s), but table only has {len(key_typ)} key field(s)""; ); if key_typ[i] != ts[i]:; raise ValueError(; f""query_table: key mismatch at key field {i} ({list(ts.keys())[i]!r}): query type is {ts[i]}, table key type is {key_typ[i]}""; ); i += 1. if i == 0:; raise ValueError(""query_table: cannot query with empty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:186286,safe,safeguards,186286,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['safe'],['safeguards']
Safety,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6804,avoid,avoid,6804,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['avoid'],['avoid']
Safety,"s.” bioRxiv (2019): 538421. https://www.biorxiv.org/content/10.1101/538421v1; Kurki, Mitja I., et al. “Contribution of rare and common variants to intellectual disability in a sub-isolate of Northern Finland.” Nature Communications 10.1 (2019): 410. https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/30679432/; Martin, Alicia R., et al. “Current clinical use of polygenic scores will risk exacerbating health disparities.” bioRxiv (2019): 441261. https://www.biorxiv.org/content/10.1101/441261v3; Collaborative, Epi25, et al. “Ultra-rare genetic variation in the epilepsies: a whole-exome sequencing study of 17,606 individuals.” American Journal of Human Genetics (2019): https://www.cell.com/ajhg/fulltext/S0002-9297(19)30207-1; Karczewski, Konrad J., et al. “The mutational constraint spectrum quantified from variation in 141,456 humans.” bioRxiv (2019): 531210. https://www.biorxiv.org/content/10.1101/531210v4; Whiffin, Nicola, et al. “Human loss-of-function variants suggest that partial LRRK2 inhibition is a safe therapeutic strategy for Parkinsons disease.” bioRxiv() (2019): 561472. https://www.biorxiv.org/content/10.1101/561472v1; Cummings, Beryl B., et al. “Transcript expression-aware annotation improves rare variant discovery and interpretation.” bioRxiv (2019): 554444. https://www.biorxiv.org/content/10.1101/554444v1; Wang, Qingbo, et al. “Landscape of multi-nucleotide variants in 125,748 human exomes and 15,708 genomes.” bioRxiv (2019): 573378. https://www.biorxiv.org/content/10.1101/573378v2; Minikel, Eric Vallabh, et al. “Evaluating potential drug targets through human loss-of-function genetic variation.” bioRxiv (2019): 530881. https://www.biorxiv.org/content/10.1101/530881v2; Collins, Ryan L., et al. “An open resource of structural variation for medical and population genetics.” bioRxiv (2019): 578674. https://www.biorxiv.org/content/10.1101/578674v1; Whiffin, Nicola, et al. “Characterising the loss-of-function impact of 5’untranslated region variants in whole genom",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:14889,safe,safe,14889,references.html,https://hail.is,https://hail.is/references.html,1,['safe'],['safe']
Safety,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:14061,avoid,avoid,14061,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avoid'],['avoid']
Safety,"ssociated type.; Hail defines the following types:; Primitives: - Int -; Double -; Float -; Long -; Boolean -; String; Compound Types: - Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; Genetic Types: - Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call. Primitive Types¶; Let’s start with simple primitive types. Primitive types are a basic; building block for any programming language - these are things like; numbers and strings and boolean values.; Hail expressions are passed as Python strings to Hail methods. In [2]:. # the Boolean literals are 'true' and 'false'; hc.eval_expr_typed('true'). Out[2]:. (True, Boolean). The return value is True, not true. Why? When values are; returned by Hail methods, they are returned as the corresponding Python; value. In [3]:. hc.eval_expr_typed('123'). Out[3]:. (123, Int). In [4]:. hc.eval_expr_typed('123.45'). Out[4]:. (123.45, Double). String literals are denoted with double-quotes. The ‘u’ preceding the; printed result denotes a unicode string, and is safe to ignore. In [5]:. hc.eval_expr_typed('""Hello, world""'). Out[5]:. (u'Hello, world', String). Primitive types support all the usual operations you’d expect. For; details, refer to the documentation on; operators and; types. Here are some examples. In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:4038,safe,safe,4038,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['safe'],['safe']
Safety,"tDataset`; """""". jvds = self._jvdf.lmmreg(kinshipMatrix._jkm, y, jarray(Env.jvm().java.lang.String, covariates),; use_ml, global_root, va_root, run_assoc, joption(delta), sparsity_threshold,; use_dosages, joption(n_eigs), joption(dropped_variance_fraction)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(test=strlike,; y=strlike,; covariates=listof(strlike),; root=strlike,; use_dosages=bool); def logreg(self, test, y, covariates=[], root='va.logreg', use_dosages=False):; """"""Test each variant for association using logistic regression. .. include:: requireTGenotype.rst. **Examples**. Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:. >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). **Notes**. The :py:meth:`~hail.VariantDataset.logreg` method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth'). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes. By default, genotypes values are given by hard call genotypes (``g.gt``).; If ``use_dosages=True``, then genotype values are defined by the dosage; :math:`\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})`. For Phred-scaled values,; :math:`\mathrm{P}(\mathrm{Het})` and :math:`\mathrm{P}(\mathrm{HomVar})` are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1. The example above considers a model of the form. .. math::.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:139834,predict,predicting,139834,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['predict'],['predicting']
Safety,"t_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = state. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). @typecheck_method(item=str); def __getitem__(self, item):; return self._get_field(item). def __setattr__(self, key, value):; if key in self._fields:; raise ValueError(""Structs are immutable, cannot overwrite a field.""); else:; super().__setattr__(key, value). def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]; elif item in self._fields:; return self._fields[item]; else:; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __repr__(self):; return str(self). def __str__(self):; if a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:1718,avoid,avoid,1718,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['avoid'],['avoid']
Safety,"top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42629,safe,safer,42629,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['safe'],['safer']
Safety,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22265,avoid,avoid,22265,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,"['avoid', 'redund']","['avoid', 'redundant']"
Safety,"ult_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. read_input_group(**kwargs); Create a new resource group representing a mapping of identifier to; input resource files. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read a binary PLINK file:; >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() . Read a FASTA file and it’s index (file extensions matter!):; >>> fasta =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:8116,avoid,avoid,8116,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['avoid'],['avoid']
Safety,"v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:; >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:; >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; Rows with a constant value (i.e., zero variance) will result nan; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of aggregators.stats()). Notes; In this method, each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:20894,avoid,avoid,20894,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['avoid'],['avoid']
Safety,"variant dataset to have 500 partitions:; >>> vds_result = vds.repartition(500). Notes; Check the current number of partitions with num_partitions().; The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with \(M\) variants is first imported, each of the \(k\) partition will contain about \(M/k\) of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it’s recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores.; Partitions are a core concept of distributed computation in Spark, see here for details. With shuffle=True, Hail does a full shuffle of the data and creates equal sized partitions. With shuffle=False, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the repartition and coalesce commands in Spark, respectively. In particular, when shuffle=False, num_partitions cannot exceed current number of partitions. Parameters:; num_partitions (int) – Desired number of partitions, must be less than the current number if shuffle=False; shuffle (bool) – If true, use full shuffle to repartition. Returns:Variant dataset with the number of partitions equal to at most num_partitions. Return type:VariantDataset. rowkey_schema¶; Returns the signature of the row key (variant) contained in this VDS.; Examples; >>> print(vds.rowkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.rowkey_schema). Return type:Type. rrm(force_block=False, force_gramian=False)[source]¶; Computes the Realized Relationship Matrix (RRM).; Examples; >>> kinship_matrix = vds.rrm(). Notes; The Realized Relationship Matrix i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:152622,avoid,avoid,152622,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['avoid'],['avoid']
Safety,"white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters:. fn (Callable) – The function to execute.; iterables (Iterable[Any]) – The iterables are zipped together and each tuple is used as; arguments to fn. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of iterables must; have the same length.; timeout (Union[int, float, None]) – This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator’s; BatchPoolFuture; iterator.__next__() invokes BatchPoolFuture.result() with this; timeout.; chunksize (int) – The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Py",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:5671,timeout,timeout,5671,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['timeout'],['timeout']
Safety,"with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.row_key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_matrix_table(tmp2).add_row_index(uid).key_rows_by(uid); ht.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions).drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_matrix_table(tmp, _n_partitions=n_partitions). return MatrixTable(; ir.MatrixRepartition(; self._mir, n_partitions",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:108218,avoid,avoid,108218,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['avoid'],['avoid']
Safety,"xecutes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:1633,safe,safely,1633,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['safe'],['safely']
Safety,"üsün S Kizilkaya, Lærke S; 	 Gasbjerg, Alexander S Hauser, Jørgen Rungby, Henrik T Sørensen, Allan Vaag, Jens S; 	 Nielsen, Oluf Pedersen, Allan Linneberg, Bolette Hartmann, Anette P Gjesing, Jens J; 	 Holst, Torben Hansen, Mette M Rosenkilde, Niels Grarup, Rare Heterozygous; 	 Loss-of-Function Variants in the Human GLP-1 Receptor Are Not Associated With; 	 Cardiometabolic Phenotypes, The Journal of Clinical Endocrinology & Metabolism, Volume; 	 108, Issue 11, November 2023, Pages; 	 2821–2833, https://doi.org/10.1210/clinem/dgad290. https://academic.oup.com/jcem/article/108/11/2821/7180819. 	 Vukadinovic, Milos et al. Deep learning-enabled analysis of medical images identifies; 	 cardiac sphericity as an early marker of cardiomyopathy and related outcomes. Med,; 	 Volume 4, Issue 4, 252 - 262.e3. https://www.cell.com/med/fulltext/S2666-6340(23)00069-7. 	 Epi25 Collaborative; Chen S, Neale BM, Berkovic SF. Shared and distinct ultra-rare; 	 genetic risk for diverse epilepsies: A whole-exome sequencing study of 54,423; 	 individuals across multiple genetic ancestries. medRxiv [Preprint]. 2023 Feb; 	 24:2023.02.22.23286310. doi: 10.1101/2023.02.22.23286310. PMID: 36865150; PMCID:; 	 PMC9980234. https://pubmed.ncbi.nlm.nih.gov/36865150/. 	 Kurki, M.I., Karjalainen, J., Palta, P. et al. FinnGen provides genetic insights from a; 	 well-phenotyped isolated population. Nature 613, 508–518; 	 (2023). https://doi.org/10.1038/s41586-022-05473-8 https://www.nature.com/articles/s41586-022-05473-8. 	 Mortensen, Ó., Thomsen, E., Lydersen, L.N. et al. FarGen: Elucidating the distribution; 	 of coding variants in the isolated population of the Faroe Islands. Eur J Hum Genet 31,; 	 329–337; 	 (2023). https://doi.org/10.1038/s41431-022-01227-2 https://www.nature.com/articles/s41431-022-01227-2. 	 Steiner, H.E., Carrion, K.C., Giles, J.B., Lima, A.R., Yee, K., Sun, X., Cavallari,; 	 L.H., Perera, M.A., Duconge, J. and Karnes, J.H. (2023), Local Ancestry-Informed; 	 Candidate Pathway Ana",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:5415,risk,risk,5415,references.html,https://hail.is,https://hail.is/references.html,1,['risk'],['risk']
Security," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:1320,access,access,1320,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['access'],['access']
Security," Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Table; GroupedTable; MatrixTable; GroupedMatrixTable. Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions; init(); asc(); desc(); stop(); spark_context(); tmp_dir(); default_reference(); get_reference(); set_global_seed(); reset_global_randomness(); citation(); version(). hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API. View page source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:1059,access,access,1059,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['access'],['access']
Security," HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Aggregable; Aggregable[Array[Double]]; Aggregable[Array[Float]]; Aggregable[Array[Int]]; Aggregable[Array[Long]]; Aggregable[Double]; Aggregable[Float]; Aggregable[Genotype]; Aggregable[Int]; Aggregable[Long]; Aggregable[T]. AltAllele; Array; Array[Array[T]]; Array[Boolean]; Array[Double]; Array[Float]; Array[Int]; Array[Long]; Array[String]; Array[T]. Boolean; Call; Dict; Double; Float; Genotype; Int; Interval; Locus; Long; Set; Set[Double]; Set[Float]; Set[Int]; Set[Long]; Set[Set[T]]; Set[String]; Set[T]. String; Struct; Variant. Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Types. View page source. Types¶. Aggregable¶; An Aggregable is a Hail data type representing a distributed row or column of a matrix. Hail exposes a number of methods to compute on aggregables depending on the data type. Aggregable[Array[Double]]¶. sum(): Array[Double] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Float]]¶. sum(): Array[Float] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Array[Int]]¶. sum(): Array[Int]. Compute the sum by index. All elements in the aggregable must have the same length.; Examples; Count the total number of occurrences of each allele across samples, per variant:; >>> vds_result = vds.annotate_variants_expr('va.AC = gs.map(g => g.oneHotAlleles(v)).sum()'). Aggregable[Array[Long]]¶. sum(): Array[Long] – Compute the sum by index. All elements in the aggregable must have the same length. Aggregable[Double]¶. hist(start: Double, end: Double, bins: Int): Struct{binEdges:Array[Double],binFrequencies:Array[Long],nLess:Long,nGreater:Long}. binEdges (Array[Double]) – Array of bin cutoffs; binFrequencies (Array[Long]) – Number of e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:1000,expose,exposes,1000,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['expose'],['exposes']
Security," In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11560,expose,exposes,11560,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['expose'],['exposes']
Security," Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:1207,password,password,1207,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['password'],['password']
Security," Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.expr. Source code for hail.expr; import abc; from hail.java import scala_object, Env, jset; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call. class TypeCheckError(Exception):; """"""; Error thrown at mismatch between expected and supplied python types. :param str message: Error message; """""". def __init__(self, message):; self.msg = message; super(TypeCheckError).__init__(TypeCheckError). def __str__(self):; return self.msg. [docs]class Type(object):; """"""; Hail type superclass used for annotations and expression language.; """"""; __metaclass__ = abc.ABCMeta. def __init__(self, jtype):; self._jtype = jtype. def __repr__(self):; return str(self). def __str__(self):; return self._jtype.toPrettyString(0, True, False). def __eq__(self, other):; return self._jtype.equals(other._jtype). def __hash__(self):; return self._jtype.hashCode(). [docs] def pretty(self, indent=0, attrs=False):; """"""Returns a prettily formatted string representation of the type. :param int indent: Number of spaces to indent. :param bool attrs: Print struct field attributes. :rtype: str; """""". return self._jtype.toPrettyString(indent, False, attrs). @classmethod; def _from_java(cls, jtype):; # FIXME string matching is pretty hacky; class_name = jtype.getClass().getCanonicalName(). if class_name in __singletons__:; return __singletons__[class_name](); elif class_name == 'is.hail.expr.TArray':; return TArray._from_java(jtype); elif class_name == 'is.hail.expr.TSet':; return TSet._from_java(jtype); elif class_name == 'is.hail.expr.TDict':; return TDict._from_java(jtype); elif class_name == 'is.hail.expr.TStruct':; return TStruct._from_java(jtype); else:; raise TypeError(""unknown type class: '%s'"" % class_name). @abc.abstractmethod; def _typecheck(self, annotation):; """"""; Raise an exception if the given annotation is not the appropriate type. :pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/expr.html:1141,hash,hashCode,1141,docs/0.1/_modules/hail/expr.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html,1,['hash'],['hashCode']
Security," ReferenceGenome. View page source. ReferenceGenome. class hail.genetics.ReferenceGenome[source]; An object that represents a reference genome.; Examples; >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; Hail comes with predefined reference genomes (case sensitive!):. GRCh37, Genome Reference Consortium Human Build 37; GRCh38, Genome Reference Consortium Human Build 38; GRCm38, Genome Reference Consortium Mouse Build 38; CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using get_reference():; >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using read will add the reference genome to the list of; known references; it is possible to access the reference genome using; get_reference() anytime afterwards. Note; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; Hail allows setting a default reference so that the reference_genome; argument of import_vcf() does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the default_reference argument of; init(). In order to set a custom reference genome as default,; pass the reference as an argument to default_reference() after; initializing Hail. Parameters:. name (str) – Name of reference. Must be unique and NOT one of Hail’s; predefined references: 'GRCh37', 'GRCh38', 'GRCm38',; 'CanFam3' and 'default'.; contigs (list of str) – Contig names.; lengths (dict of str to int) – Dict of contig names to contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contig",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:1714,access,access,1714,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['access'],['access']
Security," Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. You can count the rows of a table. [6]:. users.count(). [6]:. 943. You can access fields of tables with the Python attribute notation table.field, or with index notation table['field']. The latter is useful when the field names are not valid Python identifiers (if a field name includes a space, for example). [7]:. users.occupation.describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. [8]:. users['occupation'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f39046280d0>; Index:; ['row']; --------------------------------------------------------. users.occupation and users['occupation'] are Hail Expressions; Lets peak at their using show. Notice that the key is shown as well!. [9]:. users.occupation.show(). idoccupationint32str; 1""technician""; 2""oth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:3604,access,access,3604,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['access'],['access']
Security," check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the ca",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:2313,hash,hash,2313,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['hash'],['hash']
Security," form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which keys index the; expression, if any. For example, mt.DP.describe() tells us that mt.DP; has type int32 and is an entry field of mt, since it is indexed; by both rows and columns:; >>> mt.DP.describe() ; ---------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:2917,access,access,2917,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['access'],['access']
Security," import Locus; from hail.typecheck import *. interval_type = lazy(). [docs]class Interval(object):; """"""; A genomic interval marked by start and end loci. .. testsetup::. interval1 = Interval.parse('X:100005-X:150020'); interval2 = Interval.parse('16:29500000-30200000'). :param start: inclusive start locus; :type start: :class:`.Locus`; :param end: exclusive end locus; :type end: :class:`.Locus`; """""". @handle_py4j; def __init__(self, start, end):; if not (isinstance(start, Locus) and isinstance(end, Locus)):; raise TypeError('expect arguments of type (Locus, Locus) but found (%s, %s)' %; (str(type(start)), str(type(end)))); jrep = scala_object(Env.hail().variant, 'Locus').makeInterval(start._jrep, end._jrep); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Interval(start=%s, end=%s)' % (repr(self.start), repr(self.end)). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._start = Locus._from_java(self._jrep.start()). @classmethod; def _from_java(cls, jrep):; interval = Interval.__new__(cls); interval._init_from_java(jrep); return interval. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a genomic interval from string representation. **Examples**:. >>> interval_1 = Interval.parse('X:100005-X:150020'); >>> interval_2 = Interval.parse('16:29500000-30200000'); >>> interval_3 = Interval.parse('16:29.5M-30.2M') # same as interval_2; >>> interval_4 = Interval.parse('16:30000000-END'); >>> interval_5 = Interval.parse('16:30M-END') # same as interval_4; >>> interval_6 = Interval.parse('1-22') # autosomes; >>> interval_7 = Interval.parse('X') # all of chromosome X. There are several acceptable representations. ``CHR1:POS1-CHR2:POS2`` is the fully specified representation, and; we use this to define the various shortcut representations. In a ``POS`` field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/interval.html:1415,hash,hashCode,1415,docs/0.1/_modules/hail/representation/interval.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/interval.html,1,['hash'],['hashCode']
Security," in a dictionary, indexing; with exprs.; Examples; In the example below, both table1 and table2 are keyed by one; field ID of type int.; >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using key as the sole index expression is equivalent to passing all; key fields individually:; >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:35650,expose,expose,35650,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['expose'],['expose']
Security," less than the threshold given by maf-threshold are removed; Variants in the pseudoautosomal region (X:60001-2699520) || (X:154931044-155260560) are included if the include_par optional parameter is set to true.; The minor allele frequency (maf) per variant is calculated.; For each variant and sample with a non-missing genotype call, \(E\), the expected number of homozygotes (from population MAF), is computed as \(1.0 - (2.0*maf*(1.0-maf))\).; For each variant and sample with a non-missing genotype call, \(O\), the observed number of homozygotes, is computed as 0 = heterozygote; 1 = homozygote; For each variant and sample with a non-missing genotype call, \(N\) is incremented by 1; For each sample, \(E\), \(O\), and \(N\) are combined across variants; \(F\) is calculated by \((O - E) / (N - E)\); A sex is assigned to each sample with the following criteria: F < 0.2 => Female; F > 0.8 => Male. Use female-threshold and male-threshold to change this behavior. Annotations; The below annotations can be accessed with sa.imputesex. isFemale (Boolean) – True if the imputed sex is female, false if male, missing if undetermined; Fstat (Double) – Inbreeding coefficient; nTotal (Long) – Total number of variants considered; nCalled (Long) – Number of variants with a genotype call; expectedHoms (Double) – Expected number of homozygotes; observedHoms (Long) – Observed number of homozygotes. Parameters:; maf_threshold (float) – Minimum minor allele frequency threshold.; include_par (bool) – Include pseudoautosomal regions.; female_threshold (float) – Samples are called females if F < femaleThreshold; male_threshold (float) – Samples are called males if F > maleThreshold; pop_freq (str) – Variant annotation for estimate of MAF.; If None, MAF will be computed. Returns:Annotated dataset. Return type:VariantDataset. join(right)[source]¶; Join two variant datasets.; Notes; This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations fr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:72812,access,accessed,72812,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessed']
Security," method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8341,authenticat,authenticate,8341,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['authenticat'],['authenticate']
Security," variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8446,validat,validate,8446,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['validat'],['validate']
Security,"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:1471,hash,hashCode,1471,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['hash'],['hashCode']
Security,"(f""\n {k!r}"" for k in mt.entry if k in used_ref_block_fields); ). rmt = mt.filter_entries(; hl.case(); .when(hl.is_missing(mt.END), False); .when(hl.is_defined(mt.END) & mt[gt_field].is_hom_ref(), True); .or_error(; hl.str(; 'cannot create VDS from merged representation -' ' found END field with non-reference genotype at '; ); + hl.str(mt.locus); + hl.str(' / '); + hl.str(mt.col_key[0]); ); ); rmt = rmt.select_entries(*(x for x in rmt.entry if x in used_ref_block_fields)); rmt = rmt.filter_rows(hl.agg.count() > 0). rmt = rmt.key_rows_by('locus').select_rows().select_cols(). if is_split:; rmt = rmt.distinct_by_row(). vmt = mt.filter_entries(hl.is_missing(mt.END)).drop('END')._key_rows_by_assert_sorted('locus', 'alleles'); vmt = vmt.filter_rows(hl.agg.count() > 0). return VariantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:7475,validat,validate,7475,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['validat'],['validate']
Security,"(self._ir.ref.name, f, t), t, self._indices, self._aggregations; ); elif isinstance(self._ir, ir.SelectFields):; expr = construct_expr(ir.GetField(self._ir.old, f), t, self._indices, self._aggregations); else:; expr = construct_expr(ir.GetField(self._ir, f), t, self._indices, self._aggregations); self._set_field(f, expr). def _set_field(self, key, value):; if key not in self._fields:; # Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.; if key in self.__dict__ or hasattr(super(), key):; self._warn_on_shadowed_name.add(key); else:; self.__dict__[key] = value; self._fields[key] = value. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). def __getattribute__(self, item):; if item in super().__getattribute__('_warn_on_shadowed_name'):; warning(; f'Field {item} is shadowed by another method or attribute. '; f'Use [""{item}""] syntax to access the field.'; ); self._warn_on_shadowed_name.remove(item); return super().__getattribute__(item). def __getattr__(self, item):; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __bool__(self):; return bool(len(self)). [docs] @typecheck_method(item=oneof(str, int, slice)); def __getitem__(self, item):; """"""Access a field of the struct by name or index. Examples; --------. >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.Expression`; Struct field.; """"""; if isinstance(item, str):; return self._get_field(item); if isinstance(item, int):; return self._get_field(self.dtype.fields[item]); else:; assert item.start is None or isinstance(item.start, int); assert item.stop is None or isinstance(item.stop, int); assert item.step is None or isinstance(item.step, int); return self.select(*self.dtype.fields[item.start : item.stop : item.ste",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:45057,access,access,45057,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['access']
Security,", 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression). Returns:; ArrayNumericExpression. __sub__(other)[source]; Positionally subtract an array or a scalar.; Examples; >>> hl.eval(a2 - 1); [0, -2, 0, -2, 0, -2]. >>> hl.eval(a1 - a2); [-1, 2, 1, 4, 3, 6]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to subtract. Returns:; ArrayNumericExpression – Array of positional differences. __truediv__(other)[source]; Positionally divide by an array or a scalar.; Examples; >>> hl.eval(a1 / 10) ; [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. >>> hl.eval(a2 / a1) ; [inf, -1.0, 0.5, -0.3333333333333333, 0.25, -0.2]. Parameters:; other (NumericExpression or ArrayNumericExpression) – Value or array to divide by. Returns:; ArrayNumericExpression – Array of positional quotients. aggregate(f); Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:5205,access,accessing,5205,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['access'],['accessing']
Security,",; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:3271,access,accessible,3271,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['access'],['accessible']
Security,"-----+-------+-----+-------+-------+-------+-------+-------+-------+. but the value 5 is only stored once for the entire dataset and NOT once per; row of the table. The output of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:3728,access,accessed,3728,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['access'],['accessed']
Security,"--------------------------------. [4]:. mt.GT.describe(). --------------------------------------------------------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6040,access,accessed,6040,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,2,['access'],['accessed']
Security,"----------; Type:; call; --------------------------------------------------------; Source:; <hail.matrixtable.MatrixTable object at 0x7efd1f44dc10>; Index:; ['row', 'column']; --------------------------------------------------------. MatrixTable operations; We belabored the operations on tables because they all have natural analogs (sometimes several) on matrix tables. For example:. count => count_{rows, cols} (and count which returns both); filter => filter_{rows, cols, entries}; annotate => annotate_{rows, cols, entries} (and globals for both); select => select_{rows, cols, entries} (and globals for both); transmute => transmute_{rows, cols, entries} (and globals for both); group_by => group_{rows, cols}_by; explode => expode_{rows, cols}; aggregate => aggregate_{rows, cols, entries}. Some operations are unique to MatrixTable:. The row fields can be accessed as a Table with rows; The column fields can be accessed as a Table with cols.; The entire field space of a MatrixTable can be accessed as a coordinate-form Table with entries. Be careful with this! While it’s fast to aggregate or query, trying to write this Table to disk could produce files thousands of times larger than the corresponding MatrixTable. Let’s explore mt using these tools. Let’s get the size of the dataset. [5]:. mt.count() # (rows, cols). [5]:. (10879, 284). Let’s look at the first few row keys (variants) and column keys (sample IDs). [6]:. mt.rows().select().show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; 1:2252970[""C"",""T""]; 1:2284195[""T"",""C""]; 1:2779043[""T"",""C""]; 1:2944527[""G"",""A""]; 1:3761547[""C"",""A""]; showing top 10 rows. [7]:. mt.s.show(). sstr; ""HG00096""; ""HG00099""; ""HG00105""; ""HG00118""; ""HG",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:6175,access,accessed,6175,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['access'],['accessed']
Security,"---; :class:`.ArrayExpression`; All key/value pairs in the dictionary.; """"""; return hl.array(self). def _extra_summary_fields(self, agg_result):; return {; 'Min Size': agg_result[0],; 'Max Size': agg_result[1],; 'Mean Size': agg_result[2],; }. def _nested_summary(self, agg_result, top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42371,access,accessible,42371,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['accessible']
Security,"---; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; # pile of hacks to preserve some legacy behavior, like auto gzip; fs = Env.fs(); if isinstance(fs, HadoopFS):; return fs.legacy_open(path, mode, buffer_size); _, ext = os.path.splitext(path); if ext in ('.gz', '.bgz'):; binary_mode = 'wb' if mode[0] == 'w' else 'rb'; file = fs.open(path, binary_mode, buffer_size); file = gzip.GzipFile(fileobj=file, mode=mode); if 'b' not in mode:; file = io.TextIOWrapper(file, encoding='utf-8'); else:; file = fs.open(path, mode, buffer_size); return file. [docs]@typecheck(src=str, dest=str); def hadoop_copy(src, dest):; """"""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:2988,access,access,2988,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,1,['access'],['access']
Security,". ; . Installation; Mac OS X; Linux; Google Dataproc; Azure HDInsight; Next Steps. Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/azure.html:1156,password,password,1156,docs/0.2/install/azure.html,https://hail.is,https://hail.is/docs/0.2/install/azure.html,2,"['access', 'password']","['access', 'password']"
Security,". Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million. How can we make this more useful? With filter!. In [28]:. vds.query_genotypes('gs.filter(g => g.isHet()).count()'). Out[28]:. 2583309L. Of the 11 million genotypes in the dataset, about 2.5M are heterozygous.; What about combining sample annotations with genotype information? How; many heterozygote genotypes are found in the American samples? A simple; way to implement this is by filtering to American samples first and then; running the same query. In [29]:. (vds.filter_samples_expr('sa.metadata.SuperPopulation == ""AMR""'); .query_genotypes('gs.filter(g => g.isHet()).count()')). Out[29]:. 754850L. The next cell is a bit tricky - aggregables have an extra “context” that; they carry around. We can actually access the sample, sample; annotations, variant, and variant annotations inside of operations on; gs. We don’t need to filter samples first, we can do it inside the; query:. In [30]:. vds.query_genotypes('gs.filter(g => g.isHet() && sa.metadata.SuperPopulation == ""AMR"").count()'). Out[30]:. 754850L. Here’s an example where we use the variant annotations to count the; number of heterozygous genotypes in Americans at rare loci. In [31]:. vds.query_genotypes('''gs.filter(g => g.isHet(); && sa.metadata.SuperPopulation == ""AMR""; && va.qc.AF < 0.01).count()'''). Out[31]:. 1879L. Sum¶; The sum aggregator can be used to compute useful statistics per; sample or variant. For example, we may want to count the total number of; non-reference alleles per sample:. In [32]:. (vds.annotate_samples_expr('sa.nNonRefAlleles = gs.map(g => g.nNonRefAlleles()).sum()'); .query_samples('samples.map(s => sa.nNonRefAlleles).take(10)')). Out[32]:. [6423, 6530, 6606, 6638, 6570, 6572, 6542, 6490, 6606, 6464]. Fraction¶; The fraction aggregator can actuall",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:13208,access,access,13208,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['access'],['access']
Security,". Python API — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob. Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture. Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend. Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API. View page source. Python API; This is the API documentation for Batch, and provides detailed information; on the Python programming interface.; Use import hailtop.batch to access this functionality. Batches; A Batch is an object that represents the set of jobs to run; and the order or dependencies between the jobs. Each Job has; an image in which to execute commands and settings for storage,; memory, and CPU. A BashJob is a subclass of Job; that runs bash commands while a PythonJob executes Python; functions. batch.Batch; Object representing the distributed acyclic graph (DAG) of jobs to run. job.Job; Object representing a single job to execute. job.BashJob; Object representing a single bash job to execute. job.PythonJob; Object representing a single Python job to execute. Resources; A Resource is an abstract class that represents files in a Batch and; has two subtypes: ResourceFile and ResourceGroup.; A single file is represented by a ResourceFile which has two subtypes:; InputResourceFile and JobResourceFile. An InputResourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:814,access,access,814,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['access'],['access']
Security,". Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the loc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8872,access,access,8872,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['access'],['access']
Security,".tdt.chi2** (*Double*) -- TDT statistic. - **va.tdt.pval** (*Double*) -- p-value. :param pedigree: Sample pedigree.; :type pedigree: :class:`~hail.representation.Pedigree`. :param root: Variant annotation root to store TDT result. :return: Variant dataset with TDT association results added to variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.tdt(pedigree._jrep, root); return VariantDataset(self.hc, jvds). @handle_py4j; def _typecheck(self):; """"""Check if all sample, variant and global annotations are consistent with the schema."""""". self._jvds.typecheck(). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike); def variant_qc(self, root='va.qc'):; """"""Compute common variant statistics (quality control metrics). .. include:: requireTGenotype.rst. **Examples**. >>> vds_result = vds.variant_qc(). .. _variantqc_annotations:. **Annotations**. :py:meth:`~hail.VariantDataset.variant_qc` computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with ``va.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+--------------------------------------------------------+; | Name | Type | Description |; +===========================+========+========================================================+; | ``callRate`` | Double | Fraction of samples with called genotypes |; +---------------------------+--------+--------------------------------------------------------+; | ``AF`` | Double | Calculated alternate allele frequency (q) |; +---------------------------+--------+--------------------------------------------------------+; | ``AC`` | Int | Count of alternate alleles |; +---------------------------+--------+--------------------------------------------------------+; | ``rHeterozygosity`` | Double | Proportion of heterozygotes |; +---------------------------+--------+--------------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:217372,access,accessed,217372,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessed']
Security,"0012:A:T`` or ``22:140012:A:TTT``. - The variant ``22:140012:A:T`` will not be annotated by; ``22:140012:A:T,TTT``. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run :py:meth:`.split_multi` before :py:meth:`.annotate_variants_vds`. :param VariantDataset other: Variant dataset to annotate with. :param str root: Sample annotation path to add variant annotations. :param str expr: Annotation expression. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; '''. jvds = self._jvds.annotateVariantsVDS(other._jvds, joption(root), joption(expr)). return VariantDataset(self.hc, jvds). [docs] def annotate_variants_db(self, annotations, gene_key=None):; """"""; Annotate variants using the Hail annotation database. .. warning::. Experimental. Supported only while running Hail on the Google Cloud Platform. Documentation describing the annotations that are accessible through this method can be found :ref:`here <sec-annotationdb>`. **Examples**. Annotate variants with CADD raw and PHRED scores:. >>> vds = vds.annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) # doctest: +SKIP. Annotate variants with gene-level PLI score, using the VEP-generated gene symbol to map variants to genes: . >>> pli_vds = vds.annotate_variants_db('va.gene.constraint.pli') # doctest: +SKIP. Again annotate variants with gene-level PLI score, this time using the existing ``va.gene_symbol`` annotation ; to map variants to genes:. >>> vds = vds.annotate_variants_db('va.gene.constraint.pli', gene_key='va.gene_symbol') # doctest: +SKIP. **Notes**. Annotations in the database are bi-allelic, so splitting multi-allelic variants in the VDS before using this ; method is recommended to capture all appropriate annotations from the database. To do this, run :py:meth:`split_multi` ; prior to annotating variants with this method:. >>> vds = vds.split_multi().annotate_variants_db(['",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:32875,access,accessible,32875,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previously internal _num_allele_type as numeric_allele_type; and deprecated it. Add new AlleleType enumeration for users to be; able to easily use the values returned by numeric_allele_type.; (#14297); vds.sample_gc now uses independent aggregators. Users may now; import these functions and use them directly.; (#14405); VariantDataset.validate now checks that all ref blocks are no; longer than the ref_block_max_length field, if it exists. Bug Fixes. (#14420) Fixes a; serious, but likely rare, bug in the Table/MatrixTable reader, which; has been present since Sep 2020. It manifests as many (around half or; more) of the rows being dropped. This could only happen when 1); reading a (matrix)table whose partitioning metadata allows rows with; the same key to be split across neighboring partitions, and 2); reading it with a different partitioning than it was written. 1); would likely only happen by reading data keyed by locus and alleles,; and rekeying it to only locus before writing. 2) would likely only; happen by using the _intervals or _n_partitions arguments to; read_(matrix)_table, or possibly repartition. Please reach; out to us if you’re concerned you may have been affected by this.; (#14330) Fixes; erroneous error in export_vcf with unphased haploid Calls.; (#14303) Fix; missingness error when sampling entries from a MatrixTable.; (#14288) Contigs may; now be comp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:13792,validat,validate,13792,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['validat'],['validate']
Security,"971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:19000,access,accessible,19000,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['access'],['accessible']
Security,":`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2160,hash,hash,2160,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,1,['hash'],['hash']
Security,"; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2472,access,accessed,2472,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['access'],['accessed']
Security,"; if is_female is not None:; jsex = jsome(jobject.Female()) if is_female else jsome(jobject.Male()); else:; jsex = jnone(). self._jrep = Env.hail().methods.BaseTrio(proband, joption(fam), joption(father), joption(mother), jsex); self._fam = fam; self._proband = proband; self._father = father; self._mother = mother; self._is_female = is_female. @classmethod; def _from_java(cls, jrep):; trio = Trio.__new__(cls); trio._jrep = jrep; return trio. def __repr__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; repr(self.proband), repr(self.fam), repr(self.father),; repr(self.mother), repr(self.is_female)). def __str__(self):; return 'Trio(proband=%s, fam=%s, father=%s, mother=%s, is_female=%s)' % (; str(self.proband), str(self.fam), str(self.father),; str(self.mother), str(self.is_female)). def __eq__(self, other):; if not isinstance(other, Trio):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). @property; @handle_py4j; def proband(self):; """"""ID of proband in trio, never missing. :rtype: str; """"""; if not hasattr(self, '_proband'):; self._proband = self._jrep.kid(); return self._proband. @property; @handle_py4j; def father(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_father'):; self._father = from_option(self._jrep.dad()); return self._father. @property; @handle_py4j; def mother(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". if not hasattr(self, '_mother'):; self._mother = from_option(self._jrep.mom()); return self._mother. @property; @handle_py4j; def fam(self):; """"""Family ID. :rtype: str or None; """""". if not hasattr(self, '_fam'):; self._fam = from_option(self._jrep.fam()); return self._fam. @property; @handle_py4j; def is_male(self):; """"""Returns True if the proband is a reported male, False if reported female, and None if no sex is defined. :rtype: bool or None; """"""; if not hasattr(self, '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:2072,hash,hashCode,2072,docs/0.1/_modules/hail/representation/pedigree.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html,1,['hash'],['hashCode']
Security,"; return 'MatrixTable', MatrixTable, table_error(obj), True; elif isinstance(obj, GroupedMatrixTable):; return 'GroupedMatrixTable', GroupedMatrixTable, table_error(obj._parent), True; elif isinstance(obj, Table):; return 'Table', Table, table_error(obj), True; elif isinstance(obj, GroupedTable):; return 'GroupedTable', GroupedTable, table_error(obj), False; elif isinstance(obj, Struct):; return 'Struct', Struct, struct_error(obj), False; elif isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:7854,access,access,7854,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,1,['access'],['access']
Security,"; self._s,; self._fam_id,; self._pat_id if self._pat_id in ids else None,; self._mat_id if self._mat_id in ids else None,; self._is_female,; ). def _sex_as_numeric_string(self):; if self._is_female is None:; return ""0""; return ""2"" if self.is_female else ""1"". def _to_fam_file_line(self):; def sample_id_or_else_zero(sample_id):; if sample_id is None:; return ""0""; return sample_id. line_list = [; sample_id_or_else_zero(self._fam_id),; self._s,; sample_id_or_else_zero(self._pat_id),; sample_id_or_else_zero(self._mat_id),; self._sex_as_numeric_string(),; ""0"",; ]; return ""\t"".join(line_list). [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @typecheck_method(trios=sequenceof(Trio)); def __init__(self, trios):; self._trios = tuple(trios). def __eq__(self, other):; return isinstance(other, Pedigree) and self._trios == other._trios. def __hash__(self):; return hash(self._trios). def __iter__(self):; return self._trios.__iter__(). [docs] @classmethod; @typecheck_method(fam_path=str, delimiter=str); def read(cls, fam_path, delimiter='\\s+') -> 'Pedigree':; """"""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". trios = []; missing_sex_count = 0; missing_sex_values = set(); with Env.fs().open(fam_path) as file:; for line in file:; split_line = re.split(delimiter, line.strip()); num_fields = len(split_line); if num_fields != 6:; raise FatalError(; ""Require 6 fields per line in .fam, but this line has {}: {}"".format(num_fields, line); ); (fam, kid, dad, mom, sex, _) = tuple(split_line); # 1 is male, 2 is female, 0 is unknown.; is_female = sex == ""2"" if sex ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:4463,hash,hash,4463,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,1,['hash'],['hash']
Security,"CSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:1791,access,access,1791,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,1,['access'],['access']
Security,"E2',; u'isoform': u'GENE2.1'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.2'},; {u'canonical': False,; u'consequence': u'MIS',; u'gene': u'GENE2',; u'isoform': u'GENE2.3'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.1'},; {u'canonical': False,; u'consequence': u'SYN',; u'gene': u'GENE3',; u'isoform': u'GENE3.2'}]},; Struct{; info: Struct{; AC: Array[Int],; AN: Int,; AF: Array[Double]; },; transcripts: Array[Struct{; gene: String,; isoform: String,; canonical: Boolean,; consequence: String; }]; }). You’ll rarely need to construct a Variant or Genotype object; inside the Hail expression language. More commonly, these objects will; be provided to you as variables. In the remainder of this notebook, we; will explore how to to manipulate the demo variables. In the next; notebook, we start using the expression langauge to annotate and filter; a dataset.; First, a short demonstration of some of the methods accessible on; Variant and Genotype objects:. In [52]:. hc.eval_expr_typed('v'). Out[52]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [53]:. hc.eval_expr_typed('v.contig'). Out[53]:. (u'16', String). In [54]:. hc.eval_expr_typed('v.start'). Out[54]:. (19200405, Int). In [55]:. hc.eval_expr_typed('v.ref'). Out[55]:. (u'C', String). In [56]:. hc.eval_expr_typed('v.altAlleles'). Out[56]:. ([AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)], Array[AltAllele]). In [57]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isSNP())'). Out[57]:. ([True, False], Array[Boolean]). In [58]:. hc.eval_expr_typed('v.altAlleles.map(aa => aa.isInsertion())'). Out[58]:. ([False, True], Array[Boolean]). In [59]:. hc.eval_expr_typed('g'). Out[59]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [60]:. hc.eval_expr_typed('g.dp'). Out[60]:. (26, Int). In [61]:. hc.eval_expr_typed('g.ad'). O",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:15814,access,accessible,15814,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['access'],['accessible']
Security,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:2430,access,access,2430,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['access'],['access']
Security,"False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (struct.foo) or string indexing (struct['foo']). Warning; Using this method can cause out of memory errors. Only collect small tables. Returns:; list of Struct – List of rows. collect_by_key(name='values')[source]; Collect values for each unique key into an array. Note; Requires a keyed table. Examples; >>> t1 = hl.Table.parallelize([; ... {'t': 'foo', 'x': 4, 'y': 'A'},; ... {'t': 'bar', 'x': 2, 'y': 'B'},; ... {'t': 'bar', 'x': -3, 'y': 'C'},; ... {'t': 'quam', 'x': 0, 'y': 'D'}],; ... hl.tstruct(t=hl.tstr, x=hl.tint32, y=hl.tstr),; ... key='t'). >>> t1.show(); +--------+-------+-----+; | t | x | y |; +--------+-------+-----+; | str | int32 | str |; +--------+-------+-----+; | ""bar"" | 2 | ""B"" |; | ""bar"" | -3 | ""C"" |; | ""foo"" | 4 | ""A"" |; | ""quam"" | 0 | ""D"" |; +--------+-------+-----+. >>> t1.collect_by_key().show(); +--------+---------------------------------+; | t | values |; +--------+---------------------------------+; | str | array<struc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:16388,access,accessed,16388,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['access'],['accessed']
Security,"Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. B",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:30515,access,accessing,30515,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['accessing']
Security,"GT', 'GT'); }; if set(call_fields) != vds_call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30495,hash,hashlib,30495,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['hash'],['hashlib']
Security,"Index with a single integer:; >>> hl.eval(names[1]); 'Bob'. >>> hl.eval(names[-1]); 'Charlie'. Slicing is also supported:; >>> hl.eval(names[1:]); ['Bob', 'Charlie']. Parameters:; item (slice or Expression of type tint32) – Index or slice. Returns:; Expression – Element or array slice. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. aggregate(f)[source]; Uses the aggregator library to compute a summary from an array.; This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, call_stats(). Parameters:; f – Aggregation function. Returns:; Expression. all(f); Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f); Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any elemen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:3056,access,accessing,3056,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['access'],['accessing']
Security,"LECT variant.contig, variant.start, variant.ref, gs.pos AS genotype_pos, gs.item.gt AS gt FROM variants, variants.gs WHERE variant.start = 13090728 AND gs.pos >= 20 AND gs.pos < 25;"". +----------------+---------------+-------------+--------------+----+; | variant.contig | variant.start | variant.ref | genotype_pos | gt |; +----------------+---------------+-------------+--------------+----+; | 20 | 13090728 | A | 20 | 1 |; | 20 | 13090728 | A | 21 | 0 |; | 20 | 13090728 | A | 22 | 0 |; | 20 | 13090728 | A | 23 | 0 |; | 20 | 13090728 | A | 24 | 0 |; +----------------+---------------+-------------+--------------+----+. We can also retrieve the values from the AD (Allelic Depths) array by doing a nested; query that returns one row per genotype and per AD value. The ad_pos column is; the index of the value in the AD array.; $ impala-shell -q ""SELECT variant.contig, variant.start, variant.ref, gs.pos AS genotype_pos, gs.item.gt AS gt, ad.pos AS ad_pos, ad.item AS ad FROM variants, variants.gs, gs.ad WHERE variant.start = 13090728 LIMIT 6;"". +----------------+---------------+-------------+--------------+----+--------+----+; | variant.contig | variant.start | variant.ref | genotype_pos | gt | ad_pos | ad |; +----------------+---------------+-------------+--------------+----+--------+----+; | 20 | 13090728 | A | 0 | 0 | 0 | 28 |; | 20 | 13090728 | A | 0 | 0 | 1 | 0 |; | 20 | 13090728 | A | 1 | 0 | 0 | 20 |; | 20 | 13090728 | A | 1 | 0 | 1 | 0 |; | 20 | 13090728 | A | 2 | 0 | 0 | 11 |; | 20 | 13090728 | A | 2 | 0 | 1 | 0 |; +----------------+---------------+-------------+--------------+----+--------+----+. If you no longer need to use SQL you can delete the table definition. Since the table; was registered as an external table the underlying data is not affected, so you can; still access the VDS from Hail.; $ impala-shell -q ""DROP TABLE variants""; $ hadoop fs -ls sample.vds. Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/sql.html:6623,access,access,6623,docs/0.1/sql.html,https://hail.is,https://hail.is/docs/0.1/sql.html,1,['access'],['access']
Security,"ND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().unpersist(self). @overload; def collect(self) -> List[hl.Struct]: ... @overload; def collect(self, _localize=False) -> ArrayExpression: ... [docs] @typecheck_method(_localize=bool, _timed=bool); def collect(self, _localize=True, *, _timed=False):; """"""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""; if len(self.key) > 0:; t = self.order_by(*self.key); else:; t = self; rows_ir = ir.GetField(ir.TableCollect(t._tir), 'rows'); e = construct_expr(rows_ir, hl.tarray(t.row.dtype)); if _localize:; return Env.backend().execute(e._ir, timed=_timed); else:; return e. [docs] def describe(self, handler=print, *, widget=False):; """"""Print information about the fields in the table. Note; ----; The `widget` argument is **experimental**. Parameters; ----------; handler : Callable[[str], None]; Handler function for returned string.; widget : bool; Create an interactive IPython widget.; """"""; if widget:; from hail.experimental.interact import interact. return interact(self). def format_type(typ):; return typ.pretty(indent=4).lstrip(). if len(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:81648,access,accessed,81648,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['access'],['accessed']
Security,"ON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""; assert 'annotation_db' in doc, doc; assert 'key_properties' in doc['annotation_db'], doc['annotation_db']; assert 'description' in doc, doc; assert 'url' in doc, doc; assert 'versions' in doc, doc; key_properties = set(x for x in doc['annotation_db']['key_properties'] if x is not None); versions = [; DatasetVersion.from_json(x, cloud); for x in doc['versions']; if DatasetVersion.from_json(x, cloud) is not None; ]; versions_in_region = DatasetVersion.get_region(name, versions, region); if versions_in_region:; return Dataset(name, doc['description'], doc['url'], key_properties, versions_in_region). def __init__(self, name: str, description: str, url: str, key_properties: Set[str], versions: List[DatasetVersion]):; assert set(key_properties).issubset(DB._valid_key_properties); self.name = name; self.description = d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:6373,access,access,6373,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:1977,access,access,1977,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['access'],['access']
Security,"a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2499,access,accessing,2499,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['access'],['accessing']
Security,"agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.dtype.items()):; if isinstance(self._ir, ir.MakeStruct):; expr = construct_expr(self._ir.fields[i][1], t, self._indices,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42888,access,access,42888,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['access']
Security,"all_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields of a Struct should become top level fields.; However, be aware that all top-level field names must be unique. In the; following example, **mt[‘info’] would fail if DP already exists as an entry; field.; >>> mt_new = mt.select_rows(**mt['info']) . The example below adds two new row fields. Keys are always preserved, so the; row keys locus and alleles will also be present in the new table.; AC = mt.info.AC turns the subfield AC into a top-level field.; >>> mt_new = mt.select_rows(AC = mt.info.AC,; ... n_filters = hl.len(mt['filters'])). The order of the fields entered as arguments will be maintained in the new; matrix table.; Drop; The complement of select methods, MatrixTable.drop() can remove any top; level field. An example of removing the GQ entry field is:; >>> mt_new = mt.drop('GQ'). Explode; Explode operations can is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:7794,access,accessing,7794,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['access'],['accessing']
Security,"alueError(; ""'gvcf_sample_names' and 'gvcf_paths' must have the same length ""; f'{len(gvcf_sample_names)} != {len(gvcf_paths)}'; ). if batch_size is None:; if gvcf_batch_size is None:; gvcf_batch_size = VariantDatasetCombiner._default_gvcf_batch_size; else:; pass; elif gvcf_batch_size is None:; warning(; 'The batch_size parameter is deprecated. '; 'The batch_size parameter will be removed in a future version of Hail. '; 'Please use gvcf_batch_size instead.'; ); gvcf_batch_size = batch_size; else:; raise ValueError(; 'Specify only one of batch_size and gvcf_batch_size. ' f'Received {batch_size} and {gvcf_batch_size}.'; ); del batch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': requ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26034,hash,hash,26034,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['hash'],['hash']
Security,"and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7742,access,accessing,7742,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['access'],['accessing']
Security,"annotations = select(table, toKeep1, toKeep2, toKeep3)'. The above is equivalent to; expr='''sa.annotations.toKeep1 = table.toKeep1,; sa.annotations.toKeep2 = table.toKeep2,; sa.annotations.toKeep3 = table.toKeep3'''. Finally, for more information about importing key tables from text, ; see the documentation for HailContext.import_table(). Parameters:; table (KeyTable) – Key table.; root (str or None) – Sample annotation path to store text table. (This or expr required).; expr (str or None) – Annotation expression. (This or root required).; vds_key (str, list of str, or None.) – Join key for the dataset, if not sample ID.; product (bool) – Join with all matching keys (see note). Returns:Annotated variant dataset. Return type:VariantDataset. annotate_variants_db(annotations, gene_key=None)[source]¶; Annotate variants using the Hail annotation database. Warning; Experimental. Supported only while running Hail on the Google Cloud Platform. Documentation describing the annotations that are accessible through this method can be found here.; Examples; Annotate variants with CADD raw and PHRED scores:; >>> vds = vds.annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) . Annotate variants with gene-level PLI score, using the VEP-generated gene symbol to map variants to genes:; >>> pli_vds = vds.annotate_variants_db('va.gene.constraint.pli') . Again annotate variants with gene-level PLI score, this time using the existing va.gene_symbol annotation ; to map variants to genes:; >>> vds = vds.annotate_variants_db('va.gene.constraint.pli', gene_key='va.gene_symbol') . Notes; Annotations in the database are bi-allelic, so splitting multi-allelic variants in the VDS before using this ; method is recommended to capture all appropriate annotations from the database. To do this, run split_multi() ; prior to annotating variants with this method:; >>> vds = vds.split_multi().annotate_variants_db(['va.cadd.RawScore', 'va.cadd.PHRED']) . To add VEP annotations, or to add gene-leve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:19691,access,accessible,19691,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"aram bool export_pp: If true, export linear-scaled probabilities (Hail's `pp` field on genotype) as the VCF PP FORMAT field. :param bool parallel: If true, return a set of VCF files (one per partition) rather than serially concatenating these files.; """""". self._jvdf.exportVCF(output, joption(append_to_header), export_pp, parallel). [docs] @handle_py4j; @convertVDS; @typecheck_method(output=strlike,; overwrite=bool,; parquet_genotypes=bool); def write(self, output, overwrite=False, parquet_genotypes=False):; """"""Write variant dataset as VDS file. **Examples**. Import data from a VCF file and then write the data to a VDS file:. >>> vds.write(""output/sample.vds""). :param str output: Path of VDS file to write. :param bool overwrite: If true, overwrite any existing VDS file. Cannot be used to read from and write to the same path. :param bool parquet_genotypes: If true, store genotypes as Parquet rather than Hail's serialization. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. """""". if self._is_generic_genotype:; self._jvdf.write(output, overwrite); else:; self._jvdf.write(output, overwrite, parquet_genotypes). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=strlike,; annotation=strlike,; subset=bool,; keep=bool,; filter_altered_genotypes=bool,; max_shift=integral,; keep_star=bool); def filter_alleles(self, expr, annotation='va = va', subset=True, keep=True,; filter_altered_genotypes=False, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:60256,access,accessible,60256,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"ariant datasets have the same variants, samples, genotypes, and annotation schemata and values.; Examples; This will return True:; >>> vds.same(vds). Notes; The tolerance parameter sets the tolerance for equality when comparing floating-point fields. More precisely, \(x\) and \(y\) are equal if. \[bs{x - y} \leq tolerance * \max{bs{x}, bs{y}}\]. Parameters:; other (VariantDataset) – variant dataset to compare against; tolerance (float) – floating-point tolerance for equality. Return type:bool. sample_annotations¶; Return a dict of sample annotations.; The keys of this dictionary are the sample IDs (strings).; The values are sample annotations. Returns:dict. sample_ids¶; Return sampleIDs. Returns:List of sample IDs. Return type:list of str. sample_qc(root='sa.qc', keep_star=False)[source]¶; Compute per-sample QC metrics. Important; The genotype_schema() must be of type TGenotype in order to use this method. Annotations; sample_qc() computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; sa.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of genotypes called. nHomRef; Int; Number of homozygous reference genotypes. nHet; Int; Number of heterozygous genotypes. nHomVar; Int; Number of homozygous alternate genotypes. nCalled; Int; Sum of nHomRef + nHet + nHomVar. nNotCalled; Int; Number of uncalled genotypes. nSNP; Int; Number of SNP alternate alleles. nInsertion; Int; Number of insertion alternate alleles. nDeletion; Int; Number of deletion alternate alleles. nSingleton; Int; Number of private alleles. nTransition; Int; Number of transition (A-G, C-T) alternate alleles. nTransversion; Int; Number of transversion alternate alleles. nNonRef; Int; Sum of nHet and nHomVar. rTiTv; Double; Transition/Transversion ratio. rHetHomVar; Double; Het/HomVar genotype ratio. rInsertionDeletion; Double; Insertion/Deletion ratio. dpMean;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:156225,access,accessed,156225,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessed']
Security,"ary version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:10582,access,access,10582,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['access']
Security,"ary.; """"""; return hl.array(self). def _extra_summary_fields(self, agg_result):; return {; 'Min Size': agg_result[0],; 'Max Size': agg_result[1],; 'Mean Size': agg_result[2],; }. def _nested_summary(self, agg_result, top):; k = construct_variable(Env.get_uid(), self.dtype.key_type, indices=self._indices); v = construct_variable(Env.get_uid(), self.dtype.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42435,access,access,42435,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['access']
Security,"atasets_metadata; from .lens import MatrixRows, TableRows. class DatasetVersion:; """""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:1947,access,access,1947,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"atch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is impl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4710,access,access,4710,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['access'],['access']
Security,"ated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'), Rao score test ('score'),; and Firth test ('firth') as the ``test`` parameter. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of :py:meth:`.logreg` with sample genotype ``gt`` replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples. The resulting **logistic regression key table** has key column of type String given by the ``key_name``; parameter and additional columns corresponding to the fields of the ``va.logreg`` schema given for ``test``; in :py:meth:`.logreg`. :py:meth:`.logreg_burden` returns both the logistic regression key table and the sample key table. :param str key_name: Name to assign to key column of returned key tables. :param str variant_keys: Variant annotation path for the TArray or",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:152483,access,accessible,152483,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7544,access,accessing,7544,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['access'],['accessing']
Security,"ax(series_max, overall_max). color_mapping = continuous_nums_to_colors(overall_min, overall_max, plotly.colors.sequential.Viridis). def transform(df):; df[self.aesthetic_name] = df[self.aesthetic_name].map(lambda i: color_mapping(i)); return df. return transform. class ScaleColorHue(ScaleDiscrete):; def get_values(self, categories):; num_categories = len(categories); step = 1.0 / num_categories; interpolation_values = [step * i for i in range(num_categories)]; hsv_scale = px.colors.get_colorscale(""HSV""); return px.colors.sample_colorscale(hsv_scale, interpolation_values). class ScaleShapeAuto(ScaleDiscrete):; def get_values(self, categories):; return [; ""circle"",; ""square"",; ""diamond"",; ""cross"",; ""x"",; ""triangle-up"",; ""triangle-down"",; ""triangle-left"",; ""triangle-right"",; ""triangle-ne"",; ""triangle-se"",; ""triangle-sw"",; ""triangle-nw"",; ""pentagon"",; ""hexagon"",; ""hexagon2"",; ""octagon"",; ""star"",; ""hexagram"",; ""star-triangle-up"",; ""star-triangle-down"",; ""star-square"",; ""star-diamond"",; ""diamond-tall"",; ""diamond-wide"",; ""hourglass"",; ""bowtie"",; ""circle-cross"",; ""circle-x"",; ""square-cross"",; ""square-x"",; ""diamond-cross"",; ""diamond-x"",; ""cross-thin"",; ""x-thin"",; ""asterisk"",; ""hash"",; ""y-up"",; ""y-down"",; ""y-left"",; ""y-right"",; ""line-ew"",; ""line-ns"",; ""line-ne"",; ""line-nw"",; ""arrow-up"",; ""arrow-down"",; ""arrow-left"",; ""arrow-right"",; ""arrow-bar-up"",; ""arrow-bar-down"",; ""arrow-bar-left"",; ""arrow-bar-right"",; ]. class ScaleColorContinuousIdentity(ScaleContinuous):; def valid_dtype(self, dtype):; return dtype == tstr. [docs]def scale_x_log10(name=None):; """"""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:7262,hash,hash,7262,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['hash'],['hash']
Security,"calization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:2946,access,access,2946,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['access'],['access']
Security,"ccess images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:6527,access,access,6527,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['access'],['access']
Security,"ch_size. def maybe_load_from_saved_path(save_path: str) -> Optional[VariantDatasetCombiner]:; if force:; return None; fs = hl.current_backend().fs; if fs.exists(save_path):; try:; combiner = load_combiner(save_path); warning(f'found existing combiner plan at {save_path}, using it'); # we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large); combiner._branch_factor = branch_factor; combiner._target_records = target_records; combiner._gvcf_batch_size = gvcf_batch_size; return combiner; except (ValueError, TypeError, OSError, KeyError) as e:; warning(; f'file exists at {save_path}, but it is not a valid combiner plan, overwriting\n'; f' caused by: {e}'; ); return None. # We do the first save_path check now after validating the arguments; if save_path is not None:; saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner. if len(gvcf_paths) > 0:; n_partition_args = (; int(intervals is not None); + int(import_interval_size is not None); + int(use_genome_default_intervals); + int(use_exome_default_intervals); ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': require one argument from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals' to choose GVCF partitioning""; ). if n_partition_args > 1:; warning(; ""'new_combiner': multiple colliding arguments found from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals'.""; ""\n The argument found first in the list in this warning will be used, and others ignored.""; ). if intervals is not None:; pass; elif import_interval_size is not None:; intervals = calculate_even_genome_partitioning(reference_genome, import_interval_size); elif use_genome_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:26616,validat,validating,26616,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['validat'],['validating']
Security,"cts each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2520,access,access,2520,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['access'],['access']
Security,"d exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt ; <hail.matrixtable.MatrixTable at 0x1107e54a8>. >>> mt.locus ; <LocusExpression of type locus<GRCh37>>. Likewise, mt.DP is an Int32Expression with type int32; and is an entry field of mt.; Hail expressions can also Expression.describe() themselves, providing; information about their source matrix table or table and which",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:2727,expose,exposed,2727,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['expose'],['exposed']
Security,"dditional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8436,authenticat,authenticated,8436,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['authenticat'],['authenticated']
Security,"described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1413,access,access,1413,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['access'],['access']
Security,"e incompatible types: '{}', '{}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2856,hash,hash,2856,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['hash'],['hash']
Security,"e of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to; access the binary PLINK file output and association results in downstream jobs.; def gwas(batch, vcf, phenotypes):; """"""; QC data and get association test statistics; """"""; cores = 2; g = batch.new_job(name='run-gwas'); g.image('us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas:latest'); g.cpu(cores); g.declare_resource_group(ofile={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:6081,access,access,6081,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['access'],['access']
Security,"e resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotation to add to global; annotation_type (Type) – Hail type of annotation. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global_expr(expr)[source]¶; Annotate global with expression.; Example; Annotate global with an array of populations:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:; >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. global: global annotations. Parameters:expr (str or list of str) – Annotation expression. Returns:Annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:12127,access,accessed,12127,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessed']
Security,"e.value_type, indices=self._indices); return {; '[<keys>]': k._summarize(agg_result[3][0]),; '[<values>]': v._summarize(agg_result[3][1]),; }. def _summary_aggs(self):; length = hl.len(self); return hl.tuple((; hl.agg.min(length),; hl.agg.max(length),; hl.agg.mean(length),; hl.agg.explode(; lambda elt: hl.tuple((elt[0]._all_summary_aggs(), elt[1]._all_summary_aggs())), hl.array(self); ),; )). [docs]class StructExpression(Mapping[Union[str, int], Expression], Expression):; """"""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """""". @classmethod; def _from_fields(cls, fields: 'Dict[str, Expression]'):; t = tstruct(**{k: v.dtype for k, v in fields.items()}); x = ir.MakeStruct([(n, expr._ir) for (n, expr) in fields.items()]); indices, aggregations = unify_all(*fields.values()); s = StructExpression.__new__(cls); super(StructExpression, s).__init__(x, t, indices, aggregations); s._warn_on_shadowed_name = set(); s._fields = {}; for k, v in fields.items():; s._set_field(k, v); return s. @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(StructExpression, self).__init__(x, type, indices, aggregations); self._fields: Dict[str, Expression] = {}; self._warn_on_shadowed_name = set(). for i, (f, t) in enumerate(self.d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:42790,access,accessible,42790,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['accessible']
Security,"e: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; return hash((self._s, self._pat_id, self._mat_id, self._fam_id, self._is_female)). @property; def s(self):; """"""ID of proband in trio, never missing. :rtype: str; """""". return self._s. @property; def pat_id(self):; """"""ID of father in trio, may be missing. :rtype: str or None; """""". return self._pat_id. @property; def mat_id(self):; """"""ID of mother in trio, may be missing. :rtype: str or None; """""". return self._mat_id. @property; def fam_id(self):; """"""Family ID. :rtype: str or None; """""". return self._fam_id. @property; def is_male(self):; """"""Returns ``True`` if the proband is a reported male,; ``False`` if reported female, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self._is_female is False. @property; def is_female(self):; """"""Returns ``True`` if the proband is a reported female,; ``False`` if reported male, and ``None`` if no sex is defined. :rtype: bool or None; """""". if self._is_female is None:; return None. return self",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:2003,hash,hash,2003,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,1,['hash'],['hash']
Security,"e_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotation path starting in 'global'. :param annotation: annotation to add to global. :param annotation_type: Hail type of annotation; :type annotation_type: :py:class:`.Type`. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". annotation_type._typecheck(annotation). annotated = self._jvds.annotateGlobal(annotation_type._convert_to_j(annotation), annotation_type._jtype, path); assert annotated.globalSignature().typeCheck(annotated.globalAnnotation()), 'error in java type checking'; return VariantDataset(self.hc, annotated). [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_samples_expr(self, expr):; """"""Annotate samples with expression. **Examples**. Compute per-sample GQ statistics for hets:. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:12834,access,accessed,12834,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessed']
Security,"ed types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary; statistics about variants and samples, generating synthetic data,; plotting, exporting, and more. The Hail expression language takes the; form of Python strings passed into various Hail methods like; filter_variants_expr; and linear; regression.; The expression language is a programming language just like Python or R; or Scala. While the syntax is different, programming experience will; certainly translate. We have built the expression language with the hope; that even people new to pro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:1512,access,accessed,1512,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['access'],['accessed']
Security,"ed=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functions. GWAS; To start, we will write a function that creates a new Job on an existing Batch that; takes as arguments the VCF file and the phenotypes file. The return value of this; function is the Job that is created in the function, which can be used later to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5511,access,access,5511,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['access'],['access']
Security,"ee ServiceBackend for details.; default_timeout (Union[int, float, None]) – Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int]) – Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4298,authenticat,authenticating,4298,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['authenticat'],['authenticating']
Security,"eight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:69027,access,accessible,69027,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['access'],['accessible']
Security,"either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared disk that; have been specified to be uploaded by the user. These files can either be specified with; Batch.write_output() or are file dependencies for downstream jobs. Service Accounts; A Google service account is automatically created for a new Batch user that is used by Batch to download data; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:3205,access,access,3205,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['access'],['access']
Security,"ele is a transversion SNP. This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. :rtype: bool; """""". return self._jrep.isTransversion(). [docs] @handle_py4j; def category(self):; """"""Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. :rtype: str; """"""; return self._jrep.altAlleleType(). [docs]class Locus(object):; """"""; An object that represents a location in the genome. :param contig: chromosome identifier; :type contig: str or int; :param int position: chromosomal position (1-indexed); """""". @handle_py4j; def __init__(self, contig, position):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Locus').apply(contig, position); self._init_from_java(jrep); self._contig = contig; self._position = position. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Locus(contig=%s, position=%s)' % (self.contig, self.position). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; l = Locus.__new__(cls); l._init_from_java(jrep); l._contig = jrep.contig(); l._position = jrep.position(); return l. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = Locus.parse('1:101230'); >>> l2 = Locus.parse('X:4201230'). :rtype: :class:`.Locus`; """""". return Locus._from_java(scala_object(Env.hail().variant, 'Locus').parse(string)). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:9758,hash,hashCode,9758,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['hash'],['hashCode']
Security,"elf, gt, ad=None, dp=None, gq=None, pl=None):; """"""Initialize a Genotype object."""""". jvm = Env.jvm(); jgt = joption(gt); if ad:; jad = jsome(jarray(jvm.int, ad)); else:; jad = jnone(); jdp = joption(dp); jgq = joption(gq); if pl:; jpl = jsome(jarray(jvm.int, pl)); else:; jpl = jnone(). jrep = scala_object(Env.hail().variant, 'Genotype').apply(; jgt, jad, jdp, jgq, jpl, False, False); self._gt = gt; self._ad = ad; self._dp = dp; self._gq = gq; self._pl = pl; self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; fake_ref = 'FakeRef=True' if self._jrep.fakeRef() else ''; if self._jrep.isLinearScale():; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, GP=%s%s)' %\; (self.gt, self.ad, self.dp, self.gq, self.gp, fake_ref); else:; return 'Genotype(GT=%s, AD=%s, DP=%s, GQ=%s, PL=%s%s)' % \; (self.gt, self.ad, self.dp, self.gq, self.pl, fake_ref). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; g = Genotype.__new__(cls); g._init_from_java(jrep); g._gt = from_option(jrep.gt()); g._ad = jarray_to_list(from_option(jrep.ad())); g._dp = from_option(jrep.dp()); g._gq = from_option(jrep.gq()); g._pl = jarray_to_list(from_option(jrep.pl())); return g. @property; def gt(self):; """"""Returns the hard genotype call. :rtype: int or None; """""". return self._gt. @property; def ad(self):; """"""Returns the allelic depth. :rtype: list of int or None; """""". return self._ad. @property; def dp(self):; """"""Returns the total depth. :rtype: int or None; """""". return self._dp. @property; def gq(self):; """"""Returns the phred-scaled genotype quality. :return: int or None; """""". return self._gq. @property; def pl(self):; """"""Returns the phred-scaled genotype posterior likelihoods. :rtype: list of int or None; """""". return self._pl. [docs] def od(self):; """"""Returns the difference between the total depth and the alle",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:2001,hash,hashCode,2001,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['hash'],['hashCode']
Security,"epth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""Returns the fraction of reads that are reference reads. Equivalent to:. >>> g.ad[0] / sum(g.ad). :rtype: float or None; """""". return from_option(self._jrep.fractionReadsRef()). [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. :param call: Genotype hard call; :type call: int or None; """""". _call_jobject = None. @handle_py4j; def __init__(self, call):; """"""Initialize a Call object."""""". if not Call._call_jobject:; Call._call_jobject = scala_object(Env.hail().variant, 'Call'). jrep = Call._call_jobject.apply(call); self._init_from_java(jrep). def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Call(gt=%s)' % self._jrep. def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jcall = Call._call_jobject; self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; c = Call.__new__(cls); c._init_from_java(jrep); return c. @property; def gt(self):; """"""Returns the hard call. :rtype: int or None; """""". return self._jrep. [docs] def is_hom_ref(self):; """"""True if the call is 0/0. :rtype: bool; """""". return self._jcall.isHomRef(self._jrep). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """""". return self._jcall.isHet(self._jrep). [docs] def is_hom_var(self):; """"""True if the call contains two identical alternate alleles. :rtype: bool; """""". return self._jcall.isHomVar(self._jrep). [docs] def is_called_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """""". return self._jcall.isCalledNonRef(self._jrep). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """""". return self._jcall.isHetNonRef(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:8100,hash,hashCode,8100,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['hash'],['hashCode']
Security,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:1718,access,accessed,1718,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['access'],['accessed']
Security,"experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:1332,access,access,1332,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['access'],['access']
Security,"ferences) == 0. def _to_json_context(self):; if self._json is None:; self._json = {'reference_genomes': {r: hl.get_reference(r)._config for r in self.references}}; return self._json. @classmethod; def union(cls, *types):; ctxs = [t.get_context() for t in types if not t.get_context().is_empty]; if len(ctxs) == 0:; return _empty_context; if len(ctxs) == 1:; return ctxs[0]; refs = ctxs[0].references.union(*[ctx.references for ctx in ctxs[1:]]); return HailTypeContext(refs). _empty_context = HailTypeContext(). [docs]class HailType(object):; """"""; Hail type superclass.; """""". def __init__(self):; super(HailType, self).__init__(); self._context = None. def __repr__(self):; s = str(self).replace(""'"", ""\\'""); return ""dtype('{}')"".format(s). @abc.abstractmethod; def _eq(self, other):; raise NotImplementedError. def __eq__(self, other):; return isinstance(other, HailType) and self._eq(other). @abc.abstractmethod; def __str__(self):; raise NotImplementedError. def __hash__(self):; # FIXME this is a bit weird; return 43 + hash(str(self)). def pretty(self, indent=0, increment=4):; """"""Returns a prettily formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._conv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:4154,hash,hash,4154,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['hash'],['hash']
Security,"fields[item]; else:; raise KeyError(get_nice_field_error(self, item)). @typecheck_method(item=str); def __getitem__(self, item):; return self._get_field(item). def __setattr__(self, key, value):; if key in self._fields:; raise ValueError(""Structs are immutable, cannot overwrite a field.""); else:; super().__setattr__(key, value). def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]; elif item in self._fields:; return self._fields[item]; else:; raise AttributeError(get_nice_attr_error(self, item)). def __len__(self):; return len(self._fields). def __repr__(self):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:3080,hash,hash,3080,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['hash'],['hash']
Security,"g the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a si",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:120481,access,accessible,120481,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"g,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file to write.; overwrite (bool) – If true, overwrite any existing VDS file. Cannot be used to read from and write to the same path.; parquet_genotypes (bool) – If true, store genotypes as Parquet rather than Hail’s serialization. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:180864,access,accessible,180864,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"h a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_expr:. In [45]:. # 'v' is used to indicate 'Variant' in Hail; hc.eval_expr_typed('v'). Out[45]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [46]:. # 's' is used to refer to sample ID in Hail; hc.eval_expr_typed('s'). Out[46]:. (u'NA12878', String). In [47]:. # 'g' is used to refer to the genotype in Hail; hc.eval_expr_typed('g'). Out[47]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [48]:. # 'sa' is used to refer to sample annotations; hc.eval_expr_typed('sa'). Out[48]:. (Struct{u'cohort': u'1KG', u'covariates': Struct{u'PC2': -0.61512, u'PC3': 0.3166666, u'age': 34, u'PC1': 0.102312, u'isFemale': True}},; Struct{cohort:String,covariates:Struct{PC1:Double,PC2:Double,PC3:Double,age:Int,isFemale:Boolean}}). The above output is a bit wordy. Let’s try 'va':. In [49]:. # 'va' is used to refer to variant annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:12259,access,access,12259,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['access'],['access']
Security,"h/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4016,authoriz,authorization,4016,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['authoriz'],['authorization']
Security,"he Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; files are then passed on to the main container via a shared disk where the user’s code is; executed. Finally, the output container runs and uploads any files from the shared ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:1442,authenticat,authenticate,1442,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['authenticat'],['authenticate']
Security,"hile MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global, entry) are top-level and exposed as attributes; on the MatrixTable object. For example, if the matrix table mt had a; row field locus, this field could be referenced with either mt.locus or; mt['locus']. The former access pattern does not work with field names with; spaces or punctuation.; The result of referencing a field from a matrix table is an Expression; which knows its type, its source matrix table, and whether it is a row field,; column field, entry field, or global field. Hail uses this context to know which; operations are allowed for a given expression.; When evaluated in a Python interpreter, we can see mt.locus is a; LocusExpression with type locus<GRCh37>.; >>> mt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:2386,access,accessed,2386,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['access'],['accessed']
Security,"hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:1567,access,access,1567,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['access'],['access']
Security,"hon API; Annotation Database; Other Resources. Hail. Docs »; Overview. View page source. Overview¶; A typical workflow in Hail begins with importing genotype data from a standard file format such as VCF, PLINK Binary files, GEN, or BGEN files into Hail’s Variant Dataset format.; Next, samples and variants are annotated with additional meta-information such as phenotype for samples and functional consequence for variants.; Samples, variants, and genotypes are filtered from the dataset based on expressions constructed using Hail’s Domain-Specific Language.; Once the dataset has been cleaned, various analytic methods such as PCA and logistic regression are used to find genetic associations.; Lastly, data is exported to a variety of file formats. Variant Dataset (VDS)¶. Hail represents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:1600,access,access,1600,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['access'],['access']
Security,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:8690,access,accessed,8690,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,4,['access'],['accessed']
Security,"ich is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2181,hash,hash,2181,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,1,['hash'],['hash']
Security,"icks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 var",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:1373,access,access,1373,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['access'],['access']
Security,"igned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56841,validat,validation,56841,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['validat'],['validation']
Security,"in the; struct, and names them according to the struct field name. For example, the following invocation (assuming ``va.qc`` was generated; by :py:meth:`.variant_qc`):. >>> vds.export_variants('output/file.tsv', 'variant = v, va.qc.*'). will produce the following set of columns:. .. code-block:: text. variant callRate AC AF nCalled ... Note that using the ``.*`` syntax always results in named arguments, so it; is not possible to export header-less files in this manner. However,; naming the ""splatted"" struct will apply the name in front of each column; like so:. >>> vds.export_variants('output/file.tsv', 'variant = v, QC = va.qc.*'). which produces these columns:. .. code-block:: text. variant QC.callRate QC.AC QC.AF QC.nCalled ... **Notes**. This module takes a comma-delimited list of fields or expressions to; print. These fields will be printed in the order they appear in the; expression in the header and on each line. One line per variant in the VDS will be printed. The accessible namespace includes:. - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for variant ``v``. **Designating output with an expression**. Much like the filtering methods, this method uses the Hail expression language.; While the filtering methods expect an; expression that evaluates to true or false, this method expects a; comma-separated list of fields to print. These fields take the; form ``IDENTIFIER = <expression>``. :param str output: Output file. :param str expr: Export expression for values to export. :param bool types: Write types of exported columns to a file at (output + "".types""). :param bool parallel: If true, writes a set of files (one per partition) rather than serially concatenating these files.; """""". self._jvds.exportVariants(output, expr, types, parallel). [docs] @handle_py4j; @typecheck_method(output=strlike,; append_to_header=nullable(strlike),; export_pp=boo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:55465,access,accessible,55465,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"ing the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:2204,hash,hash,2204,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,1,['hash'],['hash']
Security,"ions can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31472,access,access,31472,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['access']
Security,"ions. get_movie_lens(output_dir[, overwrite]); Download public Movie Lens dataset. class hail.utils.Interval(start, end, includes_start=True, includes_end=False, point_type=None)[source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.interval.take(5). This is rare; it is much; more common to manipulate the IntervalExpression object, which is; constructed using the following functions:. interval(); locus_interval(); parse_locus_interval(). class hail.utils.Struct(**kwargs)[source]; Nested annotation structure.; >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:; >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:; attributes – Field names and values. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.info.take(5). This is rare; it is much; more common to manipulate the StructExpression object, which is; constructed using the struct() function. class hail.utils.frozendict(d)[source]; An object representing an immutable dictionary.; >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a frozendict:; >>> dict(frozendict({'a': 1, 'b': 2})). Note; This object refers to the Pyth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:3058,access,accessing,3058,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['access'],['accessing']
Security,"ions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.StructExpression'>>. __eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:1241,access,accessible,1241,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['access'],['accessible']
Security,"ith identifiers that form the header:. >>> vds.export_genotypes('output/genotypes.tsv', 'SAMPLE=s, VARIANT=v, GQ=g.gq, DP=g.dp, ANNO1=va.anno1, ANNO2=va.anno2'). Export the same information without identifiers, resulting in a file with no header:. >>> vds.export_genotypes('output/genotypes.tsv', 's, v, g.gq, g.dp, va.anno1, va.anno2'). **Notes**. :py:meth:`~hail.VariantDataset.export_genotypes` outputs one line per cell (genotype) in the data set, though HomRef and missing genotypes are not output by default if the genotype schema is equal to :py:class:`~hail.expr.TGenotype`. Use the ``export_ref`` and ``export_missing`` parameters to force export of HomRef and missing genotypes, respectively. The ``expr`` argument is a comma-separated list of fields or expressions, all of which must be of the form ``IDENTIFIER = <expression>``, or else of the form ``<expression>``. If some fields have identifiers and some do not, Hail will throw an exception. The accessible namespace includes ``g``, ``s``, ``sa``, ``v``, ``va``, and ``global``. .. warning::. If the genotype schema does not have the type :py:class:`~hail.expr.TGenotype`, all genotypes will be exported unless the value of ``g`` is missing.; Use :py:meth:`~hail.VariantDataset.filter_genotypes` to filter out genotypes based on an expression before exporting. :param str output: Output path. :param str expr: Export expression for values to export. :param bool types: Write types of exported columns to a file at (output + "".types""). :param bool export_ref: If true, export reference genotypes. Only applicable if the genotype schema is :py:class:`~hail.expr.TGenotype`. :param bool export_missing: If true, export missing genotypes. :param bool parallel: If true, writes a set of files (one per partition) rather than serially concatenating these files.; """""". if self._is_generic_genotype:; self._jvdf.exportGenotypes(output, expr, types, export_missing, parallel); else:; self._jvdf.exportGenotypes(output, expr, types, export_ref, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:49427,access,accessible,49427,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"jects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:3645,access,access,3645,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"king directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the Lo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4272,authoriz,authorization,4272,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['authoriz'],['authorization']
Security,"lease 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict asser",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70611,access,accessing,70611,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['accessing']
Security,"lete(self):; """"""Returns True if the trio has a defined mother, father, and sex. The considered fields are ``mother``, ``father``, and ``sex``.; Recall that ``proband`` may never be missing. The ``fam`` field; may be missing in a complete trio. :rtype: bool; """""". if not hasattr(self, '_complete'):; self._complete = self._jrep.isComplete(); return self._complete. [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @handle_py4j; def __init__(self, trios):. self._jrep = Env.hail().methods.Pedigree(jindexed_seq([t._jrep for t in trios])); self._trios = trios. @classmethod; def _from_java(cls, jrep):; ped = Pedigree.__new__(cls); ped._jrep = jrep; ped._trios = None; return ped. def __eq__(self, other):; if not isinstance(other, Pedigree):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). [docs] @staticmethod; @handle_py4j; @typecheck(fam_path=strlike,; delimiter=strlike); def read(fam_path, delimiter='\\s+'):; """"""Read a .fam file and return a pedigree object. **Examples**. >>> ped = Pedigree.read('data/test.fam'). **Notes**. This method reads a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. Hail expects a file in the same spec as PLINK outlines. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:4720,hash,hashCode,4720,docs/0.1/_modules/hail/representation/pedigree.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html,1,['hash'],['hashCode']
Security,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:1881,hash,hash,1881,docs/0.1/_modules/hail/representation/annotations.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html,1,['hash'],['hash']
Security,"log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Bac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:3080,access,accessing,3080,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['access'],['accessing']
Security,"lot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; ReferenceGenome. View page source. ReferenceGenome. class hail.genetics.ReferenceGenome[source]; An object that represents a reference genome.; Examples; >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; Hail comes with predefined reference genomes (case sensitive!):. GRCh37, Genome Reference Consortium Human Build 37; GRCh38, Genome Reference Consortium Human Build 38; GRCm38, Genome Reference Consortium Mouse Build 38; CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using get_reference():; >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using read will add the reference genome to the list of; known references; it is possible to access the reference genome using; get_reference() anytime afterwards. Note; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; Hail allows setting a default reference so that the reference_genome; argument of import_vcf() does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the default_reference argument of; init(). In order to set a custom reference genome as default,; pass the reference as an argument to default_reference() after; initializing Hail. Parameters:. name (str) – Name of reference. Must b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:1318,access,access,1318,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['access'],['access']
Security,"lt2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:3217,access,access,3217,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,1,['access'],['access']
Security,"n API; Hail Query Python API; Variant Dataset; VariantDataset. View page source. VariantDataset. class hail.vds.VariantDataset[source]; Class for representing cohort-level genomic data.; This class facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:1580,validat,validate,1580,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['validat'],['validate']
Security,n module hail.methods). trios (hail.genetics.Pedigree property). truncate_reference_blocks() (in module hail.vds). tset (class in hail.expr.types). tstr (in module hail.expr.types). tstruct (class in hail.expr.types). ttuple (class in hail.expr.types). tuple() (in module hail.expr.functions). TupleExpression (class in hail.expr). U. unfilter_entries() (hail.MatrixTable method). union() (hail.expr.SetExpression method). (hail.Table method). union_cols() (hail.MatrixTable method). union_rows() (hail.MatrixTable method). (hail.vds.VariantDataset method). uniroot() (in module hail.expr.functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:51244,validat,validate,51244,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['validat'],['validate']
Security,"n run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:4313,access,accessed,4313,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['access'],['accessed']
Security,"n x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; -------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:3655,interlock,interlocking,3655,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['interlock'],['interlocking']
Security,"name': c, 'length': l} for c, l in lengths.items()],; 'xContigs': x_contigs,; 'yContigs': y_contigs,; 'mtContigs': mt_contigs,; 'par': [{'start': {'contig': c, 'position': s}, 'end': {'contig': c, 'position': e}} for (c, s, e) in par],; }. self._contigs = contigs; self._lengths = lengths; self._par_tuple = par; self._par = [hl.Interval(hl.Locus(c, s, self), hl.Locus(c, e, self)) for (c, s, e) in par]; self._global_positions = None; self._global_positions_list = None. if not _builtin:; Env.backend().add_reference(self). self._sequence_files = None; self._liftovers = dict(). def __str__(self):; return self._config['name']. def __repr__(self):; return 'ReferenceGenome(name=%s, contigs=%s, lengths=%s, x_contigs=%s, y_contigs=%s, mt_contigs=%s, par=%s)' % (; self.name,; self.contigs,; self.lengths,; self.x_contigs,; self.y_contigs,; self.mt_contigs,; self._par_tuple,; ). def __eq__(self, other):; return isinstance(other, ReferenceGenome) and self._config == other._config. def __hash__(self):; return hash(self.name). @property; def name(self):; """"""Name of reference genome. Returns; -------; :class:`str`; """"""; return self._config['name']. @property; def contigs(self):; """"""Contig names. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._contigs. @property; def lengths(self):; """"""Dict of contig name to contig length. Returns; -------; :obj:`dict` of :class:`str` to :obj:`int`; """"""; return self._lengths. @property; def x_contigs(self):; """"""X contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['xContigs']. @property; def y_contigs(self):; """"""Y contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['yContigs']. @property; def mt_contigs(self):; """"""Mitochondrial contigs. Returns; -------; :obj:`list` of :class:`str`; """"""; return self._config['mtContigs']. @property; def par(self):; """"""Pseudoautosomal regions. Returns; -------; :obj:`list` of :class:`.Interval`; """""". return self._par. [docs] @typecheck_method(con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:5442,hash,hash,5442,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['hash'],['hash']
Security,"notation of type Array, then that; variant will be counted twice in that key’s group. With single_key=True, variant_keys expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. Notes; This method modifies linreg() by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample’s; genotypes and annotations over all variants with that key. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of linreg() with sample genotype gt replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples.; The resulting linear regression key table has the following columns:. value of key_name (String) – descriptor of variant group key (key column); beta (Double) – fit coefficient, \(\hat\beta_1\); se (Double) – estimated standard error, \(\widehat{\mathrm{se}}\); tstat (Double) – \(t\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); pval (Double) – \(p\)-value. linreg_burden() returns both the linear regression key table and the sample key table.; Extended example; Let’s walk through these steps in the max() toy example above.; There are six samples with th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:86995,access,accessible,86995,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"nstallation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:1168,password,password,1168,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['password'],['password']
Security,"numeric); def same(self, other, tolerance=1e-6):; """"""True if the two variant datasets have the same variants, samples, genotypes, and annotation schemata and values. **Examples**. This will return True:. >>> vds.same(vds). **Notes**. The ``tolerance`` parameter sets the tolerance for equality when comparing floating-point fields. More precisely, :math:`x` and :math:`y` are equal if. .. math::. \abs{x - y} \leq tolerance * \max{\abs{x}, \abs{y}}. :param other: variant dataset to compare against; :type other: :class:`.VariantDataset`. :param float tolerance: floating-point tolerance for equality. :rtype: bool; """""". return self._jvds.same(other._jvds, tolerance). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(root=strlike,; keep_star=bool); def sample_qc(self, root='sa.qc', keep_star=False):; """"""Compute per-sample QC metrics. .. include:: requireTGenotype.rst. **Annotations**. :py:meth:`~hail.VariantDataset.sample_qc` computes 20 sample statistics from the ; genotype data and stores the results as sample annotations that can be accessed with; ``sa.qc.<identifier>`` (or ``<root>.<identifier>`` if a non-default root was passed):. +---------------------------+--------+----------------------------------------------------------+; | Name | Type | Description |; +===========================+========+==========================================================+; | ``callRate`` | Double | Fraction of genotypes called |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomRef`` | Int | Number of homozygous reference genotypes |; +---------------------------+--------+----------------------------------------------------------+; | ``nHet`` | Int | Number of heterozygous genotypes |; +---------------------------+--------+----------------------------------------------------------+; | ``nHomVar`` | Int | Number of homozygous alternate genotypes |; +---------------------------+--------+--------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:197232,access,accessed,197232,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessed']
Security,"n{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24890,validat,validation,24890,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['validat'],['validation']
Security,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:3312,access,access,3312,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['access'],['access']
Security,"otype-level information to delimited text file.; Examples; Export genotype information with identifiers that form the header:; >>> vds.export_genotypes('output/genotypes.tsv', 'SAMPLE=s, VARIANT=v, GQ=g.gq, DP=g.dp, ANNO1=va.anno1, ANNO2=va.anno2'). Export the same information without identifiers, resulting in a file with no header:; >>> vds.export_genotypes('output/genotypes.tsv', 's, v, g.gq, g.dp, va.anno1, va.anno2'). Notes; export_genotypes() outputs one line per cell (genotype) in the data set, though HomRef and missing genotypes are not output by default if the genotype schema is equal to TGenotype. Use the export_ref and export_missing parameters to force export of HomRef and missing genotypes, respectively.; The expr argument is a comma-separated list of fields or expressions, all of which must be of the form IDENTIFIER = <expression>, or else of the form <expression>. If some fields have identifiers and some do not, Hail will throw an exception. The accessible namespace includes g, s, sa, v, va, and global. Warning; If the genotype schema does not have the type TGenotype, all genotypes will be exported unless the value of g is missing.; Use filter_genotypes() to filter out genotypes based on an expression before exporting. Parameters:; output (str) – Output path.; expr (str) – Export expression for values to export.; types (bool) – Write types of exported columns to a file at (output + “.types”); export_ref (bool) – If true, export reference genotypes. Only applicable if the genotype schema is TGenotype.; export_missing (bool) – If true, export missing genotypes.; parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_plink(output, fam_expr='id = s', parallel=False)[source]¶; Export variant dataset as PLINK2 BED, BIM and FAM. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Import data from a VCF file, split multi-allelic variants, and expor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:38783,access,accessible,38783,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"ou must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] doe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:3260,access,accessing,3260,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['access'],['accessing']
Security,"parallelism but increase the; amount of meaningful work done per-container. shutdown(wait=True); Allow temporary resources to be cleaned up.; Until shutdown is called, some temporary cloud storage files will; persist. After shutdown has been called and all outstanding jobs have; completed, these files will be deleted. Parameters:; wait (bool) – If true, wait for all jobs to complete before returning from this; method. submit(fn, *args, **kwargs); Call fn on a cloud machine with all remaining arguments and keyword arguments.; The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the default_image argument to BatchPoolExecutor; This function does not return the function’s output, it returns a; BatchPoolFuture whose BatchPoolFuture.result() method; can be used to access the value.; Examples; Do nothing, but on the cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x: x, 4); ... future.result(); 4. Call a function with two arguments and one keyword argument, on the; cloud:; >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(lambda x, y, z: x + y + z,; ... ""poly"", ""ethyl"", z=""ene""); ... future.result(); ""polyethylene"". Generate a product of two random matrices, on the cloud:; >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: ; ... future = bpe.submit(random_product, 1); ... future.result(); [23.325755364428026]. Parameters:. fn (Callable) – The function to execute.; args (Any) – Arguments for the funciton.; kwargs (Any) – Keyword arguments for the function. Return type:; BatchPoolFuture. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; prov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:6948,access,access,6948,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['access'],['access']
Security,"pe: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:6487,hash,hashCode,6487,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['hash'],['hashCode']
Security,"point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def cont",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2923,hash,hash,2923,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['hash'],['hash']
Security,"prs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:71168,expose,expose,71168,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['expose'],['expose']
Security,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:8798,expose,exposed,8798,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['expose'],['exposed']
Security,"racters, escape them with double backslash (\).; let s = ""1kg-NA12878"" in s.split(""-""); result: [""1kg"", ""NA12878""]. Arguments. delim (String) – Regular expression delimiter.; n (Int) – Number of times the pattern is applied. See the Java documentation for more information. split(delim: String): Array[String]. Returns an array of strings, split on the given regular expression delimiter. See the documentation on regular expression syntax delimiter. If you need to split on special characters, escape them with double backslash (\).; let s = ""1kg-NA12878"" in s.split(""-""); result: [""1kg"", ""NA12878""]. Arguments. delim (String) – Regular expression delimiter. toDouble(): Double – Convert value to a Double. toFloat(): Float – Convert value to a Float. toInt(): Int – Convert value to an Integer. toLong(): Long – Convert value to a Long. Struct¶; A Struct is like a Python tuple where the fields are named and the set of fields is fixed.; An example of constructing and accessing the fields in a Struct is; let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene; result: ""ACBD"". A field of the Struct can also be another Struct. For example, va.info.AC selects the struct info from the struct va, and then selects the array AC from the struct info. Variant¶; A Variant is a Hail data type representing a variant in the Variant Dataset. It is referred to as v in the expression language.; The pseudoautosomal region (PAR) is currently defined with respect to reference GRCh37:. X: 60001 - 2699520, 154931044 - 155260560; Y: 10001 - 2649520, 59034050 - 59363566. Most callers assign variants in PAR to X. alt(): String – Alternate allele sequence. Assumes biallelic.; altAllele(): AltAllele – The alternate allele. Assumes biallelic.; altAlleles: Array[AltAllele] – The alternate alleles.; contig: String – String representation of contig, exactly as imported. NB: Hail stores contigs as strings. Use double-quotes when checking contig equality.; inXNonPar(): Boolean – True if chromosome is X and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:48459,access,accessing,48459,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['access'],['accessing']
Security,"rameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""; current_version = self.version; available_regions = [k for k in self.url.keys()]; valid_region = region in available_regions; if not valid_region:; message = (; f'\nName: {name}\n'; f'Version: {current_version}\n'; f'This dataset exists but is not yet available in the'; f' {region} region bucket.\n'; f'Dataset is currently available in the'; f' {"", "".join(available_regions)} region bucket(s).\n'; f'Reach out to the Hail team at https://discuss.hail.is/'; f' to request this dataset in your region.'; ); warnings.warn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:3507,access,accessible,3507,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['accessible']
Security,"rapped in constructor functions like; hl.int32(3). In the same way that Hail expressions can be combined together; via operations like addition and multiplication, they can also be combined with; Python objects.; For example, we can add a Python int to an Int32Expression.; >>> x + 3; <Int32Expression of type int32>. Addition is commutative, so we can also add an Int32Expression to an; int.; >>> 3 + x; <Int32Expression of type int32>. Note that Hail expressions cannot be used in other modules, like numpy; or scipy.; Hail has many subclasses of Expression – one for each Hail type. Each; subclass has its own constructor method. For example, if we have a list of Python; integers, we can convert this to a Hail ArrayNumericExpression with; array():; >>> a = hl.array([1, 2, -3, 0, 5]); >>> a; <ArrayNumericExpression of type array<int32>>. Expression objects keep track of their data type, which is; why we can see that a is of type array<int32> in the output above. An; expression’s type can also be accessed with Expression.dtype().; >>> a.dtype; dtype('array<int32>'). Hail arrays can be indexed and sliced like Python lists or numpy arrays:; >>> a[1]; <Int32Expression of type int32>. >>> a[1:-1]; <ArrayNumericExpression of type array<int32>>. In addition to constructor methods like array() and bool(),; Hail expressions can also be constructed with the literal() method,; which will impute the type of of the expression.; >>> hl.literal([0,1,2]); <ArrayNumericExpression of type array<int32>>. Boolean Logic; Unlike Python, a Hail BooleanExpression cannot be used with the Python; keywords and, or, and not. The Hail substitutes are &, |,; and ~.; >>> s1 = hl.int32(3) == 4; >>> s2 = hl.int32(3) != 4. >>> s1 & s2; <BooleanExpression of type bool>. >>> s1 | s2; <BooleanExpression of type bool>. >>> ~s1; <BooleanExpression of type bool>. Remember that you can use eval(): to evaluate the expression.; >>> hl.eval(~s1); True. Caution; The operator precedence of & and | is different from ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:4157,access,accessed,4157,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['access'],['accessed']
Security,"riantDataset(rmt, vmt). def __init__(self, reference_data: MatrixTable, variant_data: MatrixTable):; self.reference_data: MatrixTable = reference_data; self.variant_data: MatrixTable = variant_data. self.validate(check_data=False). [docs] def write(self, path, **kwargs):; """"""Write to `path`.""""""; self.reference_data.write(VariantDataset._reference_path(path), **kwargs); self.variant_data.write(VariantDataset._variants_path(path), **kwargs). [docs] def checkpoint(self, path, **kwargs) -> 'VariantDataset':; """"""Write to `path` and then read from `path`.""""""; self.write(path, **kwargs); return read_vds(path). [docs] def n_samples(self) -> int:; """"""The number of samples present.""""""; return self.reference_data.count_cols(). @property; def reference_genome(self) -> ReferenceGenome:; """"""Dataset reference genome. Returns; -------; :class:`.ReferenceGenome`; """"""; return self.reference_data.locus.dtype.reference_genome. [docs] @typecheck_method(check_data=bool); def validate(self, *, check_data: bool = True):; """"""Eagerly checks necessary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:8239,validat,validate,8239,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['validat'],['validate']
Security,"rtion, Deletion, Star, MNP, Complex; isComplex(): Boolean – True if not a SNP, MNP, star, insertion, or deletion.; isDeletion(): Boolean – True if v.ref begins with and is longer than v.alt.; isIndel(): Boolean – True if an insertion or a deletion.; isInsertion(): Boolean – True if v.alt begins with and is longer than v.ref.; isMNP(): Boolean – True if v.ref and v.alt are the same length and differ in more than one position.; isSNP(): Boolean – True if v.ref and v.alt are the same length and differ in one position.; isStar(): Boolean – True if v.alt is *.; isTransition(): Boolean – True if a purine-purine or pyrimidine-pyrimidine SNP.; isTransversion(): Boolean – True if a purine-pyrimidine SNP.; ref: String – Reference allele base sequence. Array¶; An Array is a collection of items that all have the same data type (ex: Int, String) and are indexed. Arrays can be constructed by specifying [item1, item2, ...] and they are 0-indexed.; An example of constructing an array and accessing an element is:; let a = [1, 10, 3, 7] in a[1]; result: 10. They can also be nested such as Array[Array[Int]]:; let a = [[1, 2, 3], [4, 5], [], [6, 7]] in a[1]; result: [4, 5]. Array[Array[T]]¶. flatten(): Array[T]. Flattens a nested array by concatenating all its rows into a single array.; let a = [[1, 3], [2, 4]] in a.flatten(); result: [1, 3, 2, 4]. Array[Boolean]¶. sort(ascending: Boolean): Array[Boolean]. Sort the collection with the ordering specified by ascending.; Arguments. ascending (Boolean) – If true, sort the collection in ascending order. Otherwise, sort in descending order. sort(): Array[Boolean] – Sort the collection in ascending order. Array[Double]¶. max(): Double – Largest element in the collection. mean(): Double – Mean value of the collection. median(): Double – Median value of the collection. min(): Double – Smallest element in the collection. product(): Double – Product of all elements in the collection (returns 1 if empty). sort(ascending: Boolean): Array[Double]. S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/types.html:21782,access,accessing,21782,docs/0.1/types.html,https://hail.is,https://hail.is/docs/0.1/types.html,1,['access'],['accessing']
Security,"ruct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). Caution; These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use hadoop_copy(); to move your file to a distributed file system. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hail.utils.hadoop_copy(src, dest)[source]; Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') . Notes; Try using hadoop_open() first, it’s simpler, but not great; for large data! For example:; >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: ; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters:. src (str) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:6305,access,access,6305,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['access'],['access']
Security,"s). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""; assert 'annotation_db' in doc, doc; assert 'key_properties' in doc['annotation_db'], doc['annotation_db']; assert 'description' in doc, doc; assert 'url' in doc, doc; assert 'versions' in doc, doc; key_properties = set(x for x in doc['annotation_db']['key_properties'] if x is not None); versions = [; DatasetVersion.from_json(x, cloud); for x in doc['versions']; if DatasetVersion.from_json(x, cloud) is not None; ]; versions_in_region = DatasetVersion.get_region(name, versions, region); if versions_in_region:; return Dataset(name, doc['description'], doc['url'], key_properties, versions_in_region). def __init__(self, name: str, description: str, url: str, key_properties: Set[str], versions: Li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:6248,access,access,6248,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"s:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""; assert 'annotation_db' in doc, doc; assert 'key_prope",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:5615,access,access,5615,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"s_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with an actual file path into the command for s.; We use another f-string in t’s command where we print the contents of s.ofile to stdout.; s.ofile is the same temporary file that was created in the command for t. Therefore,; Batch deduces that t must depend on s and thus creates an implicit dependency for t on s.; In both the LocalBackend and ServiceBackend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo """,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4637,access,accessed,4637,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['access'],['accessed']
Security,"se TypeError(; ""array expects key to be type 'slice' or expression of type 'int32', ""; ""found expression of type '{}'"".format(item._type); ); else:; return self._method(""indexArray"", self.dtype.element_type, item). @typecheck_method(start=nullable(expr_int32), stop=nullable(expr_int32), step=nullable(expr_int32)); def _slice(self, start=None, stop=None, step=None):; indices, aggregations = unify_all(self, *(x for x in (start, stop, step) if x is not None)); if step is None:; step = hl.int(1); if start is None:; start = hl.if_else(step >= 0, 0, -1); if stop is not None:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop._ir, step._ir); else:; slice_ir = ir.ArraySlice(self._ir, start._ir, stop, step._ir). return construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @dep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13231,access,accessing,13231,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['access'],['accessing']
Security,"se dirname is path. Warning; On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. hailtop.fs.open(path, mode='r', buffer_size=8192, *, requester_pays_config=None)[source]; Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS.; Examples; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: ; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt') as f: ; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:4651,access,access,4651,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['access'],['access']
Security,"sents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2394,access,accessed,2394,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['access'],['accessed']
Security,"sion keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:2800,access,access,2800,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['access'],['access']
Security,"slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:2334,hash,hash,2334,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['hash'],['hash']
Security,"somal region `(X:60001-2699520) || (X:154931044-155260560)` are included if the ``include_par`` optional parameter is set to true.; 4. The minor allele frequency (maf) per variant is calculated.; 5. For each variant and sample with a non-missing genotype call, :math:`E`, the expected number of homozygotes (from population MAF), is computed as :math:`1.0 - (2.0*maf*(1.0-maf))`.; 6. For each variant and sample with a non-missing genotype call, :math:`O`, the observed number of homozygotes, is computed as `0 = heterozygote; 1 = homozygote`; 7. For each variant and sample with a non-missing genotype call, :math:`N` is incremented by 1; 8. For each sample, :math:`E`, :math:`O`, and :math:`N` are combined across variants; 9. :math:`F` is calculated by :math:`(O - E) / (N - E)`; 10. A sex is assigned to each sample with the following criteria: `F < 0.2 => Female; F > 0.8 => Male`. Use ``female-threshold`` and ``male-threshold`` to change this behavior. **Annotations**. The below annotations can be accessed with ``sa.imputesex``. - **isFemale** (*Boolean*) -- True if the imputed sex is female, false if male, missing if undetermined; - **Fstat** (*Double*) -- Inbreeding coefficient; - **nTotal** (*Long*) -- Total number of variants considered; - **nCalled** (*Long*) -- Number of variants with a genotype call; - **expectedHoms** (*Double*) -- Expected number of homozygotes; - **observedHoms** (*Long*) -- Observed number of homozygotes. :param float maf_threshold: Minimum minor allele frequency threshold. :param bool include_par: Include pseudoautosomal regions. :param float female_threshold: Samples are called females if F < femaleThreshold. :param float male_threshold: Samples are called males if F > maleThreshold. :param str pop_freq: Variant annotation for estimate of MAF.; If None, MAF will be computed. :return: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:89399,access,accessed,89399,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessed']
Security,"source]; An object representing a range of values between start and end.; >>> interval2 = hl.Interval(3, 6). Parameters:. start (any type) – Object with type point_type.; end (any type) – Object with type point_type.; includes_start (bool) – Interval includes start.; includes_end (bool) – Interval includes end. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.interval.take(5). This is rare; it is much; more common to manipulate the IntervalExpression object, which is; constructed using the following functions:. interval(); locus_interval(); parse_locus_interval(). class hail.utils.Struct(**kwargs)[source]; Nested annotation structure.; >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:; >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:; attributes – Field names and values. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.info.take(5). This is rare; it is much; more common to manipulate the StructExpression object, which is; constructed using the struct() function. class hail.utils.frozendict(d)[source]; An object representing an immutable dictionary.; >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a frozendict:; >>> dict(frozendict({'a': 1, 'b': 2})). Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.my_dict.take(5). This is rare; it is much; more common to manipulate the DictExpression object, which is; cons",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:3251,access,accessed,3251,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['access'],['accessed']
Security,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:2577,validat,validate,2577,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['validat'],['validate']
Security,"t already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2328,access,accessing,2328,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['access'],['accessing']
Security,"ta; on your behalf. To get the name of the service account, click on your name on the header bar or go to; https://auth.hail.is/user.; To give the service account read and write access to a Google Storage bucket, run the following command substituting; SERVICE_ACCOUNT_NAME with the full service account name (ex: test@my-project.iam.gserviceaccount.com) and BUCKET_NAME; with your bucket name. See this page; for more information about access control.; gcloud storage buckets add-iam-policy-binding gs://<BUCKET_NAME> \; --member=serviceAccount:<SERVICE_ACCOUNT_NAME> \; --role=roles/storage.objectAdmin. The Google Artifact Registry is a Docker repository hosted by Google that is an alternative to; Docker Hub for storing images. It is recommended to use the artifact registry for images that; shouldn’t be publically available. If you have an artifact registry associated with your project, then you can enable the service account to; view Docker images with the command below where SERVICE_ACCOUNT_NAME is your full service account; name, and <REPO> is the name of your repository you want to grant access to and has a path that; has the following prefix us-docker.pkg.dev/<MY_PROJECT>:; gcloud artifacts repositories add-iam-policy-binding <REPO> \; --member=<SERVICE_ACCOUNT_NAME> --role=roles/artifactregistry.repoAdmin. Billing; The cost for executing a job depends on the underlying machine type, the region in which the VM is running in,; and how much CPU and memory is being requested. Currently, Batch runs most jobs on 16 core, spot, n1; machines with 10 GB of persistent SSD boot disk and 375 GB of local SSD. The costs are as follows:. Compute cost. Caution; The prices shown below are approximate prices based on us-central1. Actual prices are; based on the current spot prices for a given worker type and the region in which the worker is running in.; You can use Job.regions() to specify which regions to run a job in. = $0.01 per core per hour for spot standard worker types; = $0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:3872,access,access,3872,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['access'],['access']
Security,"taset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with va.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of samples with called genotypes. AF; Double; Calculated alternate allele frequency (q). AC; Int; Count of alternate alleles. rHeterozygosity; Double; Proportion of heterozygotes. rHetHomVar; Double; Ratio of heterozygotes to homozygous alternates. rExpectedHetFrequency; Double; Expected rHeterozygosity based on HWE. pHWE; Double; p-value from Hardy Weinberg Equilibrium null model. nHomRef; Int; Number of homozygous reference samples. nHet; Int; Number of heterozygous samples. nHomVar; Int; Number of homozygous alternate samples. nCalled; Int; Sum of nHomRef, nHet, and nHomVar. nNotCalled; Int; Number of uncalled samples. nNonRef; Int; Sum of nHet and nHomVar. rHetHomVar; Double; Het/HomVar ratio across all samples. dpMean; Double; Depth mean across all samples. dpStDev; Double; De",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:172035,access,accessed,172035,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessed']
Security,"tch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = state. def _get_field(self, item):; if item in self._fields:; return self._fields[item]; el",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:1132,access,accessed,1132,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['access'],['accessed']
Security,"th the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.8",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:51377,access,accessible,51377,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['accessible']
Security,"than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and exe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8060,authenticat,authenticate,8060,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['authenticat'],['authenticate']
Security,"the; struct.* syntax. This syntax produces one column per field in the; struct, and names them according to the struct field name.; For example, the following invocation (assuming va.qc was generated; by variant_qc()):; >>> vds.export_variants('output/file.tsv', 'variant = v, va.qc.*'). will produce the following set of columns:; variant callRate AC AF nCalled ... Note that using the .* syntax always results in named arguments, so it; is not possible to export header-less files in this manner. However,; naming the “splatted” struct will apply the name in front of each column; like so:; >>> vds.export_variants('output/file.tsv', 'variant = v, QC = va.qc.*'). which produces these columns:; variant QC.callRate QC.AC QC.AF QC.nCalled ... Notes; This module takes a comma-delimited list of fields or expressions to; print. These fields will be printed in the order they appear in the; expression in the header and on each line.; One line per variant in the VDS will be printed. The accessible namespace includes:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. Designating output with an expression; Much like the filtering methods, this method uses the Hail expression language.; While the filtering methods expect an; expression that evaluates to true or false, this method expects a; comma-separated list of fields to print. These fields take the; form IDENTIFIER = <expression>. Parameters:; output (str) – Output file.; expr (str) – Export expression for values to export.; types (bool) – Write types of exported columns to a file at (output + “.types”); parallel (bool) – If true, writes a set of files (one per partition) rather than serially concatenating these files. export_vcf(output, append_to_header=None, export_pp=False, parallel=False)[source]¶; Export variant dataset as a .vcf or .vcf.bgz file.; Examples; Export to VCF as a block-compressed file:; >>> vds.export_vcf('output/example.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:43797,access,accessible,43797,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['access'],['accessible']
Security,"torage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:5897,access,access,5897,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['access'],['access']
Security,"tput of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fract",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:3835,access,accessing,3835,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['access'],['accessing']
Security,"type; var = Env.get_uid(); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); self._agg_bindings.add(var); aggregated = f(ref); _check_agg_bindings(aggregated, self._agg_bindings); self._agg_bindings.remove(var). if not self._as_scan and not aggregated._aggregations:; raise ExpressionException(; f""'hl.{self.correct_prefix()}.array_agg' "" f""must take mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); aggregations = hl.utils.LinkedList(Aggregation); if not self._as_scan:; aggregations = aggregations.push(Aggregation(array, aggregated)); return construct_expr(; ir.AggArrayPerElement(array._ir, var, 'unused', aggregated._ir, self._as_scan),; tarray(aggregated.dtype),; Indices(indices.source, aggregated._indices.axes),; aggregations,; ). @property; def context(self):; if self._as_scan:; return 'scan'; else:; return 'agg'. def _aggregate_local_array(array, f):; """"""Compute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""; elt = array.dtype.element_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:10319,access,accessing,10319,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['access'],['accessing']
Security,"uired when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25349,access,access,25349,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['access']
Security,"ulates the sum of the squared residuals (x; - mean) of an array. In [41]:. # Uncomment the below code by deleting the triple-quotes and write an expression to calculate the residuals. """"""; result, t = hc.eval_expr_typed('''; let a = [1, -2, 11, 3, -2]; and mean = <FILL IN>; in a.map(x => <FILL IN> ).sum(); '''); """""". try:; print('Your result: %s (%s)' % (result, t)); print('Expected answer: 114.8 (Double)'); except NameError:; print('### Remove the triple quotes around the above code to start the exercise ### '). ### Remove the triple quotes around the above code to start the exercise ###. What if a contains a missing value NA: Int? Will your code still; work?. Structs¶; Structs are a collection of named values known as fields. Hail; does not have tuples like Python. Unlike arrays, the values can be; heterogenous. Unlike Dicts, the set of names are part of the type; and must be known statically. Structs are constructed with a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:11274,access,accessed,11274,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['access'],['accessed']
Security,"up. With ``single_key=True``, ``variant_keys`` expects a; variant annotation whose value is itself the key of interest. In bose cases, variants with missing keys are; ignored. **Notes**. This method modifies :py:meth:`.linreg` by replacing the genotype covariate per variant and sample with; an aggregated (i.e., collapsed) score per key and sample. This numeric score is computed from the sample's; genotypes and annotations over all variants with that key. Conceptually, the method proceeds as follows:. 1) Filter to the set of samples for which all phenotype and covariates are defined. 2) For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; ``agg_expr`` must be of numeric type and has the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. Note that ``v``, ``va``, and ``g`` are accessible through; `Aggregable methods <https://hail.is/hail/types.html#aggregable>`_ on ``gs``. The resulting **sample key table** has key column ``key_name`` and a numeric column of scores for each sample; named by the sample ID. 3) For each key, fit the linear regression model using the supplied phenotype and covariates.; The model is that of :py:meth:`.linreg` with sample genotype ``gt`` replaced by the score in the sample; key table. For each key, missing scores are mean-imputed across all samples. The resulting **linear regression key table** has the following columns:. - value of ``key_name`` (*String*) -- descriptor of variant group key (key column); - **beta** (*Double*) -- fit coefficient, :math:`\hat\beta_1`; - **se** (*Double*) -- estimated standard error, :math:`\widehat{\mathrm{se}}`; - **tstat** (*Double*) -- :math:`t`-statistic, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **pval** (*Double*) -- :math:`p`-value. :py:meth:`.linreg_burden` returns both the linear reg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:105414,access,accessible,105414,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['access'],['accessible']
Security,"xpression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1366,access,accessed,1366,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['access'],['accessed']
Security,"xtension of a; Table.; Unlike a table, which has two field groups (row fields and global; fields), a matrix table consists of four components:. a two-dimensional matrix of entry fields where each entry is indexed by; row key(s) and column key(s); a corresponding rows table that stores all of the row fields that are; constant for every column in the dataset; a corresponding columns table that stores all of the column fields that; are constant for; every row in the dataset; a set of global fields that are constant for every entry in the dataset. There are different operations on the matrix for each field group.; For instance, Table has Table.select() and; Table.select_globals(), while MatrixTable has; MatrixTable.select_rows(), MatrixTable.select_cols(),; MatrixTable.select_entries(), and MatrixTable.select_globals().; It is possible to represent matrix data by coordinate in a table , storing one; record per entry of the matrix. However, the MatrixTable represents; this data far more efficiently and exposes natural interfaces for computing on; it.; The MatrixTable.rows() and MatrixTable.cols() methods return the; row and column fields as separate tables. The MatrixTable.entries(); method returns the matrix as a table in coordinate form – use this object with; caution, because this representation is costly to compute and is significantly; larger in memory. Keys; Matrix tables have keys just as tables do. However, instead of one key, matrix; tables have two keys: a row key and a column key. Row fields are indexed by the; row key, column fields are indexed by the column key, and entry fields are; indexed by the row key and the column key. The key structs can be accessed with; MatrixTable.row_key and MatrixTable.col_key. It is possible to; change the keys with MatrixTable.key_rows_by() and; MatrixTable.key_cols_by().; Due to the data representation of a matrix table, changing a row key is often an; expensive operation. Referencing Fields; All fields (row, column, global,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:1713,expose,exposes,1713,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['expose'],['exposes']
Security,"y leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:41847,access,access,41847,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['access'],['access']
Security,"ype, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2894,hash,hash,2894,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['hash'],['hash']
Security,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56604,validat,validation,56604,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['validat'],['validation']
Security,"| 5 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 5 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. but the value 5 is only stored once for the entire dataset and NOT once per; row of the table. The output of Table.describe() lists what all of the row; fields and global fields are.; >>> ht.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ID': int32; 'HT': int32; 'SEX': str; 'X': int32; 'Z': int32; 'C1': int32; 'C2': int32; 'C3': int32; ----------------------------------------; Key:; None; ----------------------------------------. Keys; Row fields can be specified to be the key of the table with the method; Table.key_by(). Keys are important for joining tables together (discussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:3621,access,accessed,3621,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['access'],['accessed']
Security,"}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start = includes_start; self._includes_end = includes_end. def __str__(self):; if isinstance(self._start, hl.genetics.Locus) and self._start.contig == self._end.contig:; bounds = f'{self._start}-{self._end.position}'; else:; bounds = f'{self._start}-{self._end}'; open = '[' if self._includes_start else '('; close = ']' if self._includes_end else ')'; return f'{open}{bounds}{close}'. def __repr__(self):; return 'Interval(start={}, end={}, includes_start={}, includes_end={})'.format(; repr(self.start), repr(self.end), repr(self.includes_start), repr(self._includes_end); ). def __eq__(self, other):; return (; (; self._start == other._start; and self._end == other._end; and self._includes_start == other._includes_start; and self._includes_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:2876,hash,hash,2876,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['hash'],['hash']
Security,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24653,validat,validation,24653,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['validat'],['validation']
Security,"﻿. . Querying using SQL — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources; Hadoop Glob Patterns; SQL; Impala. Hail. Docs »; Other Resources »; Querying using SQL. View page source. Querying using SQL¶; Since Hail uses the Parquet file format for data storage, a Hail VDS can be queried using; Hadoop SQL tools, like Hive or Impala. This mode of access may be convenient for users; who have ad hoc queries that they are able to express in SQL.; Note that SQL access is read-only: it is not possible to write Hail datasets using; SQL at the current time. Impala¶; Each VDS should be registered in the Hive metastore to allow Impala to query it (Impala uses Hive’s metastore to store table metadata). This is done by creating an external table in Hive, the “external” part means that the data is managed by an entity outside Hive (and; Impala). The table schema is read from one of the Parquet files in the VDS file; hierarchy.; To generate a Hive file:. Copy a VCF file into HDFS. $ hadoop fs -put src/test/resources/sample.vcf.bgz sample.vcf.bgz. Convert the VCF file into a VDS using Hail:; >>> hc.import_vcf(""sample.vcf.bgz"").write(""sample.vds"", parquet_genotypes=True). Note the use of parquet_genotypes=True, which writes the genotype; information using Parquet structures, rather than an opaque binary; representation that cannot be queried using SQL. Register the VDS as a Hive table. $ PARQUET_DATA_FILE=$(hadoop fs -stat '%n' hdfs:///user/$USER/sample.vds/rdd.parquet/*.parquet | head -1); $ impala-shell -q ""CREATE EXTERNAL TABLE variants LIKE PARQUET 'hdfs:///user/$USER/sample.vds/rdd.parquet/$PARQUET_DATA_FILE' STORED AS PARQUET LOCATION 'hdfs:///user/$USER/sample.vds/rdd.parquet'"". It is good practice to run Impala’s COMPUTE STATS command on the newly-created table, so that subsequent queries run efficiently.; $",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/sql.html:518,access,access,518,docs/0.1/sql.html,https://hail.is,https://hail.is/docs/0.1/sql.html,2,['access'],['access']
Security,"﻿. . Struct — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Struct. View page source. Struct¶. class hail.representation.Struct(attributes)[source]¶; Nested annotation structure.; >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both ‘items’ and ‘attributes’, which; allows either syntax for accessing the element “foo” of struct “bar”:; >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name “1kg”, for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:; >>> getattr(bar, '1kg'); >>> bar['1kg']. The pprint module can be used to print nested Structs in a more; human-readable fashion:; >>> from pprint import pprint; >>> pprint(bar). Parameters:attributes (dict) – struct members. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. get; Get an item, or return a default value if the item is not found. get(item, default=None)[source]¶; Get an item, or return a default value if the item is not found. Parameters:; item (str) – Name of attribute.; default – Default value. Returns:Value of item if found, or default value if not. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Struct.html:707,access,accessing,707,docs/0.1/representation/hail.representation.Struct.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Struct.html,2,['access'],"['access', 'accessing']"
Security,"﻿. . hail.representation.annotations — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.annotations. Source code for hail.representation.annotations; from hail.typecheck import *. [docs]class Struct(object):; """"""; Nested annotation structure. >>> bar = Struct({'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Note that it is possible to use Hail to define struct fields inside; of a key table or variant dataset that do not match python syntax.; The name ""1kg"", for example, will not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:600,access,accessing,600,docs/0.1/_modules/hail/representation/annotations.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html,2,['access'],"['access', 'accessing']"
Security,"﻿. Hail | ; Aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Aggregators. View page source. Aggregators; The aggregators module is exposed as hl.agg, e.g. hl.agg.sum. collect(expr); Collect records into an array. collect_as_set(expr); Collect records into a set. count(); Count the number of records. count_where(condition); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:714,expose,exposed,714,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['expose'],['exposed']
Security,"﻿. Hail | ; Annotation Database. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Database Query. Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Annotation Database. View page source. Annotation Database. Warning; All functionality described on this page is experimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the pan",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:656,access,accessible,656,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['access'],['accessible']
Security,"﻿. Hail | ; Functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions. View page source. Functions; These functions are exposed at the top level of the module, e.g. hl.case. Core language functions; literal(); cond(); if_else(); switch(); case(); bind(); rbind(); missing(); null(); str(); is_missing(); is_defined(); coalesce(); or_else(); or_missing(); range(); query_table(); CaseBuilder; SwitchBuilder. Constructor functions; bool(); float(); float32(); float64(); int(); int32(); int64(); interval(); struct(); tuple(); array(); empty_array(); set(); empty_set(); dict(); empty_dict(). Collection functions; len(); map(); flatmap(); starmap(); zip(); enumerate(); zip_with_index(); flatten(); any(); all(); filter(); sorted(); find(); group_by(); fold(); array_scan(); reversed(); keyed_intersection(); keyed_union(). Numeric functions; abs(); approx_equal(); bit_and(); bit_or(); bit_xor(); bit_lshift(); bit_rshift(); bit_not(); bit_count(); exp(); expit(); is_nan(); is_finite(); is_infinite(); log(); log10(); logit(); floor(); ceil(); sqrt(); sign(); min(); nanmin(); max(); nanmax(); mean(); median(); product(); sum(); cumulative_sum(); argmin(); argmax(); corr(); uniroot(); binary_search(). String functions; format(); json(); parse_json(); hamming(); delimit(); entropy(); parse_int(); parse_int32(); parse_int64(); parse_float(); parse_float32(); parse_float64(). Statistical functions; chi_squared_test",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:702,expose,exposed,702,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['expose'],['exposed']
Security,"﻿. Hail | ; Scans. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Scans. View page source. Scans; The scan module is exposed as hl.scan, e.g. hl.scan.sum.; The functions in this module perform rolling aggregations along the rows of a; table, or along the rows or columns of a matrix table. The value of the scan at; a given row (or column) is the result of applying the corresponding aggregator; to all previous rows (or columns). Scans directly over entries are not currently; supported.; For example, the count aggregator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:689,expose,exposed,689,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['expose'],['exposed']
Security,"﻿. Hail | ; StructExpression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; StructExpression. View page source. StructExpression. class hail.expr.StructExpression[source]; Expression of type tstruct.; >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field a of struct s with dot syntax:; >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:; >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of StructExpression (keys, values,; annotate, drop, etc.) will only be accessible using the; StructExpression.__getitem__() syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; Attributes. dtype; The data type of the expression. Methods. annotate; Add new fields or recompute existing fields. drop; Drop fields from the struct. flatten; Recursively eliminate struct fields by adding their fields to this struct. get; See StructExpression.__getitem__(). items; A list of pairs of field name and expression for said field. keys; The list of field names. rename; Rename fields of the struct. select; Select existing fields and compute new ones. values; A list of expressions for each field. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:844,access,accessible,844,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,2,['access'],"['access', 'accessible']"
Security,"﻿. Hail | ; hail.utils.struct. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:939,access,accessing,939,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['access'],['accessing']
Security,"﻿. Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:582,hash,hashlib,582,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['hash'],['hashlib']
Security,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:1416,hash,hash,1416,docs/0.2/_modules/hailtop/frozendict.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html,1,['hash'],['hash']
Security,"﻿. Hail | ; hailtop.fs Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; Top-Level Functions; copy(); exists(); is_dir(); is_file(); ls(); mkdir(); open(); remove(); rmtree(); stat(). hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.fs Python API. View page source. hailtop.fs Python API; This is the API documentation for Hail’s cloud-agnostic file system implementation in hailtop.fs.; Use import hailtop.fs as hfs to access this functionality. Top-Level Functions. copy(src, dest, *[, requester_pays_config]); Copy a file between filesystems. exists(path, *[, requester_pays_config]); Returns True if path exists. is_dir(path, *[, requester_pays_config]); Returns True if path both exists and is a directory. is_file(path, *[, requester_pays_config]); Returns True if path both exists and is a file. ls(path, *[, requester_pays_config]); Returns information about files at path. mkdir(path, *[, requester_pays_config]); Ensure files can be created whose dirname is path. open(path[, mode, buffer_size, ...]); Open a file from the local filesystem of from blob storage. remove(path, *[, requester_pays_config]); Removes the file at path. rmtree(path, *[, requester_pays_config]); Recursively remove all files under the given path. stat(path, *[, requester_pays_config]); Returns information about the file or directory at a given path. hailtop.fs.copy(src, dest, *, requester_pays_config=None)[source]; Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS.; Examples; Copy a file from Google Cloud Storage to a local file:; >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:769,access,access,769,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['access'],['access']
Testability," 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:989,test,test,989,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['test'],['test']
Testability," """"""Get a field from each struct in this set. Examples; --------. >>> x = hl.set({hl.struct(a='foo', b=3), hl.struct(a='bar', b=4)}); >>> hl.eval(x.a) == {'foo', 'bar'}; True. >>> a = hl.set({hl.struct(b={hl.struct(inner=1),; ... hl.struct(inner=2)}),; ... hl.struct(b={hl.struct(inner=3)})}); >>> hl.eval(hl.flatten(a.b).inner) == {1, 2, 3}; True; >>> hl.eval(hl.flatten(a.b.inner)) == {1, 2, 3}; True. Parameters; ----------; item : :class:`str`; Field name. Returns; -------; :class:`.SetExpression`; A set formed by getting the given field for each struct in; this set; """""". return self.map(lambda x: x[item]). [docs]class DictExpression(Expression):; """"""Expression of type :class:`.tdict`. >>> d = hl.literal({'Alice': 43, 'Bob': 33, 'Charles': 44}); """""". @typecheck_method(x=ir.IR, type=HailType, indices=Indices, aggregations=LinkedList); def __init__(self, x, type, indices=Indices(), aggregations=LinkedList(Aggregation)):; super(DictExpression, self).__init__(x, type, indices, aggregations); assert isinstance(type, tdict); self._kc = coercer_from_dtype(type.key_type); self._vc = coercer_from_dtype(type.value_type). [docs] @typecheck_method(item=expr_any); def __getitem__(self, item):; """"""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""; if not self._kc.can_coerce(item.dtype):; raise TypeError(; ""dict encountered an invalid key type\n"" "" dict key type: '{}'\n"" "" type of 'item': '{}'"".format(; self.dtype.key_type, item.dtype; ); ); return self._index(self.dtype.value_type, self._kc.coerce(item)). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns whether a given key is present in the dictionary. Examples; --------. >>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:36628,assert,assert,36628,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['assert'],['assert']
Testability," ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """""". def __init__(self, bmir):; self._bmir = bmir. [docs] @classmethod; @typecheck_method(path=str, _assert_type=nullable(tblockmatrix)); def read(cls, path, *, _assert_type=None):; """"""Reads a block matrix. Parameters; ----------; path: :class:`str`; Path to input file. Returns; -------; :class:`.BlockMatrix`; """"""; return cls(BlockMatrixRead(BlockMatrixNativeReader(path), _assert_type=_assert_type)). [docs] @classmethod; @typecheck_method(uri=str, n_rows=int, n_cols=int, block_size=nullable(int), _assert_type=nullable(tblockmatrix)); def fromfile(cls, uri, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https:/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:9580,log,logarithm,9580,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['log'],"['log', 'logarithm']"
Testability," && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/ldpruned.variants"", ""v"")). Notes; Variants are pruned in each contig from smallest to largest start position. The LD pruning algorithm is as follows:; pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and correlation(v1, v2) >= r2):; keep = False; if keep:; pruned_set.append(v1). The parameter window defines the maximum distance in base pairs between two variants to check whether; the variants are independent (\(R^2\) < r2) where r2 is the maximum \(R^2\) allowed.; \(R^2\) is defined as the square of Pearson’s correlation coefficient; \({\rho}_{x,y}\) between the two genotype vectors \({\mathbf{x}}\) and \({\mathbf{y}}\). \[{\rho}_{x,y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}\]; ld_prune() with default arguments is equivalent to plink --indep-pairwise 1000kb 1 0.2.; The list of pruned variants returned by Hail and PLINK will differ because Hail mean-imputes missing values and tests pairs of variants in a different order than PLINK.; Be sure to provide enough disk space per worker because ld_prune() persists up to 3 copies of the data to both memory and disk.; The amount of disk space required will depend on the size and minor allele frequency of the input data and the prune parameters r2 and window. The number of bytes stored in memory per variant is about nSamples / 4 + 50. Warning; The variants in the pruned set are not guaranteed to be identical each time ld_prune() is run. We recommend running ld_prune() once and exporting the list of LD pruned variants using; export_variants() for future use. Parameters:; r2 (float) – Maximum \(R^2\) threshold between two variants in the pruned set within a given window.; window (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:76873,test,tests,76873,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['tests']
Testability," (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:3508,log,log,3508,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['log'],['log']
Testability," (int) – Width of window in base-pairs for computing pair-wise \(R^2\) values.; memory_per_core (int) – Total amount of memory available for each core in MB. If unsure, use the default value.; num_cores (int) – The number of cores available. Equivalent to the total number of workers times the number of cores per worker. Returns:Variant dataset filtered to those variants which remain after LD pruning. Return type:VariantDataset. linreg(y, covariates=[], root='va.linreg', use_dosages=False, min_ac=1, min_af=0.0)[source]¶; Test each variant for association using linear regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run linear regression per variant using a phenotype and two covariates stored in sample annotations:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The linreg() method computes, for each variant, statistics of; the \(t\)-test for the genotype coefficient of the linear function; of best fit from sample genotype and covariates to quantitative; phenotype or case-control status. Hail only includes samples for which; phenotype and all covariates are defined. For each variant, missing genotypes; as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Assuming there are sample annotations sa.pheno.height,; sa.pheno.age, sa.pheno.isFemale, and sa.cov.PC1, the code:; >>> vds_result = vds.linreg('sa.pheno.height', covariates=['sa.pheno.age', 'sa.pheno.isFemale', 'sa.cov.PC1']). considers a model of the form. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:78591,test,test,78591,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['test']
Testability," (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; +-----------+------------+. Parallelizing with a specified number of partitions:. >>> rows = [ {'a': i} for i in range(100) ]; >>> ht = hl.Table.parallelize(rows, n_partitions=10); >>> ht.n_partitions(); 10; >>> ht.count(); 100. Parallelizing with some global",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:17223,log,login,17223,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['log'],['login']
Testability," ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and pyth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4435,log,log,4435,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,2,['log'],['log']
Testability," ). if n_partition_args == 0:; raise ValueError(; ""'new_combiner': require one argument from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals' to choose GVCF partitioning""; ). if n_partition_args > 1:; warning(; ""'new_combiner': multiple colliding arguments found from 'intervals', 'import_interval_size', ""; ""'use_genome_default_intervals', or 'use_exome_default_intervals'.""; ""\n The argument found first in the list in this warning will be used, and others ignored.""; ). if intervals is not None:; pass; elif import_interval_size is not None:; intervals = calculate_even_genome_partitioning(reference_genome, import_interval_size); elif use_genome_default_intervals:; size = VariantDatasetCombiner.default_genome_interval_size; intervals = calculate_even_genome_partitioning(reference_genome, size); elif use_exome_default_intervals:; size = VariantDatasetCombiner.default_exome_interval_size; intervals = calculate_even_genome_partitioning(reference_genome, size); assert intervals is not None; else:; intervals = []. if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). # we need to compute the type that the combiner will have, this will allow us to read matrix; # tables quickly, especially in an asynchronous environment like query on batch where typing; # a read uses a blocking round trip.; vds = None; gvcf_type = None; if vds_paths:; # sync up gvcf_reference_entry_fields_to_keep and they reference entry types from the VDS; vds = hl.vds.read_vds(vds_paths[0], _warn_no_ref_block_max_length=False); vds_ref_entry = set(; name[1:] if name in ('LGT', 'LPGT') else name for name in vds.reference_data.entry if name != 'END'; ); if gvcf_reference_entry_fields_to_keep is not None and vds_ref_entry != gvcf_reference_entry_fields_to_keep:; warning(; ""Mismatch between 'gvcf_reference_entry_fields' to keep and VDS reference data ""; ""entry types. Overwriting with reference entry fields from suppli",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:27993,assert,assert,27993,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['assert'],['assert']
Testability," + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ========== ================== ======= ============================================; Wald `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; Wald `standard_error` float64 estimated standard error,; :math:`\widehat{\mathrm{se}}`; Wald `z_stat` float64 Wald :math:`z`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; Wald `p_value` float64 Wald p-value testing :math:`\beta_1 = 0`; LRT, Firth `beta` float64 fit effect coefficient,; :math:`\hat\beta_1`; LRT, Firth `chi_sq_stat` float64 deviance statistic; LRT, Firth `p_value` float64 LRT / Firth p-value testing; :math:`\beta_1 = 0`; Score `chi_sq_stat` float64 score statistic; Score `p_value` float64 score p-value testing :math:`\beta_1 = 0`; ========== ================== ======= ============================================. For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. ================ =================== ======= ===============================; Test Field Type Value; ================ =================== ======= ===============",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:29849,test,testing,29849,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,6,['test'],['testing']
Testability," - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.expl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32528,test,test,32528,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['test'],['test']
Testability," - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""; require_alleles_field(mt, 'variant_qc'). bound_exprs = {}; gq_dp_exprs = {}. def has_field_of_type(name, dtype):; return name in mt.entry ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:10330,test,test,10330,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['test'],['test']
Testability," .. code-block:: text. if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation ``sa.fam.isCase`` added by importing a FAM file with case-control phenotype, case is 1 and control is 0. Hail's logistic regression tests correspond to the ``b.wald``, ``b.lrt``, and ``b.score`` tests in `EPACTS <http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests>`__. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:148536,log,logistic,148536,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['logistic']
Testability," ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_erro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:8267,test,test,8267,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,4,['test'],['test']
Testability," .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; If there are no records to aggregate, the result is False.; Missing records are not considered. If every record is missing,; the result is also False. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.all(condition)[source]; Returns True if condition is True for every record.; Examples; >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; If there are no records to aggregate, the result is True.; Missing records are not considered. If every record is missing,; the result is also True. Parameters:; condition (BooleanExpression) – Condition to test. Returns:; BooleanExpression. hail.expr.aggregators.take(expr, n, ordering=None)[source]; Take n records of expr, optionally ordered by ordering.; Examples; Take 3 elements of field X:; >>> table1.aggregate(hl.agg.take(table1.X, 3)); [5, 6, 7]. Take the ID and HT fields, ordered by HT (descending):; >>> table1.aggregate(hl.agg.take(hl.struct(ID=table1.ID, HT=table1.HT),; ... 3,; ... ordering=-table1.HT)); [Struct(ID=2, HT=72), Struct(ID=3, HT=70), Struct(ID=1, HT=65)]. Notes; The resulting array can include fewer than n elements if there are fewer; than n total records.; The ordering argument may be an expression, a function, or None.; If ordering is an expression, this expression’s type should be one with; a natural ordering (e.g. numeric).; If ordering is a function, it will be evaluated on each record of expr; to compute the value used for ordering. In the above example,; ordering=-table1.HT and ordering=lambda x: -x.HT would be; equivalent.; If orde",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:7741,test,test,7741,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability," / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:100844,test,test,100844,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['test'],['test']
Testability," 0 else int_rep + 2**32. ploidy = (int_rep >> 1) & 0x3; phased = (int_rep & 1) == 1. def allele_repr(c):; return c >> 3. def ap_j(p):; return p & 0xFFFF. def ap_k(p):; return (p >> 16) & 0xFFFF. def gt_allele_pair(i):; assert i >= 0, ""allele pair value should never be negative""; if i < len(small_allele_pair):; return small_allele_pair[i]; return allele_pair_sqrt(i). def call_allele_pair(i):; if phased:; rep = allele_repr(i); p = gt_allele_pair(rep); j = ap_j(p); k = ap_k(p); return allele_pair(j, k - j); else:; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploidy == 2:; int_rep |= allele_pair_rep(value) << 3; int_rep = int_rep if 0 <= int_rep < 2**31 - 1 else int_rep - 2**32. byte_writer.write_int32(int_rep). def unify(self, t):; return t == tcall. def subst(self):; return self. def clear(self):; pass. [docs]class tlocus(HailType):; """"""Hail type for a genomic coordinate with a contig and a position. In Python, these are represented by :class:`.Locus`. Parameters; ----------; reference_genome: :class:`.ReferenceGenome` or :class:`str`; Reference genome to use. See Also; --------; :class:`.LocusExpression`, :func:`.locus`, :func:`.parse_locus`,; :class:`.Locus`; """""". struct_repr = tstruct(contig=_tstr(), pos=_tint32()). @classmethod; @typecheck_method(reference_genome=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:45042,assert,assert,45042,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['assert'],['assert']
Testability," 0 or 1. :param covariates: list of covariate expressions; :type covariates: list of str. :param str root: Variant annotation path to store result of logistic regression. :param bool use_dosages: If true, use genotype dosage rather than hard call. :return: Variant dataset with logistic regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.logreg(test, y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; test=strlike,; y=strlike,; covariates=listof(strlike)); def logreg_burden(self, key_name, variant_keys, single_key, agg_expr, test, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; logistic regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using the logistic Wald test on the maximum genotype per gene. Here ``va.genes`` is; a variant annotation of type Set[String] giving the set of genes containing the variant; (see **Extended example** in :py:meth:`.linreg_burden` for a deeper dive in the context of linear regression):. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... test='wald',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using the logistic score test on the weighted sum of genotypes per gene.; Here ``va.gene`` is a variant annotation of type String giving a single gene per variant (or no gene if; missing), and ``va.weight`` is a numeric variant annotation:. >>> logreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .logreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:149362,test,test,149362,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,3,"['log', 'test']","['logistic', 'test']"
Testability," 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile function of a normal distribution with mean mu and standard deviation sigma, inverts pnorm(). qpois(p, lamb[, lower_tail, log_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). Randomness.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:10503,log,log,10503,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['log'],['log']
Testability," 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, odds_ratio=tfloat64); return _func(""chi_squared_test"", ret_type, c1, c2, c3, c4). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32, min_cell_count=expr_int32); def contingency_table_test(c1, c2, c3, c4, min_cell_count) -> StructExpression:; """"""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, odds_ratio=tfloat64); return _func(""contingency_table_test"", ret_type, c1, c2, c3, c4, min_cell_count). # We use 64-bit integers.; # It is relatively easy to encounter an integer overflow bug with 32-bit integers.; [docs]@typecheck(a=expr_array(expr_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:20936,test,test,20936,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['test'],['test']
Testability," 8.0, 27.0, 64.0, 125.0]. >>> hl.eval(s3.map(lambda x: x.length())); {3, 5, 7}. Parameters:; f (function ( (arg) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:10206,log,logging,10206,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['log'],['logging']
Testability," :math:`r_g`; is assumed to be 0 between traits. If `rg` and `cov_matrix` are both; not None, :math:`r_g` values from `cov_matrix` take precedence.; cov_matrix : :class:`numpy.ndarray`, optional; Covariance matrix for traits, **unscaled by :math:`M`**, the number of SNPs.; Overrides `h2` and `rg` even when `h2` or `rg` are not ``None``.; seed : :obj:`int`, optional; Seed for random number generator. If `seed` is ``None``, `seed` is set randomly. Returns; -------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` with simulated SNP effects as a row field of arrays.; rg : :obj:`list`; Genetic correlation between traits, possibly altered from input `rg` if; covariance matrix was not positive semi-definite.; """"""; uid = Env.get_uid(base=100); h2 = h2.tolist() if isinstance(h2, np.ndarray) else ([h2] if not isinstance(h2, list) else h2); rg = rg.tolist() if isinstance(rg, np.ndarray) else ([rg] if not isinstance(rg, list) else rg); assert all(x >= 0 and x <= 1 for x in h2), 'h2 values must be between 0 and 1'; assert h2 is not [None] or cov_matrix is not None, 'h2 and cov_matrix cannot both be None'; M = mt.count_rows(); if cov_matrix is not None:; n_phens = cov_matrix.shape[0]; else:; n_phens = len(h2); if rg == [None]:; print(f'Assuming rg=0 for all {n_phens} traits'); rg = [0] * int((n_phens**2 - n_phens) / 2); assert all(x >= -1 and x <= 1 for x in rg), 'rg values must be between 0 and 1'; cov, rg = get_cov_matrix(h2, rg); cov = (1 / M) * cov; # seed random state for replicability; randstate = np.random.RandomState(int(seed)); betas = randstate.multivariate_normal(; mean=np.zeros(n_phens),; cov=cov,; size=[; M,; ],; ); df = pd.DataFrame([0] * M, columns=['beta']); tb = hl.Table.from_pandas(df); tb = tb.add_index().key_by('idx'); tb = tb.annotate(beta=hl.literal(betas.tolist())[hl.int32(tb.idx)]); mt = mt.add_row_index(name='row_idx' + uid); mt = mt.annotate_rows(beta=tb[mt['row_idx' + uid]]['beta']); mt = _clean_fields(mt, uid); return mt, rg. [docs]@typecheck(; mt=Mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:10401,assert,assert,10401,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,['assert'],['assert']
Testability," :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\sigma}_{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:135489,test,test,135489,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability," :meth:`.key_rows_by`; or :meth:`.key_cols_by` to remove the field from the key before dropping. While many operations exist independently for rows, columns, entries, and; globals, only one is needed for dropping due to the lack of any necessary; contextual information. Parameters; ----------; exprs : varargs of :class:`str` or :class:`.Expression`; Names of fields to drop or field reference expressions. Returns; -------; :class:`.MatrixTable`; Matrix table without specified fields.; """""". def check_key(name, keys):; if name in keys:; raise ValueError(""MatrixTable.drop: cannot drop key field '{}'"".format(name)); return name. all_field_exprs = {e: k for k, e in self._fields.items()}; fields_to_drop = set(); for e in exprs:; if isinstance(e, Expression):; if e in all_field_exprs:; fields_to_drop.add(all_field_exprs[e]); else:; raise ExpressionException(; ""Method 'drop' expects string field names or top-level field expressions""; "" (e.g. 'foo', matrix.foo, or matrix['foo'])""; ); else:; assert isinstance(e, str); if e not in self._fields:; raise IndexError(""MatrixTable has no field '{}'"".format(e)); fields_to_drop.add(e). m = self; global_fields = [field for field in fields_to_drop if self._fields[field]._indices == self._global_indices]; if global_fields:; m = m._select_globals(""MatrixTable.drop"", m.globals.drop(*global_fields)). row_fields = [; check_key(field, list(self.row_key)); for field in fields_to_drop; if self._fields[field]._indices == self._row_indices; ]; if row_fields:; m = m._select_rows(""MatrixTable.drop"", row=m.row.drop(*row_fields)). col_fields = [; check_key(field, list(self.col_key)); for field in fields_to_drop; if self._fields[field]._indices == self._col_indices; ]; if col_fields:; m = m._select_cols(""MatrixTable.drop"", m.col.drop(*col_fields)). entry_fields = [field for field in fields_to_drop if self._fields[field]._indices == self._entry_indices]; if entry_fields:; m = m._select_entries(""MatrixTable.drop"", m.entry.drop(*entry_fields)). return m. [",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:44107,assert,assert,44107,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['assert'],['assert']
Testability," = Indices.unify(*[e._indices for e in exprs]); except ExpressionException:; # source mismatch; from collections import defaultdict. sources = defaultdict(lambda: []); for e in exprs:; from .expression_utils import get_refs. for name, inds in get_refs(e, *[e for a in e._aggregations for e in a.exprs]).items():; sources[inds.source].append(str(name)); raise ExpressionException(; ""Cannot combine expressions from different source objects.""; ""\n Found fields from {n} objects:{fields}"".format(; n=len(sources), fields=''.join(""\n {}: {}"".format(src, fds) for src, fds in sources.items()); ); ) from None; first, rest = exprs[0], exprs[1:]; aggregations = first._aggregations; for e in rest:; aggregations = aggregations.push(*e._aggregations); return new_indices, aggregations. def unify_types_limited(*ts):; type_set = set(ts); if len(type_set) == 1:; # only one distinct class; return next(iter(type_set)); elif all(is_numeric(t) for t in ts):; # assert there are at least 2 numeric types; assert len(type_set) > 1; if tfloat64 in type_set:; return tfloat64; elif tfloat32 in type_set:; return tfloat32; elif tint64 in type_set:; return tint64; else:; assert type_set == {tint32, tbool}; return tint32; else:; return None. def unify_types(*ts):; limited_unify = unify_types_limited(*ts); if limited_unify is not None:; return limited_unify; elif all(isinstance(t, tarray) for t in ts):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:13852,assert,assert,13852,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,3,['assert'],['assert']
Testability," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:5381,assert,assertion,5381,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,1,['assert'],['assertion']
Testability," = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; fy = y[ui] + e; new_y[ui] = fy; keep[ui] = True; j = ui + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; if j >= len(x):; break; if compare(udx, udy, dx, judy) < 0:; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; if compare(ldx, ldy, dx, jldy) > 0:; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; j += 1; return new_y, keep. [docs]def smoothed_pdf(; data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices is None:; raise ValueError('Invalid input'); agg_f = data._aggregation_method(); data = agg_f(aggregators.approx_cdf(data, k)). if legend is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'. if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:7512,log,log,7512,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['log'],['log']
Testability," >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_rshift(x, y, logical=False)[source]; Bitwise right-shift x by y.; Examples; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With logical=False (default), the sign is preserved:; >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With logical=True, the sign bit is treated as any other:; >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; If logical is False, then the shift is a sign-preserving right shift.; If logical is True, then the shift is logical, with the sign bit; treated as any other bit.; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.3025850",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:6474,log,logical,6474,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['log'],['logical']
Testability," API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Localization; A job is executed in three separate Docker containers: input, main, output. The input container; downloads files from Google Storage to the input container. These input files are either inputs; to the batch or are output files that have been generated by a dependent job. The downloaded; fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:1327,log,logs,1327,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['log'],['logs']
Testability," Addition and subtraction “union” realized blocks.; Element-wise multiplication “intersects” realized blocks.; Transpose “transposes” realized blocks.; abs() and sqrt() preserve the realized blocks.; sum() along an axis realizes those blocks for which at least one; block summand is realized.; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7570,log,logarithm,7570,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,2,['log'],"['log', 'logarithm']"
Testability," Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approxim",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5248,test,test,5248,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['test'],['test']
Testability," Boolean – Returns true if item is missing. Otherwise, false. isnan(a: Double): Boolean – Returns true if the argument is NaN (not a number), false if the argument is defined but not NaN. Returns missing if the argument is missing. json(x: T): String – Returns the JSON representation of a data type. Locus(contig: String, pos: Int): Locus. Construct a Locus object.; let l = Locus(""1"", 10040532) in l.position; result: 10040532. Arguments. contig (String) – String representation of contig.; pos (Int) – SNP position or start of an indel. Locus(s: String): Locus. Construct a Locus object.; let l = Locus(""1:10040532"") in l.position; result: 10040532. Arguments. s (String) – String of the form CHR:POS. log(x: Double, b: Double): Double. Returns the base b logarithm of the given value x.; Arguments. x (Double) – the number to take the base b logarithm of.; b (Double) – the base. log(x: Double): Double. Returns the natural logarithm of the given value x.; Arguments. x (Double) – the number to take the natural logarithm of. log10(x: Double): Double. Returns the base 10 logarithm of the given value x.; Arguments. x (Double) – the number to take the base 10 logarithm of. merge(s1: Struct, s2: Struct): Struct. Create a new Struct with all fields in s1 and s2.; let s1 = {gene: ""ACBD"", function: ""LOF""} and s2 = {a: 20, b: ""hello""} in merge(s1, s2); result: {gene: ""ACBD"", function: ""LOF"", a: 20, b: ""hello""}. orElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:12212,log,logarithm,12212,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['log'],['logarithm']
Testability," Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105000,log,logistic,105000,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability," Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; AC / AN.; AC (array<int32>) – Calculated allele count, one element per; allele, including the reference. Sums to AN.; AN (int32) – Total number of called alleles.; homozygote_count (array<int32>) – Number of homozygotes per; allele. One element per allele, including the reference.; call_rate (float64) – Fraction of calls neither missing nor filtered.; Equivalent to n_called / count_cols().; n_called (int64) – Number of samples with a defined GT.; n_not_called (int64) – Number of samples with a missing GT.; n_filtered (int64) – Number of filtered entries.; n_het (int64) – Number of heterozygous samples.; n_non_ref (int64) – Number of samples with at least one called; non-reference allele.; het_freq_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:100428,test,test,100428,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['test'],['test']
Testability," Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum element of a collection or of given numeric expressions. nanmin(*exprs[, filter_missing]); Retur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:1083,log,logical,1083,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['log'],['logical']
Testability," Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) changes by less than \(10^{-6}\). For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:112588,test,testing,112588,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['testing']
Testability," ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i in range(len(new_fields)); ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:9518,assert,assert,9518,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability," Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:. 66. [4]:. users.aggregate(hl.agg.counter(users.occupation == 'programmer'))[True]. [4]:. 6",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1213,log,logger,1213,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['log'],['logger']
Testability," Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1343,assert,assert,1343,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,2,['assert'],['assert']
Testability," None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; return False. def is_continuous(self):; return False. class PositionScaleContinuous(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None, transformation=""identity""):; super().__init__(axis, name, breaks, labels); self.transformation = transformation. def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far); if self.transformation == ""identity"":; pass; elif self.transformation == ""log10"":; self.update_axis(fig_so_far)(type=""log""); elif self.transformation == ""reverse"":; self.update_axis(fig_so_far)(autorange=""reversed""); else:; raise ValueError(f""Unrecognized transformation {self.transformation}""). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. class PositionScaleDiscrete(PositionScale):; def __init__(self, axis=None, name=None, breaks=None, labels=None):; super().__init__(axis, name, breaks, labels). def apply_to_fig(self, parent, fig_so_far):; super().apply_to_fig(parent, fig_so_far). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return True. def is_continuous(self):; return False. class ScaleContinuous(Scale):; def __init__(self, aesthetic_name):; super().__init__(aesthetic_name). def transform_data(self, field_expr):; return field_expr. def is_discrete(self):; return False. def is_continuous(self):; return True. def valid_dtype(self, dtype):; return is_continuous_type(dtype",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:3024,log,log,3024,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['log'],['log']
Testability," Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:9003,log,log,9003,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['log'],['log']
Testability," Python API; Hail Query Python API; Functions; Core language functions; SwitchBuilder. View page source. SwitchBuilder. class hail.expr.builders.SwitchBuilder[source]; Class for generating conditional trees based on value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; Swi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:1609,test,test,1609,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['test'],['test']
Testability," R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:29083,test,test,29083,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability," Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence, explosion, or; reaching the max (by default,; 25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; fit.converged; bool; True if iteration converged. Wald, LRT, Firth; fit.exploded; bool; True if iteration exploded. We consider iteration to have converged when every coordinate of; \(\beta\) changes by less than \(10^{-6}\) by default. For Wald and; LRT, up to 25 iterations are attempted by default; in testing we find 4 or 5; iterations nearly always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:10732,test,testing,10732,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['test'],['testing']
Testability," Suppose the variant dataset saved at *data/example_lmmreg.vds* has a Boolean variant annotation ``va.useInKinship`` and numeric or Boolean sample annotations ``sa.pheno``, ``sa.cov1``, ``sa.cov2``. Then the :py:meth:`.lmmreg` function in. >>> assoc_vds = hc.read(""data/example_lmmreg.vds""); >>> kinship_matrix = assoc_vds.filter_variants_expr('va.useInKinship').rrm(); >>> lmm_vds = assoc_vds.lmmreg(kinship_matrix, 'sa.pheno', ['sa.cov1', 'sa.cov2']). will execute the following four steps in order:. 1) filter to samples in given kinship matrix to those for which ``sa.pheno``, ``sa.cov``, and ``sa.cov2`` are all defined; 2) compute the eigendecomposition :math:`K = USU^T` of the kinship matrix; 3) fit covariate coefficients and variance parameters in the sample-covariates-only (global) model using restricted maximum likelihood (`REML <https://en.wikipedia.org/wiki/Restricted_maximum_likelihood>`__), storing results in global annotations under ``global.lmmreg``; 4) test each variant for association, storing results under ``va.lmmreg`` in variant annotations. This plan can be modified as follows:. - Set ``run_assoc=False`` to not test any variants for association, i.e. skip Step 5.; - Set ``use_ml=True`` to use maximum likelihood instead of REML in Steps 4 and 5.; - Set the ``delta`` argument to manually set the value of :math:`\delta` rather that fitting :math:`\delta` in Step 4.; - Set the ``global_root`` argument to change the global annotation root in Step 4.; - Set the ``va_root`` argument to change the variant annotation root in Step 5. :py:meth:`.lmmreg` adds 9 or 13 global annotations in Step 4, depending on whether :math:`\delta` is set or fit. +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | Annotation | Type | Value |; +==============================================+=====================",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:116155,test,test,116155,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability," This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:16447,test,test,16447,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability," [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26766,log,logistic,26766,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'test']"
Testability," \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if iteration converged. Wald, LRT, Firth; va.logreg.fit.exploded; Boolean; true if iteration exploded. We consider iteration to have converged when every coordinate of \(\beta\) chang",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:111499,log,logreg,111499,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['log'],['logreg']
Testability," \\varepsilon), \quad \\varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid; function <https://en.wikipedia.org/wiki/Sigmoid_function>`__, the; genotype :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; :math:`\mathrm{isFemale}` is coded as 1 for true (female) and; 0 for false (male). The null model sets :math:`\\beta_1 = 0`. The resulting variant annotations depend on the test statistic; as shown in the tables below. ========== =================== ====== =====; Test Annotation Type Value; ========== =================== ====== =====; Wald ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; Wald ``va.logreg.se`` Double estimated standard error, :math:`\widehat{\mathrm{se}}`; Wald ``va.logreg.zstat`` Double Wald :math:`z`-statistic, equal to :math:`\hat\\beta_1 / \widehat{\mathrm{se}}`; Wald ``va.logreg.pval`` Double Wald p-value testing :math:`\\beta_1 = 0`; LRT, Firth ``va.logreg.beta`` Double fit genotype coefficient, :math:`\hat\\beta_1`; LRT, Firth ``va.logreg.chi2`` Double deviance statistic; LRT, Firth ``va.logreg.pval`` Double LRT / Firth p-value testing :math:`\\beta_1 = 0`; Score ``va.logreg.chi2`` Double score statistic; Score ``va.logreg.pval`` Double score p-value testing :math:`\\beta_1 = 0`; ========== =================== ====== =====. For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. ================ =========================== ======= =====; Test Annotation Type Value; ================ =========================== ======= =====; Wald, LRT, Firth ``va.logreg.fit.nIter`` Int number of iterations until co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:141978,log,logreg,141978,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['logreg']
Testability," ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.MakeArray([e._ir for e in exprs], None); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tset):; elements = []; found_expr = False; for element in e:; value = _to_expr(element, dtype.element_type); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; assert len(elements) > 0; exprs = [; element if isinstance(element, Expression) else hl.literal(element, dtype.element_type); for element in elements; ]; indices, aggregations = unify_all(*exprs); x = ir.ToSet(ir.toStream(ir.MakeArray([e._ir for e in exprs], None))); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, ttuple):; elements = []; found_expr = False; assert len(e) == len(dtype.types); for i in range(len(e)):; value = _to_expr(e[i], dtype.types[i]); found_expr = found_expr or isinstance(value, Expression); elements.append(value); if not found_expr:; return e; else:; exprs = [; elements[i] if isinstance(elements[i], Expression) else hl.literal(elements[i], dtype.types[i]); for i in range(len(elements)); ]; indices, aggregations = unify_all(*exprs); x = ir.MakeTuple([expr._ir for expr in exprs]); return expressions.construct_expr(x, dtype, indices, aggregations); elif isinstance(dtype, tdict):; keys = []; values = []; found_expr = False; for k, v in e.item",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:11060,assert,assert,11060,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability," __ __ <>__\n'; ' / /_/ /__ __/ /\n'; ' / __ / _ `/ / /\n'; ' /_/ /_/\\_,_/_/_/ version {}\n'.format(py_version); ). if py_version.startswith('devel'):; sys.stderr.write(; 'NOTE: This is a beta version. Interfaces may change\n'; ' during the beta period. We recommend pulling\n'; ' the latest changes weekly.\n'; ); sys.stderr.write(f'LOGGING: writing to {log}\n'). self._user_specified_rng_nonce = True; if global_seed is None:; if 'rng_nonce' not in backend.get_flags('rng_nonce'):; backend.set_flags(rng_nonce=hex(Random().randrange(-(2**63), 2**63 - 1))); self._user_specified_rng_nonce = False; else:; backend.set_flags(rng_nonce=hex(global_seed)); Env._hc = self. def initialize_references(self, default_reference):; assert self._backend; self._backend.initialize_references(); if default_reference in BUILTIN_REFERENCES:; self._default_ref = self._backend.get_reference(default_reference); else:; self._default_ref = ReferenceGenome.read(default_reference). @property; def default_reference(self) -> ReferenceGenome:; assert self._default_ref is not None, '_default_ref should have been initialized in HailContext.create'; return self._default_ref. @default_reference.setter; def default_reference(self, value):; if not isinstance(value, ReferenceGenome):; raise TypeError(f'{value} is {type(value)} not a ReferenceGenome'); self._default_ref = value. def stop(self):; assert self._backend; self._backend.stop(); self._backend = None; Env._hc = None; Env._dummy_table = None; Env._seed_generator = None; hail.ir.clear_session_functions(). [docs]@typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:4377,assert,assert,4377,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['assert'],['assert']
Testability," _convert_to_encoding(self, byte_writer, value):; length = len(self); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[i + j]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8; for i, t in enumerate(self.types):; if not HailType._missing(value[i]):; t._convert_to_encoding(byte_writer, value[i]). def unify(self, t):; if not (isinstance(t, ttuple) and len(self.types) == len(t.types)):; return False; for t1, t2 in zip(self.types, t.types):; if not t1.unify(t2):; return False; return True. def subst(self):; return ttuple(*[t.subst() for t in self.types]). def clear(self):; for t in self.types:; t.clear(). def _get_context(self):; return HailTypeContext.union(*self.types). def allele_pair(j: int, k: int):; assert j >= 0 and j <= 0xFFFF; assert k >= 0 and k <= 0xFFFF; return j | (k << 16). def allele_pair_sqrt(i):; k = int(math.sqrt(8 * float(i) + 1) / 2 - 0.5); assert k * (k + 1) // 2 <= i; j = i - k * (k + 1) // 2; # TODO another assert; return allele_pair(j, k). small_allele_pair = [; allele_pair(0, 0),; allele_pair(0, 1),; allele_pair(1, 1),; allele_pair(0, 2),; allele_pair(1, 2),; allele_pair(2, 2),; allele_pair(0, 3),; allele_pair(1, 3),; allele_pair(2, 3),; allele_pair(3, 3),; allele_pair(0, 4),; allele_pair(1, 4),; allele_pair(2, 4),; allele_pair(3, 4),; allele_pair(4, 4),; allele_pair(0, 5),; allele_pair(1, 5),; allele_pair(2, 5),; allele_pair(3, 5),; allele_pair(4, 5),; allele_pair(5, 5),; allele_pair(0, 6),; allele_pair(1, 6),; allele_pair(2, 6),; allele_pair(3, 6),; allele_pair(4, 6),; allele_pair(5, 6),; allele_pair(6, 6),; allele_pair(0, 7),; allele_pair(1, 7),; allele_pair(2, 7),; allele_pair(3, 7),; allele_pair(4, 7),; allele_pair(5, 7),; allele_pair(6, 7),; allele_pair(7, 7),; ]. class _tcall(HailType):; """"""Hail type for a diploid genotype. In Python, these are represented by :class:`.Call`.; """""". def __init__(self):; super(_tcall, self).__init__(). def _typecheck_one_level(s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:42023,assert,assert,42023,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['assert'],['assert']
Testability," `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pnorm"", tfloat64, x, mu, sigma, lower_tail, log_p). [docs]@typecheck(x=expr_float64, n=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pT(x, n, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikipedia.org/wiki/Student%27s_t-distribution>`__ with; `n` degrees of freedom. Examples; --------. >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a t-distributed random variable with `n` degrees of freedom. If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:73044,log,logarithm,73044,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logarithm']
Testability," `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Also; --------; :func:`.or_else`. Parameters; ----------; args : variable-length args of :cl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57859,log,logistic,57859,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logistic']
Testability," a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:105201,log,logistic,105201,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability," a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:16895,log,login,16895,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['log'],['login']
Testability," about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression); logical (bool). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_not(x)[source]; Bitwise invert x.; Examples; >>> hl.eval(hl.bit_not(0)); -1. Notes; See the Python wiki; for more information about bit operators. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_count(x)[source]; Count the number of 1s in the in the two’s complement binary representation of x.; Examples; The binary representation of 7 is 111, so:; >>> hl.eval(hl.bit_count(7)); 3. Parameters:; x (Int32Expression or Int64Expression). Returns:; Int32Expression. hail.expr.functions.exp(x)[source]. hail.expr.functions.expit(x)[source]. hail.expr.functions.is_nan(x)[source]. hail.expr.functions.is_finite(x)[source]. hail.expr.functions.is_infinite(x)[source]. hail.expr.functions.log(x, base=None)[source]; Take the logarithm of the x with base base.; Examples; >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; If the base argument is not supplied, then the natural logarithm is used. Parameters:. x (float or Expression of type tfloat64); base (float or Expression of type tfloat64). Returns:; Expression of type tfloat64. hail.expr.functions.log10(x)[source]. hail.expr.functions.logit(x)[source]. hail.expr.functions.floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Retu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:7324,log,log,7324,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,2,['log'],"['log', 'logarithm']"
Testability," allele. :rtype: bool; """""". return self._jrep.isHetRef(). [docs] def is_not_called(self):; """"""True if the genotype call is missing. :rtype: bool; """""". return self._jrep.isNotCalled(). [docs] def is_called(self):; """"""True if the genotype call is non-missing. :rtype: bool; """""". return self._jrep.isCalled(). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return from_option(self._jrep.nNonRefAlleles()). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = G",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:5375,test,testcode,5375,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['test'],['testcode']
Testability," allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC; 3; 4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing valu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:80836,log,logistic,80836,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['log'],['logistic']
Testability," always suffice. Convergence may also fail due to; explosion, which refers to low-level numerical linear algebra exceptions; caused by manipulating ill-conditioned matrices. Explosion may result from; (nearly) linearly dependent covariates or complete separation.; A more common situation in genetics is quasi-complete seperation, e.g.; variants that are observed only in cases (or controls). Such variants; inevitably arise when testing millions of variants with very low minor; allele count. The maximum likelihood estimate of \(\beta\) under; logistic regression is then undefined but convergence may still occur; after a large number of iterations due to a very flat likelihood; surface. In testing, we find that such variants produce a secondary bump; from 10 to 15 iterations in the histogram of number of iterations per; variant. We also find that this faux convergence produces large standard; errors and large (insignificant) p-values. To not miss such variants,; consider using Firth logistic regression, linear regression, or; group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose; we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic,; and linear regression models to this data, where x is genotype,; y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; signifi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:11767,log,logistic,11767,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,"['log', 'test']","['logistic', 'tests']"
Testability," approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\sigma}_{g,v}^2)` via rotation of the model. .. math::. y \\sim \\mathrm{N}\\left(X_v\\beta_v, \sigma_{g,v}^2 (K + \\hat{\delta} I)\\right). Note that the only new rotation to compute here is :math:`U^T v`. To test the null hypothesis that the genotype coefficient :math:`\\beta^0_v` is zero, we consider the restricted model with parameters :math:`((0, \\beta^1_v, \ldots, \\beta^c_v), \sigma_{g,v}^2)` within the full model with parameters :math:`(\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller vari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:134978,test,test,134978,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability," be less than 1.; a (float or Expression of type tfloat64) – The alpha parameter in the beta distribution. The result is undefined; for non-positive a.; b (float or Expression of type tfloat64) – The beta parameter in the beta distribution. The result is undefined; for non-positive b. Returns:; Float64Expression. hail.expr.functions.dchisq(x, df, ncp=None, log_p=False)[source]; Compute the probability density at x of a chi-squared distribution with df; degrees of freedom.; Examples; >>> hl.eval(hl.dchisq(1, 2)); 0.3032653298563167. >>> hl.eval(hl.dchisq(1, 2, ncp=2)); 0.17472016746112667. >>> hl.eval(hl.dchisq(1, 2, log_p=True)); -1.1931471805599454. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dnorm(x, mu=0, sigma=1, log_p=False)[source]; Compute the probability density at x of a normal distribution with mean; mu and standard deviation sigma. Returns density of standard normal; distribution by default.; Examples; >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:9473,log,logarithm,9473,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['log'],['logarithm']
Testability," common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97362,test,testing,97362,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['testing']
Testability," default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; regions=regions,; gcs_bucket_allow_list=gcs_bucket_allow_list,; ); ); if backend == 'spark':; return init_spark(; sc=sc,; app_name=app_name,; master=master,; local=local,; min_block_size=min_block_size,; branching_factor=branching_factor,; spark_conf=spark_conf,; _optimizer_iterations=_optimizer_iterations,; log=log,; quiet=quiet,; append=append,; tmp_dir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; idempotent=idempotent,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; copy_log_on_error=copy_spark_log_on_error,; ); if backend == 'local':; return init_local(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; default_reference=default_reference,; global_seed=global_seed,; skip_logging_configuration=skip_logging_configuration,; gcs_requester_pays_configuration=gcs_requester_pays_configuration,; ); raise ValueError(f'unknown Hail Query backend: {backend}'). @typecheck(; sc=nullable(SparkContext),; app_name=nullable(str),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=enumeration(*BUILTIN_REFERENCES),; idempotent=bool,; global_seed=nu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12885,log,log,12885,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,6,['log'],['log']
Testability," defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and one or more columns per sample.; Examples; Consider a VariantDataset vds with 2 variants and 3 samples:; V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:121105,log,logreg,121105,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,3,"['log', 'test']","['logreg', 'test']"
Testability," density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, height=int(height / 3), width=width, x_range=sp.x_range); xp, x_renderers, x_max_densities = get_density_plot_items(; source_pd,; _x[0],; xp,; x_axis=True,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); xp.xaxis.visible = False; yp = figure(height=height, width=int(width / 3), y_range=sp.y_range); yp, y_renderers, y_max_densities = get_density_plot_items(; source_pd,; _y[0],; yp,; x_axis=False,; colors=sp_color_mappers,; continuous_cols=continuous_cols,; factor_cols=factor_cols,; ); yp.yaxis.visible = False; density_renderers = x_renderers + y_renderers; first_row = [xp]. if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; for factor_col, _, renderer in density_renderers:; renderer.visible = factor_col == label_cols[0]. if label_cols[0] in factor_cols:; xp.y_range.start = 0; xp.y_range.end = x_max_densities[label_cols[0]]; yp.x_range.start = 0; yp.x_range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:43470,assert,assert,43470,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['assert'],['assert']
Testability," else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': object did not match the passed type '{}'"".format(dtype)) from e. if wrapper['has_free_vars']:; raise ValueError(; ""'literal' cannot be used with hail expressions that depend ""; ""on other expressions. Use expression 'x' directly ""; ""instead of passing it to 'literal'.""; ). if wrapper['has_expr']:; return literal(hl.eval(to_expr(x, dtype)), dtype). if x is None or x is pd.NA:; return hl.missing(dtype); elif is_primitive(dtype):; if dtype == tint32:; assert is_int32(x); assert tint32.min_value <= x <= tint32.max_value; return construct_expr(ir.I32(x), tint32); elif dtype == tint64:; assert is_int64(x); assert tint64.min_value <= x <= tint64.max_value; return construct_expr(ir.I64(x), tint64); elif dtype == tfloat32:; assert is_float32(x); return construct_expr(ir.F32(x), tfloat32); elif dtype == tfloat64:; assert is_float64(x); return construct_expr(ir.F64(x), tfloat64); elif dtype == tbool:; assert isinstance(x, builtins.bool); return construct_expr(ir.TrueIR() if x else ir.FalseIR(), tbool); else:; assert dtype == tstr; assert isinstance(x, builtins.str); return construct_expr(ir.Str(x), tstr); else:; return construct_expr(ir.EncodedLiteral(dtype, x), dtype). [docs]@deprecated(version=""0.2.59"", reason=""Replaced by hl.if_else""); @typecheck(condition=expr_bool, consequent=expr_any, alternate=expr_any, missing_false=bool); def cond(condition, consequent, alternate, missing_false: bool = False):; """"""Deprecated in favor of :func:`.if_else`. Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `pr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:12046,assert,assert,12046,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['assert'],['assert']
Testability," expressions to group by. Returns; -------; :class:`.GroupedMatrixTable`; Grouped matrix, can be used to call :meth:`.GroupedMatrixTable.aggregate`.; """"""; if self._row_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by rows; cannot also group by cols.""); if self._col_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by cols.""). caller = 'group_cols_by'; col_key, computed_key = get_key_by_exprs(; caller,; exprs,; named_exprs,; self._parent._col_indices,; override_protected_indices={self._parent._global_indices, self._parent._row_indices},; ). self._check_bindings(caller, computed_key, self._parent._col_indices); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some place",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:7180,assert,assert,7180,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['assert'],['assert']
Testability," fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.arr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:57228,assert,assert,57228,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['assert'],['assert']
Testability," firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54705,log,logreg,54705,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logreg']
Testability," for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:2337,test,test,2337,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['test'],['test']
Testability," for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create a metadata plot for a Hail Table or MatrixTable. plot_roc_curve(ht, scores[, t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:1705,test,tests,1705,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,2,['test'],['tests']
Testability," for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:55552,log,logistic,55552,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability," for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:35107,log,logistic,35107,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability," for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+------+--------+---------+-------+; | id | size | q_stat | p_value | fault |; +=======+======+========+=========+=======+; | geneA | 2 | 4.136 | 0.205 | 0 |; +-------+------+--------+---------+-------+; | geneB | 1 | 5.659 | 0.195 | 0 |; +-------+------+--------+---------+-------+; | geneC | 3 | 4.122 | 0.192 | 0 |; +-------+------+--------+---------+-------+. Groups larger than `max_size` appear with missing `q_stat`, `p_value`, and; `fault`. The hard limit on the number of rows in a group is 46340. Note that the variance component score `q_stat` agrees with ``Q`` in the R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103848,log,logistic,103848,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability," from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34557,log,logistic,34557,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'tests']"
Testability," from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations which summarize the iterative fitting process:. Test; Annotation; Type; Value. Wald, LRT, Firth; va.logreg.fit.nIter; Int; number of iterations until convergence, explosion, or reaching the max (25 for Wald, LRT; 100 for Firth). Wald, LRT, Firth; va.logreg.fit.converged; Boolean; true if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:111324,test,testing,111324,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['testing']
Testability," ht.Q,; w=w,; k=hl.nd.ones(hl.len(w), dtype=hl.tint32),; lam=hl.nd.zeros(hl.len(w)),; mu=0,; sigma=0,; min_accuracy=accuracy,; max_iterations=iterations,; ); ht = ht.select(; 'size',; # for reasons unknown, the R implementation calls this expression the Q statistic (which is; # *not* what they write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and resi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87086,log,logistic,87086,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'test']"
Testability," i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f. Returns:; ArrayExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:14006,log,logging,14006,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,2,['log'],['logging']
Testability," ignored (treated as one).; If expr is an expression of type tint32, tint64 or; tbool, then the result is an expression of type; tint64. If expr is an expression of type tfloat32; or tfloat64, then the result is an expression of type; tfloat64. Warning; Boolean values are cast to integers before computing the product. Parameters:; expr (NumericExpression) – Numeric expression. Returns:; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:15127,test,test,15127,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability," in a complete trio. :rtype: bool; """""". if not hasattr(self, '_complete'):; self._complete = self._jrep.isComplete(); return self._complete. [docs]class Pedigree(object):; """"""Class containing a list of trios, with extra functionality. :param trios: list of trio objects to include in pedigree; :type trios: list of :class:`.Trio`; """""". @handle_py4j; def __init__(self, trios):. self._jrep = Env.hail().methods.Pedigree(jindexed_seq([t._jrep for t in trios])); self._trios = trios. @classmethod; def _from_java(cls, jrep):; ped = Pedigree.__new__(cls); ped._jrep = jrep; ped._trios = None; return ped. def __eq__(self, other):; if not isinstance(other, Pedigree):; return False; else:; return self._jrep == other._jrep. @handle_py4j; def __hash__(self):; return self._jrep.hashCode(). [docs] @staticmethod; @handle_py4j; @typecheck(fam_path=strlike,; delimiter=strlike); def read(fam_path, delimiter='\\s+'):; """"""Read a .fam file and return a pedigree object. **Examples**. >>> ped = Pedigree.read('data/test.fam'). **Notes**. This method reads a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. Hail expects a file in the same spec as PLINK outlines. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """""". jrep = Env.hail().methods.Pedigree.read(fam_path, Env.hc()._jhc.hadoopConf(), delimiter); return Pedigree._from_java(jrep). @property; @handle_py4j; def trios(self):; """"""List of trio objects in this pedigree. :rtype: list of :class:`.Trio`; """""". if not self._trios:; self._trios = [Trio._from_java(t) for t in jiterable_to_list(self._jrep.trios())]; return self._trios. [docs] def complete_trios(self):; """"""List of trio objects that have a defined father, mother, and sex. :rtype: list of :class:`.Trio`; """"""; return filter(lambda t: t.is_complete(), self.trios). [docs] @handle_py4j; @typecheck_method(samples=listof(strlike)); def filter_to(self, samples):; """"""Filter the pedigree to a given list of sample IDs. **",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html:4951,test,test,4951,docs/0.1/_modules/hail/representation/pedigree.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/pedigree.html,1,['test'],['test']
Testability," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7965,log,log,7965,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,6,"['log', 'test']","['log', 'logistf', 'test', 'tests']"
Testability," interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:2437,log,log,2437,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['log'],['log']
Testability," is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Retu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57441,log,logit,57441,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logit']
Testability," isinstance(x, np.generic):; x = x.item(); elif isinstance(x, np.ndarray):; pass; else:; try:; dtype._traverse(x, typecheck_expr); except TypeError as e:; raise TypeError(""'literal': object did not match the passed type '{}'"".format(dtype)) from e. if wrapper['has_free_vars']:; raise ValueError(; ""'literal' cannot be used with hail expressions that depend ""; ""on other expressions. Use expression 'x' directly ""; ""instead of passing it to 'literal'.""; ). if wrapper['has_expr']:; return literal(hl.eval(to_expr(x, dtype)), dtype). if x is None or x is pd.NA:; return hl.missing(dtype); elif is_primitive(dtype):; if dtype == tint32:; assert is_int32(x); assert tint32.min_value <= x <= tint32.max_value; return construct_expr(ir.I32(x), tint32); elif dtype == tint64:; assert is_int64(x); assert tint64.min_value <= x <= tint64.max_value; return construct_expr(ir.I64(x), tint64); elif dtype == tfloat32:; assert is_float32(x); return construct_expr(ir.F32(x), tfloat32); elif dtype == tfloat64:; assert is_float64(x); return construct_expr(ir.F64(x), tfloat64); elif dtype == tbool:; assert isinstance(x, builtins.bool); return construct_expr(ir.TrueIR() if x else ir.FalseIR(), tbool); else:; assert dtype == tstr; assert isinstance(x, builtins.str); return construct_expr(ir.Str(x), tstr); else:; return construct_expr(ir.EncodedLiteral(dtype, x), dtype). [docs]@deprecated(version=""0.2.59"", reason=""Replaced by hl.if_else""); @typecheck(condition=expr_bool, consequent=expr_any, alternate=expr_any, missing_false=bool); def cond(condition, consequent, alternate, missing_false: bool = False):; """"""Deprecated in favor of :func:`.if_else`. Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:11958,assert,assert,11958,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['assert'],['assert']
Testability," key. The phenotype type must either be numeric; (with all present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:120677,log,logistic,120677,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['log', 'test']","['logistic', 'test']"
Testability," last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-API-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:9377,log,log,9377,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,['log'],"['log', 'logs']"
Testability," list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:3556,test,test,3556,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['test'],['test']
Testability," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5417,log,logging,5417,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['log'],['logging']
Testability," low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.age else (2 * sa.cov.age + 10). For Boolean covariate types, true is coded as 1 and false as 0. In particular, for the sample annotation sa.fam.isCase added by importing a FAM file with case-control phenotype, case is 1 and control is 0.; Hail’s logistic regression tests correspond to the b.wald, b.lrt, and b.score tests in EPACTS. For each variant, Hail imputes missing genotypes as the mean of called genotypes, whereas EPACTS subsets to those samples with called genotypes. Hence, Hail and EPACTS results will currently only agree for variants with no missing genotypes. Parameters:; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression. Must evaluate to Boolean or; numeric with all values 0 or 1.; covariates (list of str) – list of covariate expressions; root (str) – Variant annotation path to store result of logistic regression.; use_dosages (bool) – If true, use genotype dosage rather than hard call. Returns:Variant dataset with logistic regression variant annotations. Return type:VariantDataset. logreg_burden(key_name, variant_keys, single_key, agg_expr, test, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:116331,log,logistic,116331,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['log', 'test']","['logistic', 'tests']"
Testability," mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:6414,test,test,6414,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['test'],['test']
Testability," method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8262,log,login,8262,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['log'],['login']
Testability," n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----; The aggregation scope includes all column fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.Expression`; Aggregation expressions. Returns; -------; :class:`.GroupedMatrixTable`; """"""; if self._row_keys is not None:; raise NotImplementedError(""GroupedMatrixTable is already grouped by rows. Cannot aggregate over cols.""); assert self._col_keys is not None. base = self._col_fields if self._col_fields is not None else hl.struct(); for k, e in named_exprs.items():; analyze('GroupedMatrixTable.aggregate_cols', e, self._parent._global_indices, {self._parent._col_axis}). self._check_bindings('aggregate_cols', named_exprs, self._parent._col_indices); return self._copy(col_fields=base.annotate(**named_exprs)). [docs] @typecheck_method(named_exprs=expr_any); def aggregate_rows(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate rows by group. Examples; --------; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a set as a new row field:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .result()). Notes; -----; The aggregation scope includes all row fields and global fields. See Also; --------; :meth:`.result`. Parameters; ----------; named_exprs : varargs of :class:`.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:9521,assert,assert,9521,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['assert'],['assert']
Testability," n_singleton (int64) – Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; n_transition (int64) – Number of transition (A-G, C-T) alternate alleles.; n_transversion (int64) – Number of transversion alternate alleles.; n_star (int64) – Number of star (upstream deletion) alleles.; r_ti_tv (float64) – Transition/Transversion ratio.; r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:68101,test,tests,68101,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['test'],['tests']
Testability," native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as inte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92578,log,log,92578,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['log'],['log']
Testability," ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dnorm(x, mu=0, sigma=1, log_p=False)[source]; Compute the probability density at x of a normal distribution with mean; mu and standard deviation sigma. Returns density of standard normal; distribution by default.; Examples; >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters:. x (float or Expression of type tfloat64) – Real number at which to compute the probability density.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The probability density. hail.expr.functions.dpois(x, lamb, log_p=False)[source]; Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:10319,log,logarithm,10319,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['log'],['logarithm']
Testability," number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-tw",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12817,test,test,12817,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['test'],['test']
Testability," objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:2159,log,log,2159,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,2,['log'],['log']
Testability," of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; dr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12020,log,log,12020,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['log'],['log']
Testability," of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ploidy == 1. :rtype: bool; """"""; return self.ploidy == 1. [docs] def is_diploid(self):; """"""True if the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """"""; if self.ploidy < 2:; return False; return self._alleles[0] != self._alleles[1]. [docs] def is_hom_var(self):; """"""True if the call contains identical alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n == 0:; return False. a0 = self._alleles[0]; if a0 == 0:; return False. if n == 1:; return True. assert n == 2; return self._alleles[1] == a0. [docs] def is_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """"""; return any(a > 0 for a in self._alleles). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:3663,assert,assert,3663,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['assert'],['assert']
Testability," of str) – List of covariate sample annotations.; global_root (str) – Global annotation root, a period-delimited path starting with global.; va_root (str) – Variant annotation root, a period-delimited path starting with va.; run_assoc (bool) – If true, run association testing in addition to fitting the global model.; use_ml (bool) – Use ML instead of REML throughout.; delta (float or None) – Fixed delta value to use in the global model, overrides fitting delta.; sparsity_threshold (float) – Genotype vector sparsity at or below which to use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:108868,log,logreg,108868,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['log', 'test']","['logreg', 'test']"
Testability," of variants with very low minor allele count. The maximum likelihood estimate of \(\beta\) under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests.; Here’s a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. Status; HomRef; Het; HomVar. Case; 1000; 10; 0. Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:113927,log,logistic,113927,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,6,['log'],"['logfit', 'logistf', 'logistic']"
Testability," operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:58678,log,logic,58678,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['log'],['logic']
Testability," page source. Experimental; This module serves two functions: as a staging area for extensions of Hail; not ready for inclusion in the main package, and as a library of lightly reviewed; community submissions.; At present, the experimental module is organized into a few freestanding; modules, linked immediately below, and many freestanding functions, documented; on this page. Warning; The functionality in this module may change or disappear entirely between different versions of; Hail. If you critically depend on functionality in this module, please create an issue to request; promotion of that functionality to non-experimental. Otherwise, that functionality may disappear!. ldscsim. Contribution Guidelines; Submissions from the community are welcome! The criteria for inclusion in the; experimental module are loose and subject to change:. Function docstrings are required. Hail uses; NumPy style docstrings.; Tests are not required, but are encouraged. If you do include tests, they must; run in no more than a few seconds. Place tests as a class method on Tests in; python/tests/experimental/test_experimental.py; Code style is not strictly enforced, aside from egregious violations. We do; recommend using autopep8 though!. Annotation Database; Classes. hail.experimental.DB; An annotation database instance. Genetics Methods. load_dataset(name, version, reference_genome); Load a genetic dataset from Hail's repository. ld_score(entry_expr, locus_expr, radius[, ...]); Calculate LD scores. ld_score_regression(weight_expr, ...[, ...]); Estimate SNP-heritability and level of confounding biases from genome-wide association study (GWAS) summary statistics. write_expression(expr, path[, overwrite]); Write an Expression. read_expression(path[, _assert_type]); Read an Expression written with experimental.write_expression(). filtering_allele_frequency(ac, an, ci); Computes a filtering allele frequency (described below) for ac and an with confidence ci. hail_metadata(t_path); Create",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:1646,test,tests,1646,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['test'],['tests']
Testability," parameter.; size : tint64, the number of variants in this group.; q_stat : tfloat64, the \(Q\) statistic, see Notes for why this differs from the paper.; p_value : tfloat64, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes.; fault : tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:77684,log,log-likelihood,77684,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['log'],['log-likelihood']
Testability," pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; },; linreg: Struct{; beta: Double,; se: Double,; tstat: Double,; pval: Double; }; }. Looking at the bottom of the above printout, you can see the linear; regression adds new variant annotations for the beta, standard error,; t-statistic, and p-value. In [46]:. def qqplot(pvals, xMax, yMax):; spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals)); exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]; obs = [-log(p, 10) for p in spvals]; plt.clf(); plt.scatter(exp, obs); plt.plot(np.arange(0, max(xMax, yMax)), c=""red""); plt.xlabel(""Expected p-value (-log10 scale)""); plt.ylabel(""Observed p-value (-log10 scale)""); plt.xlim(0, xMax); plt.ylim(0, yMax); plt.show(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:23078,log,log,23078,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['log'],['log']
Testability," r_het_hom_var (float64) – Het/HomVar call ratio.; r_insertion_deletion (float64) – Insertion/Deletion allele ratio. Missing values NA may result from division by zero. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable – Dataset with a new column-indexed field name. hail.methods._logistic_skat(group, weight, y, x, covariates, max_size=46340, null_max_iterations=25, null_tolerance=1e-06, accuracy=1e-06, iterations=10000)[source]; The logistic sequence kernel association test (SKAT).; Logistic SKAT tests if the phenotype, y, is significantly associated with the genotype,; x. For \(N\) samples, in a group of \(M\) variants, with \(K\) covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. Each sample’s phenotype is Bernoulli distributed with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a sy",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:68495,test,tests,68495,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['test'],['tests']
Testability," raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i in range(len(new_fields)); ]; fields = {name: expr for name, expr in zip(dtype.keys(), exprs)}; from .typed_expressions import StructExpression. return StructExpression._from_fields(fields). elif isinstance(dtype, tarray):; elements = []; found_expr = False; for eleme",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:9304,assert,assert,9304,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability," rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsorize_fraction < 1; ), 'truncate_reference_blocks: ""ref_block_winsorize_fraction"" must be between 0 and 1 (e.g. 0.01 to truncate the top 1% of reference blocks)'; if ref_block_winsorize_fraction > 0.1:; warning(; f""'truncate_reference_blocks': ref_block_winsorize_fraction of {ref_block_winsorize_fraction} will lead to significant data duplication,""; f"" recommended values are <0.05.""; ); max_ref_block_base_pairs = rd.aggregate_entries(; hl.agg.approx_quantiles(rd.END - rd.locus.position + 1, 1 - ref_block_winsorize_fraction, k=200); ). assert (; max_ref_block_base_pairs > 0; ), 'truncate_reference_blocks: ""max_ref_block_base_pairs"" must be between greater than zero'; info(f""splitting VDS reference blocks at {max_ref_block_base_pairs} base pairs""). rd_under_limit = rd.filter_entries(rd.END - rd.locus.position < max_ref_block_base_pairs).localize_entries(; 'fixed_blocks', 'cols'; ). rd_over_limit = rd.filter_entries(rd.END - rd.locus.position >= max_ref_block_base_pairs).key_cols_by(; col_idx=hl.scan.count(); ); rd_over_limit = rd_over_limit.select_rows().select_cols().key_rows_by().key_cols_by(); es = rd_over_limit.entries(); es = es.annotate(new_start=hl.range(es.locus.position, es.END + 1, max_ref_block_base_pairs)); es = es.explode('new_start'); es = es.transmute(; locus=hl.locus(es.locus.contig, es.new_start, reference_genome=es.locus.dtype.reference_genome),; END=hl.min(es.new_start + max_ref_block_base_pairs - 1, es.END),; ); es = es.key_by(es.locus).collect_by_key(""new_blocks""); es = es.transmute(moved_blocks_dict=hl.dict(es.new_blocks.map(lambda x: (x.col_idx, x.drop('col_idx'))))). joined = rd_under_limit.join(es,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:34754,assert,assert,34754,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,1,['assert'],['assert']
Testability," return isinstance(other, _tcall). def _parsable_string(self):; return ""Call"". def _convert_from_json(self, x, _should_freeze: bool = False) -> genetics.Call:; if x == '-':; return genetics.Call([]); if x == '|-':; return genetics.Call([], phased=True); if x[0] == '|':; return genetics.Call([int(x[1:])], phased=True). n = len(x); i = 0; while i < n:; c = x[i]; if c in '|/':; break; i += 1. if i == n:; return genetics.Call([int(x)]). return genetics.Call([int(x[0:i]), int(x[i + 1 :])], phased=(c == '|')). def _convert_to_json(self, x):; return str(x). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False) -> genetics.Call:; int_rep = byte_reader.read_int32(); int_rep = int_rep if int_rep >= 0 else int_rep + 2**32. ploidy = (int_rep >> 1) & 0x3; phased = (int_rep & 1) == 1. def allele_repr(c):; return c >> 3. def ap_j(p):; return p & 0xFFFF. def ap_k(p):; return (p >> 16) & 0xFFFF. def gt_allele_pair(i):; assert i >= 0, ""allele pair value should never be negative""; if i < len(small_allele_pair):; return small_allele_pair[i]; return allele_pair_sqrt(i). def call_allele_pair(i):; if phased:; rep = allele_repr(i); p = gt_allele_pair(rep); j = ap_j(p); k = ap_k(p); return allele_pair(j, k - j); else:; rep = allele_repr(i); return gt_allele_pair(rep). if ploidy == 0:; alleles = []; elif ploidy == 1:; alleles = [allele_repr(int_rep)]; elif ploidy == 2:; p = call_allele_pair(int_rep); alleles = [ap_j(p), ap_k(p)]; else:; raise ValueError(""Unsupported Ploidy""). return genetics.Call(alleles, phased). def _convert_to_encoding(self, byte_writer, value: genetics.Call):; int_rep = 0. int_rep |= value.ploidy << 1; if value.phased:; int_rep |= 1. def diploid_gt_index(j: int, k: int):; assert j <= k; return k * (k + 1) // 2 + j. def allele_pair_rep(c: genetics.Call):; [j, k] = c.alleles; if c.phased:; return diploid_gt_index(j, j + k); return diploid_gt_index(j, k). assert value.ploidy <= 2; if value.ploidy == 1:; int_rep |= value.alleles[0] << 3; elif value.ploi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:44262,assert,assert,44262,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['assert'],['assert']
Testability," return p. @typecheck(p=figure, font_size=str); def set_font_size(p, font_size: str = '12pt'):; """"""Set most of the font sizes in a bokeh figure. Parameters; ----------; p : :class:`bokeh.plotting.figure`; Input figure.; font_size : str; String of font size in points (e.g. '12pt'). Returns; -------; :class:`bokeh.plotting.figure`; """"""; p.legend.label_text_font_size = font_size; p.xaxis.axis_label_text_font_size = font_size; p.yaxis.axis_label_text_font_size = font_size; p.xaxis.major_label_text_font_size = font_size; p.yaxis.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:16666,log,log,16666,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['log'],['log']
Testability," right_key_types:; raise ValueError(; f""'join': key mismatch:\n ""; f"" left: [{', '.join(str(t) for t in left_key_types)}]\n ""; f"" right: [{', '.join(str(t) for t in right_key_types)}]""; ); left_fields = set(self._fields); right_fields = set(right._fields) - set(right.key). renames, _ = deduplicate(right_fields, max_attempts=100, already_used=left_fields). if renames:; renames = dict(renames); right = right.rename(renames); info(; 'Table.join: renamed the following fields on the right to avoid name conflicts:'; + ''.join(f'\n {k!r} -> {v!r}' for k, v in renames.items()); ). return Table(ir.TableJoin(self._tir, right._tir, how, _join_key)). [docs] @typecheck_method(expr=BooleanExpression); def all(self, expr):; """"""Evaluate whether a boolean expression is true for all rows. Examples; --------; Test whether `C1` is greater than 5 in all rows of the table:. >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Expression to test. Returns; -------; :obj:`bool`; """"""; return self.aggregate(hl.agg.all(expr)). [docs] @typecheck_method(expr=BooleanExpression); def any(self, expr):; """"""Evaluate whether a Boolean expression is true for at least one row. Examples; --------. Test whether `C1` is equal to 5 any row in any row of the table:. >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Boolean expression. Returns; -------; :obj:`bool`; ``True`` if the predicate evaluated for ``True`` for any row, otherwise ``False``.; """"""; return self.aggregate(hl.agg.any(expr)). [docs] @typecheck_method(mapping=dictof(str, str)); def rename(self, mapping) -> 'Table':; """"""Rename fields of the table. Examples; --------; Rename `C1` to `col1` and `C2` to `col2`:. >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters; ----------; mapping : :obj:`dict` of :class:`str`, :obj:`str`; Mapping from old field n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:103604,test,test,103604,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['test'],['test']
Testability," root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosages genotypes rather than hard call genotypes. :param int min_ac: Minimum alternate allele count. :param float min_af: Minimum alternate allele frequency. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.linreg(y, jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, min_ac, min_af); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(key_name=strlike,; variant_keys=strlike,; single_key=bool,; agg_expr=strlike,; y=strlike,; covariates=listof(strlike)); def linreg_burden(self, key_name, variant_keys, single_key, agg_expr, y, covariates=[]):; r""""""Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. .. include:: requireTGenotype.rst. **Examples**. Run a gene burden test using linear regression on the maximum genotype per gene. Here ``va.genes`` is a variant; annotation of type Set[String] giving the set of genes containing the variant (see **Extended example** below; for a deep dive):. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.cov2'])). Run a gene burden test using linear regression on the weighted sum of genotypes per gene. Here ``va.gene`` is; a variant annotation of type String giving a single gene per variant (or no gene if missing), and ``va.weight``; is a numeric variant annotation:. >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.gene',; ... single_key=True,; ... agg_expr='gs.map(g => va.weight * g.gt).sum()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:102703,test,test,102703,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32021,test,test,32021,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['test'],['test']
Testability," sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4648,log,log,4648,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['log'],['log']
Testability," self._jrep = jrep. @classmethod; def _from_java(cls, jrep):; g = Genotype.__new__(cls); g._init_from_java(jrep); g._gt = from_option(jrep.gt()); g._ad = jarray_to_list(from_option(jrep.ad())); g._dp = from_option(jrep.dp()); g._gq = from_option(jrep.gq()); g._pl = jarray_to_list(from_option(jrep.pl())); return g. @property; def gt(self):; """"""Returns the hard genotype call. :rtype: int or None; """""". return self._gt. @property; def ad(self):; """"""Returns the allelic depth. :rtype: list of int or None; """""". return self._ad. @property; def dp(self):; """"""Returns the total depth. :rtype: int or None; """""". return self._dp. @property; def gq(self):; """"""Returns the phred-scaled genotype quality. :return: int or None; """""". return self._gq. @property; def pl(self):; """"""Returns the phred-scaled genotype posterior likelihoods. :rtype: list of int or None; """""". return self._pl. [docs] def od(self):; """"""Returns the difference between the total depth and the allelic depth sum. Equivalent to:. .. testcode::. g.dp - sum(g.ad). :rtype: int or None; """""". return from_option(self._jrep.od()). @property; def gp(self):; """"""Returns the linear-scaled genotype probabilities. :rtype: list of float of None; """""". return jarray_to_list(from_option(self._jrep.gp())). [docs] def dosage(self):; """"""Returns the expected value of the genotype based on genotype probabilities,; :math:`\\mathrm{P}(\\mathrm{Het}) + 2 \\mathrm{P}(\\mathrm{HomVar})`. Genotype must be bi-allelic. :rtype: float; """""". return from_option(self._jrep.dosage()). [docs] def is_hom_ref(self):; """"""True if the genotype call is 0/0. :rtype: bool; """""". return self._jrep.isHomRef(). [docs] def is_het(self):; """"""True if the genotype call contains two different alleles. :rtype: bool; """""". return self._jrep.isHet(). [docs] def is_hom_var(self):; """"""True if the genotype call contains two identical alternate alleles. :rtype: bool; """""". return self._jrep.isHomVar(). [docs] def is_called_non_ref(self):; """"""True if the genotype call contains any ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:3041,test,testcode,3041,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['test'],['testcode']
Testability," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:2462,test,test,2462,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,1,['test'],['test']
Testability," source. Hail Query Python API; This is the API documentation for Hail Query, and provides detailed information; on the Python programming interface.; Use import hail as hl to access this functionality. Classes. hail.Table; Hail's distributed implementation of a dataframe or SQL table. hail.GroupedTable; Table grouped by row that can be aggregated into a new table. hail.MatrixTable; Hail's distributed implementation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:1665,log,log,1665,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['log'],['log']
Testability," specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95351,test,test,95351,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['test']
Testability," t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:16767,log,login,16767,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['log'],['login']
Testability," taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return f'{a0}|{a1}'; return f'{a0}/{a1}'. def __repr__(self):; return 'Call(alleles=%s, phased=%s)' % (self._alleles, self._phased). def __eq__(self, other):; return (; (self._phased == other._phased and self._alleles == other._alleles); if isinstance(other, Call); else NotImplemented; ). def __hash__(self):; return hash(self._phased) ^ hash(tuple(self._alleles)). def __getitem__(self, item):; """"""Get the i*th* allele. Returns; -------; :obj:`int`; """"""; return self._alleles[item]. @property; def alleles(self) -> Sequence[int]:; """"""Get the alleles of this call. Returns; -------; :obj:`list` of :obj:`int`; """"""; return self._alleles. @property; def ploidy(self):; """"""The number of alleles for this call. Returns; -------; :obj:`int`; """"""; return len(self._alleles). @property; def phased(self):; """"""True if the call is phased. Returns; -------; :obj:`bool`; """"""; return self._phased. [docs] def is_haploid(self):; """"""True if the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:1916,assert,assert,1916,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['assert'],['assert']
Testability," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:5784,assert,assert,5784,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['assert'],['assert']
Testability," the ploidy == 2. :rtype: bool; """"""; return self.ploidy == 2. [docs] def is_hom_ref(self):; """"""True if the call has no alternate alleles. :rtype: bool; """"""; if self.ploidy == 0:; return False. return all(a == 0 for a in self._alleles). [docs] def is_het(self):; """"""True if the call contains two different alleles. :rtype: bool; """"""; if self.ploidy < 2:; return False; return self._alleles[0] != self._alleles[1]. [docs] def is_hom_var(self):; """"""True if the call contains identical alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n == 0:; return False. a0 = self._alleles[0]; if a0 == 0:; return False. if n == 1:; return True. assert n == 2; return self._alleles[1] == a0. [docs] def is_non_ref(self):; """"""True if the call contains any non-reference alleles. :rtype: bool; """"""; return any(a > 0 for a in self._alleles). [docs] def is_het_non_ref(self):; """"""True if the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:4013,assert,assert,4013,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['assert'],['assert']
Testability," the variant dataset was imported with :py:meth:`~hail.HailContext.import_plink`, :py:meth:`~hail.HailContext.import_gen`,; or :py:meth:`~hail.HailContext.import_bgen`, or if the variant dataset was simulated with :py:meth:`~hail.HailContext.balding_nichols_model`. :rtype: bool; """""". return self._jvds.wasSplit(). [docs] @handle_py4j; def file_version(self):; """"""File version of variant dataset. :rtype: int; """""". return self._jvds.fileVersion(). [docs] @handle_py4j; @typecheck_method(key_exprs=oneof(strlike, listof(strlike)),; agg_exprs=oneof(strlike, listof(strlike))); def aggregate_by_key(self, key_exprs, agg_exprs):; """"""Aggregate by user-defined key and aggregation expressions to produce a KeyTable.; Equivalent to a group-by operation in SQL. **Examples**. Compute the number of LOF heterozygote calls per gene per sample:. >>> kt_result = (vds; ... .aggregate_by_key(['Sample = s', 'Gene = va.gene'],; ... 'nHet = g.filter(g => g.isHet() && va.consequence == ""LOF"").count()'); ... .export(""test.tsv"")). This will produce a :class:`KeyTable` with 3 columns (`Sample`, `Gene`, `nHet`). :param key_exprs: Named expression(s) for which fields are keys.; :type key_exprs: str or list of str. :param agg_exprs: Named aggregation expression(s).; :type agg_exprs: str or list of str. :rtype: :class:`.KeyTable`; """""". if isinstance(key_exprs, list):; key_exprs = "","".join(key_exprs); if isinstance(agg_exprs, list):; agg_exprs = "","".join(agg_exprs). return KeyTable(self.hc, self._jvds.aggregateByKey(key_exprs, agg_exprs)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=oneof(strlike, listof(strlike)),; propagate_gq=bool); def annotate_alleles_expr(self, expr, propagate_gq=False):; """"""Annotate alleles with expression. .. include:: requireTGenotype.rst. **Examples**. To create a variant annotation ``va.nNonRefSamples: Array[Int]`` where the ith entry of; the array is the number of samples carrying the ith alternate allele:. >>> vds_result = vds.annotate_alleles_expr('va.nNo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:6740,test,test,6740,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability," tint32, the fault flag from pgenchisq(). The global fields are:. n_complete_samples : tint32, the number of samples with neither a missing; phenotype nor a missing covariate.; y_residual : tint32, the residual phenotype from the null model. This may be; interpreted as the component of the phenotype not explained by the covariates alone.; s2 : tfloat64, the variance of the residuals, \(\sigma^2\) in the paper.; null_fit:. b : tndarray vector of coefficients.; score : tndarray vector of score statistics.; fisher : tndarray matrix of fisher statistics.; mu : tndarray the expected value under the null model.; n_iterations : tint32 the number of iterations before termination.; log_lkhd : tfloat64 the log-likelihood of the final iteration.; converged : tbool True if the null model converged.; exploded : tbool True if the null model failed to converge due to numerical; explosion. hail.methods.skat(key_expr, weight_expr, y, x, covariates, logistic=False, max_size=46340, accuracy=1e-06, iterations=10000)[source]; Test each keyed group of rows for association by linear or logistic; SKAT test.; Examples; Test each gene for association using the linear sequence kernel association; test:; >>> skat_table = hl.skat(key_expr=burden_ds.gene,; ... weight_expr=burden_ds.weight,; ... y=burden_ds.burden.pheno,; ... x=burden_ds.GT.n_alt_alleles(),; ... covariates=[1, burden_ds.burden.cov1, burden_ds.burden.cov2]). Caution; By default, the Davies algorithm iterates up to 10k times until an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:77924,log,logistic,77924,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,"['log', 'test']","['logistic', 'test']"
Testability," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:10507,test,tested,10507,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['test'],['tested']
Testability," to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34426,test,testing,34426,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['testing']
Testability," true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’) as the test parameter. Conceptually, the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:120770,test,tests,120770,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['log', 'test']","['logreg', 'tests']"
Testability," type :py:data:`.tfloat64`; Standard deviation (default = 1).; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; return _func(""dpois"", tfloat64, x, lamb, log_p). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def exp(x) -> Float64Expression:; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""exp"", tfloat64, x). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def fisher_exact_test(c1, c2, c3, c4) -> StructExpression:; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:30183,log,logarithm,30183,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logarithm']
Testability," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:109349,log,logreg,109349,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,3,"['log', 'test']","['logistic', 'logreg', 'test']"
Testability," use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{isCase}) = \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt} + \beta_2 \, \mathrm{age} + \beta_3 \, \mathrm{isFemale} + \varepsilon), \quad \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid; function, the; genotype \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for; Het, and 2 for HomVar, and the Boolean covariate; \(\mathrm{isFemale}\) is coded as 1 for true (female) and; 0 for false (male). The null model sets \(\beta_1 = 0\).; The resulting variant annotations depend on the test statistic; as shown in the tables below. Test; Annotation; Type; Value. Wald; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). Wald; va.logreg.se; Double; estimated standard error, \(\widehat{\mathrm{se}}\). Wald; va.logreg.zstat; Double; Wald \(z\)-statistic, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; va.logreg.pval; Double; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; va.logreg.beta; Double; fit genotype coefficient, \(\hat\beta_1\). LRT, Firth; va.logreg.chi2; Double; deviance statistic. LRT, Firth; va.logreg.pval; Double; LRT / Firth p-value testing \(\beta_1 = 0\). Score; va.logreg.chi2; Double; score statistic. Score; va.logreg.pval; Double; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for each variant using Newton iteration and only emits the above annotations when the maximum likelihood estimate of the coefficients converges. The Firth test uses a modified form of Newton iteration. To help diagnose convergence issues, Hail also emits three variant annotations whi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:111033,log,logreg,111033,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['log'],['logreg']
Testability," values should be ordered in; the order they appear in the upper triangle of the covariance matrix,; from left to right, top to bottom.; psd_rg : :obj:`bool`; Whether to automatically adjust rg values to get a positive semi-definite; covariance matrix, which ensures that SNP effects simulated with that; covariance matrix have the desired variance and correlation properties; specified by the h2 and rg parameters. Returns; -------; cov_matrix : :class:`numpy.ndarray`; Covariance matrix calculated using `h2` and (possibly altered) `rg` values.; rg : :obj:`list`; Genetic correlation between traits, possibly altered from input `rg` if; covariance matrix was not positive semi-definite.; """"""; assert all(x >= 0 and x <= 1 for x in h2), 'h2 values must be between 0 and 1'; assert all(x >= -1 and x <= 1 for x in rg), 'rg values must be between -1 and 1'; rg = np.asarray(rg) if isinstance(rg, list) else rg; n_rg = len(rg); n_h2 = len(h2); # expected number of rg values, given number of traits; exp_n_rg = int((n_h2**2 - n_h2) / 2); assert n_rg == exp_n_rg, f'The number of rg values given is {n_rg}, expected {exp_n_rg}'; cor = np.zeros(shape=(n_h2, n_h2)); # set upper triangle of correlation matrix to be rg; cor[np.triu_indices(n=n_h2, k=1)] = rg; cor += cor.T; cor[np.diag_indices(n=n_h2)] = 1; if psd_rg:; cor0 = cor; cor = _nearpsd(cor); idx = np.triu_indices(n=n_h2, k=1); maxlines = 50; msg = ['Adjusting rg values to make covariance matrix positive semidefinite']; msg += (; [(f'{cor0[idx[0][i],idx[1][i]]} -> {cor[idx[0][i],idx[1][i]]}') for i in range(n_rg)]; if n_rg <= maxlines; else [(f'{cor0[idx[0][i],idx[1][i]]} -> {cor[idx[0][i],idx[1][i]]}') for i in range(maxlines)]; + [f'[ printed first {maxlines} rg changes -- omitted {n_rg - maxlines} ]']; ); print('\n'.join(msg)); rg = np.ravel(cor[idx]); S = np.diag(h2) ** (1 / 2); cov_matrix = S @ cor @ S # covariance matrix decomposition. # check positive semidefinite; if not np.all(np.linalg.eigvals(cov_matrix) >= 0) and not psd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:19700,assert,assert,19700,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,1,['assert'],['assert']
Testability," will; return rows in order.; This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters:; name (str) – Name of index field. Returns:; Table – Table with a new index field. aggregate(expr, _localize=True)[source]; Aggregate over rows into a local value.; Examples; Aggregate over rows:; >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; This method supports (and expects!) aggregation over rows. Parameters:; expr (Expression) – Aggregation expression. Returns:; any – Aggregated value dependent on expr. all(expr)[source]; Evaluate whether a boolean expression is true for all rows.; Examples; Test whether C1 is greater than 5 in all rows of the table:; >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters:; expr (BooleanExpression) – Expression to test. Returns:; bool. annotate(**named_exprs)[source]; Add new fields.; New Table fields may be defined in several ways:. In terms of constant values. Every row will have the same value.; In terms of other fields in the table.; In terms of fields in other tables, this is called “joining”. Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Add field Y containing the square of field X; >>> ht = ht.annotate(Y = ht.X ** 2); >>> ht.show(); +-------+-------+-----+-------+-------+----------+; | ID | HT | SEX | X | Z | Y |; +-------+-------+-----+-------+-------+----------+; | int32 | int32 | str | int32 | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:8434,test,test,8434,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['test'],['test']
Testability," write in the paper); q_stat=ht.Q / 2 / ht.s2,; # The reasoning for taking the complement of the CDF value is:; #; # 1. Q is a measure of variance and thus positive.; #; # 2. We want to know the probability of obtaining a variance even larger (""more extreme""); #; # Ergo, we want to check the right-tail of the distribution.; p_value=1.0 - genchisq_data.value,; fault=genchisq_data.fault,; ); return ht.select_globals('y_residual', 's2', 'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87150,test,tests,87150,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['tests']
Testability," | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:16506,log,login,16506,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['log'],['login']
Testability,"' requires the 'a' and 'b' arguments to have the same type\n""; f"" a: type '{a.dtype}'\n""; f"" b: type '{b.dtype}'""; ); return coalesce(a, b). [docs]@typecheck(predicate=expr_bool, value=expr_any); def or_missing(predicate, value):; """"""Returns `value` if `predicate` is ``True``, otherwise returns missing. Examples; --------. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:60839,test,test,60839,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['test'],['test']
Testability,"', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36973,test,test,36973,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['test']
Testability,"': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; # X is samples by covs.; # y is length num samples, for one cov.; n = X.shape[0]; m = X.shape[1]. if null_fit is None:; avg = y.sum() / n; logit_avg = hl.log(avg / (1 - avg)); b = hl.nd.hstack([hl.nd.array([logit_avg]), hl.nd.zeros((hl.int32(m - 1)))]); mu = sigmoid(X @ b); score = X.T @ (y - mu); # Reshape so we do a rowwise multiply; fisher = X.T @ (X * (mu * (1 - mu)).reshape(-1, 1)); else:; # num covs used to fit null model.; m0 = null_fit.b.shape[0]; m_diff = m - m0. X0 = X[:, 0:m0]; X1 = X[:, m0:]. b = hl.nd.hstack([null_fit.b, hl.nd.zeros((m_diff,))]); mu = sigmoid(X @ b); score = hl.nd.hstack([null_fit.score, X1.T @ (y - mu)]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). dtype = numerical_regression_fit_dtype; b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:39409,assert,assert,39409,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['assert'],['assert']
Testability,"'To include ignored fields, change str_expr to match desired fields'); fields = list(in_ref_coef_dict); return {k: ref_coef_dict[k] for k in fields}. [docs]@typecheck(mt=MatrixTable, y=expr_int32, P=oneof(int, float)); def ascertainment_bias(mt, y, P):; r""""""Adds ascertainment bias to a binary phenotype to give it a sample; prevalence of `P` = cases/(cases+controls). Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing binary phenotype to be used.; y : :class:`.Expression`; Column field of binary phenotype.; P : :obj:`int` or :obj:`float`; Desired ""sample prevalence"" of phenotype. Returns; -------; :class:`.MatrixTable`; :class:`.MatrixTable` containing binary phenotype with prevalence of approx. P; """"""; assert P >= 0 and P <= 1, 'P must be in [0,1]'; uid = Env.get_uid(base=100); mt = mt.annotate_cols(y_w_asc_bias=y); y_stats = mt.aggregate_cols(hl.agg.stats(mt.y_w_asc_bias)); K = y_stats.mean; n = y_stats.n; assert abs(P - K) < 1, 'Specified sample prevalence is incompatible with population prevalence.'; if P < K:; p = (1 - K) * P / (K * (1 - P)); con = mt.filter_cols(mt.y_w_asc_bias == 0); cas = mt.filter_cols(mt.y_w_asc_bias == 1).add_col_index(name='col_idx_' + uid); keep = round(p * n * K) * [1] + round((1 - p) * n * K) * [0]; cas = cas.annotate_cols(**{'keep_' + uid: hl.literal(keep)[hl.int32(cas['col_idx_' + uid])]}); cas = cas.filter_cols(cas['keep_' + uid] == 1); cas = _clean_fields(cas, uid); mt = cas.union_cols(con); elif P > K:; p = K * (1 - P) / ((1 - K) * P); cas = mt.filter_cols(mt.y_w_asc_bias == 1); con = mt.filter_cols(mt.y_w_asc_bias == 0).add_col_index(name='col_idx_' + uid); keep = round(p * n * (1 - K)) * [1] + round((1 - p) * n * (1 - K)) * [0]; con = con.annotate_cols(**{'keep_' + uid: hl.literal(keep)[hl.int32(con['col_idx_' + uid])]}); con = con.filter_cols(con['keep_' + uid] == 1); con = _clean_fields(con, uid); mt = con.union_cols(cas); return mt. [docs]@typecheck(mt=MatrixTable, y=oneof(expr_int32, expr_flo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:32837,assert,assert,32837,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,1,['assert'],['assert']
Testability,"']). If we now export this VDS as VCF, it would produce the following header (for these new fields):. .. code-block:: text. ##INFO=<ID=AC_HC,Number=.,Type=String,Description="""". This header doesn't contain all information that should be present in an optimal VCF header:; 1) There is no FILTER entry for `HardFilter`; 2) Since `AC_HC` has one entry per non-reference allele, its `Number` should be `A`; 3) `AC_HC` should have a Description. We can fix this by setting the attributes of these fields:. >>> annotated_vds = (annotated_vds; ... .set_va_attributes(; ... 'va.info.AC_HC',; ... {'Description': 'Allele count for high quality genotypes (DP >= 10, GQ >= 20)',; ... 'Number': 'A'}); ... .set_va_attributes(; ... 'va.filters',; ... {'HardFilter': 'This site fails GATK suggested hard filters.'})). Exporting the VDS with the attributes now prints the following header lines:. .. code-block:: text. ##INFO=<ID=test,Number=A,Type=String,Description=""Allele count for high quality genotypes (DP >= 10, GQ >= 20)""; ##FILTER=<ID=HardFilter,Description=""This site fails GATK suggested hard filters."">. :param str ann_path: Path to variant annotation beginning with `va`. :param dict attributes: A str-str dict containing the attributes to set. :return: Annotated dataset with the attribute added to the variant annotation.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.setVaAttributes(ann_path, Env.jutils().javaMapToMap(attributes))). [docs] @handle_py4j; @typecheck_method(ann_path=strlike,; attribute=strlike); def delete_va_attribute(self, ann_path, attribute):; """"""Removes an attribute from a variant annotation field.; Attributes are key/value pairs that can be attached to a variant annotation field. The following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:206098,test,test,206098,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['test']
Testability,"'cannot impute {k}th element') from exc; return; if isinstance(t, (tarray, tset)):; try:; raise_for_holes(t.element_type); except ExpressionException as exc:; raise ExpressionException('cannot impute array elements') from exc; return; if isinstance(t, tdict):; try:; raise_for_holes(t.key_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict keys') from exc; try:; raise_for_holes(t.value_type); except ExpressionException as exc:; raise ExpressionException('cannot impute dict values') from exc; return. def to_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if isinstance(e, Expression):; if dtype and not dtype == e.dtype:; raise TypeError(""expected expression of type '{}', found expression of type '{}'"".format(dtype, e.dtype)); return e; return cast_expr(e, dtype, partial_type). def cast_expr(e, dtype=None, partial_type=None) -> 'Expression':; assert dtype is None or partial_type is None; if not dtype:; dtype = impute_type(e, partial_type); x = _to_expr(e, dtype); if isinstance(x, Expression):; return x; else:; return hl.literal(x, dtype). def _to_expr(e, dtype):; if e is None:; return None; elif isinstance(e, Expression):; if e.dtype != dtype:; assert is_numeric(dtype), 'expected {}, got {}'.format(dtype, e.dtype); if dtype == tfloat64:; return hl.float64(e); elif dtype == tfloat32:; return hl.float32(e); elif dtype == tint64:; return hl.int64(e); else:; assert dtype == tint32; return hl.int32(e); return e; elif not is_compound(dtype):; # these are not container types and cannot contain expressions if we got here; return e; elif isinstance(dtype, tstruct):; new_fields = []; found_expr = False; for f, t in dtype.items():; value = _to_expr(e[f], t); found_expr = found_expr or isinstance(value, Expression); new_fields.append(value). if not found_expr:; return e; else:; exprs = [; new_fields[i] if isinstance(new_fields[i], Expression) else hl.literal(new_fields[i], dtype[i]); for i i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:8996,assert,assert,8996,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability,"'n_complete_samples'). [docs]@typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; null_max_iterations=int,; null_tolerance=float,; accuracy=numeric,; iterations=int,; ); def _logistic_skat(; group,; weight,; y,; x,; covariates,; max_size: int = 46340,; null_max_iterations: int = 25,; null_tolerance: float = 1e-6,; accuracy: float = 1e-6,; iterations: int = 10000,; ):; r""""""The logistic sequence kernel association test (SKAT). Logistic SKAT tests if the phenotype, `y`, is significantly associated with the genotype,; `x`. For :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. 1. Each sample's phenotype is Bernoulli distributed with mean :math:`p_i` and variance; :math:`\sigma^2_i = p_i(1 - p_i)`, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:87571,test,tests,87571,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['tests']
Testability,"():; raise NotImplementedError('cannot convert aggregated expression to table'). if source is None:; return fallback_name, hl.Table.parallelize([hl.struct(**{fallback_name: self})], n_partitions=1). name = source._fields_inverse.get(self); top_level = name is not None; if not top_level:; name = fallback_name; named_self = {name: self}; if len(axes) == 0:; x = source.select_globals(**named_self); ds = hl.Table.parallelize([x.index_globals()], n_partitions=1); elif isinstance(source, hail.Table):; if top_level and name in source.key:; named_self = {}; ds = source.select(**named_self).select_globals(); elif isinstance(source, hail.MatrixTable):; if self._indices == source._row_indices:; if top_level and name in source.row_key:; named_self = {}; ds = source.select_rows(**named_self).select_globals().rows(); elif self._indices == source._col_indices:; if top_level and name in source.col_key:; named_self = {}; ds = source.select_cols(**named_self).select_globals().key_cols_by().cols(); else:; assert self._indices == source._entry_indices; ds = source.select_entries(**named_self).select_globals().select_cols().select_rows(); return name, ds. [docs] @typecheck_method(; n=nullable(int),; width=nullable(int),; truncate=nullable(int),; types=bool,; handler=nullable(anyfunc),; n_rows=nullable(int),; n_cols=nullable(int),; ); def show(self, n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None):; """"""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; -----; The output can be passed piped to a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:25718,assert,assert,25718,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability,"(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; Phenotype and covariate sample annotations may also be specified using programmatic expressions without identifiers, such as:; if (sa.isFemale) sa.cov.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:115061,test,testing,115061,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['testing']
Testability,"(0-indexed) and the column keys 0, 1, … N.; force_bgz (bool) – If True, load .gz files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep (str) – This parameter is a deprecated name for delimiter, please use that; instead.; delimiter (str) – A single character string which separates values in the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns:; MatrixTable – MatrixTable constructed from imported data. hail.methods.import_plink(bed, bim, fam, min_partitions=None, delimiter='\\\\s+', missing='NA', quant_pheno=False, a2_reference=True, reference_genome='default', contig_recoding=None, skip_invalid_loci=False, n_partitions=None, block_size=None)[source]; Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable.; Examples; >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the --make-bed option.; Hail uses the individual ID (column 2 in FAM file) as the sample id (s).; The individual IDs must be unique.; The resulting MatrixTable has the following fields:. Row fields:. locus (tlocus or tstruct) – Row key. The; chromosome and position. If reference_genome is defined, the type; will be tlocus parameterized by reference_genome.; Otherwise, the type will be a tstruct with two fields:; contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference allele (A2; if a2_reference is True) is the first element in the array.; rsid (tstr) – Column 2 in the BIM file.; cm_position (tfloat64) – Column 3 in ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:27156,test,test,27156,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['test'],['test']
Testability,"(Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, ste",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:15305,log,logP,15305,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['log'],['logP']
Testability,"(X @ b). score_0 = null_fit.score; score_1 = X1.T @ (y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b));",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43650,assert,assert,43650,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['assert'],['assert']
Testability,"(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@type",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56788,log,logarithm,56788,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logarithm']
Testability,"(the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; +-----------+------------+. Parallelizing with a specified number of partitions:; >>> rows = [ {'a': i} for i in range(100) ]; >>> ht = hl.Table.parallelize(rows, n_partitions=10); >>> ht.n_partitions(); 10; >>> ht.count(); 100. Parallelizing with some global",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:51992,log,login,51992,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['log'],['login']
Testability,"(y - mu); score = hl.nd.hstack([score_0, score_1]). fisher00 = null_fit.fisher; fisher01 = X0.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)); fisher10 = fisher01.T; fisher11 = X1.T @ (X1 * (mu * (1 - mu)).reshape(-1, 1)). fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). solve_attempt = hl.nd.solve(fisher, score, no_crash=True). chi_sq = hl.or_missing(~solve_attempt.failed, (score * solve_attempt.solution).sum()). p = hl.pchisqtail(chi_sq, m - m0). return hl.struct(chi_sq_stat=chi_sq, p_value=p). def _firth_fit(; b: NDArrayNumericExpression, # (K,); X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1. dtype = numerical_regression_fit_dtype._drop_fields(['score', 'fisher']); blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}); X_bslice = X[:, : b.shape[0]]. def fit(recur, iteration, b):; def cont(exploded, delta_b, max_delta_b):; log_lkhd_left = hl.log(y * mu + (hl.literal(1.0) - y) * (1 - mu)).sum(); log_lkhd_right = hl.log(hl.abs(hl.nd.diagonal(r))).sum(); log_lkhd = log_lkhd_left + log_lkhd_right. next_b = b + delta_b. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(b=b, mu=mu, n_iterations=iteration, log_lkhd=log_lkhd, converged=True, exploded=False),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b)); ). m = b.shape[0] # n_covariates or n_covariates + ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:43718,assert,assert,43718,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['assert'],['assert']
Testability,") Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:59639,log,logs,59639,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['log'],['logs']
Testability,") covariates, the; model is given by:. \[\begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}\]; The usual null hypothesis is \(\beta_1 = 0\). SKAT tests for an association, but does not; provide an effect size or other information about the association.; Wu et al. argue that, under the null hypothesis, a particular value, \(Q\), is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of \(Q\). If \(\widehat{\beta_\textrm{null}}\) is the best-fit beta under the; null model:. \[Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X))\]; Then \(Q\) is defined by Wu et al. as:. \[\begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}\]; Therefore \(r_i\), the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. Each sample’s phenotype is Bernoulli distributed with mean \(p_i\) and variance; \(\sigma^2_i = p_i(1 - p_i)\), the binomial variance.; \(G W G^T\), is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of \(Q\) is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call \(Z Z^T\):. \[\begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}\]; The eigenvalues of \(Z Z^T\) and \(Z^T Z\) are the squared singular values of \(Z\);; therefore, we instead focus on \(Z",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:69134,log,logit,69134,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['log'],['logit']
Testability,")); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters; ----------; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:55999,log,log,55999,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['log']
Testability,"),; master=nullable(str),; local=str,; log=nullable(str),; quiet=bool,; append=bool,; min_block_size=int,; branching_factor=int,; tmp_dir=nullable(str),; default_reference=nullable(enumeration(*BUILTIN_REFERENCES)),; idempotent=bool,; global_seed=nullable(int),; spark_conf=nullable(dictof(str, str)),; skip_logging_configuration=bool,; local_tmpdir=nullable(str),; _optimizer_iterations=nullable(int),; backend=nullable(enumeration(*BackendType.__args__)),; driver_cores=nullable(oneof(str, int)),; driver_memory=nullable(str),; worker_cores=nullable(oneof(str, int)),; worker_memory=nullable(str),; gcs_requester_pays_configuration=nullable(oneof(str, sized_tupleof(str, sequenceof(str)))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:5873,log,log,5873,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['log'],['log']
Testability,"). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Bo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47659,log,logistic,47659,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'test']"
Testability,"):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure_prob):; return hl.sqrt(hl.log(2 / failure_prob) * s / 2). def compute_global_error(s):; return hl.rbind(compute_grid_size(s), lambda p: 1 / p + compute_single_error(s, failure_prob / p)). if all_quantiles:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_global_error)); else:; return hl.rbind(cdf, lambda cdf: hl.rbind(compute_sum(cdf), compute_single_error)). def _error_from_cdf_python(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""; import math. s = 0; for i in builtins.range(builtins.len(cdf._compaction_counts)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:6131,log,log,6131,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['log']
Testability,"):; et = unify_types_limited(*(t.element_type for t in ts)); if et is not None:; return tarray(et); else:; return None; else:; return None. def super_unify_types(*ts):; ts = [t for t in ts if t is not None]; if len(ts) == 0:; return None; t0 = ts[0]; if all(is_numeric(t) for t in ts):; return unify_types_limited(*ts); if any(not isinstance(t, type(t0)) for t in ts):; return None; if isinstance(t0, tarray):; et = super_unify_types(*[t.element_type for t in ts]); return tarray(et); if isinstance(t0, tset):; et = super_unify_types(*[t.element_type for t in ts]); return tset(et); if isinstance(t0, tdict):; kt = super_unify_types(*[t.key_type for t in ts]); vt = super_unify_types(*[t.value_type for t in ts]); return tdict(kt, vt); if isinstance(t0, tstruct):; keys = [k for t in ts for k in t.fields]; kvs = {k: super_unify_types(*[t.get(k, None) for t in ts]) for k in keys}; return tstruct(**kvs); if all(t0 == t for t in ts):; return t0. return None. def unify_exprs(*exprs: 'Expression') -> Tuple:; assert len(exprs) > 0; types = {e.dtype for e in exprs}. # all types are the same; if len(types) == 1:; return (*exprs, True). for t in types:; c = expressions.coercer_from_dtype(t); if all(c.can_coerce(e.dtype) for e in exprs):; return (*tuple([c.coerce(e) for e in exprs]), True). # cannot coerce all types to the same type; return (*exprs, False). [docs]class Expression(object):; """"""Base class for Hail expressions."""""". __array_ufunc__ = None # disable NumPy coercions, so Hail coercions take priority. @typecheck_method(x=ir.IR, type=nullable(HailType), indices=Indices, aggregations=linked_list(Aggregation)); def __init__(; self, x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; self._ir: ir.IR = x; self._type = type; self._indices = indices; self._aggregations = aggregations; self._summary = None. [docs] def describe(self, handler=print):; """"""Print information about type, index, and dependencies.""""""; if self._aggrega",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:15296,assert,assert,15296,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['assert'],['assert']
Testability,"); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""log10"", tfloat64, x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def logit(x) -> Float64Expression:; """"""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.log(x / (1 - x)). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Als",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:57706,log,log,57706,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['log']
Testability,"); Count the number of records where a predicate is True. counter(expr, *[, weight]); Count the occurrences of each unique record and return a dictionary. any(condition); Returns True if condition is True for any record. all(condition); Returns True if condition is True for every record. take(expr, n[, ordering]); Take n records of expr, optionally ordered by ordering. min(expr); Compute the minimum expr. max(expr); Compute the maximum expr. sum(expr); Compute the sum of all records of expr. array_sum(expr); Compute the coordinate-wise sum of all records of expr. mean(expr); Compute the mean value of records of expr. approx_quantiles(expr, qs[, k]); Compute an array of approximate quantiles. approx_median(expr[, k]); Compute the approximate median. stats(expr); Compute a number of useful statistics about expr. product(expr); Compute the product of all records of expr. fraction(predicate); Compute the fraction of records where predicate is True. hardy_weinberg_test(expr[, one_sided]); Performs test of Hardy-Weinberg equilibrium. explode(f, array_agg_expr); Explode an array or set expression to aggregate the elements of all records. filter(condition, aggregation); Filter records according to a predicate. inbreeding(expr, prior); Compute inbreeding statistics on calls. call_stats(call, alleles); Compute useful call statistics. info_score(gp); Compute the IMPUTE information score. hist(expr, start, end, bins); Compute binned counts of a numeric expression. linreg(y, x[, nested_dim, weight]); Compute multivariate linear regression statistics. corr(x, y); Computes the Pearson correlation coefficient between x and y. group_by(group, agg_expr); Compute aggregation statistics stratified by one or more groups. array_agg(f, array); Aggregate an array element-wise using a user-specified aggregation function. downsample(x, y[, label, n_divisions]); Downsample (x, y) coordinate datapoints. approx_cdf(expr[, k, _raw]); Produce a summary of the distribution of values. hail.expr.aggr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:1913,test,test,1913,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability,"); else:; assert isinstance(src, MatrixTable); row_uid = Env.get_uid(); uids.append(row_uid); col_uid = Env.get_uid(); uids.append(col_uid). def joiner(left: MatrixTable):; localized = self._localize_entries(row_uid, col_uid); src_cols_indexed = self.add_col_index(col_uid).cols(); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(; row_exprs={row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs={col_uid: src_cols_indexed.index(*col_exprs)[col_uid]},; ); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}). join_ir = ir.Join(; ir.ProjectedTopLevelReference('g', uid, self.entry.dtype), uids, [*row_exprs, *col_exprs], joiner; ); return construct_expr(join_ir, self.entry.dtype, indices, aggregations). @typecheck_method(entries_field_name=str, cols_field_name=str); def _localize_entries(self, entries_field_name, cols_field_name) -> 'Table':; assert entries_field_name not in self.row; assert cols_field_name not in self.globals; return Table(ir.CastMatrixToTable(self._mir, entries_field_name, cols_field_name)). [docs] @typecheck_method(entries_array_field_name=nullable(str), columns_array_field_name=nullable(str)); def localize_entries(self, entries_array_field_name=None, columns_array_field_name=None) -> 'Table':; """"""Convert the matrix table to a table with entries localized as an array of structs. Examples; --------; Build a numpy ndarray from a small :class:`.MatrixTable`:. >>> mt = hl.utils.range_matrix_table(3,3); >>> mt = mt.select_entries(x = mt.row_idx * mt.col_idx); >>> mt.show(); +---------+-------+-------+-------+; | row_idx | 0.x | 1.x | 2.x |; +---------+-------+-------+-------+; | int32 | int32 | int32 | int32 |; +---------+-------+-------+-------+; | 0 | 0 | 0 | 0 |; | 1 | 0 | 1 | 2 |; | 2 | 0 | 2 | 4 |; +---------+-------+-------+-------+. >>> t = mt.localize_entries('entry_structs', 'columns'); >>> t.describe(); ----------------------------------------; Global fiel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:97898,assert,assert,97898,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['assert'],['assert']
Testability,"); else:; return _func(""dnchisq"", tfloat64, x, df, ncp, log_p). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, log_p=expr_bool); def dnorm(x, mu=0, sigma=1, log_p=False) -> Float64Expression:; """"""Compute the probability density at `x` of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns density of standard normal; distribution by default. Examples; --------. >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Real number at which to compute the probability density.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability den",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:29325,log,logarithm,29325,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['logarithm']
Testability,"*tup))). test_statistic = numerator / denominator; p_value = pchisqtail(test_statistic, 1); return struct(test_statistic=test_statistic, p_value=p_value). [docs]@typecheck(; collection=expr_oneof(; expr_dict(), expr_set(expr_tuple([expr_any, expr_any])), expr_array(expr_tuple([expr_any, expr_any])); ); ); def dict(collection) -> DictExpression:; """"""Creates a dictionary. Examples; --------. >>> hl.eval(hl.dict([('foo', 1), ('bar', 2), ('baz', 3)])); {'bar': 2, 'baz': 3, 'foo': 1}. Notes; -----; This method expects arrays or sets with elements of type :class:`.ttuple`; with 2 fields. The first field of the tuple becomes the key, and the second; field becomes the value. Parameters; ----------; collection : :class:`.DictExpression` or :class:`.ArrayExpression` or :class:`.SetExpression`. Returns; -------; :class:`.DictExpression`; """"""; if isinstance(collection.dtype, (tarray, tset)):; key_type, value_type = collection.dtype.element_type.types; return _func('dict', tdict(key_type, value_type), collection); else:; assert isinstance(collection.dtype, tdict); return collection. [docs]@typecheck(x=expr_float64, a=expr_float64, b=expr_float64); def dbeta(x, a, b) -> Float64Expression:; """"""; Returns the probability density at `x` of a `beta distribution; <https://en.wikipedia.org/wiki/Beta_distribution>`__ with parameters `a`; (alpha) and `b` (beta). Examples; --------. >>> hl.eval(hl.dbeta(.2, 5, 20)); 4.900377563180943. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Point in [0,1] at which to sample. If a < 1 then x must be positive.; If b < 1 then x must be less than 1.; a : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The alpha parameter in the beta distribution. The result is undefined; for non-positive a.; b : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The beta parameter in the beta distribution. The result is undefined; for non-positive b. Returns; -------; :class:`.Float64Expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:26198,assert,assert,26198,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['assert'],['assert']
Testability,"+ n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:; >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:; >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters:. x (i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12903,test,test,12903,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['test'],['test']
Testability,"+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail Struct or dict s.; >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in partial_type. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly.; >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:51411,log,login,51411,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['log'],['login']
Testability,", and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:16617,test,test,16617,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['test'],['test']
Testability,", axis=0)); array([1., 2., 3., 4.]). Parameters; ----------; nds : a sequence of :class:`.NDArrayNumericExpression`; The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).; Note: unlike Numpy, the numerical element type of each array_like must match.; axis : int, optional; The axis along which the arrays will be joined. Default is 0.; Note: unlike Numpy, if provided, axis cannot be None. Returns; -------; :class:`.NDArrayExpression`; The concatenated array; """"""; head_nd = nds[0]. if isinstance(nds, list):; indices, aggregations = unify_all(*nds); typs = {x.dtype for x in nds}. if len(typs) != 1:; element_types = {t.element_type for t in typs}; if len(element_types) != 1:; argument_element_types_str = "", "".join(str(nd.dtype.element_type) for nd in nds); raise ValueError(; f'hl.nd.concatenate: ndarrays must have same element types, found these element types: ({argument_element_types_str})'; ). ndims = {t.ndim for t in typs}; assert len(ndims) != 1; ndims_str = "", "".join(str(nd.dtype.ndim) for nd in nds); raise ValueError(f'hl.nd.concatenate: ndarrays must have same number of dimensions, found: {ndims_str}.'); else:; indices = nds._indices; aggregations = nds._aggregations. makearr = aarray(nds); concat_ir = NDArrayConcat(makearr._ir, axis). return construct_expr(concat_ir, tndarray(head_nd._type.element_type, head_nd.ndim), indices, aggregations). [docs]@typecheck(N=expr_numeric, M=nullable(expr_numeric), dtype=HailType); def eye(N, M=None, dtype=tfloat64):; """"""; Construct a 2-D :class:`.NDArrayExpression` with ones on the *main* diagonal; and zeros elsewhere. Examples; --------; >>> hl.eval(hl.nd.eye(3)); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]); >>> hl.eval(hl.nd.eye(2, 5, dtype=hl.tint32)); array([[1, 0, 0, 0, 0],; [0, 1, 0, 0, 0]], dtype=int32). Parameters; ----------; N : :class:`.NumericExpression` or Python number; Number of rows in the output.; M : :class:`.NumericExpression` or Python number, opt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:15058,assert,assert,15058,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['assert'],['assert']
Testability,", explosion, or reaching the max (25 for Wald, LRT; 100 for Firth); Wald, LRT, Firth ``va.logreg.fit.converged`` Boolean true if iteration converged; Wald, LRT, Firth ``va.logreg.fit.exploded`` Boolean true if iteration exploded; ================ =========================== ======= =====. We consider iteration to have converged when every coordinate of :math:`\\beta` changes by less than :math:`10^{-6}`. For Wald and LRT, up to 25 iterations are attempted; in testing we find 4 or 5 iterations nearly always suffice. Convergence may also fail due to explosion, which refers to low-level numerical linear algebra exceptions caused by manipulating ill-conditioned matrices. Explosion may result from (nearly) linearly dependent covariates or complete `separation <https://en.wikipedia.org/wiki/Separation_(statistics)>`__. A more common situation in genetics is quasi-complete seperation, e.g. variants that are observed only in cases (or controls). Such variants inevitably arise when testing millions of variants with very low minor allele count. The maximum likelihood estimate of :math:`\\beta` under logistic regression is then undefined but convergence may still occur after a large number of iterations due to a very flat likelihood surface. In testing, we find that such variants produce a secondary bump from 10 to 15 iterations in the histogram of number of iterations per variant. We also find that this faux convergence produces large standard errors and large (insignificant) p-values. To not miss such variants, consider using Firth logistic regression, linear regression, or group-based tests. Here's a concrete illustration of quasi-complete seperation in R. Suppose we have 2010 samples distributed as follows for a particular variant:. ======= ====== === ======; Status HomRef Het HomVar; ======= ====== === ======; Case 1000 10 0; Control 1000 0 0; ======= ====== === ======. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:143979,test,testing,143979,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['test'],['testing']
Testability,", nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir is None:; log_dir = os.getcwd(); log = hail.utils.timestamp_path(os.path.join(log_dir, 'hail'), suffix=f'-{py_version}.log'); return log. def convert_gcs_requester_pays_configuration_to_hadoop_conf_style(; x: Optional[Union[str, Tuple[str, List[str]]]],; ) -> Tuple[Optional[str], Optional[str]]:; if isinstance(x, str):; return x, None; if isinstance(x, tuple):; return x[0], "","".join(x[1]); return None, None. class HailContext(object):; @staticmethod; def create(; log: str,; quiet: bool,; append: bool,; tmpdir: str,; local_tmpdir: str,; default_reference: str,; global_seed: Optional[int],; backend: Backend,; ):; hc = HailContext(; log=log,; quiet=quiet,; append=append,; tmpdir=tmpdir,; local_tmpdir=local_tmpdir,; global_seed=global_seed,; backend=backend,; ); hc.initialize_references(default_reference); return hc. @typecheck_method(; log=str, quiet=bool, append=bool, tmpdir=str, local_tmpdir=str, global_seed=nu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:1899,log,log,1899,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['log'],['log']
Testability,", sequenceof(str)))),; ); def init_local(; log=None,; quiet=False,; append=False,; branching_factor=50,; tmpdir=None,; default_reference='GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" aut",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20073,log,log,20073,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['log'],['log']
Testability,", set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:6176,test,test,6176,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,3,"['log', 'test']","['logistic', 'test']"
Testability,", the method proceeds as follows:. Filter to the set of samples for which all phenotype and covariates are defined. For each key and sample, aggregate genotypes across variants with that key to produce a numeric score.; agg_expr must be of numeric type and has the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Note that v, va, and g are accessible through; Aggregable methods on gs.; The resulting sample key table has key column key_name and a numeric column of scores for each sample; named by the sample ID. For each key, fit the logistic regression model using the supplied phenotype, covariates, and test.; The model and tests are those of logreg() with sample genotype gt replaced by the; score in the sample key table. For each key, missing scores are mean-imputed across all samples.; The resulting logistic regression key table has key column of type String given by the key_name; parameter and additional columns corresponding to the fields of the va.logreg schema given for test; in logreg(). logreg_burden() returns both the logistic regression key table and the sample key table. Parameters:; key_name (str) – Name to assign to key column of returned key tables.; variant_keys (str) – Variant annotation path for the TArray or TSet of keys associated to each variant.; single_key (bool) – if true, variant_keys is interpreted as a single (or missing) key per variant,; rather than as a collection of keys.; agg_expr (str) – Sample aggregation expression (per key).; test (str) – Statistical test, one of: ‘wald’, ‘lrt’, ‘score’, or ‘firth’.; y (str) – Response expression.; covariates (list of str) – list of covariate expressions. Returns:Tuple of logistic regression key table and sample aggregation key table. Return type:(KeyTable, KeyTable). make_table(variant_expr, genotype_expr, key=[], separator='.')[source]¶; Produce a key with one row per variant and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:120951,log,logistic,120951,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['log'],['logistic']
Testability,", the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""; return _func(""dnorm"", tfloat64, x, mu, sigma, log_p). [docs]@typecheck(x=expr_float64, lamb=expr_float64, log_p=expr_bool); def dpois(x, lamb, log_p=False) -> Float64Expression:; """"""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""; return _func(""dpois"", tfloat64, x, lamb, log_p). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def exp(x) -> Float64Expression:; """"""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""exp"", tfloat64, x). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def fisher_exact_test(c1, c2, c3, c4) -> StructExpression:; """"""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.1007",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:30307,log,log,30307,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['log']
Testability,", with :math:`K` covariates, the; model is given by:. .. math::. \begin{align*}; X &: R^{N \times K} \\; G &: \{0, 1, 2\}^{N \times M} \\; \\; Y &\sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_0 X + \beta_1 G)); \end{align*}. The usual null hypothesis is :math:`\beta_1 = 0`. SKAT tests for an association, but does not; provide an effect size or other information about the association. Wu et al. argue that, under the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. If :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the; null model:. .. math::. Y \sim \textrm{Bernoulli}(\textrm{logit}^{-1}(\beta_\textrm{null} X)). Then :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; p_i &= \textrm{logit}^{-1}(\widehat{\beta_\textrm{null}} X) \\; r_i &= y_i - p_i \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. Therefore :math:`r_i`, the residual phenotype, is the portion of the phenotype unexplained by; the covariates alone. Also notice:. 1. Each sample's phenotype is Bernoulli distributed with mean :math:`p_i` and variance; :math:`\sigma^2_i = p_i(1 - p_i)`, the binomial variance. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We describe below our interpretation of the mathematics as described in the main body and; appendix of Wu, et al. According to the paper, the distribution of :math:`Q` is given by a; generalized chi-squared distribution whose weights are the eigenvalues of a symmetric matrix; which we call :math:`Z Z^T`:. .. math::. \begin{align*}; V_{ii} &= \sigma^2_i \\; W_{ii} &= w_i \quad\quad \textrm{the weight for variant } i \\; \\; P_0 &= V - V X (X^T V X)^{-1} X^T V \\; Z Z^T &= P_0^{1/2} G W G^T P_0^{1/2}; \end{align*}. The eigenvalues of :math:`Z Z^",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:88241,log,logit,88241,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logit']
Testability,",; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] -",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5670,log,log,5670,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['log'],['log']
Testability,",; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26964,test,test,26964,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['test']
Testability,"- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085,; and 0.0016, respectively. The erroneous value 0.991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This test; is slower, as both the null and full model must be fit per variant, and; convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted by default for the null; model and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model; otherwise,; they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:34260,log,logreg,34260,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logreg']
Testability,"-+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106961,log,logistic,106961,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,3,"['log', 'test']","['logistic', 'test']"
Testability,"-- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:7118,log,log,7118,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,1,['log'],['log']
Testability,"----+-----------------+; | ploidy | Phased | Unphased |; +========+=================+=================+; | 0 | ``|-`` | ``-`` |; +--------+-----------------+-----------------+; | 1 | ``|i`` | ``i`` |; +--------+-----------------+-----------------+; | 2 | ``i|j`` | ``i/j`` |; +--------+-----------------+-----------------+; | 3 | ``i|j|k`` | ``i/j/k`` |; +--------+-----------------+-----------------+; | N | ``i|j|k|...|N`` | ``i/j/k/.../N`` |; +--------+-----------------+-----------------+. Parameters; ----------; s : str or :class:`.StringExpression`; String to parse. Returns; -------; :class:`.CallExpression`; """"""; return _func('Call', tcall, s). [docs]@typecheck(expression=expr_any); def is_defined(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is not missing. Examples; --------. >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is not missing, ``False`` otherwise.; """"""; return ~apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(expression=expr_any); def is_missing(expression) -> BooleanExpression:; """"""Returns ``True`` if the argument is missing. Examples; --------. >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is missing, ``False`` otherwise.; """"""; return apply_expr(lambda x: ir.IsNA(x), tbool, expression). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def is_nan(x) -> BooleanExpression:; """"""Returns ``True`` if the argument is ``nan`` (not a number). Examples; --------. >>> hl.eval(hl.is_nan(0)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:51512,test,test,51512,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['test'],['test']
Testability,"------+-------+; | 0 | 1 | 2 |; | 1 | 3 | 4 |; +---------+-------+-------+. Notes. Matrix dimensions are inferred from input data.; You must provide row and column dimensions by specifying rows or; entries (inclusive) and cols or entries (inclusive).; The respective dimensions of rows, cols and entries must match should; you provide rows and entries or cols and entries (inclusive). Parameters:. globals (dict from str to any) – Global fields by name.; rows (dict from str to list of any) – Row fields by name.; cols (dict from str to list of any) – Column fields by name.; entries (dict from str to list of list of any) – Matrix entries by name in the form entry[row_idx][col_idx]. Returns:; MatrixTable – A MatrixTable assembled from inputs whose rows are keyed by row_idx; and columns are keyed by col_idx. classmethod from_rows_table(table)[source]; Construct matrix table with no columns from a table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Examples; Import a text table and construct a rows-only matrix table:; >>> table = hl.import_table('data/variant-lof.tsv'); >>> table = table.transmute(**hl.parse_variant(table['v'])).key_by('locus', 'alleles'); >>> sites_mt = hl.MatrixTable.from_rows_table(table). Notes; All fields in the table become row-indexed fields in the; result. Parameters:; table (Table) – The table to be converted. Returns:; MatrixTable. property globals; Returns a struct expression including all global fields. Returns:; StructExpression. globals_table()[source]; Returns a table with a single row with the globals of the matrix table.; Examples; Extract the globals table:; >>> globals_table = dataset.globals_table(). Returns:; Table – Table with the globals from the matrix, with a single row. group_cols_by(*exprs, **named_exprs)[source]; Group columns, used with GroupedMatrixTable.aggregate().; Examples; Aggregate to a matrix with cohort as column keys, comput",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:38134,test,tested,38134,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['test'],['tested']
Testability,"-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.show(handler=lambda x: logging.info(x)) . Parameters:. n or n_rows (int) – Maximum number of rows to show, or negative to show all rows.; width (int) – Horizontal width at which to break fields.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field.; handler (Callable[[str], Any]) – Handler function for data string. summarize(handler=None)[source]; Compute and print summary information about the fields in the table. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. tail(n)[source]; Subset table to last n rows.; Examples; Subset to the last three rows:; >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; The number of partitions in the new table is equal to the number of; partitions containing the last n rows. Parameters:; n (int) – Number of rows to include. Returns:; Table – Table including the last n rows. take(n, _localize=True)[source]; Collect the first n rows of the table into a local list.; Examples; Take the first three rows:; >>> first3 = table1.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Notes; This method does not need to look at all the data in the table, and; allows for fast queries of the start of the table.; This method is equivalent to Table.head() followed by; Table.collect(). Parameters:; n (int) – Numbe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:67391,test,tested,67391,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['test'],['tested']
Testability,"------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106463,log,logistic,106463,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['log'],['logistic']
Testability,"---------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); els",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106842,log,logistic,106842,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'test']"
Testability,"----------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+------------------------------------------------------------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:121458,log,logDeltaGrid,121458,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['logDeltaGrid']
Testability,"------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:122374,log,log,122374,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['log']
Testability,"---------------------------------------+; | ``global.lmmreg.fit.seH2`` | Double | standard error of :math:`\\hat{h}^2` under asymptotic normal approximation |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.normLkhdH2`` | Array[Double] | likelihood function of :math:`h^2` normalized on the discrete grid ``0.01, 0.02, ..., 0.99``. Index ``i`` is the likelihood for percentage ``i``. |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ra",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:121155,log,log,121155,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['log']
Testability,"-------------------------------------------------------------------------------+; | ``global.lmmreg.fit.maxLogLkhd`` | Double | (restricted) maximum log likelihood corresponding to :math:`\\hat{\delta}` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coef",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:121803,log,logLkhdVals,121803,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['log'],"['log', 'logLkhdVals']"
Testability,"------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +---------------------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:122261,log,log,122261,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['log'],['log']
Testability,"-----; mt : :class:`.MatrixTable`; :class:`.MatrixTable` with betas as a row field, simulated according to specified model.; pi : :obj:`list`; Probability of a SNP being causal for different traits, possibly altered; from input `pi` if covariance matrix for multitrait simulation was not; positive semi-definite.; rg : :obj:`list`; Genetic correlation between traits, possibly altered from input `rg` if; covariance matrix for multitrait simulation was not positive semi-definite. """"""; h2 = h2.tolist() if isinstance(h2, np.ndarray) else ([h2] if not isinstance(h2, list) else h2); pi = pi.tolist() if isinstance(pi, np.ndarray) else ([pi] if not isinstance(pi, list) else pi); rg = rg.tolist() if isinstance(rg, np.ndarray) else ([rg] if not isinstance(rg, list) else rg); assert all(x >= 0 and x <= 1 for x in h2), 'h2 values must be between 0 and 1'; assert (pi is not [None]) or all(; x >= 0 and x <= 1 for x in pi; ), 'pi values for spike & slab must be between 0 and 1'; assert rg == [None] or all(x >= -1 and x <= 1 for x in rg), 'rg values must be between -1 and 1 or None'; if annot is not None: # multi-trait annotation-informed; assert rg == [None], 'Correlated traits not supported for annotation-informed model'; h2 = h2 if isinstance(h2, list) else [h2]; annot_sum = mt.aggregate_rows(hl.agg.sum(annot)); mt = mt.annotate_rows(beta=hl.literal(h2).map(lambda x: hl.rand_norm(0, hl.sqrt(annot * x / (annot_sum * M))))); elif len(h2) > 1 and (pi in ([None], [1])): # multi-trait correlated infinitesimal; mt, rg = multitrait_inf(mt=mt, h2=h2, rg=rg); elif len(h2) == 2 and len(pi) > 1 and len(rg) == 1: # two trait correlated spike & slab; print('multitrait ss'); mt, pi, rg = multitrait_ss(mt=mt, h2=h2, rg=0 if rg is [None] else rg[0], pi=pi); elif len(h2) == 1 and len(pi) == 1: # single trait infinitesimal/spike & slab; M = mt.count_rows(); pi_temp = 1 if pi == [None] else pi[0]; mt = mt.annotate_rows(beta=hl.rand_bool(pi_temp) * hl.rand_norm(0, hl.sqrt(h2[0] / (M * pi_temp)))); el",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:7113,assert,assert,7113,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,4,['assert'],['assert']
Testability,"--. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:61060,test,test,61060,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,3,['test'],['test']
Testability,"-; x; Expression to convert. Returns; -------; :class:`.StringExpression`; String expression with JSON representation of `x`.; """"""; return _func(""json"", tstr, x). [docs]@typecheck(x=expr_str, dtype=hail_type); def parse_json(x, dtype):; """"""Convert a JSON string to a structured expression. Examples; --------; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters; ----------; x : :class:`.StringExpression`; JSON string.; dtype; Type of value to parse. Returns; -------; :class:`.Expression`; """"""; return _func(""parse_json"", ttuple(dtype), x, type_args=(dtype,))[0]. [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64)), base=nullable(expr_float64)); def log(x, base=None) -> Float64Expression:; """"""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """""". def scalar_log(x):; if base is not None:; return _func(""log"", tfloat64, x, to_expr(base)); else:; return _func(""log"", tfloat64, x). x = to_expr(x); if isinstance(x.dtype, tndarray):; return x.map(scalar_log); return scalar_log(x). [docs]@typecheck(x=oneof(expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def log10(x) -> Float64Expression:; """"""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Ex",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:56077,log,log,56077,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['log'],['log']
Testability,"-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pnorm(x[, mu, sigma, lower_tail, log_p]); The cumulative probability function of a normal distribution with mean mu and standard deviation sigma. ppois(x, lamb[, lower_tail, log_p]); The cumulative probability function of a Poisson distribution. qchisqtail(p, df[, ncp, lower_tail, log_p]); The quantile function of a chi-squared distribution with df degrees of freedom, inverts pchisqtail(). qnorm(p[, mu, sigma, lower_tail, log_p]); The quantile function of a normal distribution with mean mu and standard deviation sigma, inverts pnorm(). qpois(p, lamb[, lower_tail, log_p]); The quantile function of a Poisson distribution with rate parameter lamb, inverts ppois(). Randomness. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:10644,test,test,10644,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['test'],['test']
Testability,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:10000,test,testing,10000,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['test'],['testing']
Testability,". Control; 1000; 0; 0. The following R code fits the (standard) logistic, Firth logistic, and linear regression models to this data, where x is genotype, y is phenotype, and logistf is from the logistf package:; x <- c(rep(0,1000), rep(1,1000), rep(1,10); y <- c(rep(0,1000), rep(0,1000), rep(1,10)); logfit <- glm(y ~ x, family=binomial()); firthfit <- logistf(y ~ x); linfit <- lm(y ~ x). The resulting p-values for the genotype coefficient are 0.991, 0.00085, and 0.0016, respectively. The erroneous value 0.991 is due to quasi-complete separation. Moving one of the 10 hets from case to control eliminates this quasi-complete separation; the p-values from R are then 0.0373, 0.0111, and 0.0116, respectively, as expected for a less significant association.; The Firth test reduces bias from small counts and resolves the issue of separation by penalizing maximum likelihood estimation by the Jeffrey’s invariant prior. This test is slower, as both the null and full model must be fit per variant, and convergence of the modified Newton method is linear rather than quadratic. For Firth, 100 iterations are attempted for the null model and, if that is successful, for the full model as well. In testing we find 20 iterations nearly always suffices. If the null model fails to converge, then the sa.lmmreg.fit annotations reflect the null model; otherwise, they reflect the full model.; See Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants for an empirical comparison of the logistic Wald, LRT, score, and Firth tests. The theoretical foundations of the Wald, likelihood ratio, and score tests may be found in Chapter 3 of Gesine Reinert’s notes Statistical Theory. Firth introduced his approach in Bias reduction of maximum likelihood estimates, 1993. Heinze and Schemper further analyze Firth’s approach in A solution to the problem of separation in logistic regression, 2002.; Those variants that don’t vary across the included samp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:114791,test,test,114791,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['test'],['test']
Testability,". In [6]:. hc.eval_expr_typed('3 + 8'). Out[6]:. (11, Int). In [7]:. hc.eval_expr_typed('3.2 * 0.5'). Out[7]:. (1.6, Double). In [8]:. hc.eval_expr_typed('3 ** 3'). Out[8]:. (27.0, Double). In [9]:. hc.eval_expr_typed('25 ** 0.5'). Out[9]:. (5.0, Double). In [10]:. hc.eval_expr_typed('true || false'). Out[10]:. (True, Boolean). In [11]:. hc.eval_expr_typed('true && false'). Out[11]:. (False, Boolean). Missingness¶; Like R, all values in Hail can be missing. Most operations, like; addition, return missing if any of their inputs is missing. There are a; few special operations for manipulating missing values. There is also a; missing literal, but you have to specify it’s type. Missing Hail values; are converted to None in Python. In [12]:. hc.eval_expr_typed('NA: Int') # missing Int. Out[12]:. (None, Int). In [13]:. hc.eval_expr_typed('NA: Dict[String, Int]'). Out[13]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:5271,test,test,5271,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['test'],['test']
Testability,". Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Statistical functions. View page source. Statistical functions. chi_squared_test(c1, c2, c3, c4); Performs chi-squared test of independence on a 2x2 contingency table. fisher_exact_test(c1, c2, c3, c4); Calculates the p-value, odds ratio, and 95% confidence interval using Fisher's exact test for a 2x2 table. contingency_table_test(c1, c2, c3, c4, ...); Performs chi-squared or Fisher's exact test of independence on a 2x2 contingency table. cochran_mantel_haenszel_test(a, b, c, d); Perform the Cochran-Mantel-Haenszel test for association. dbeta(x, a, b); Returns the probability density at x of a beta distribution with parameters a (alpha) and b (beta). dchisq(x, df[, ncp, log_p]); Compute the probability density at x of a chi-squared distribution with df degrees of freedom. dnorm(x[, mu, sigma, log_p]); Compute the probability density at x of a normal distribution with mean mu and standard deviation sigma. dpois(x, lamb[, log_p]); Compute the (log) probability density at x of a Poisson distribution with rate parameter lamb. hardy_weinberg_test(n_hom_ref, n_het, n_hom_var); Performs test of Hardy-Weinberg equilibrium. binom_test(x, n, p, alternative); Performs a binomial test on p given x successes in n trials. pchisqtail(x, df[, ncp, lower_tail, log_p]); Returns the probability under the right-tail starting at x for a chi-squared distribution with df degrees of freedom. pgenchisq(x, w, k, lam, mu, sigma, *[, ...]); The cumulative probability function of a generalized ch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:1186,test,test,1186,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['test'],['test']
Testability,". Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for any element, False otherwise. append(item)[source]; Append an element to the array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5026,test,test,5026,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['test'],['test']
Testability,". res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26430,log,logistic,26430,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,"['log', 'test']","['logistic', 'test']"
Testability,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12356,test,test,12356,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['test'],['test']
Testability,".2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:54135,log,log,54135,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['log'],['log']
Testability,".991 is due to; quasi-complete separation. Moving one of the 10 hets from case to control; eliminates this quasi-complete separation; the p-values from R are then; 0.0373, 0.0111, and 0.0116, respectively, as expected for a less; significant association. The Firth test reduces bias from small counts and resolves the issue of; separation by penalizing maximum likelihood estimation by the `Jeffrey's; invariant prior <https://en.wikipedia.org/wiki/Jeffreys_prior>`__. This; test is slower, as both the null and full model must be fit per variant,; and convergence of the modified Newton method is linear rather than; quadratic. For Firth, 100 iterations are attempted for the null model; and, if that is successful, for the full model as well. In testing we; find 20 iterations nearly always suffices. If the null model fails to; converge, then the `logreg.fit` fields reflect the null model;; otherwise, they reflect the full model. See; `Recommended joint and meta-analysis strategies for case-control association testing of single low-count variants <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4049324/>`__; for an empirical comparison of the logistic Wald, LRT, score, and Firth; tests. The theoretical foundations of the Wald, likelihood ratio, and score; tests may be found in Chapter 3 of Gesine Reinert's notes; `Statistical Theory <http://www.stats.ox.ac.uk/~reinert/stattheory/theoryshort09.pdf>`__.; Firth introduced his approach in; `Bias reduction of maximum likelihood estimates, 1993 <http://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf>`__.; Heinze and Schemper further analyze Firth's approach in; `A solution to the problem of separation in logistic regression, 2002 <https://cemsiis.meduniwien.ac.at/fileadmin/msi_akim/CeMSIIS/KB/volltexte/Heinze_Schemper_2002_Statistics_in_Medicine.pdf>`__. Hail's logistic regression tests correspond to the ``b.wald``,; ``b.lrt``, and ``b.score`` tests in `EPACTS`_. For each variant, Hail; imputes missin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:54871,test,testing,54871,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['testing']
Testability,".; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. p_ab(theta=0.5)[source]¶; Returns the p-value associated with finding the given allele depth ratio.; This function uses a one-tailed binomial test.; This function returns None if the allelic depth (ad) is missing. Parameters:theta (float) – null reference probability for binomial model. Return type:float. pl¶; Returns the phred-scaled genotype posterior likelihoods. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:6017,test,test,6017,docs/0.1/representation/hail.representation.Genotype.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html,1,['test'],['test']
Testability,".; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_standoff = 0; import math. p.xaxis.major_label_orientation = math.pi / 3. p.rect(; x='x",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:18735,log,log,18735,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['log'],['log']
Testability,".; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.pdf(data, k=1000, confidence=5, legend=None, title=None, log=False, interactive=False)[source]. hail.plot.smoothed_pdf(data, k=350, smoothing=0.5, legend=None, title=None, log=False, interactive=False, figure=None)[source]; Create a density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter.; smoothing (float) – Degree of smoothing.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts.; interactive (bool) – If True, return a handle to pass to bokeh.io.show().; figure (bokeh.plotting.figure) – If not None, add density plot to figure. Otherwise, create a new figure. Returns:; bokeh.plotting.figure. hail.plot.histogram(data, range=None, bins=50, legend=None, title=None, log=False, interactive=False)[source]; Create a histogram.; Notes; data can be a Float64Expression, or the result of the hist(); or approx_cdf() aggregators. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; log (bool) – Plot the log10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalize",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:3081,log,log,3081,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['log'],['log']
Testability,".Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; logistic : :obj:`bool` or :obj:`tuple` of :obj:`int` and :obj:`float`; If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size : :obj:`int`; Maximum size of group on which to run the test.; accuracy : :obj:`float`; Accuracy achieved by the Davies algorithm if fault value is zero.; iterations : :obj:`int`; Maximum number of iterations attempted by the Davies algorithm. Returns; -------; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:107266,test,test,107266,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['test'],['test']
Testability,".T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < tolerance,; hl.struct(; b=b,; score=score,; fisher=fisher,; mu=mu,; n_iterations=iteration,; log_lkhd=log_lkhd,; converged=True,; exploded=False,; ),; ); .when(; iteration == max_iterations,; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=False),; ); .default(recur(iteration + 1, next_b, next_m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:68131,assert,assert,68131,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['assert'],['assert']
Testability,".bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; if logical:; return _shift_op(x, y, '>>>'); else:; return _shift_op(x, y, '>>'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64)); def bit_not(x):; """"""Bitwise invert `x`. Examples; --------; >>> hl.eval(hl.bit_not(0)); -1. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return construct_expr(ir.ApplyUnaryPrimOp('~', x._ir), x.dtype, x._i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:181653,log,logical,181653,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['log'],['logical']
Testability,".eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; -----; The odds ratio is given by ``(c1 / c2) / (c3 / c4)``. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, odds_ratio=tfloat64); return _func(""chi_squared_test"", ret_type, c1, c2, c3, c4). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32, min_cell_count=expr_int32); def contingency_table_test(c1, c2, c3, c4, min_cell_count) -> StructExpression:; """"""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :cla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:20455,test,test,20455,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['test'],['test']
Testability,".genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [doc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:1550,assert,assert,1550,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['assert'],['assert']
