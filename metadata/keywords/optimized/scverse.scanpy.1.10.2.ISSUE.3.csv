quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:121,install,install,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:129,install,install,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008,1,['install'],['install']
Deployability,"Not a bug, _per se_, as the code runs fine, but you currently cannot install scanpy directly with the rapids extras. Running `pip install scanpy[rapids]` fails with the following error:. ```python; ERROR: Could not find a version that satisfies the requirement cudf>=0.9; extra == ""rapids"" (from scanpy[rapids]) (from versions: 0.5.0, 0.6.1, 0.6.1.post1); ERROR: No matching distribution found for cudf>=0.9; extra == ""rapids"" (from scanpy[rapids]); ```; ; A quick check shows that `PyPi` has the most recent version of rapids suite (cuML, cuDF, cuGRAPH) at 0.6.1 - hence the fail. The only place I can see to get versions of the cu suite that meets the requirements in `setup.py`is from the rapidsAI conda channel, where the most recent version is 0.140. However, if I try to `pip install scanpy` followed by the necessary conda command to install the rapids packages, I get a broken environment that won't let me `import scanpy as sc`, frequently - though not always - because of issues with pandas. How should I install the rapids extras?. Anyways, I recognise these features are experimental, but would love to see them become standard as they are a huge boon when working on big datasets, particularly as I have access to some really good GPUs remotely.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1280:69,install,install,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1280,5,['install'],['install']
Deployability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:96,upgrade,upgraded,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,2,['upgrade'],['upgraded']
Deployability,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706; * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`; * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`; * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658; * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852:86,Update,Updated,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852,2,"['Update', 'update']","['Updated', 'update']"
Deployability,"Now only mpl versions start to break more important stuff than 3d plotting, so let’s update. <details><summary>Outdated</summary>. The umap_with_edges went . | from | to |; | ---- | -- |; | ![grafik](https://user-images.githubusercontent.com/291575/73608411-df447900-45c2-11ea-948f-164ba4204416.png) | ![grafik](https://user-images.githubusercontent.com/291575/73608416-e8354a80-45c2-11ea-9456-82a00cfca79d.png) |. Is the new one correct? If not, what did I do wrong?. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1020:85,update,update,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020,1,['update'],['update']
Deployability,"Numba can’t correctly detect when a threading backend is available. This was fine in the past, since `pynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, y",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:406,install,install,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['install'],['install']
Deployability,"OK I install umap 0.4 . ```; pip install git+git://github.com/lmcinnes/umap@0.4dev; ```. However, it doesn't seem to run any faster and actually throws an error now. ```; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```; gives; ```; AttributeError Traceback (most recent call last); <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440,2,['install'],['install']
Deployability,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:18,install,install,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862,1,['install'],['install']
Deployability,OK! Please add a release note and we’re good to go I think.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1943677733:17,release,release,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1943677733,1,['release'],['release']
Deployability,"OK! This is a bugfix, so I added this PR to the bugfix milestone. Please add a release note for 1.9.6, then it’s ready!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2698#issuecomment-1770654161:79,release,release,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2698#issuecomment-1770654161,1,['release'],['release']
Deployability,"OK, I backported the fix and will do a hotfix today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830#issuecomment-1911978854:39,hotfix,hotfix,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830#issuecomment-1911978854,1,['hotfix'],['hotfix']
Deployability,"OK, I updated the docs so the `.copy()` is in there!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2368351194:6,update,updated,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2368351194,1,['update'],['updated']
Deployability,"OK, done! https://github.com/theislab/scanpydoc/pull/128 is released and this PR is updated. I’m not touching the `/` shortcut: meta/ctrl+k is advertised in a prominent spot and I don’t want to accidentally swallow a user trying to type `/` somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889299384:60,release,released,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805#issuecomment-1889299384,2,"['release', 'update']","['released', 'updated']"
Deployability,"OK, let’s close this in favor of #1733 and update that one with the newest information if necessary",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2088#issuecomment-1787310117:43,update,update,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2088#issuecomment-1787310117,1,['update'],['update']
Deployability,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:518,release,release,518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['release'],['release']
Deployability,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672:352,upgrade,upgraded,352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494485672,1,['upgrade'],['upgraded']
Deployability,"OK, so we want to build and upload the notebooks. With a bit of convincing, we can get readthedocs to do that for us. But (for a good reason) `scanpy_usage` is a different repo. This means:. 1. Changing something there should trigger a rebuild, not changing the scanpy repo; 2. We probably need to put them on https://scanpy_usage.readthedocs.io; 3. We can convince sphinx to create an index entry for the docs in the navigation sidebar that leads to the scanpy_usage site. Then it would look like this:. https://scanpy.readthedocs.io:. - Examples → Converted to a fake index entry that is a link to https://scanpy_usage.readthedocs.io; - Basic Usage → …/basic_usage.html; - Installation → …/installation.html; - API → …/api/index.html; - References → …/references.html. https://scanpy_usage.readthedocs.io:. - Examples → /index.html; - Basic Usage → Fake index extry to https://scanpy.readthedocs.io/…/basic_usage.html; - Installation → Fake index extry to https://scanpy.readthedocs.io/…/installation.html; - API → Fake index extry to https://scanpy.readthedocs.io/…/api/index.html; - References → Fake index extry to https://scanpy.readthedocs.io/…/references.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125:675,Install,Installation,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-364060125,4,"['Install', 'install']","['Installation', 'installation']"
Deployability,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:491,install,installed,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962,1,['install'],['installed']
Deployability,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619:87,pipeline,pipeline,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619,1,['pipeline'],['pipeline']
Deployability,"OK, this is indeed fixed in scanpy master and the bugfix branch (1.9.x). Please try installing the current bugfix branch:. ```bash; pip install 'scanpy @ git+https://github.com/scverse/scanpy.git@1.9.x'; ```. We will release a new feature version in 3 weeks, so there will probably be no bugfix release before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806#issuecomment-1893367676:84,install,installing,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806#issuecomment-1893367676,4,"['install', 'release']","['install', 'installing', 'release']"
Deployability,"OK, this should work. The only issue is that if users check “No release notes necessary” while not checking another box, the “check-relnotes” job still runs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2569#issuecomment-1759813642:64,release,release,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569#issuecomment-1759813642,1,['release'],['release']
Deployability,"OK, updated this so it follows the decision implemented in #1244",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1898572961:4,update,updated,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1898572961,1,['update'],['updated']
Deployability,Offsets for factors were being calculated wrong. It was being done with a sum when it should have been product. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2964; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2965:301,release,release,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2965,2,"['Release', 'release']","['Release', 'release']"
Deployability,"Oh! I did upgrade pip and all the packages needed by scanpy, but didn't have the idea of doing:. pip3 install --upgrade setuptools. This fixed it! Many thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705:10,upgrade,upgrade,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90#issuecomment-367380705,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing.; ; I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528:546,update,update,546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433771528,2,['update'],['update']
Deployability,"Oh, I was too hasty in merging this. Thanks for clarifying more of this. I think it's perfectly fine to have this better and more stringent behavior. . Added a note in the release notes: https://github.com/theislab/scanpy/commit/f428848ece1d7a4794090eb70a34a3b8f1953dee. Btw: I think we should have much nicer release notes with batches both for subversions and author contributions. I'll try improving them very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/442#issuecomment-457870095:172,release,release,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442#issuecomment-457870095,2,['release'],['release']
Deployability,"Ok , thanks for letting me know. Please check the pull request. I have; verified my code by keeping weights 1 and it has same values when; observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,; Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, I’ll close this one then 🙂; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492237208:212,update,update,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492237208,1,['update'],['update']
Deployability,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package 😅 ; https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616:161,release,release,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616,1,['release'],['release']
Deployability,"Ok, it's related to pandas 0.23 - runs on pandas 0.22. Don't know what happened but I'll fix this tomorrow. In the meanwhile you could `pip install pandas==0.22.0`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390823067:140,install,install,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390823067,1,['install'],['install']
Deployability,"Ok, updated the docs... Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/150#issuecomment-387215323:4,update,updated,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/150#issuecomment-387215323,1,['update'],['updated']
Deployability,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:21,install,install,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479,3,['install'],"['install', 'installs']"
Deployability,"Okay, I solved the issue. In my environment, scanpy==1.7.2 does not work with umap==0.5.2. I pip uninstall scanpy and umap-learn. Next I installed umap-learn through conda which was umap==0.5.1. When installing scanpy again it works as expected. . Not sure if its worth looking further into it but pip install scanpy also installs umap==0.5.2 (which does not work at least for me). . Many thanks! ; ![umap_0 5 1](https://user-images.githubusercontent.com/20926246/148250255-5ac00a46-cbc9-4608-a893-0472e32f5fb5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077:137,install,installed,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005863077,4,['install'],"['install', 'installed', 'installing', 'installs']"
Deployability,"Okay, it’s merged! @taopeng1100, please install the dev version of scanpy like this, and retry:. ```bash; pip install git+https://github.com/theislab/scanpy.git; # or; pip install --user git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278:40,install,install,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278,3,['install'],['install']
Deployability,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320:55,install,installation,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320,3,['install'],"['install', 'installation']"
Deployability,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:74,update,updated,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138,1,['update'],['updated']
Deployability,"Once https://github.com/lmcinnes/pynndescent/issues/241 is released, we should be able to get numpy 2 compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3065:59,release,released,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3065,1,['release'],['released']
Deployability,One idea: ; 1) Cluster the graph with leiden; 2) Coarsen the graph (collapse cells in a single cluster into super nodes); 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color; 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984:326,patch,patches,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984,1,['patch'],['patches']
Deployability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:62,install,install,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,3,"['install', 'integrat']","['install', 'installation', 'integration']"
Deployability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,1,['integrat'],['integrated']
Deployability,"One question related to #891, is there any plotting order for continuous values like higher expression plotted on top? That's more controversial but sometimes dropout et al might obscure cells expressing a gene. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/893#issuecomment-546341102:62,continuous,continuous,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546341102,1,['continuous'],['continuous']
Deployability,"Oops, forgot to ask for a release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933:26,release,release,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933,1,['release'],['release']
Deployability,Or you install the development version of scanpy: `pip install git+https://github.com/scverse/scanpy.git`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607290916:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607290916,2,['install'],['install']
Deployability,"Out of experience, I don't really feel like conda-forge is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:901,release,release,901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['release'],['release']
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5c0e89e99dc2461c654c549435a73f547f3573ce; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3339: Add PYI lints'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3339-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3339 on branch 1.10.x (Add PYI lints)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5d5d873b1fb0353089569f85580b43437df9c6cd; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3104: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3104-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3104 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 8d046ff37e024ae88eadfb22ea8fd142a6b95aa1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3093: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3093-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3093 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 05dcf68f32ce255447ea804de55babefb3c47c92; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2753: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2753-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2753 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 330a099ffe76286f0f047387701af7e9fd58831a; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2838: Fix pytest 8 compat'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2838-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2838 on branch 1.9.x (Fix pytest 8 compat)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036,6,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 47664d83a7bc47756356b907e5719076ab187361; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2784: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2784-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2784 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 4f4b1c3a655546d981360bcce625d354a4291385; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2811: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2811-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2811 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 585f58c9e4dd82dd7809a831538c4e230b008818; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2841: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2841-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2841 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5ccce795b19a5aa59a6b1f1c3552884ed6fc94d1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2544: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2544-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2544 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 86dc4d5d96eb7547833e7805ea2f7d603bd3ba2d; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2779: Fix anndata warnings'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2779-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2779 on branch 1.9.x (Fix anndata warnings)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 95206dc54c8bb0d9d478f09f47dff9477a5c58c4; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2704: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2704-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2704 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 b23229f9bfc95ff90a5d6393b4d53d062190d5bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2732: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2732-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2732 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 bf5f27aa9e968de6e73fc7abb46a89084ddf6880; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2831: Prepare 1.9.8, stop ignoring citation errors'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2831-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2831 on branch 1.9.x (Prepare 1.9.8, stop ignoring citation errors)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c2f706b35d52a5e21ccf84f1cd299b0dadf49668; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2716: Add missing link targets'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2716-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2716 on branch 1.9.x (Add missing link targets)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c410cd123f5487f25c08b421c8d06da50551ff73; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2799: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2799-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2799 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 e5d41d4aa58a925f0fa5cfcf580cb975167a71c9; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2235: Separate test utils from tests'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2235-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2235 on branch 1.9.x (Separate test utils from tests)"". And apply the correct labels and milestones. Congratulations — you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870:157,update,update,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870,3,"['continuous', 'integrat', 'update']","['continuous', 'integration', 'update']"
Deployability,"PI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cached_property 1.5.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.09.1; dateutil 2.8.2; debugpy 1.5.0; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fsspec 2021.10.0; google NA; h5py 3.4.0; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.2; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; mudata 0.1.0; muon 0.1.1; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 2.0.10; psutil 5.9.0; ptyprocess 0.7.0; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pynndescent 0.5.4; pyparsing 2.4.7; pyreadr 0.4.4; pytz 2021.3; scipy 1.7.1; seaborn 0.11.2; six 1.16.0; sklearn 1.0; sphinxcontrib NA; statsmodels 0.13.0; storemagic NA; tables 3.6.1; threadpoolctl 3.0.0; tlz 0.11.1; toolz 0.11.1; tornado 6.1; tqdm 4.62.3; traitlets 5.1.0; typing_extensions NA; umap 0.5.1; wcwidth 0.2.5; yaml 5.4.1; zipp NA; zmq 22.3.0; -----; IPython 7.28.0; jupyter_client 7.0.6; jupyter_core 4.8.1; notebook 6.4.4; -----; Python 3.7.2 (default, Dec 29 2018, 06:19:36) [GCC 7.3.0]; Linux-4.15.0-70-generic-x86_64-with-debian-buster-sid; 4 logical CPU cores, x86_64; -----; Session information updated at 2022-01-25 10:12. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2122:4613,update,updated,4613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2122,1,['update'],['updated']
Deployability,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358#issuecomment-438006625:96,update,update,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358#issuecomment-438006625,1,['update'],['update']
Deployability,PS: I updated the [seurat-based tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) and added a few of your plotting functions and a link to your gist. Feel free to update it further!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737:6,update,updated,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-420689737,2,['update'],"['update', 'updated']"
Deployability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:55,install,installing,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['install'],['installing']
Deployability,PS: `pbmc68k_reduced` and `toggleswitch` are from back in the days; they should remain the only examples that actually have the data in the repository and the PyPI distributions. All other datasets should download their data from some stable URL...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476589204:27,toggle,toggleswitch,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476589204,1,['toggle'],['toggleswitch']
Deployability,"Palantir is not yet in a released version: https://scanpy.readthedocs.io/en/latest/#on-master-march-21-2019. We'll soon have 1.4.1. Until then, you need to install from GitHub as described in the installation section. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-477679626:25,release,released,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-477679626,3,"['install', 'release']","['install', 'installation', 'released']"
Deployability,"Part of [scanpy 2.0](https://github.com/orgs/scverse/projects/18/views/1). <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. I implemented a Ruff check (PLR0917) for this, but setting `max-positional-args = 3` would make this massive PR even larger, so I opted to do it in a separate one. ## Reviewers. Your main job is to check if the position of the `*` makes sense for each exported function (i.e. the ones with the `@legacy_api` decorator). I tried my best to base it on internal usage of each API, but one placement or the other might be to early. The only real logic changes are in `scanpy/tests/test_package_structure.py`. This PR:. - makes public APIs with more than 5 parameters keyword-only without breaking backwards compatibility (for now); - makes private APIs with more than 5 parameters keyword-only; - checks that we don’t internally use the old positional APIs using a warning filter with `action='error'`; - checks that APIs use our convention for the `copy` parameter (`adata` as first param, returns `adata` type or None`); - manually checked that APIs use our convention `filename: Path | str`/`path: Path | str`. ## Follow-up changes. - [ ] Set `max-positional-args = 3`; - [ ] make sure that no new public API gets introduced without being included in the `api_module_names` list; - [ ] enforce convention `show`, `return_fig`, `ax`; - [ ] enforce convention for `random_seed: AnyRandom`; - [ ] https://github.com/scverse/scanpy/issues/2331",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2702:260,release,release,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2702,2,"['Release', 'release']","['Release', 'release']"
Deployability,Patching scanpy for xeus,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667:0,Patch,Patching,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667,1,['Patch'],['Patching']
Deployability,Pca loadings n points patch,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075:22,patch,patch,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075,1,['patch'],['patch']
Deployability,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:24,update,updated,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788,1,['update'],['updated']
Deployability,"Ping @flying-sheep @giovp @Mirkazemi. Turns out that the `sc.datasets.visium_sge` would just read in whatever images were most recently added, not the one that fit the dataset. This fixes that. Additionally, `sc.read_visium` now takes a directory as the first argument. If a reading function assumes a directory structure, that directory should be passed, not a specific path inside of it. This follows our other functions like: `sc.read_10x_mtx`. Similarly, I've rearranged how the example datasets are stored and how the test data is stored to better match the outputs from 10x pipelines. A few more changes I'd like to make:. * Restructure how elements are added to `uns`, as mentioned in https://github.com/theislab/anndata/issues/295#issuecomment-596164456; * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`.; * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088:580,pipeline,pipelines,580,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088,1,['pipeline'],['pipelines']
Deployability,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python; adata.X = adata.X.astype('<f8') # Make float64 to ensure stability; sc.tl.score_genes_cell_cycle(adata, use_raw=False,; s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,; random_state=0); adata.X = adata.X.astype('<f4') # Return to float32 for consistency; ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924:145,pipeline,pipeline,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313#issuecomment-849730924,1,['pipeline'],['pipeline']
Deployability,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:721,pipeline,pipeline,721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['pipeline'],['pipeline']
Deployability,Please ask usage questions here: https://discourse.scverse.org/. You should not integrate normalized and unnormalized counts. Consider getting the raw counts or integrating on the normalized counts,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652:80,integrat,integrate,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652,2,['integrat'],"['integrate', 'integrating']"
Deployability,Please install the latest scanpy in a clean environment. Feel free to reopen if this issue persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127,1,['install'],['install']
Deployability,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746:7,update,update,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746,2,"['release', 'update']","['release', 'update']"
Deployability,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:74,install,install,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746,2,['install'],['install']
Deployability,"Potentially fixes #1355. * Would still need tests/ further consideration.; * Need to fix missing values not being plotted below present ones. Using this branch:. ```python; import scanpy as sc; import numpy as np; import matplotlib as mpl. pbmc = sc.datasets.pbmc3k_processed(); pbmc.obs[""louvain""].iloc[::2] = np.nan; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain""); ```. ![image](https://user-images.githubusercontent.com/8238804/89258138-e54b0d80-d66a-11ea-8d13-e7bf975c3203.png). ```python; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain"", groups=list(pbmc.obs[""louvain""].cat.categories[:3])); ```. ![image](https://user-images.githubusercontent.com/8238804/89258165-f6941a00-d66a-11ea-9eaf-3a51a5a49a38.png). ## Update:. This PR expanded in scope quite a bit, so I'd like to wrap it up. Most things are implemented and seem to work. Regression tests need to be added, for these cases. - [x] Tests for all fixed cases (probably write these down as well); - [x] Decide on adding arguments, and default values; - [x] Decide on whether continuous legend update happens in this PR",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356:774,Update,Update,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356,3,"['Update', 'continuous', 'update']","['Update', 'continuous', 'update']"
Deployability,Pre-release jobs fail because scipy removed `.A` from csr/csc matrices,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3083:4,release,release,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3083,1,['release'],['release']
Deployability,Prep 1.8.2 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2037:11,release,release,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2037,1,['release'],['release']
Deployability,Prep release again,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1586:5,release,release,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1586,1,['release'],['release']
Deployability,Prep release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2196:5,release,release,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2196,1,['release'],['release']
Deployability,Preparations for UMAP 0.4 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779:26,release,release,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779,2,['release'],['release']
Deployability,Prepare 1.9.4 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2639:14,release,release,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2639,1,['release'],['release']
Deployability,Prepare 1.9.6 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2724:14,release,release,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2724,1,['release'],['release']
Deployability,Prepare 1.9.7 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2826:14,release,release,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2826,1,['release'],['release']
Deployability,"Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/960:8,release,release,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960,1,['release'],['release']
Deployability,Prepping for 1.8.1 bugfix release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1937:26,release,release,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1937,1,['release'],['release']
Deployability,Prepping release notes for 1.9.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2196:9,release,release,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2196,1,['release'],['release']
Deployability,Prepping the 1.8.2 release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2037:19,release,release,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2037,1,['release'],['release']
Deployability,Problem with 3D UMAP after package updates,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663:35,update,updates,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663,1,['update'],['updates']
Deployability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:733,release,releases,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['release'],['releases']
Deployability,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:22,release,released,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,1,['release'],['released']
Deployability,"Pytables is in requirements.txt (the PyPI package is called “tables”), how did y’all get Scanpy installed without all its dependencies?. https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198:96,install,installed,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198,1,['install'],['installed']
Deployability,"Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.ne",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:13199,Pipeline,PipelineDevelope,13199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4531,Install,Installing,4531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,Qc metrics update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:11,update,update,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,1,['update'],['update']
Deployability,Question: plans for scATAC integration with scRNA?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/725:27,integrat,integration,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725,1,['integrat'],['integration']
Deployability,"Quite possibly, I need to look. Just updated from 1.3.x to 1.4.3 this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-493234847:37,update,updated,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-493234847,1,['update'],['updated']
Deployability,"Ran into issues making release (PyPi didn't like the upload), trying again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1586:23,release,release,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1586,1,['release'],['release']
Deployability,"Ran into this today as a coworker wanted to stick a UMAP legend into `lower left` and couldn't. I whipped up a hotfix, which turned out to be exactly the same syntax as you've got going on here, and was on my way to mention it when I found this. Would be nice to see this integrated as the docs imply this is possible, and it seems useful to have on occasion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426:111,hotfix,hotfix,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426,2,"['hotfix', 'integrat']","['hotfix', 'integrated']"
Deployability,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:251,install,installing,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897,4,"['install', 'release']","['installed', 'installing', 'releases']"
Deployability,Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:124,update,update,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['update'],['update']
Deployability,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682,1,['install'],['install']
Deployability,"Really awesome changes! Now things are nicely tiled along the same grid independent of whether categorical or continuous annotation is plotted. :smile:. ![image](https://user-images.githubusercontent.com/16916678/46213585-e8567380-c306-11e8-9bdf-eb38d3410c3b.png). I added a docstring for `panels_per_row`; maybe one should simply call it `ncols` as in `matplotlib.GridSpec`? Essentially, in scanpy, sklearn and many other packages all things that are integer numebers are called `nsomething` or `n_something`. I'd merge immediately, things seem to work perfectly now. Just one tiny cosmetic thing; for these scatter plots, don't you think it would be nice to have them be a perfect square? As there is no meaningful scales on x and y axis? It gave me a bit of a headache when I first wrote it (couldn't make it work with GridSpec, hence all the mess that you encountered)... You're new clean code is definitely more important than this cosmetic thing, but if you have a quick solution, that's the last thing I can think of... ; ![image](https://user-images.githubusercontent.com/16916678/46212751-b9d79900-c304-11e8-9511-3e19559e8e83.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932:110,continuous,continuous,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425450932,1,['continuous'],['continuous']
Deployability,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56:11,install,installed,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56,1,['install'],['installed']
Deployability,"Recently, this tutorial was updated with what you need:; https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html. You can find the link to that tutorial from; https://scanpy.readthedocs.io/en/latest/. On Fri, Mar 15, 2019 at 3:34 PM jiawen wang <notifications@github.com>; wrote:. > Thanks a lot. All of these new features are what we need!; >; > I notice that the tutorial has not been updated yet (such as; > sc.tl.filter_rank_genes_groups( ) and rna velocity function in; > https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find; > these features occasionally. Could you add them in scanpy tutorial ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/425#issuecomment-473309434>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1S565VMCgOnXCQetV2R6_A1HONPZks5vW69qgaJpZM4Z-M3d>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-474006729:28,update,updated,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-474006729,2,['update'],['updated']
Deployability,"Ref https://github.com/theislab/scanpy/issues/1698#issuecomment-824634639, cut down in https://github.com/theislab/scanpy/issues/1698#issuecomment-826712541. Basically, `gearys_c`, `morans_i` can run into silent numba parallelization errors when all values for a variable are the same. We should specifically address these cases. As suggested by @Hrovatin, `nan` and a warning is probably sufficient for this case. That said, it may take some work to consistently throw a warning. * Depending on the threading backend, this may error (solving the problem of incorrect values); * We may be able to control this, but how the parallel backend interacts with the `error_model` seems unclear; * I don't think we can throw a warning from numba code, let alone parallel numba code; * It would be nice if we could guarantee a `tbb` or `omp` threading backend, since they seem more consistent, but I personally have not been able to `pip install` these for a bit over a year now. This may be macOS specific.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806:929,install,install,929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806,1,['install'],['install']
Deployability,"Regarding Q3 from my previous comment, I tried few things and I think it is the easiest to keep the coexpression data as continuous and remove the colorbar afterwards. . I have, however, correction to what what was written before. `ax.images.im[-1].colorbar.remove()` doesn't work (in the case of umap) since it is a scatter plot. `ax.collections[-1].colorbar.remove()` needs to be used instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-588981048:121,continuous,continuous,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-588981048,1,['continuous'],['continuous']
Deployability,"Regarding running CI with minimal optional deps, I’d say we could change this line:. https://github.com/scverse/scanpy/blob/86e2a35c1df2b61772e5f898bfcd11abb8d9fb2c/.azure-pipelines.yml#L46. … to be parametric like `pip install .[dev,test$(test_extras))]`, and run things once with `test_extras=''` and once with `test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'`. we’d probably have to make a lot of tests optional with `@skipif(not find_spec('thing'), ...)` though, and of course remove some things from the `test` extra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088710180:172,pipeline,pipelines,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088710180,2,"['install', 'pipeline']","['install', 'pipelines']"
Deployability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:277,install,installed,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,1,['install'],['installed']
Deployability,Release date for next version?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1365:0,Release,Release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1365,1,['Release'],['Release']
Deployability,Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1623:0,Release,Release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623,3,"['Release', 'release', 'update']","['Release', 'release', 'update']"
Deployability,Release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/951:0,Release,Release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/951,1,['Release'],['Release']
Deployability,Release notes reorganization,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1628:0,Release,Release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628,1,['Release'],['Release']
Deployability,"Remove batch effect (""Integrate"" in Seurat"")",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873:22,Integrat,Integrate,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873,1,['Integrat'],['Integrate']
Deployability,"Removes the need for a pytables dependency. * pytables has been a source of installation heisenbugs, particularly on windows (#1468, #1284, #454); * why use two hdf5 libraries; * Makes it easier to move reading 10x files into anndata or elsewhere #1387. I've edited the code as lightly as possible, since these readers were originally contributed by someone at 10X, so I assume they had better knowledge of possible edge cases. - [ ] ~~Test with h5py 2~~; - [ ] Add release note",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064:76,install,installation,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064,2,"['install', 'release']","['installation', 'release']"
Deployability,"Reproducible example. ```py; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.pca(adata); sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False)`; ```. ```pytb; TypeError Traceback (most recent call last); <ipython-input-231-50baef9a10a9> in <module>; 5 pbmc = sc.datasets.pbmc68k_reduced(); 6 sc.pp.pca(adata); ----> 7 sce.pp.bbknn(adata, batch_key='batch', copy=False,trim=0, save_knn=False). ~/Library/Python/3.7/lib/python/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 except ImportError:; 83 raise ImportError('Please install bbknn: `pip install bbknn`.'); ---> 84 return bbknn(**params, **kwargs). ~/Bioinformatics/usr/local/lib/python3.7/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, approx, metric, copy, **kwargs); 281 ; 282 	logg.info('	finished', time=start,; --> 283 		deep=('added to `.uns[\'neighbors\']`\n'; 284 ' \'distances\', weighted adjacency matrix\n'; 285 		'	\'connectivities\', weighted adjacency matrix')). ~/Library/Python/3.7/lib/python/site-packages/scanpy/logging.py in info(*args, **kwargs); 17 ; 18 def info(*args, **kwargs):; ---> 19 return msg(*args, v='info', **kwargs); 20 ; 21 . TypeError: msg() got an unexpected keyword argument 'deep'`; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/770:674,install,install,674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770,2,['install'],['install']
Deployability,Restricting umap to below `0.5` so we can fix changes before it's released (see #1509).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1543:66,release,released,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1543,1,['release'],['released']
Deployability,"Revert ""Update external page""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2415:8,Update,Update,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2415,1,['Update'],['Update']
Deployability,"Right after booting up my Docker container, this is the result of `conda list | grep anndata`:. `anndata 0.8.0 pyhd8ed1ab_1 conda-forge`. Then, after running `pip install anndata --upgrade`, and then again `conda list | grep anndata`:. `anndata 0.10.6 pypi_0 pypi`. At this point, `pkg_version('anndata')` errors out as described.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037957505:163,install,install,163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037957505,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Right now, figures for tutorials and documentation have to be manually generated when we make a release. This often leads to out of date figures, and a fair amount of pain for otherwise small style changes. It would be good if this could be part of an automatic build process. For the sphinx docs, we could probably do this on the readthedocs servers since we have extra capacity. For notebooks, this could take some consideration. Some notebooks deal with large data that we don't want to reprocess frequently. A good step could just be some simple scripting around automatically running the notebooks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357:96,release,release,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357,1,['release'],['release']
Deployability,Run ci on release branch,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2834:10,release,release,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2834,1,['release'],['release']
Deployability,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/811:123,release,released,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811,1,['release'],['released']
Deployability,"Running sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3') produces scikit-misc error; package not installable with either pip or conda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:121,install,installable,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['install'],['installable']
Deployability,SAM: Updated default parameters to suit large datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540:5,Update,Updated,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540,1,['Update'],['Updated']
Deployability,"Same error here...any ideas?. ```; -----; anndata 0.8.0; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 9.0.1; PyQt5 NA; anndata 0.8.0; anndata2ri 0.0.0; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; backports NA; beta_ufunc NA; binom_ufunc NA; bs4 4.10.0; cached_property 1.5.2; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.02.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.10.0; entrypoints 0.4; fsspec 2022.02.0; get_version 3.5.4; h5py 3.6.0; igraph 0.9.9; ipykernel 6.9.1; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; packaging 21.3; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.7; pytz 2021.3; pytz_deprecation_shim NA; rpy2 3.4.2; scanpy 1.8.2; scipy 1.7.3; seaborn 0.11.2; setuptools 59.8.0; sinfo 0.3.1; sip NA; six 1.16.0; sklearn 1.0.2; soupsieve 2.3.1; sphinxcontrib NA; spyder 5.2.2; spyder_kernels 2.2.1; spydercustomize NA; statsmodels 0.13.2; storemagic NA; tables 3.7.0; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.1.1; typing_extensions NA; tzlocal NA; wcwidth 0.2.5; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 7.32.0; jupyter_client 7.1.2; jupyter_core 4.9.2; -----; Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) [GCC 9.4.0]; Linux-5.4.0-109-generic-x86_64-with-debian-bullseye-sid; 16 logical CPU cores, x86_64; -----; Session information updated at 2022-04-20 18:16; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1104127300:1933,update,updated,1933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1104127300,1,['update'],['updated']
Deployability,Same here. I would like to run Scanpy on the latest Python version. Does anyone have an ETA or a way to force installation (at our own risk of course)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1327933295:110,install,installation,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1327933295,1,['install'],['installation']
Deployability,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda?. Logs:. ```; [dilawars@chamcham scanpy_exp]$ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:53.98); saving figure to file ./figures/tsne_full.pdf; computing neighbors; using data matrix X directly; Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518:57,install,install,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427357518,2,['install'],"['install', 'installing']"
Deployability,"Same issue with OSX python 3.7, solved simply with `conda install pytables`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014:58,install,install,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462042014,1,['install'],['install']
Deployability,"Same! Seems like -maxiter gets set/clobbered to 1. I'm seeing it on one machine (which I have limited access too, its a galaxy installation using scanpy scripts) but not another (my local), both of which are apparently running scanpy 1.8.1. . Im wondering if there's a umap-learn version issue? In order to set the umap n_epochs(aka maxiter) default , it looks like older versions of umap-learn expected 0 (e.g. https://github.com/lmcinnes/umap/blob/0.5.0/umap/umap_.py), whereas the newer expect None. My working installation has umap-learn 0.5.2 (which seems to expect None), and I'm not sure about the one on the other server. Might be barking up the wrong tree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544:127,install,installation,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2337#issuecomment-1371840544,2,['install'],['installation']
Deployability,"Scanorama's `nn_approx` uses annoy, which is a package that has caused many a headache due to its instability. From my experience, these segfaults started showing up since 1.17.x got released, and tend to begin happening more consistently if anything is installed into the environment after annoy itself somehow. Downgrading to 1.16.3 via pip tends to make them go away. It would be neat if there was some sort of more reliable workaround, keeping annoy stable and not bothering the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866#issuecomment-1976231395:183,release,released,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866#issuecomment-1976231395,2,"['install', 'release']","['installed', 'released']"
Deployability,"Scanpy analysis for a combined species. I have tried following the normal pipeline to work with the data, but it fails to find MT/mt genes in single cell dataset aligned to both mouse & human genes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2393:74,pipeline,pipeline,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2393,1,['pipeline'],['pipeline']
Deployability,Scanpy cant't installed in two environment in anaconda,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1136:14,install,installed,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136,1,['install'],['installed']
Deployability,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:258,continuous,continuous,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721,1,['continuous'],['continuous']
Deployability,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/405:18,install,installed,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405,2,"['install', 'update']","['installed', 'update']"
Deployability,"See https://github.com/scverse/anndata/pull/1346. Also. - add anchors to headers for stable links; - fill in missing release notes for the releases 0.4.4, 1.3.7, and 1.3.8",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3172:117,release,release,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3172,2,['release'],"['release', 'releases']"
Deployability,"See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3239:134,release,release,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239,4,['release'],['release']
Deployability,"See my last comment. After fixing the colormaps in this PR, I didn’t update the images, but the tests still pass. What’s up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449:69,update,update,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441571449,1,['update'],['update']
Deployability,Seems like a release is in order. that code was merged in April,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651773726:13,release,release,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565#issuecomment-1651773726,1,['release'],['release']
Deployability,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:144,release,released,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294,1,['release'],['released']
Deployability,"Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598:190,install,installing,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598,1,['install'],['installing']
Deployability,"Sergei (@Koncopd) tested it out and will get back to you. He also found a peak memory usage of 121 GB. I have to admit that I never made checks with that degree of detail and I fear that for now, I'll simply update the documentation stating that peak memory usage can go up to ~120 GB. I'm still puzzled by that, and maybe some efficiency found it's way into the code which wasn't there (simple guess: is everything in `float32`?). But we'll need some time to work it out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466:208,update,update,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470050466,1,['update'],['update']
Deployability,Set 1.7.2 release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1785:10,release,release,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1785,1,['release'],['release']
Deployability,Set up Azure Pipelines with initial configuration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516:13,Pipeline,Pipelines,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516,2,"['Pipeline', 'configurat']","['Pipelines', 'configuration']"
Deployability,"Should be fixed as of `1.7.2`, which was just released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904:46,release,released,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904,1,['release'],['released']
Deployability,"Should be fixed in scipy 1.11.3: https://github.com/scipy/scipy/issues/18716. You only have 1.10.1 installed. Please try upgrading. If the error persists, feel free to follow up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3141#issuecomment-2210675845:99,install,installed,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3141#issuecomment-2210675845,1,['install'],['installed']
Deployability,Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1620:88,pipeline,pipelines,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,Shouldn’t we just depend on `requests` if it’s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:145,install,installed,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642,1,['install'],['installed']
Deployability,Should’ve checked the docs for installation. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-477692840:31,install,installation,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-477692840,1,['install'],['installation']
Deployability,"Since I saw this just linked to another issue (#2540), I'll post my updates and ways I got around this (since I had to re-investigate this last week for a separate script). We have not updated our Scanpy on our platform since I posted this ticket (though will soon), but I ran into another situation where the code worked on an older version of scanpy but not on a later version which was running on my machine. I just wrote a try/except block to handle this. ```python3; """"""; One of the scanpy versions introduced a bug that was recently fixed, ; where pl.rank_genes_groups works but not pl.rank_genes_groups_<plot>. ; I believe it is because adata.var.index is being stored as the ; adata.uns ""gene_symbol"" output for tl.rank_genes_groups, ; and pl.rank_genes_groups correctly looks for the adata.var.index, ; but pl.rank_genes_groups_<plot> is erroneously looking for the adata.var.gene_symbol there instead (per my supplied ""gene_symbols"" arg). Seems to work in scanpy 1.7.2 but is broke in 1.8.2; """""". ax = sc.pl.rank_genes_groups(adata, groups=[query_cluster],; gene_symbols='gene_symbol', n_genes=n_genes, save=""_comp_ranked.png""). try:; # Try 1.7.2 way first; ax = sc.pl.rank_genes_groups_violin(adata, groups=query_cluster, use_raw = False,; gene_symbols=""gene_symbol"", n_genes=n_genes, save=""_comp_violin.png""); except:; # Use gene names if that doesn't work; gene_names = adata.var.loc[adata.uns[""rank_genes_groups""]['names'][query_cluster]][""gene_symbol""][:n_genes].tolist(); ax = sc.pl.rank_genes_groups_violin(adata, groups=query_cluster, use_raw = False,; gene_symbols=""gene_symbol"", gene_names=gene_names, save=""_comp_violin.png""). ```. Earlier, I had mentioned using `use_raw=True` for the previous script I had this issue with, but ultimately removed that parameter due to some other downstream things",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2258#issuecomment-1625573706:68,update,updates,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2258#issuecomment-1625573706,2,['update'],"['updated', 'updates']"
Deployability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:19,update,updated,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['update'],['updated']
Deployability,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865:49,update,updated,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865,1,['update'],['updated']
Deployability,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:27,release,release,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133,3,['release'],['release']
Deployability,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715,8,"['Install', 'install', 'update', 'upgrade']","['Installation', 'install', 'installation', 'update', 'upgrade']"
Deployability,"So actually, I run a test on a fresh docker image (with this [Dockerfile](https://gist.github.com/pwl/005c781cbe19f5e961b59366f738caaf)) and it still fails to install scanpy with the same error. I had some success with changing the default python encoding to utf-8 as shown in the Dockerfile but it only works when calling python3 directly and not for pip3. However, it worked with python2. I guess python3 is not supported by scanpy, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579:159,install,install,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343252579,1,['install'],['install']
Deployability,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:65,continuous,continuous,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247,2,['continuous'],['continuous']
Deployability,So it seems ComBat outputs np.float64 😐. I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/324#issuecomment-433441386:110,update,update,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433441386,1,['update'],['update']
Deployability,So my X actually contained negative values. I removed my _scanpy.pp.scale_ step and tried this downsampling step earlier in my pipeline and its working. Thanks for taking time to help with this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2131#issuecomment-1033885282:127,pipeline,pipeline,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2131#issuecomment-1033885282,1,['pipeline'],['pipeline']
Deployability,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:21,install,installed,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310,1,['install'],['installed']
Deployability,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768,2,"['continuous', 'update']","['continuous', 'update']"
Deployability,"So the issue is incompatibilities between versions of sphinx and their `objects.inv`? Is there an open issue in sphinx for this?. > > Do you expect this to be compatible with older sphinx versions?; >; > Of course! Why would it not be?. Mostly because the inner workings of Sphinx are a mystery to me, so I have no idea what features your changes rely on 😆. This was mainly me asking if we should bump the minimum version of sphinx allowed. Maybe we should if there are issues with the `objects.inv`s?. > Maybe we can link to the dev docs?. Or we could add the classes to nitpick ignore? Then once the docs are rebuilt it will do the right thing without any intervention, and we don't have to be keeping an eye out for this. My main concern here is that the `scipy.github.io` address may not be permanent, similar to how numpy temporarily used a github.io address while they revamped their docs. Basically, it may just break or go down without notice. I thought we could even just trigger a new build of the anndata stable docs, but there's an issue there, probably to do with sphinx not being pinned on release. I do want to make a new anndata release soon-ish though. -----------. I'm happy for you to pick one of the approaches and merge it. AnnData could also use a fix for this, I've temporarily just pinned sphinx below 4.1 there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295:1104,release,release,1104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295,2,['release'],['release']
Deployability,"So this is possibly related to #1136 (pure speculation 😅 ). Basically, on a Vm with ubuntu 18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:201,install,install,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,2,['install'],"['install', 'installed']"
Deployability,"So we can’t forget adding them, thereby ensuring bugfixes make it into the next bugfix release (including CI fixes)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2518:87,release,release,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2518,1,['release'],['release']
Deployability,"So we're really not accepting any packages into scanpy.external anymore and will deprecate external soon. We've now updated our documentation to reflect this. However, we'd be very happy to welcome your package in the scverse ecosystem -> https://scverse.org/packages/#ecosystem. I'm sorry that you put in all this work but then get denied by us like this :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355#issuecomment-1431674978:116,update,updated,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355#issuecomment-1431674978,1,['update'],['updated']
Deployability,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon.; I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261,2,['release'],['release']
Deployability,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797,2,['release'],['release']
Deployability,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:60,continuous,continuous,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['continuous'],['continuous']
Deployability,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303,1,['install'],['install']
Deployability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:464,Integrat,Integration,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,"So, this is solved in anndata 0.5 and scanpy 0.4.3. See the release notes (https://scanpy.readthedocs.io) and https://github.com/theislab/anndata/commit/63500075e926f202e856bd04ec673df55bbd2460 and the [example](http://anndata.readthedocs.io/en/latest/anndata.AnnData.concatenate.html). Hope this is a meaningful default. If you pass `index_unique=None`, then it keeps the previous indices including duplicates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-364922530:60,release,release,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-364922530,1,['release'],['release']
Deployability,"Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1580:108,update,update,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580,1,['update'],['update']
Deployability,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:306,update,update,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225,1,['update'],['update']
Deployability,"Some of the preprocessing recipes have a `plot` argument, but as far as I can tell, they'll only throw an error. `recipe_zheng17` and `recipe_seurat` have the lines:. ```python; if plot:; from .. import plotting as pl # should not import at the top of the file; pl.filter_genes_dispersion(filter_result, log=True); ```. But `plotting` doesn't have the function `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/153:495,release,release,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153,1,['release'],['release']
Deployability,"Some tests are broken, here are some fixes for that. * Visium reference test was just wrong, dots should have been transparent.; * igraph 0.9.11 has different results for `""fr""` layout, updated tests accordingly (https://github.com/igraph/python-igraph/issues/545); * 32 bit PCA computation is now accurate within 32 bit rounding, so tests checking for random seed changes have been disabled",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2274:186,update,updated,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2274,1,['update'],['updated']
Deployability,Some update has flipped the plots…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1037:5,update,update,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1037,1,['update'],['update']
Deployability,"Sorry about that bug and thanks for reporting it. It only occurred with the `copy` parameter set, which is why no one noticed it till now. The bug is fixed: https://github.com/theislab/scanpy/commit/f6a41f140a646c350ab12d8bd6aeff7499df069e. The docs are updated, there's now an example: http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. I wrote a test that checks that this doesn't break again in the future: https://github.com/theislab/scanpy/blob/f6a41f140a646c350ab12d8bd6aeff7499df069e/scanpy/tests/preprocessing.py#L11-L31. There will be a new release 0.3 with many improvements tomorrow. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517:254,update,updated,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/47#issuecomment-344400517,2,"['release', 'update']","['release', 'updated']"
Deployability,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:744,release,release,744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,1,['release'],['release']
Deployability,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:55,release,release,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['release'],['release']
Deployability,Sorry about this and thanks for pointing it out! I'm currently intensively working on the revision of the method... a lot will become better. What you mention probably got lost on the way. I'm hoping to release a new version within a week.; Alex,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/110#issuecomment-376174066:203,release,release,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/110#issuecomment-376174066,1,['release'],['release']
Deployability,"Sorry about this bug in AnnData views, which have only recently been introduced. Is fixed in anndata 0.4.4 `pip install anndata --upgrade` and on the master branch: https://github.com/theislab/anndata/commit/ba9b3eed381ce427920ec67e13331d5423a5d9b3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449:112,install,install,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62#issuecomment-355731449,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,integrat,integration,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857,1,['integrat'],['integration']
Deployability,"Sorry for being offline for a few hours and great that you could resolve it. In the next version, this confusion will not appear again. Even though, obviously, any Scanpy release reproduces all examples on https://github.com/theislab/scanpy_usage, we're still at an early stage in the package. Things are progressing very fast and structure and maintenance of the package are becoming more and more professional. Also, soon, many new features and examples where other Python packages are used will be added. Thank you for a bit of patience at this stage.; Alex. PS: We now have an initial version of the documentation: https://scanpy.readthedocs.io.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985:171,release,release,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324638985,1,['release'],['release']
Deployability,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again?; For me it seems to work; ```python; fig, ax = plt.subplots(1,3, figsize=(20,6)); sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); plt.tight_layout(pad=3.0); plt.show(); ```; ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787:65,update,update,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787,1,['update'],['update']
Deployability,"Sorry for that, fixed this bug. I think there is no way to make ; ```; pip install git+https://github.com/theislab/scanpy.git; ```; work if you want to install using symbolic links (to your local clone of the git repo). Until versioning starts, it's good to be able to type `git pull` in the local clone and by that automatically update the `scanpy` installation without having to do anything else. But you're right, this is should just be a temporary solution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075:75,install,install,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284355075,4,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:31,upgrade,upgraded,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575,2,['upgrade'],['upgraded']
Deployability,"Sorry for the late reply, thought I responded to this!. > would also make sense to have this as colorbar_loc as this only really applies for continuous coloring. I think that would make sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199:141,continuous,continuous,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199,1,['continuous'],['continuous']
Deployability,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620:212,install,installation,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437067620,1,['install'],['installation']
Deployability,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572:133,install,installed,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305#issuecomment-430195572,1,['install'],['installed']
Deployability,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:108,update,updated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549,1,['update'],['updated']
Deployability,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:222,install,installation,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243,4,['install'],"['install', 'installation', 'installing']"
Deployability,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,configurat,configuration,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773,2,"['configurat', 'upgrade']","['configuration', 'upgrade']"
Deployability,Sounds good - added an entry to the release note and updated the tests to not use the parameterization.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767,2,"['release', 'update']","['release', 'updated']"
Deployability,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:126,install,installable,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478,2,['install'],['installable']
Deployability,Sounds great to me! Looking forward. Would be interesting to compare these three t-SNEs: ; ```; sc.pp.neighbors(); sc.tl.tsne(binarize=True); sc.tl.tsne(binarize=False); sc.pp.neighbors_tsne(); sc.tl.tsne(binarize=False); ```; on a couple of datasets after the standard scanpy preprocessing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594:291,pipeline,pipeline,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594,1,['pipeline'],['pipeline']
Deployability,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:42,install,install,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560,4,['install'],"['install', 'installing']"
Deployability,Start 1.8.1 release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1907:12,release,release,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1907,1,['release'],['release']
Deployability,Start 1.8.2 release notes file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1945:12,release,release,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1945,1,['release'],['release']
Deployability,Start release notes for 1.9.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1906:6,release,release,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1906,1,['release'],['release']
Deployability,"Supersedes #1540, just adding a release note. (I didn't want to push to your master branch @atarashansky)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1595:32,release,release,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595,1,['release'],['release']
Deployability,"Sure! I wansn't sure if there were other bugs to fix or PRs to merge before we wanted to make a release. I'd also like to get @fidelram to give this a look over. I think I didn't break anything, but he'd be in a better position to tell if that were the case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-509060284:96,release,release,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-509060284,1,['release'],['release']
Deployability,"Sure, by all means, open a PR at https://github.com/ebi-gene-expression-group/scanpy-scripts with the directory reformatting and needed metadata files for pip installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437046217:159,install,installation,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437046217,1,['install'],['installation']
Deployability,"Sure, thank you! Care to do a quick PR? Then we can point @taopeng1100 in the direction of installing scanpy’s dev version and everyone’s happy. @taopeng1100 please reply by GitHub comment and not by email anymore, it spams up this comment section. I always have to remove some junk your email program adds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281:91,install,installing,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281,1,['install'],['installing']
Deployability,Switch from using rubric in release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3172:28,release,release,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3172,1,['release'],['release']
Deployability,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:120,UPDATE,UPDATE,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,2,"['UPDATE', 'integrat']","['UPDATE', 'integration-scanorama']"
Deployability,Tansfering data integration fom scanorama to a new dataset,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162:16,integrat,integration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162,1,['integrat'],['integration']
Deployability,"Temporary fix for anyone looking in case an upgrade is not yet possible:; ```python; import scipy.spatial.distance as ssd; from contextlib import contextmanager. def squareform_force_zero_diagonal(X, *args, **kwargs):; if len(X.shape) == 2:; if isinstance(X, pd.DataFrame):; X.iloc[(np.arange(X.shape[0]), np.arange(X.shape[0]))] = 0; else:; X[(np.arange(X.shape[0]), np.arange(X.shape[0]))] = 0; return _squareform(X, *args, **kwargs). @contextmanager; def patch_squareform():; _squareform = ssd.squareform; ssd.squareform = squareform_force_zero_diagonal; try:; yield; finally:; ssd.squareform = _squareform. import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); with patch_squareform():; sc.tl.dendrogram(adata, groupby='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2125#issuecomment-1042300588:44,upgrade,upgrade,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2125#issuecomment-1042300588,1,['upgrade'],['upgrade']
Deployability,Test against pre-release dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:17,release,release,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['release'],['release']
Deployability,"Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings; * I'd like to have test coverage; * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)); * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1564:335,pipeline,pipelines,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564,1,['pipeline'],['pipelines']
Deployability,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284,1,['update'],['update']
Deployability,Tests were updated to include instances of the requirements.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/304:11,update,updated,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304,1,['update'],['updated']
Deployability,Thank you again! I'm merging this. Release prior to this is 1.3.4.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-443067468:35,Release,Release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-443067468,1,['Release'],['Release']
Deployability,"Thank you and sorry about the confusion, I remembered this was an option present in earlier releases, but I was wrong!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/560#issuecomment-476837808:92,release,releases,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560#issuecomment-476837808,1,['release'],['releases']
Deployability,"Thank you for all your thoughts! That's very interesting and helpful!. > although it would also make sense for log1p to be a class method, given that it only needs to exist for AnnData objects. Yes! I also think so. But then the question is which function makes into AnnData and which doesn't. Right now we only put functionality that is related to bookkeeping of the data into AnnData. Everything else remains out of it, even it's something as simple as `log1p`... but that's just a safeguard towards cluttering the object... I agree that it would be more convenient to have some of this in `AnnData`. I guess numpy went a similar way: not all of numpy's functions are available as `np.ndarray`'s class methods. > In such a library it's easy to switch between an in-place or copying workflow, to inspect intermediate output if desired. Interesting! I never thought of this. > This behavior is what numpy.log1p itself is doing here, for that matter–with an out argument it still returns the array. Yes! I think that's a good solution. The `out` argument is very verbose and allows setting a second name for the reference to the modified object, which is returned in addition. I thought about making `inplace` the default for Scanpy's function or not for a long time and finally decided for the unorthodox choice of making it the default - having in mind that AnnData's will become pretty large and at some point backed on disk (which hugely limits the possibilities of how you can write pipelines). Then the `out` rationale doesn't work anymore, as, by default, there simply is no second reference around... Again, thank you for your perspective. And, I'll merge this as soon as having figured out the `chunked` issue. Should be tomorrow or so...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076:1487,pipeline,pipelines,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403313076,1,['pipeline'],['pipelines']
Deployability,"Thank you for checking the code. I have also been thinking about possible test(s), the only thing that comes to my mind is to check the output against a fixed reference result, which I could verify on a few different machines. But I'm not sure whether this wouldn't cause problems in the future. This could be troublesome to maintain as the matrix will need be stored somewhere and may need re-checking if e.g. numpy has some relevant updates. I am open to suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349:435,update,updates,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349,1,['update'],['updates']
Deployability,"Thank you for creating `scanpy`! It's such a useful tool!. ## Overview of request; Is there any chance that you can start tagging versions of your documentation on readthedocs? Currently, the only available versions are **stable** and **latest**. That makes it easy to view the current documentation but difficult to view docs from old releases of `scanpy`. ## Desired behavior; I would like to be able to...; 1. Click on `v: stable` in the flyout menu at the bottom-left corner of the page; ![image](https://user-images.githubusercontent.com/23412689/220255767-3fbf84e9-ccf5-420b-b067-7a4054aed047.png); 2. Click on the version of the documentation tagged for the older release of `scanpy` that I'm using. ## Resources; - [How to version RTD documentation](https://docs.readthedocs.io/en/stable/versions.html); - [How to automatically maintain versions upon creating new git tags](https://docs.readthedocs.io/en/stable/automation-rules.html#activate-all-new-tags); - [Automated versioning](https://docs.readthedocs.io/en/stable/integrations.html#automated-versioning) as part of continuous documentation deployment. ## An example; As an example of versioned documentation, take a look at [Snakemake's documentation](https://snakemake.readthedocs.io/). ![image](https://user-images.githubusercontent.com/23412689/220255676-39e9eee9-e658-4866-b8b9-63a6174577ba.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2425:336,release,releases,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425,5,"['continuous', 'deploy', 'integrat', 'release']","['continuous', 'deployment', 'integrations', 'release', 'releases']"
Deployability,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275#issuecomment-427036065:78,update,updated,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275#issuecomment-427036065,1,['update'],['updated']
Deployability,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:162,update,update,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324,1,['update'],['update']
Deployability,"Thank you for the comment. Do you mean these packages will be installed when we install Scanpy? Sorry that I don't understand. When I try to use `scanpy.tl.louvain`, it says `ModuleNotFoundError: No module named 'louvain'`, and I don't know how to solve it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938:62,install,installed,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938,2,['install'],"['install', 'installed']"
Deployability,"Thank you for the detailed explanation @takluyver, this helps a lot!. @ivirshup A dev environment should successfully execute the `try` block. I designed that `except` clause to be hit when people import the installed production version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490:208,install,installed,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490,1,['install'],['installed']
Deployability,"Thank you for the prompt response! I did not install anndata separately, I just followed instructions from https://scanpy.readthedocs.io/en/latest/installation.html to install scanpy using `pip install scanpy` in miniconda environment. Do I need to reinstall another version of anndata? Or of scanpy? Sorry, still not sure how to fix this. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292:45,install,install,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392070292,4,['install'],"['install', 'installation']"
Deployability,Thank you for the release update! I just had to; `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475:18,release,release,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475,3,"['install', 'release', 'update']","['install', 'release', 'update']"
Deployability,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:139,toggle,toggleswitch,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055,1,['toggle'],['toggleswitch']
Deployability,"Thank you so much for the feedback!; I'll definitely talk to the admin, but I am not sure he would update. Considering conda, I've tried using ; conda create -n scanpy python=3.6 scanpy; conda activate scanpy. It creates the environment, but then apparently I need to run a jupyter notebook from the terminal for the environment to be activated. When trying to do it, I am getting a ""Jupyter Notebook requires JavaScript"" error, and I can't figure out how to solve it while connecting through ssh, because running ""jupyter notebook --no-browser"" generates a token I can use only on the local machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341:99,update,update,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477340341,1,['update'],['update']
Deployability,Thank you very much for pointing me to this! What you assume is absolutely right! I just replaced the file. The previous file was created when there wasn't even a function `read_10x_mtx`... I added a section to the docstring describing how the file was produced: https://github.com/theislab/scanpy/commit/fcd125252c307b5ecc077ad0e69fa9d6a1106ebb. See the updated docs: https://scanpy.readthedocs.io/en/latest/api/scanpy.datasets.pbmc3k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/428#issuecomment-456015073:355,update,updated,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/428#issuecomment-456015073,1,['update'],['updated']
Deployability,Thank you very much for this remark! I'll update the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/143#issuecomment-386718816:42,update,update,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143#issuecomment-386718816,1,['update'],['update']
Deployability,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:82,update,update,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480,1,['update'],['update']
Deployability,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:371,install,install,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244,1,['install'],['install']
Deployability,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:73,update,updates,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980,1,['update'],['updates']
Deployability,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good!. The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434085531,1,['update'],['updated']
Deployability,"Thank you, this obviously makes sense. I'll first merge another update on plotting functions and then get back to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/227#issuecomment-411656281:64,update,update,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/227#issuecomment-411656281,1,['update'],['update']
Deployability,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:388,release,release,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969,1,['release'],['release']
Deployability,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? 😄. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,integrat,integrating,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335,1,['integrat'],['integrating']
Deployability,Thanks @falexwolf. The tests are not run by default since dask etc are not installed. I install them with the following to get them to be picked up:. ```; pip install dask[array] zappy zarr; pip install pytest; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-456825801:75,install,installed,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-456825801,4,['install'],"['install', 'installed']"
Deployability,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,integrat,integrated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439,1,['integrat'],['integrated']
Deployability,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:38,upgrade,upgraded,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097,1,['upgrade'],['upgraded']
Deployability,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545:708,update,updates,708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545,1,['update'],['updates']
Deployability,"Thanks Gökcen... Yes, try `pip3 install --upgrade setuptools` or `pip` if this defaults to Python 3 already...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148#issuecomment-386887420,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"Thanks Philipp, I have updated and push the code. I hope you will accept; this pull request now. To support PCA and scanpy for weighted sampling, you; can just set a parameter , observations/samples weights at the time user; input matrix and then we can modify PCA and remaining this code is fine. I; am asking for weights because user may extracted those weights either with; sampling technique or may be sometime user can give weights of his own; desired e.g. he want to focus one cell type etc. So we should support; weights generally rather specifically. Thanks,; Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,; >; > But i will suggest to just support weights instead of coreset, may be user; > want to sample data with some other weighting technique. So we should ask; > them to just put the weights for observations, then we need to modify PCA; > as well and i think my code will support most of plots and marker genes,; > but not PCA, because my input is PCA matrix with weights for each; > observations.; >; > Thanks,; > Khalid; >; > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>; > wrote:; >; >> Long-term, we should think about the design here: Specifying weights all; >> the time is possible, but not very nice for users. So a few questions come; >> to mind:; >>; >> Should we add scanpy.pp.coreset, which would create a sampling and add; >> adata.obs['coreset_weights'] or simply adata.obs['weights']?; >>; >> If we do that or plan to in the future, how should the added weights; >> parameter to all these functions work?; >>; >> I think it might default to 'coreset_weights', and the functions would; >> automatically use that .obs column if it exists. Users should also still; >> be able to specify weights manually as in this PR.; >>; >> So the type of the parameter would be Union[str, pd.DataFrame,; >> Sequence[Union[float, int]]].; >> ------------------------------; >>; >> All of that doesn’t really affect th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004:23,update,updated,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494846004,1,['update'],['updated']
Deployability,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:297,install,install,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596,2,['install'],"['install', 'installed']"
Deployability,Thanks a lot. All of these new features are what we need!. I notice that the tutorial has not been updated yet (such as sc.tl.filter_rank_genes_groups( ) and rna velocity function in https://github.com/theislab/scanpy/tree/master/scanpy/tools). I find these features occasionally. Could you add them in scanpy tutorial ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-473309434:99,update,updated,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-473309434,1,['update'],['updated']
Deployability,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and; push when it passed tests for all 5 plots. Thanks,; Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,; >; > I have removed issue from the pull request by the testing tool, now the; > tools showed me duplications, which are mostly from other code and 1-2 from; > my code. Please have a look into it. It's my first pull request and its; > taking too much time :(; >; > Thanks; > Khalid; >; > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:; >; >> Ok , thanks for letting me know. Please check the pull request. I have; >> verified my code by keeping weights 1 and it has same values when; >> observations has no weights or all weights equal to 1.; >>; >> I also suggest to update PCA for weighted sampled data.; >>; >> Thanks,; >> Khalid Usman; >>; >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>; >> wrote:; >>; >>> You can just open a new one, I’ll close this one then 🙂; >>>; >>> —; >>> You are receiving this because you authored the thread.; >>> Reply to this email directly, view it on GitHub; >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,; >>> or mute the thread; >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>; >>> .; >>>; >>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520:883,update,update,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-494076520,1,['update'],['update']
Deployability,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:47,pipeline,pipeline-for-everything,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593,2,['pipeline'],"['pipeline-for-everything', 'pipelines']"
Deployability,"Thanks for catching this! Could you add a test so this doesn't happen again in the future?. Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802:155,release,release,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802,1,['release'],['release']
Deployability,"Thanks for clarification. I had thought it is for filtering out genes. On Mon, May 18, 2020 at 2:21 AM Rachel Ng <notifications@github.com> wrote:. > It makes sense to use AND logic, because the function keeps genes that; > satisfy all three conditions.; >; > 1. Fraction of cells inside the cluster expressing the gene must be; > greater than min_in_group_fraction; > 2. Fractions of cells outside the cluster expressing the gene must be; > less than max_out_group_fraction; > 3. Fold change must be greater than min_fold_change; >; > But there are remaining issues (calculation of fold change and using the; > absolute value of the fold change) in this function that needs to be; > updated #863 <https://github.com/theislab/scanpy/issues/863>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1213#issuecomment-629970781>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADG2PVZVFSYU3ST4ESJFS33RSDHVXANCNFSM4NAA5V2A>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750:684,update,updated,684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750,1,['update'],['updated']
Deployability,"Thanks for clarifying this again, @ivirshup! We should have changed the default value already for 1.4. I'll add a note to the release notes and it's fine... ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-475422571:126,release,release,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-475422571,1,['release'],['release']
Deployability,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:397,update,updated,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151,1,['update'],['updated']
Deployability,"Thanks for getting back to me. I tried updating the environment (spicy now at 1.11.0), and it's still hanging during the PCA step. Here are the updated versions:. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; CoreFoundation NA; Foundation NA; PIL 9.5.0; PyObjCTools NA; anyio NA; appnope 0.1.3; argcomplete NA; asttokens NA; attr 23.1.0; babel 2.12.1; backcall 0.2.0; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; fastjsonschema NA; google NA; h5py 3.9.0; idna 3.4; igraph 0.10.4; importlib_resources NA; ipykernel 6.23.3; ipython_genutils 0.2.0; ipywidgets 8.0.6; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonschema 4.17.3; jupyter_events 0.6.3; jupyter_server 2.6.0; jupyterlab_server 2.23.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; louvain 0.8.0; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.0; numba 0.57.0; numexpr 2.8.4; numpy 1.24.3; objc 9.2; overrides NA; packaging 23.1; pandas 2.0.2; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.0; prometheus_client NA; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pydev_jupyter_utils NA; pydev_jupyter_vars NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pygments 2.15.1; pyparsing 3.1.0; pyrsistent NA; pythonjsonlogger NA; pytz 2023.3; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; scipy 1.11.0; seaborn 0.12.2; send2trash NA; session_info 1.0.0; setuptools 68.0.0; six 1.16.0; sklearn 1.2.2; sniffio 1.3.0; socks 1.7.1; stack_data 0.6.2; statsmodels 0.14.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 1.12.1; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; urllib3 2.0.3; wcwidth 0.2.6; w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1608050519:144,update,updated,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1608050519,1,['update'],['updated']
Deployability,"Thanks for getting back. I ran the reproducer on my system and it indeed works perfectly. Very weird. . When I ran the reproducer, I did get one warning:; ```; WARNING: The candidate selected for download or install is a yanked version: 'scipy' candidate (version 1.11.0 at https://files.pythonhosted.org/packages/2f/b5/b5387cdafc66805907424c3a95f773b84a5d452a0925801c6218727a766e/scipy-1.11.0-cp311-cp311-macosx_10_9_x86_64.whl (from https://pypi.org/simple/scipy/) (requires-python:<3.13,>=3.9)); Reason for being yanked: License Violation; ```; Other than that, it worked fine. I have a feeling it might be an issue with the installation? That scipy warning is suspicious? I'm using mamba (mambaforge specifically) to manage my packages, maybe something went wrong there. Have you hear of any issues with mamba? Let me trouble shoot my environment and I'll report back. . Yes, I am running macOS, but it's not the Apple Silicon, I'm still using the older Intel processors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615:208,install,install,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2531#issuecomment-1619051615,2,['install'],"['install', 'installation']"
Deployability,"Thanks for letting me know!. If it's working in the newest version, there's not much for us to fix. Please let us know if you run into the error on the latest release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126:159,release,release,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126,1,['release'],['release']
Deployability,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:105,install,installed,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860,1,['install'],['installed']
Deployability,Thanks for opening an issue!. I believe scipy's `mmread` (which is being used under the hood) recently switched to using `fast_matrix_market` as of scipy `1.12`. * https://github.com/alugowski/fast_matrix_market/issues/22; * https://github.com/scipy/scipy/pull/18631. So I'm sure any action here is needed. Please let me know if this isn't behaving as expected with the newest scipy release.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846#issuecomment-1938362632:383,release,release,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846#issuecomment-1938362632,1,['release'],['release']
Deployability,"Thanks for opening the issue. It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this. Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484:171,update,update,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484,2,"['install', 'update']","['installation', 'update']"
Deployability,"Thanks for opening this PR!. Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476:485,release,release,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476,1,['release'],['release']
Deployability,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:493,install,install-,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],['install-']
Deployability,"Thanks for the PR!. On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes?. I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187:144,release,release,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187,3,"['release', 'update']","['release', 'updated']"
Deployability,"Thanks for the PR. . One concern that I have is that similar solutions do not exists for other plotting functions. For coherence, ideally the `annot_col` argument should be available for other cases. Thus, I think that a better and more generic approach would be to simply modify your genes names in the `AnnData` object and let all plotting functions use those names. For this, you simply do:. ```PYTHON; adata.var = adata.var.reset_index().set_index(annot_col); # adata.var_names is automatically updated; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256:499,update,updated,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441017256,1,['update'],['updated']
Deployability,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs?. Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033:162,release,release,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033,1,['release'],['release']
Deployability,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488,1,['integrat'],['integration']
Deployability,Thanks for the fix! . I've just added a release note.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651:40,release,release,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651,1,['release'],['release']
Deployability,Thanks for the fix! Can we include this in the next scanpy release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693958817:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1693958817,1,['release'],['release']
Deployability,"Thanks for the info!. I was about to rant that this is weird and broken, but locales are literally the first section in the Ubuntu image docs, so I can’t blame them (too much): https://hub.docker.com/_/ubuntu. They probably use `POSIX` as `C.UTF-8` isn’t standard. ([blame the C standard consortium](https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe)). They recommend `ENV LANG C.UTF-8` though, maybe that works for you. (otherwise installing `locales` and your instructions work too)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344265736:464,install,installing,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344265736,1,['install'],['installing']
Deployability,"Thanks for the information. However, I don't think the problem is the python version but libBLAS. Probably updating python also updated lot of libraries and that solved the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-410620495:128,update,updated,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-410620495,1,['update'],['updated']
Deployability,Thanks for the investigation! @metoru can you try with #2928?. ```shell; pip install git+https://github.com/scverse/scanpy.git@fix-dendro-corr; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2804#issuecomment-2006642107:77,install,install,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2804#issuecomment-2006642107,1,['install'],['install']
Deployability,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:331,update,update,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056,1,['update'],['update']
Deployability,"Thanks for the report. I'm having trouble reproducing this behaviour locally. Two thoughts:. 1. It looks like there's a newer version of leidenalg available, could you upgrade that?; 2. Maybe there is something about the neighborhood graph. Could you either: reproduce this with some dummy data (e.g. `sc.datasets.blobs`) or share the `test` object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906#issuecomment-1997818178:168,upgrade,upgrade,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906#issuecomment-1997818178,1,['upgrade'],['upgrade']
Deployability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:782,release,releases,782,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,1,['release'],['releases']
Deployability,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:74,install,install,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986,3,['install'],"['install', 'installing']"
Deployability,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:220,update,update,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133,2,"['integrat', 'update']","['integrated', 'update']"
Deployability,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/855#issuecomment-537134057:60,install,installing,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855#issuecomment-537134057,1,['install'],['installing']
Deployability,"Thanks for the suggestions and sorry for the late response. Yes, I was using google colab, and a newer version of matplotlib did the trick. I upgraded to the 3.5.3 version of matplotlib, and restarted the runtime, and that did the trick, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2332#issuecomment-1328259095:142,upgrade,upgraded,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2332#issuecomment-1328259095,1,['upgrade'],['upgraded']
Deployability,"Thanks for the thorough issue report! I recall running into this before with some other igraph algorithm, so there might be a similar patch somewhere else in the codebase. Would you be interested in opening a PR to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568:134,patch,patch,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568,1,['patch'],['patch']
Deployability,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924#issuecomment-554854817:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924#issuecomment-554854817,2,['update'],['update']
Deployability,Thanks for the update! Glad to hear it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271,1,['update'],['update']
Deployability,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992,1,['update'],['update']
Deployability,Thanks for the update! Merged via #1595,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049,1,['update'],['update']
Deployability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['update'],['update']
Deployability,Thanks for the updated preprint! It really helps better understand the updated PAGA algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814:15,update,updated,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-399275814,2,['update'],['updated']
Deployability,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:394,pipeline,pipelines,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745,1,['pipeline'],['pipelines']
Deployability,Thanks for this PR @sjfleming . If you don't mind I will integrate this functionality into a new PR that also covers #512 and #525,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944:57,integrat,integrate,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944,1,['integrat'],['integrate']
Deployability,"Thanks for this PR, this looks interesting! Sorry for taking a while to get back to you, we've been quite busy getting a release out. We'll try and get back to you with more in the next couple weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191:121,release,release,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191,1,['release'],['release']
Deployability,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:239,install,installed,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412,2,['install'],"['installation', 'installed']"
Deployability,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/276:97,install,installing,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276,3,['install'],"['install', 'installing']"
Deployability,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:25,update,updated,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,3,['update'],['updated']
Deployability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1365,release,releases,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['release'],['releases']
Deployability,"Thanks for your update @rpeys, I will try to convert to scipy csr sparse matrix :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-718667650:16,update,update,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-718667650,1,['update'],['update']
Deployability,"Thanks! And sorry, this was something I had already fixed locally, but not yet pushed. Now it's up (https://github.com/theislab/scanpy/commit/320fa421aa2e1dcb15d047c629416b8640eb1635). The first stable release is not far away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/24#issuecomment-308729587:202,release,release,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/24#issuecomment-308729587,1,['release'],['release']
Deployability,"Thanks! I've moved the release note to 1.10, since this is more of a feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1948544838:23,release,release,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1948544838,1,['release'],['release']
Deployability,Thanks! Makes sense! (Both the badge and the automatic update from PyPI).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-460613715:55,update,update,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-460613715,1,['update'],['update']
Deployability,"Thanks! This will, however, only work for `anndata>=0.7.2a1`. So you'd also have to update `scanpy/requirements.txt`. I left some more comments in #1439.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351:84,update,update,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351,1,['update'],['update']
Deployability,Thanks! you can just specify `n_top_genes=2000` or so until the fix is released,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427#issuecomment-1860902089:71,release,released,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427#issuecomment-1860902089,1,['release'],['released']
Deployability,"Thanks!. I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487264046:14,update,updated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487264046,1,['update'],['updated']
Deployability,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:39,release,released,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435,3,"['patch', 'release']","['patch', 'release', 'released']"
Deployability,"Thanks, good catch!. This is already fixed in #3194, we just need to cut a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3219#issuecomment-2348487691:75,release,release,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3219#issuecomment-2348487691,1,['release'],['release']
Deployability,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,integrat,integrated,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['integrat'],['integrated']
Deployability,"Thanks, what is the default backend on macOS? I only install matplotlib via the scanpy/anndata depenancies in a conda environment (and `MPLBACKEND` is unset). Upon installing wxpython, I can confirm the used backend. ```; >>> matplotlib.__version__; '3.2.2'; >>> matplotlib.rcParams['backend']; 'WXAgg'; >>> matplotlib.get_backend(); 'WXAgg'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483,2,['install'],"['install', 'installing']"
Deployability,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:360,install,install,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023,1,['install'],['install']
Deployability,Thanks. I merged your PR and released a new package. Closing this issue now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/793#issuecomment-526320133:29,release,released,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793#issuecomment-526320133,1,['release'],['released']
Deployability,"Thanks. On Mon, 6 May 2019 at 18:49, Koncopd <notifications@github.com> wrote:. > It should work if you install from github.; > fe2580c; > <https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/631#issuecomment-489690557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4SWBVPGTIQHOG365L3PUBOS3ANCNFSM4HLAPBTA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/631#issuecomment-489750461:104,install,install,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631#issuecomment-489750461,1,['install'],['install']
Deployability,"That error is not specific to scanpy. It would be good to know which; library is causing the problem such that it can be updated but most likely; is either numpy, scipy, matplotlib or sklearn. Maybe try to update those; packages and see if the error goes away or try to google the error to find; some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>; wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi.; > Is there a way to resolve it without installing using conda?; >; > Logs:; >; > [dilawars@chamcham scanpy_exp]$ python planaria.py; > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; > import imp; > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1; > ... storing 'clusters' as categorical; > computing tSNE; > using data matrix X directly; > using the 'MulticoreTSNE' package by Ulyanov (2017); > finished (0:02:53.98); > saving figure to file ./figures/tsne_full.pdf; > computing neighbors; > using data matrix X directly; > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171:121,update,updated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280#issuecomment-427359171,4,"['install', 'update']","['install', 'installing', 'update', 'updated']"
Deployability,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:112,install,installed,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092,1,['install'],['installed']
Deployability,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:27,install,install,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015,4,['install'],"['install', 'installed', 'installing']"
Deployability,"That'd be great, thanks!. I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270:177,pipeline,pipelines,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270,1,['pipeline'],['pipelines']
Deployability,"That's a great point, I'll definitely implement that to speed things up. I also read that same paper, which made me realize that we (and most other pipelines as far as I know) use mean log. I agree that the original interpretation is log mean. Any thoughts on switching to log mean (for everything except t-tests)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471054932:148,pipeline,pipelines,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471054932,1,['pipeline'],['pipelines']
Deployability,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,integrat,integrate,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228,1,['integrat'],['integrate']
Deployability,"That's really cool, thank you!. I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913:160,release,release,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340#issuecomment-435638913,1,['release'],['release']
Deployability,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:140,install,installation,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630,1,['install'],['installation']
Deployability,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/687#issuecomment-503484684:97,install,installing,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687#issuecomment-503484684,1,['install'],['installing']
Deployability,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,integrat,integrate,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209,2,"['integrat', 'upgrade']","['integrate', 'upgraded']"
Deployability,"The ""outs"" from `spaceranger count` v2 differ from v1. Specifically, `tissue_positions_list.csv` has been renamed `tissue_positions.csv` and now has headers. Fortunately the column names exactly match those used by `scanpy`. See https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/spatial (search for `tissue_positions.csv`). This PR adjusts `sc.read_visium()` so that it can load the outputs of `spaceranger count` v2. The API / method signature is unchanged, so no changes to the documentation are required. The version is inferred from the presence or otherwise of the old `tissue_positions_list.csv` (implying v1) and then the files in `spatial/` are handled accordingly, including differential handling of headers / column names. @linhuawang; @sopvdl; @TaopengWang",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2458:294,pipeline,pipelines,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2458,1,['pipeline'],['pipelines']
Deployability,"The API for setting the random seed changed in the recent release (`v0.7`) of `louvain`, this is fixed on master, which should see a release soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212:58,release,release,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212,2,['release'],['release']
Deployability,"The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053:125,release,release,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053,1,['release'],['release']
Deployability,"The UMAP (actually neighbours in the current implementation) is already now derived from any embedding the user wants, including integration embedding, so this is not an issue in itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985:129,integrat,integration,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985,1,['integrat'],['integration']
Deployability,The above issue was fixed by installing python-igraph and not igraph. Just run; `pip install python-igraph`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728:29,install,installing,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495769728,2,['install'],"['install', 'installing']"
Deployability,"The bio conda builds look like their broken at the moment, and we haven't had the bandwidth to fix them yet (we are not the direct maintainers of the bio-conda builds). You can find up to date installation instructions which avoid this on the [latest docs](https://scanpy.readthedocs.io/en/latest/installation.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806:193,install,installation,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806,2,['install'],['installation']
Deployability,The build environment doesn't have R installed so the checks failed....,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381973984:37,install,installed,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381973984,1,['install'],['installed']
Deployability,"The change is quite useful. Please go ahead and add a PR. On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:. > Here is a patch that fixes the above problem...; >; > import matplotlib.colors; >; > #if user defined, then use the vmax, vmin keywords, else use data to generate them...; > if ('vmax' in kwds) and ('vmin' in kwds):; > _vmax = kwds['vmax']; > _vmin = kwds['vmin']; > else:; > _vmax = max(mean_flat); > _vmin = min(mean_flat); >; > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); >; > I'll submit a pull request.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444388428:145,patch,patch,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444388428,1,['patch'],['patch']
Deployability,The color is continuous means that the color is gradient according to the default setting in package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048:13,continuous,continuous,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048,1,['continuous'],['continuous']
Deployability,"The data also allows to detect a next issue: When multiple genes have the same value of `disp_cut_off`. Can be found if here e.g. dont do `sc.pp.normalize_total`:. ```py; import scanpy as as; adata = sc.datasets.pbmc3k(); # sc.pp.normalize_total(adata, target_sum=10000); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); adata.var[""highly_variable""].sum(); ```; ```; 10367; ```; Which is due to many genes having the value selected for the `disp_cut_off` here, having . ...`x[n-2]` = `x[n-1 ]` = `x[n]` = `x[n+1] `= `x[n+2]`... https://github.com/scverse/scanpy/blob/b918a23eb77462837df90d7b3a30a573989d4d48/src/scanpy/preprocessing/_highly_variable_genes.py#L408-L418. I tried to check how Seurat is proceeding in such a case, expecting to see how it breaks the ties. (data downloaded from [here](https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz)); Here I'm actually not sure how to turn off the `scale.factor` argument? Its set to 10'000 by default. ```R; library(dplyr); library(Seurat); library(patchwork). pbmc.data <- Read10X(data.dir = ""filtered_gene_bc_matrices/hg19/""). pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k""). pbmc <- NormalizeData(pbmc, normalization.method = ""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot"", nfeatures = 10000). length(VariableFeatures(pbmc)); ```; ```; 2292; ```; However, it turns out Seurat seems to restrict to the genes which are variable in the sense of passing the set mean threshold and normalized dispersion thresholds. These thresholds are ignored in scanpy if the number of genes is given. So not really an insight of how to break ties in this case. Would suggest to make a new issue, which the potential project on comparing the frameworks could address.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888:1078,patch,patchwork,1078,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888,1,['patch'],['patchwork']
Deployability,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921,3,"['install', 'update']","['installed', 'update']"
Deployability,"The docs look great! I just wonder about the above: In the release notes, we refer to everything as `scanpy.*`, not `sc.*`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-504889159:59,release,release,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-504889159,1,['release'],['release']
Deployability,"The fix was done in #2999, but we didn’t release 1.10.2 yet. Does installing scanpy from git work for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102#issuecomment-2164806504:41,release,release,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102#issuecomment-2164806504,2,"['install', 'release']","['installing', 'release']"
Deployability,The following fix worked for me (informed by this issue: https://github.com/HumanCellAtlas/data-consumer-vignettes/issues/78). ```bash; # uninstall packages (most important one is igraph); pip uninstall igraph python-igraph leiden scanpy. # reinstall scanpy; pip install 'scanpy[leiden]'. # check to see that you can import scanpy and print the version; python -c 'import scanpy as sc; print(sc.__version__)'; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556:263,install,install,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556,1,['install'],['install']
Deployability,"The idea of having ""smart subsample"" functionality available in scanpy has been a topic of discussion for a while. I would like to see a benchmark of these methods on single cell data before choosing one to include here. Are you aware of anything in this space?. Update:. It looks like the lab it's from have put out some writing on this: https://dl.acm.org/doi/pdf/10.1145/3388440.3412409",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862#issuecomment-1948573055:263,Update,Update,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862#issuecomment-1948573055,1,['Update'],['Update']
Deployability,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I don’t think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After; --- | ---; ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441704873:623,continuous,continuous,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441704873,1,['continuous'],['continuous']
Deployability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:469,update,updated,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['update'],['updated']
Deployability,"The issue persists with anndata 0.7.6. I've also been trying to update h5py, but it has conflicts with other packages. I'll post an update if I get it updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754:64,update,update,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754,3,['update'],"['update', 'updated']"
Deployability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:113,update,updated,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,3,['update'],"['update', 'updated']"
Deployability,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665:223,install,installed,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665,3,['install'],"['install', 'installed']"
Deployability,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955,6,"['install', 'release', 'update']","['install', 'installing', 'release', 'released', 'update']"
Deployability,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:67,install,installed,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,4,"['install', 'update']","['install', 'installed', 'update']"
Deployability,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:539,upgrade,upgraded,539,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441,1,['upgrade'],['upgraded']
Deployability,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927:156,release,releases,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927,1,['release'],['releases']
Deployability,"The original line was; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata_comp.X) else True; ```; which did the expected thing, @flying-sheep introduced the bug 22 days ago in https://github.com/theislab/scanpy/commit/ce10d02f58c3308b60c23c43a36949b6aeed3ea8. Damn, I wouldn't have expected such a thing in a commit ""improved docs"". It went into release 1.3.4 and 1.3.5... Of course, it's my fault. I should have written a test in the first place. @Koncopd: can you write a test for PCA both for sparse and dense data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446372971:382,release,release,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446372971,1,['release'],['release']
Deployability,"The packages dorothea-py and progeny-py are now deprecated, instead one should use decoupler (https://github.com/saezlab/decoupler-py). It contains both prior knowledge resources plus many more since it integrates the meta-resource OmniPath, and it also contains many footprint enrichment methods instead of a fixed one.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2186:203,integrat,integrates,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2186,1,['integrat'],['integrates']
Deployability,The problem is that `np.concatenate` is being called on an `AnnData`. You may have to go back further in the scanorama releases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051044283:119,release,releases,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1051044283,1,['release'],['releases']
Deployability,"The problem is that it does not install at all. ; When I run; ```; conda create -n test; conda activate test; conda install python=3.11; conda install -c conda-forge scanpy; ```; I get an error output for the last line, which is:; ```; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:32,install,install,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,5,['install'],"['install', 'installed']"
Deployability,"The read_10x_mtx() function does not work due to the update of the anndata package to 0.10.4 (January 14, 2024; Scanpy=1.9.6)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:53,update,update,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['update'],['update']
Deployability,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,configurat,configuration,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['configurat'],['configuration']
Deployability,The release with PCA bug fix is now on pypi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151:4,release,release,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151,1,['release'],['release']
Deployability,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015:237,upgrade,upgrade,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015,1,['upgrade'],['upgrade']
Deployability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:290,install,install,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['install'],['install']
Deployability,"The scanpy install directory is super wrong, as it’s not writable for many people. There’s exactly one correct way of determining a global place for cache* files like this: [`appdirs.user_cache_dir(...)`](https://pypi.org/project/appdirs/). Alex and me talked in the past and decided for a visible directory in the working directory. I’d be up for changing it to `user_cache_dir(…)` for the data. *the data are cache files since reexccuting their function after deleting the files will redownload them without loss of information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564:11,install,install,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564,1,['install'],['install']
Deployability,"The score_genes procedure currently uses a ranking system to split genes into bins of similar expression levels. The current approach fails with some datasets. - Currently the code doesn’t produce the expected number of bins of equal or approximately equal size, see #3168; - The code fails completely when the gene set has zero expression in some cells, see #3169. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3168, closes #3169; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167:569,release,release,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167,2,"['Release', 'release']","['Release', 'release']"
Deployability,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:141,install,install,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324,1,['install'],['install']
Deployability,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```; import scanpy as sc; import pandas as pd; import numpy as np. from anndata import AnnData. def harmony_integrate(; adata: AnnData,; key: str,; basis: str = ""X_pca"",; adjusted_basis: str = ""X_pca_harmony"",; **kwargs,; ):; try:; import harmonypy; except ImportError:; raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(); adata_merge.X = adata_merge.layers['counts']; sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(); sc.experimental.pp.normalize_pearson_residuals(adata_merge); adata_merge.layers['apr'] = adata_merge.X.copy(); sc.tl.pca(adata_merge, svd_solver=""arpack""); adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(); adata2 = adata_merge.copy(); ```. The frist test:. ```; # scanpy 1.9.6 that changes of this PR won't have taken effect yet.; # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py; harmony_integrate(adata1, key='batch', basis='X_pca_30'); harmony_integrate(adata2, key='batch', basis='X_pca_30'); np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%); Max absolute difference: 1.20792265e-12; Max relative difference: 4.37537551e-09; x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,; 0.564897],; [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:409,install,install,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,2,['install'],['install']
Deployability,"The task is to write the following preprocessing sequence using an AnnData instance adata.; ```py; meanFilter = 0.01; cvFilter = 2; nr_pcs = 50. ddata = adata.to_dict(); X = ddata['X']; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary ; ddata['X'] = X; ddata['Xpca'] = Xpca; ddata['var_names'] = ddata['var_names'][gene_filter]; sett.m(0, 'Xpca has shape',; ddata['Xpca'].shape[0], 'x', ddata['Xpca'].shape[1]); from ..ann_data import AnnData; adata = AnnData(ddata); print(adata.X); ```; While the previous snippet works just as expected, when I want to do the same without a ddata object, some uncontrolled behavior comes up. Indexing doesn't work as expected anymore. @flying-sheep: could you have a look at why `adata['Xpca'] = Xpca` in the following throws an; ```py; >>> adata['Xpca'] = Xpca; IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices; ```; in the following snippet; ```py; X = adata.X; # row normalize ; X = row_norm(X, max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; X, gene_filter = filter_genes_cv(X, meanFilter, cvFilter); # compute zscore of filtered matrix ; Xz = zscore(X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update adata ; adata.X = X; adata = adata.var_names[gene_filter] # filter genes ; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape',; adata['Xpca'].shape[0], 'x', adata['Xpca'].shape[1]); print(adata.X); ```; I played around quite some bit, but the only solution that I got running then had the numerically incorrect result. It's quite to hard to keep this sequence of steps nicely organized. PS: the snippet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/4:508,update,update,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4,1,['update'],['update']
Deployability,"The tests are currently failing as scipy has updated their implementation of Mann-Whitney U tests, which we use as a reference. We should figure out what changed, and how to proceed (e.g. do we change our function, or pass different parameters to theirs in the test?). ping: @Koncopd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1892:45,update,updated,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892,1,['update'],['updated']
Deployability,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/162:201,install,installing,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162,1,['install'],['installing']
Deployability,"The upper bound for python versions we support is set by `numba` and `llvm-lite`. These rely on python internals, so can take a bit to be compatible with new releases. There should be python 3.10 support in the next numba release (https://github.com/numba/numba/issues/7562), which has a candidate out now ([see details here](https://numba.discourse.group/t/numba-0-55-0-rc1/1075)). I was able to get scanpy to import by doing:. ```sh; mamba create -yn ""numba-0.55.0rc1"" ""python=3.10""; conda activate numba-0.55.0rc1; mamba install -c numba -c numba/label/numpy numba=0.55.0rc1 numpy=1.21; pip install scanpy; python -c ""import scanpy""; ```. I couldn't install `scanpy` through conda since `pytables` doesn't have a 3.10 conda build afaict. Please let me know if try this out and run into any issues!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010871653:158,release,releases,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105#issuecomment-1010871653,5,"['install', 'release']","['install', 'release', 'releases']"
Deployability,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:71,update,update,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759,1,['update'],['update']
Deployability,"The whole PR is pretty awesome already! I wrote some comments... Can you also add the function to the docs, cross reference the deprecated and the new function in the Notes section and add it to the [release notes](https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/330#issuecomment-434090862:200,release,release,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330#issuecomment-434090862,1,['release'],['release']
Deployability,Then I guess light grays would need to be removed from the default colour maps? And the question remains for how to deal with this in a continuous covariate.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163:136,continuous,continuous,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163,1,['continuous'],['continuous']
Deployability,"There hasn't actually been a release of UMAP since the pull request that should fix this (https://github.com/lmcinnes/umap/pull/261). I think I see what happened here, so I've opened a PR to fix it here. For now, this can be worked around by running:. ```python; sc.tl.umap(adata, init_pos=sc.tl._utils.get_init_pos_from_paga(adata)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-519055058:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519055058,1,['release'],['release']
Deployability,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/660#issuecomment-495141236:102,upgrade,upgrade,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660#issuecomment-495141236,1,['upgrade'],['upgrade']
Deployability,"There is no new release with these changes, yet. You'd need to install from github if you want them. However, these updates aren't critical and you can continue using the old functions without any bad conscience. A new release is obviously pushed to GitHub, PyPI and BioConda, and announced on twitter (https://twitter.com/falexwolf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/537#issuecomment-474286898:16,release,release,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537#issuecomment-474286898,4,"['install', 'release', 'update']","['install', 'release', 'updates']"
Deployability,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,integrat,integrates,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['integrat'],['integrates']
Deployability,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:; > ; > I ran this:; > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'); > ; > But the image has something weird. Here are the snapshot:; > ; > The lines don't align well with the heatmap.; > ; > Additionally, the lines don't align well with the group ID colors, either; > ; > ; > And the ID colors seem to be not aligned well with the heatmap either.; > ; > Any thoughts?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/637#issuecomment-492510374:169,update,update,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637#issuecomment-492510374,1,['update'],['update']
Deployability,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147:61,install,installed,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147,1,['install'],['installed']
Deployability,There was an update in how `pandas` treats categoricals a couple of weeks ago (I think 0.24); we had to update `anndata` in order to incorporate the changes; that's why a newer version of `anndata` was necessary. I'm sure you unconsciously updated `pandas` before. :). Thanks for fielding this one @shayanhoss!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515#issuecomment-469639489:13,update,update,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515#issuecomment-469639489,3,['update'],"['update', 'updated']"
Deployability,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,integrat,integrated,488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['integrat'],['integrated']
Deployability,"There’s also https://github.com/FASTGenomics/base_image_alpine_scanpy, but it hasn’t been updated in a while. Should be no big problem to update it though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477937882:90,update,updated,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477937882,2,['update'],"['update', 'updated']"
Deployability,There’s an 8.2 release planning issue now: https://github.com/pytest-dev/pytest/issues/12213,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2056338635:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2056338635,1,['release'],['release']
Deployability,"These are the features that are added recently in the development version of scanpy and therefore not in any released version. You can either install the development version from github using `pip install git+https://github.com/theislab/scanpy -U` or by following the instructions here https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Note that the development version is more likely to be unstable. Alternatively, you can browse the documentation of stable (released) version of scanpy (which is what you get when scanpy is installed via pip or conda) by selecting the `stable version` on the documentation page from the menu at the bottom left:. ![image](https://user-images.githubusercontent.com/1140359/55020552-7026ed00-4fcd-11e9-8302-02a4f8f973ed.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/560#issuecomment-476770853:109,release,released,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560#issuecomment-476770853,6,"['install', 'release']","['install', 'installation', 'installed', 'released']"
Deployability,These changes enable Scanpy's pre-processing functions to run on distributed engines including Dask and Spark. The Spark integration itself relies on [Zap](https://github.com/lasersonlab/zap) for a distributed version of NumPy. . The main change is the `materialize_as_ndarray` function that is used at certain points of the computation to materialize intermediate results (not the full matrix). This is a no-op in the non-distributed case.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/283:121,integrat,integration,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/283,1,['integrat'],['integration']
Deployability,"They kept failing. Something upstream (likely numba) or Azure’s testing machines seem to have become less consistent in calculating this. [This thread](https://github.com/scverse/scanpy/pull/1740#discussion_r596827747) came across that. @ivirshup’s final statement was. > This bug seems to be based on having nested parallelism and certain reductions. We can avoid it by just not having nested parallelism, which is what I've done for gearys_c. I don’t think exact float equality is a reasonable assumption, but if we want to continue to test for it, we need to be able to force numba to be predictable or so. Needs no release note. Added https://github.com/scverse/scanpy/issues/2688 to track this regression",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2687:619,release,release,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2687,1,['release'],['release']
Deployability,"They should if you call; ```; pip install scanpy[louvain] -U; ```; anndata will also update if you call; ```; pip install scanpy -U; ```. The fact that we have `[igraph]` and `[louvain]` not in the default install is that they take quite a while. If the docs are unclear about it, happy to have you point us to it: https://scanpy.readthedocs.io/en/latest/installation.html :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518#issuecomment-470310775:34,install,install,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518#issuecomment-470310775,5,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915:189,configurat,configuration,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915,1,['configurat'],['configuration']
Deployability,This PR addresses #1562; A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1584:54,update,updated,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584,1,['update'],['updated']
Deployability,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661:1201,Update,Update,1201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661,1,['Update'],['Update']
Deployability,"This PR addresses issues from #562 #1103. Here is a list of changes for `sc.pl.dotplot` and also for `sc.pl.rank_genes_groups_dotplot`:; * Colorbar and dot size legends had been improved. Now is also possible to set a title for them.; * The dotplot functionality has been separated from the main function. This allows to take any two pandas dataFrames, one containing the color values and other containing the size to be plotted.; * A new dotplot style is added, in which a background square is colored according to the colormap instead of the dot. The style can be chosen as parameter.; * Now p-values, score and in general any parameter from `sc.rank_genes_groups` can be plotted.; * `swap_axes` has been added.; * **update**: method chaining was added to avoid explosion of parameters as suggested in #956. The caveat is that a different name is needed to keep the current functionality of `sc.pl.dotplot`. Now is possible to do `sc.pl.DotPlot(adata, ....).add_dendrogram(size=1.2).swap_axes().show()`. ![image](https://user-images.githubusercontent.com/4964309/77526545-13ce0600-6e8b-11ea-996a-3d548f52f200.png). ![image](https://user-images.githubusercontent.com/4964309/77529251-8c36c600-6e8f-11ea-8446-c1dd18374cd4.png). Open issues:; * we need a better way to filter the output from rank_genes_groups (and I would also consider a better name). One option is to filter while plotting.; * to correctly plot the results from `sc.tl.rank_genes_groups` is better to report the results for all genes (`n_genes`). The default is to keep the top 100 genes per group. Thus, for many genes, the pvalue, logfoldchange etc in other groups is not saved.; * added new parameters, for some I would be happy to have some feedback. One is `style` that has two options `square color` and `dot color`. **Update**; The following is now possible (notice the method chaining):. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], '",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127:719,update,update,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127,1,['update'],['update']
Deployability,"This PR adds a visualization of genes, where, for each category (eg. cluster), each gene is represented as a circle whose diameter is proportional to the fraction of cells expressing the gene, and the color represent the mean expression of the gene. I saw this type of visualization on a talk by Dr. Hemant Suryawanshi from Rockefeller University but I couldn't find a citation. ![image](https://user-images.githubusercontent.com/4964309/42698839-59560246-86bf-11e8-8548-2bf0e114d159.png). Furthermore, some improvements are introduced to the `pl.heatmap` and `pl.violin`. In particular, now is possible to swap the axis in multi_panel plot of `pl.violin` and to change the figure size in both types of plots. . I updated an example here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/199:714,update,updated,714,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/199,1,['update'],['updated']
Deployability,"This PR aims to add more GPU functionalities and to integrate more an exisiting one:; * `tl.draw_graph` and `tl.leiden` can now both be GPU accelerated using rapids framework.; * on `pp.neighbors`, now 'rapids' method allows for more metrics. Calculated distance does not need to be root squared anymore (_See https://github.com/rapidsai/cuml/issues/1078#issuecomment-551284134_). I also have slightly rearranged the code to integrate more 'rapids' into the general processing of neighbors, to ensure that distances and connectivities results between 'rapids' and 'umap' are the same.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533,2,['integrat'],['integrate']
Deployability,"This PR contains a reorganization of the API reference docs, mostly to ease modification and generation. Fixes #1682. ## Case sensitivity. First big issue addressed, `scanpy.plotting.dotplot.rst` and `scanpy.plotting.DotPlot.rst` are the same path on a case insensitive file system. This is what most people on macOS will have. If you have a case sensitive file system, important apps like [Adobe's](https://helpx.adobe.com/creative-suite/kb/error-case-sensitive-drives-supported.html) and [steam](https://support.steampowered.com/kb_article.php?ref=8601-RYPX-5789) won't work. So now the plotting classes are put in `generated/classes`. For me, this means:. * docs now generate without warnings; * DotPlot and MatrixPlot docs are only rebuilt if I update them. ## Separate tracked and generated content. I find it difficult to navigate the rst api docs when there is a large amount of auto generated files mixed in with manually curated ones. This becomes more of a problem since the autogenerated files are not removed by `make clean` and will still generate `html` files and generally be a nuisance while they are still around. Now all autogenerated API files are put in a `generated` directory. This is also what [xarray](https://github.com/pydata/xarray/blob/e6168ed6b771b7377e89acb47cc7e5aa9f21a574/doc/api.rst), [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.utils.extmath.randomized_range_finder.html#sklearn.utils.extmath.randomized_range_finder), and [seaborn](https://github.com/mwaskom/seaborn/blob/ba4bd0fa0a90b2bd00cb62c2b4a5e38013a73ac6/doc/api.rst) do. Now `make clean` also deletes all auto generated `rst` files. This also dramatically simplifies our `.gitignore`. ## Consolidation. I've also consolidated the managed api docs to a smaller number of files. Namely, instead of `api/*.rst` there's now just `api.rst`. Instead of `external/*.rst` there's just `external.rst`. ## Plotting functions API docs. In the current api docs, some plotting functions hav",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753:749,update,update,749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753,1,['update'],['update']
Deployability,"This PR contains a work in progress version of development docs. I would like to have an initial version of this in the next release. Right now we just have [CONTRIBUTING.md](), but this not particularly discoverable, complete, or up to date. The idea here is to be a useful resource both to first time developers and maintainers about development processes and how contributing works. Inspirational sources (which we can probably crib from with attribution):. * [Pandas development docs](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html); * [mdanalysis contributing guide](https://userguide.mdanalysis.org/stable/contributing.html). ## Key features:. * Guidelines on external tools; * Guide for testing; * Guide for writing and building docs; * Release and merging guides",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1544:125,release,release,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544,2,"['Release', 'release']","['Release', 'release']"
Deployability,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528:195,integrat,integrated,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528,1,['integrat'],['integrated']
Deployability,"This PR is for #2444 . The project ""heatgraphy"" mentioned in the issue is renamed to ""Marsilea"", https://github.com/Marsilea-viz/marsilea. The previous link to the image mentioned in the issue is not available anymore. Please refer to [this link](https://marsilea.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html). I added a minimum example in the `external` part. Here is a code example of using the plotting function. Remember to install Marsilea before proceeding:. ```shell; pip install marsilea; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); adata.var[""gene_names""] = adata.var.index. t = sc.external.pl.MarsileaHeatmap(adata, vmin=-2, vmax=2, center=0, label=""Expression""); t.add_right(""bulk_labels"", ""colors"", size=.2, label=""Cell Lables""); t.add_dendrogram(""right"", add_base=False, add_meta=True); t.h_groupby(""bulk_labels""); t.add_top(""n_counts"", ""bar"", show_value=False, label=""Counts""). genes = ['CDK6', 'UQCRFS1', 'SH2D2A', 'CD63', 'CEP152',; 'SIAH2', 'LY86', 'HSPB1', 'POLR2G', 'IGBP1']. t.add_bottom(""gene_names"", ""annot"", mark=genes); t.add_title(""Expression Heatmap""); t.legend(); t.show(); ```; If everything goes well, it should display something like this:. ![download](https://github.com/scverse/scanpy/assets/23433306/eb089061-b9ef-4d49-a2e0-fb3adcf06cd7)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512:439,install,install,439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512,2,['install'],['install']
Deployability,"This PR resolves [Issue #2608](https://github.com/scverse/scanpy/issues/2608) in Scanpy by ensuring compatibility with both [Palantir v1.3.0](https://github.com/dpeerlab/Palantir/releases/tag/v1.3.0) ([function signature](https://github.com/dpeerlab/Palantir/blob/870a9ba88b978a070fd0c29df4d1a3b94b0c9ad9/src/palantir/core.py#L34-L51)) and its [earlier versions](https://github.com/dpeerlab/Palantir/blob/4d4f9fcdbfebcf47b4ca1b7ac8a0fb1b8e5fc089/src/palantir/core.py#L31-L41). This is achieved by substituting the initial keyword argument (`data_df`) with a positional argument. The change aligns with Palantir v1.3.0's new optionality to accept AnnData objects as the first argument, making the keyword `data_df` potentially misleading. Adopting a positional argument ensures compatibility with both new and prior versions of Palantir.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2672:179,release,releases,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2672,1,['release'],['releases']
Deployability,This PR updates scanpy to accommodate the latest changes made in spaceranger 2.0 which will break the released version of scanpy. Will provide backwards compatibility to pre 2.0 releases. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2296:8,update,updates,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2296,3,"['release', 'update']","['released', 'releases', 'updates']"
Deployability,"This allows `pip install -e` to do a standardized thing! Unclear how much this helps/ hurts overall though, particularly for asking ""what version is this installation"". I think we would be able to tell that an install was ""editable"" through `importlib.metadata.files`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954:17,install,install,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954,3,['install'],"['install', 'installation']"
Deployability,This also highlights a problem: pypairs had a big upgrade in the meantime and we didn’t see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/814:50,upgrade,upgrade,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814,1,['upgrade'],['upgrade']
Deployability,"This can’t be it, `__init__.py` is [optional since Python 3.3](https://www.python.org/dev/peps/pep-0420/). I also don’t see that import error, `import scanpy` works perfectly. Can you specify how you got that?. ```py; >>> import scanpy, anndata; >>> scanpy.external.tl.palantir(anndata.AnnData()); ImportError: ; please install palantir: . git clone git://github.com/dpeerlab/Palantir.git; cd Palantir; sudo -H pip3 install .; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585#issuecomment-479508185:320,install,install,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585#issuecomment-479508185,2,['install'],['install']
Deployability,"This did fix the recursion issue, thanks. But imortlib.reload(sc) still didn't allow updated source to take effect. I don't know why. Maybe the import tree for this package too complex for importlib.reload???. But the following worked for me. ; ```; ipython; In [1]: %load_ext autoreload; In [2]: %autoreload 2; ln [3]: import scanpy as sc; ln [4]: sc.plotting._tools.scatterplots.tr_test(); [does nothing as expected, then change source to print something out]; ln [5]: sc.plotting._tools.scatterplots.tr_test(); hellya!!!. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468#issuecomment-461985115:85,update,updated,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468#issuecomment-461985115,1,['update'],['updated']
Deployability,"This does seem more like a bug now. I noticed you're using a pretty old version of scanpy. can you update to a more current version, e.g. 1.9* so that we can understand if it is reproducible on the distribution that people are using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364279207:99,update,update,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364279207,1,['update'],['update']
Deployability,"This error is certainly caused by ""scikit-learn"". I abandoned this conda environment and created a new one by `conda create -n Scanpy -c conda-forge scikit-learn`[https://scikit-learn.org/stable/install.html](url).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729:195,install,install,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2165#issuecomment-1058039729,1,['install'],['install']
Deployability,"This feature is still in the development version of scanpy, therefore not available in the released scanpy version yet. See https://github.com/theislab/scanpy/issues/560 for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/565#issuecomment-477833445:91,release,released,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565#issuecomment-477833445,1,['release'],['released']
Deployability,This has been implemented in anndata 0.5 and scanpy 0.4.3. See release notes (https://scanpy.readthedocs.io). See https://github.com/theislab/anndata/commit/8cabf9c86a38d6db88c664e2ea28e3fb29bdf99e and a few fixes after that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/69#issuecomment-364919294:63,release,release,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69#issuecomment-364919294,1,['release'],['release']
Deployability,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/471:246,update,updates,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471,1,['update'],['updates']
Deployability,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:112,integrat,integration,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,1,['integrat'],['integration']
Deployability,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:26,integrat,integrate,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['integrat'],['integrate']
Deployability,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462:16,update,update,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462,1,['update'],['update']
Deployability,"This is a version of Sphix-book-based scanpy docs. **What's lost?**; - Docsearch; - Edit on github button (was weird anyway as no other package does that). **What's gained?**; - Aesthetics, easier to read; - Git submodule for tutorials; - sphinx.ext.viewcode so each function has a source button (effectively replaces edit on github button). **What else changed?**; - I removed the release latest stuff, not sure what value it was adding; - API docs are split logically. View the rendered docs here:. https://icb-scanpy--2220.com.readthedocs.build/en/2220/index.html. What's left to do:. - [x] Update tutorials index page to use nbgallery feature of nbsphinx; - [x] Make home index page use a grid of cards, two columns; - [x] Add to contributing guide how to manage the git submodule (really just occasionally do `git submodule update --remote`); - [x] Annotate CSS overrides; - [x] Exchange sphinx ext viewcode for linkcode (probably in scanpydoc)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2220:382,release,release,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2220,3,"['Update', 'release', 'update']","['Update', 'release', 'update']"
Deployability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,configurat,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,5,"['configurat', 'install', 'update']","['configuration', 'install', 'installation', 'installed', 'updated']"
Deployability,This is also fixed by newer releases of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462142035:28,release,releases,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462142035,1,['release'],['releases']
Deployability,This is being fixed in #1210. for the time being you can try to set `dendrogram=False` or you can install the branch with the fix.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718:98,install,install,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718,1,['install'],['install']
Deployability,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:176,pipeline,pipelines,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619,1,['pipeline'],['pipelines']
Deployability,"This is fixed in pytest-dev/pytest#12169. Let’s wait for the pytest release, and then bump the min pytest version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2049963856:68,release,release,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2049963856,1,['release'],['release']
Deployability,"This is getting pretty close to done. I've updated the comment at the top with a todo list. All the ""extras"" can be moved to an issue on merge. . Let me know if any of those aren't actually optional. I'm currently inclined to drop support for `weight`. Checking older commits, it doesn't seem to actually work for the variance calculation (returns all zeros), and isn't really something we support elsewhere. It would also make adding things like ""median"" significantly more complicated in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1843407929:43,update,updated,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1843407929,1,['update'],['updated']
Deployability,This is great! 👍 . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924:32,update,update,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618#issuecomment-487026924,1,['update'],['update']
Deployability,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:117,update,update,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,3,"['install', 'update']","['install', 'update']"
Deployability,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047:129,install,installation,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047,1,['install'],['installation']
Deployability,"This is inspired by the Spring exporter. It also fills a directory with output files, but in addition to the matrix, adds cluster markers, meta data and embeddings. I keep the conversion code in the cellbrowser package, so I can change it easily in the future. We could also copy the whole function over, but I don't know if this makes a lot of sense, as people will need the cellbrowser package anyways and currently it doesn't have any dependencies, so should be easy to install. This function can optionally convert to html files, start a webserver and serve the result.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382:473,install,install,473,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382,1,['install'],['install']
Deployability,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:174,release,release,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000,1,['release'],['release']
Deployability,"This is no longer needed since UMAP 0.4 (not yet released) will use pynndescent if it is installed, see https://github.com/lmcinnes/umap/pull/278.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-534951024:49,release,released,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-534951024,2,"['install', 'release']","['installed', 'released']"
Deployability,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,integrat,integration,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730,1,['integrat'],['integration']
Deployability,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:447,pipeline,pipeline,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,1,['pipeline'],['pipeline']
Deployability,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/234:114,pipeline,pipeline,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234,1,['pipeline'],['pipeline']
Deployability,"This is the place where we can discuss the necessary workflow changes, as documented in the changed documentation:. https://icb-scanpy--1393.com.readthedocs.build/en/1393/installation.html. anndata sibling PR: theislab/anndata#427. @ivirshup here is the setup.py: https://gist.github.com/flying-sheep/9e7776f4cbe967397702e1c581e3a40a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1393:171,install,installation,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1393,1,['install'],['installation']
Deployability,"This is updated in the newest version of the module, where it looks for either `tissue_positions.csv` or `tissue_positions_list.csv`. The code on the github project works, but unfortunately the most recent version on PyPi was from Mar 2, 2023. . An easy fix is to download the github project directly, using something like ; `pip install git+https://github.com/scverse/scanpy.git#egg=scanpy`. I did it recently in a Google Colaboratory Notebook with a file with `tissue_positions.csv`, and it worked.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488#issuecomment-1650697912:8,update,updated,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488#issuecomment-1650697912,2,"['install', 'update']","['install', 'updated']"
Deployability,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811:463,update,update,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622#issuecomment-487028811,1,['update'],['update']
Deployability,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:606,install,install,606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is?. Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737:107,update,updates,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737,1,['update'],['updates']
Deployability,"This looks good!. I'd also say that we should move towards a more transparent code for the scatter plots. It's a result of 1.5 years of subsequently adding features. No good initial plan about that. The only problems I see:; - there might be a few bugs in this, so I'd like to wait until after the 1.3 release. ; - some people might want exactly the same appearance of the plots as before. I'd say that this should be possible as you just used the existing code snippets. Currently, however, there are slight differences regarding defaults of spaces etc. This is also why the tests fail. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931:302,release,release,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-416733931,1,['release'],['release']
Deployability,This looks like an bug in the most recent release of `louvain`. Try downgrading?. I would also recommend using `leiden` clustering instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246:42,release,release,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246,1,['release'],['release']
Deployability,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight?. ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages?; * How does this impact users vs. developers?; * Is IO special, or should more parts go into sub-packages?; * What gets re-exported from ""main"" modules?; * Who manages the sub-packages?. A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419:559,install,installation,559,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419,1,['install'],['installation']
Deployability,"This may be related to this issue:; https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)); [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/936:223,pipeline,pipeline,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936,1,['pipeline'],['pipeline']
Deployability,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:31,install,install,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129,2,['install'],['install']
Deployability,"This might be due to updates in pandas >1.3.0. The command ; `pbmc.rename_categories('phase', new_cluster_names)`; seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be ; `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified. . Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925:21,update,updates,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925,2,['update'],"['updated', 'updates']"
Deployability,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164:155,release,released,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164,2,"['release', 'update']","['released', 'updated']"
Deployability,"This prepares the complete centralization of all setup information in pyproject.toml once we switch to flit. To this end, it moves all setup metadata that also needs to be accessed at runtime (`__version__`, `__author__` and `__email__`) to a new `scanpy._metadata` module which retrieves them. - during development from a checked-out project's git metadata and `pyproject.toml`; - when installed from the `scanpy-$version.dist-info` metadata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1381:387,install,installed,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1381,1,['install'],['installed']
Deployability,"This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,; We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |; | -- | -- |; | Original | 297 |; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:345,Update,Updated,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['Update'],['Updated']
Deployability,This reverts commit 284c987dedfcf2fc28dd79e682ef721ccd3ff40d. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3153:481,release,release,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3153,2,"['Release', 'release']","['Release', 'release']"
Deployability,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837:129,release,release,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837,1,['release'],['release']
Deployability,"This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805:59,patch,patched,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805,2,['patch'],['patched']
Deployability,This seems to be a problem related to h5py build.; You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900#issuecomment-549045842:59,install,install,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900#issuecomment-549045842,1,['install'],['install']
Deployability,"This seems to be working now. Given that matplotlib updates are known to introduce bugs, I'm not sure we want to pin matplotlib to `3.3.3`. I'm concerned that's too strong of a restriction, since it's likely there's someone out there who needs to use an older version. Right now this is causing CI to fail, since there's a canary test that fails when the function works 😆.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803:52,update,updates,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803,2,"['canary', 'update']","['canary', 'updates']"
Deployability,This should fix the problems with the new release candidate of umap.; https://github.com/theislab/scanpy/issues/1036,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1038:42,release,release,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038,1,['release'],['release']
Deployability,"This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python; sc.tl.umap(adata, random_state=None); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/917:62,release,released,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917,1,['release'],['released']
Deployability,This skips the test `test_harmony_integrate` if harmonypy is not installed.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1428:65,install,installed,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1428,1,['install'],['installed']
Deployability,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:184,install,installations,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069,4,['install'],"['install', 'installations', 'installed']"
Deployability,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318:321,pipeline,pipeline,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318,1,['pipeline'],['pipeline']
Deployability,"This still works in `1.4.4.post1`. It's very likely caused by changes to `setup.py`. I experienced similar problems before and fixed them via `package_data`. But this got removed. It's probably only a problem for the source-based installs. https://github.com/theislab/scanpy/commit/881f0bef31cdfe0df7333641dc847a60894b5c41#diff-2eeaed663bd0d25b7e608891384b7298. ```; >>> import scanpy; >>> scanpy.__version__; <Version('1.4.5.post2')>; >>> scanpy.datasets.pbmc68k_reduced(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/datasets/__init__.py"", line 239, in pbmc68k_reduced; return read(filename); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 114, in read; **kwargs,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 524, in _read; return read_h5ad(filename, backed=backed); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 447, in read_h5ad; constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 481, in _read_args_from_h5ad; f = h5py.File(filename, 'r'); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/h5py/h5sparse.py"", line 162, in __init__; **kwds,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 312, in __init__; fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 142, in make_fid; fid = h5f.open(name, flags, fapl=fapl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/995:230,install,installs,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995,1,['install'],['installs']
Deployability,"This uses the `__array__` method on ndarray-like classes to convert from; a distributed array to a regular NumPy ndarray when `materialize_as_ndarray` is called. This was prompted by an [improvement in Zappy](https://github.com/lasersonlab/zappy/pull/7) (released in 0.2.0) that makes Zappy arrays implement the `__array__` method. As another benefit, this change should make it easier to use other implementations of the ndarray interface in Scanpy in the future. Note that there is still a Dask-specific call for the case of a sequence of arrays, since Dask can materialize these in one call to `compute`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439:255,release,released,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439,1,['release'],['released']
Deployability,This was fixed in https://github.com/scverse/scanpy/pull/2424:. @ivirshup when should we release 1.10?. ![image](https://github.com/scverse/scanpy/assets/291575/f1d7f2e3-1943-492c-a1f6-2b0499affe94),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1846864918:89,release,release,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1846864918,1,['release'],['release']
Deployability,"This what I'm currently compiling. While [Scanpy 1.1](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018) concerned some basic updates and more general features, Scanpy 1.2 will be about PAGA. No worries, everything is backward compatible... but PAGA will have many more cool features and in addition, also feature a second, better model. I will release it this weekend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757:140,update,updates,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393970757,2,"['release', 'update']","['release', 'updates']"
Deployability,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:383,install,install,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267,1,['install'],['install']
Deployability,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:58,install,installed,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267,2,['install'],"['install', 'installed']"
Deployability,"To add more confusion I created a new environment in which I only installed from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway?. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542:66,install,installed,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542,3,"['Install', 'install']","['Installed', 'installed']"
Deployability,To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1569:53,release,release,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569,2,['release'],['release']
Deployability,To make install from source tarball work. Fixes #995. @falexwolf can you check if it works for you now?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/997:8,install,install,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/997,1,['install'],['install']
Deployability,"To users: you can work around this in your environments by installing the previous version of matplotlib, for example with:. ```; $ pip install 'matplotlib<3.7'; ```. and/or adding a similar version specification to your `pyproject.toml`/`setup.py`/equivalent file, like https://github.com/single-cell-data/TileDB-SOMA/commit/0bc97e893edd2ab6b8dc7d76635a079ae7f91516.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2411#issuecomment-1429887964:59,install,installing,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2411#issuecomment-1429887964,2,['install'],"['install', 'installing']"
Deployability,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-440740958:122,update,updated,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-440740958,1,['update'],['updated']
Deployability,"Travis builds are currently breaking due to incompatible versions of numpy being present, so no compatible version gets installed. This should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320:120,install,installed,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320,1,['install'],['installed']
Deployability,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/482:9,install,install,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482,2,['install'],['install']
Deployability,Turn on code cov patch annotations,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2848:17,patch,patch,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2848,1,['patch'],['patch']
Deployability,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:26,update,updates,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,3,['update'],['updates']
Deployability,"Type `pip install pandas==0.22.0` to get the same version and run the notebook again... Of course, if we are not compatible with more recent versions of pandas, we'll immediately fix the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/158#issuecomment-390636706:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/158#issuecomment-390636706,1,['install'],['install']
Deployability,TypingError: Failed in nopython mode pipeline (step: nopython frontend),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:37,pipeline,pipeline,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['pipeline'],['pipeline']
Deployability,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/319#issuecomment-432357859:173,update,updated,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319#issuecomment-432357859,1,['update'],['updated']
Deployability,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,integrat,integrates,697,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691,1,['integrat'],['integrates']
Deployability,"UPDATE: ; So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:; `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498837162:0,UPDATE,UPDATE,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498837162,1,['UPDATE'],['UPDATE']
Deployability,"URCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py"", line 50, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148:5835,install,install,5835,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148,1,['install'],['install']
Deployability,"Umap currently (0.5.4) suppresses that warning, so I guess it’s ok: https://github.com/lmcinnes/umap/blame/master/umap/spectral.py#L532-L536. that was fixed in https://github.com/lmcinnes/umap/pull/1031. You probably just need to upgrade umap to 0.5.4 to no longer see that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3139#issuecomment-2210680950:230,upgrade,upgrade,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3139#issuecomment-2210680950,1,['upgrade'],['upgrade']
Deployability,Unable to plot embedding after Scanpy release 1.9.7,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830:38,release,release,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830,1,['release'],['release']
Deployability,"Unformatted notes by me:. Behaviors that exist for `inplace`/`copy`:. - update AnnData in place (where appropriate, choose target layer, obsm[key], …); - leave original AnnData alone, return; - new AnnData; - newly created array. `inplace=False`/`copy=True` returning array instead of whole object (AnnData) is confusing, but is sometimes done.; but having a choice to return the array makes sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664351687:72,update,update,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664351687,1,['update'],['update']
Deployability,"Unfortunately, the team did not get around to implementing the desired behavior yet and therefore, we do not have any updates yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276#issuecomment-2446254307:118,update,updates,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276#issuecomment-2446254307,1,['update'],['updates']
Deployability,Update 1.7.2 release notes on master,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1787:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1787,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update CellRank's metion in the ecosystem,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2269:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2269,1,['Update'],['Update']
Deployability,Update MAGIC API and docs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/896:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896,1,['Update'],['Update']
Deployability,Update MAGIC API to 2.x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/988:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988,1,['Update'],['Update']
Deployability,Update Neighbors docstring,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2664:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2664,1,['Update'],['Update']
Deployability,Update Preprocessing functions with numba,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3011:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3011,1,['Update'],['Update']
Deployability,Update PyPI badge,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/116:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116,1,['Update'],['Update']
Deployability,Update _dendrogram.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2765:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2765,1,['Update'],['Update']
Deployability,Update _docs.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2793:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2793,1,['Update'],['Update']
Deployability,Update _louvain.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1819:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1819,1,['Update'],['Update']
Deployability,"Update `pyproject.toml` to use [PEP-621](https://www.python.org/dev/peps/pep-0621/) and [PEP-631](https://www.python.org/dev/peps/pep-0631/) metadata. Basically, simplify by removing most of the `tool.flit` stuff. @flying-sheep, would you like to do this/ do you foresee any blockers?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1776:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776,1,['Update'],['Update']
Deployability,Update `sc.datasets.visium_sge` to include new datasets from 10x genomics and account for spaceranger version.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1473:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1473,1,['Update'],['Update']
Deployability,Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3285:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3285,1,['Update'],['Update']
Deployability,Update chat link from slack to zulip,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2192:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2192,1,['Update'],['Update']
Deployability,Update contributing guide to include tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/772:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/772,1,['Update'],['Update']
Deployability,Update coverage job,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2996:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2996,1,['Update'],['Update']
Deployability,Update dca version,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/793:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793,1,['Update'],['Update']
Deployability,Update deps pynndescent,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1927:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1927,1,['Update'],['Update']
Deployability,Update dev branch,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2421:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2421,1,['Update'],['Update']
Deployability,Update diffmap.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/54:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/54,1,['Update'],['Update']
Deployability,Update doc dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2775:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2775,1,['Update'],['Update']
Deployability,Update draw_graph.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/215:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/215,2,['Update'],['Update']
Deployability,Update ecosystem.rst,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1577:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577,1,['Update'],['Update']
Deployability,Update external page,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2400:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2400,1,['Update'],['Update']
Deployability,Update for cope with issue introduced in umap-learn 0.5.2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2028:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028,1,['Update'],['Update']
Deployability,Update function _download_visium_dataset,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1475:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1475,1,['Update'],['Update']
Deployability,Update hashsolo docstring,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2429:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2429,1,['Update'],['Update']
Deployability,"Update here: I met @ivirshup at the SCG conference in Utrecht and we briefly chatted about this issue on the way to a pub. Isaac seemed supportive of the whole setup in this PR, which would allow to do this:. ``` Python; sc.pp.neighbors(); sc.tl.tsne() # default binarize='auto' resolves to binarize=True here. UMAP kNN graph but binary weights; sc.tl.tsne(binarize=False) # this would use UMAP weights but normalize to sum to 1; sc.pp.neighbors_tsne(); sc.tl.tsne() # default binarize='auto' resolves to binarize=False here; ```. As @pavlin-policar showed above, these three t-SNE calls yield very similar embeddings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427,1,['Update'],['Update']
Deployability,Update import statements for SAM,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1052:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1052,1,['Update'],['Update']
Deployability,Update ingest example. Fixes `AttributeError: 'Ingest' object has no attribute '_pca_use_hvg'`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3075:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3075,1,['Update'],['Update']
Deployability,Update install instructions to use leiden,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1216:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1216,2,"['Update', 'install']","['Update', 'install']"
Deployability,Update installation.rst,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1317:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1317,2,"['Update', 'install']","['Update', 'installation']"
Deployability,Update intersphinx links,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2166:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2166,1,['Update'],['Update']
Deployability,Update leiden future warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2951:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2951,1,['Update'],['Update']
Deployability,Update link for moignard15,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1542:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1542,1,['Update'],['Update']
Deployability,Update log handling,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2855:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2855,1,['Update'],['Update']
Deployability,Update marsilea tutorial to use group_ methods,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3001:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3001,1,['Update'],['Update']
Deployability,Update minimum Python version to 3.10,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3282:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282,1,['Update'],['Update']
Deployability,Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216,1,['Update'],['Update']
Deployability,Update numba usage in qc metrics,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462,1,['Update'],['Update']
Deployability,Update paga.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/384:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/384,1,['Update'],['Update']
Deployability,Update palantir external,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1245:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1245,1,['Update'],['Update']
Deployability,Update policy for external tools and point to scverse ecosystem packages instead.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2400:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2400,1,['Update'],['Update']
Deployability,Update pp docs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/879:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/879,1,['Update'],['Update']
Deployability,Update pyproject.toml,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3281:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281,1,['Update'],['Update']
Deployability,Update release guide,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2184:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2184,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1218:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1218,6,"['Update', 'release']","['Update', 'release']"
Deployability,Update release notes for 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1569:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update release notes for 1.7.1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1683:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1683,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update release_notes and remove inline links,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/934:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/934,1,['Update'],['Update']
Deployability,Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1595:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595,1,['Update'],['Update']
Deployability,Update scanpydoc to fix builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2102:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2102,1,['Update'],['Update']
Deployability,Update setup.py to update dependent packages,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518,2,"['Update', 'update']","['Update', 'update']"
Deployability,Update tests now that 3d plots work,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1493:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1493,1,['Update'],['Update']
Deployability,Update the News section,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571,1,['Update'],['Update']
Deployability,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474,2,"['Update', 'integrat']","['Update', 'integration']"
Deployability,Update to `sce.external.tl._harmony_timeseries.py`.; Exposing parameters passed to `harmony.core.augmented_affinity_matrix` function. Linked to issue reported [here](https://github.com/dpeerlab/Harmony/issues/11).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1091:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1091,1,['Update'],['Update']
Deployability,Update tutorials.rst,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1230:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1230,1,['Update'],['Update']
Deployability,Update umap.py,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/217:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/217,1,['Update'],['Update']
Deployability,Update unit test for mannwhitneyu to work with scipy 1.7,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1893:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1893,1,['Update'],['Update']
Deployability,Update verbosity call in sc.external.tl.phate,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/716:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/716,1,['Update'],['Update']
Deployability,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262,1,['Update'],['Update']
Deployability,Update: I was able to get rid of the error by pip installing `dask` manually in my conda environment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2210#issuecomment-1088508941:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2210#issuecomment-1088508941,2,"['Update', 'install']","['Update', 'installing']"
Deployability,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103,2,"['Update', 'update']","['Update', 'updated']"
Deployability,"Update: The initial issue has been fixed, we don't have a more generic embedding plotting function yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-522915275:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-522915275,1,['Update'],['Update']
Deployability,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355#issuecomment-437501315,4,"['Update', 'install', 'upgrade']","['Update', 'install', 'installs', 'upgrade']"
Deployability,Update:. Docs don't build with sphinx 4.1.0 due to a error triggered by `scanpydoc`. Sphinx will be pinned until this is solved (which is when this issue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentatio,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['Update'],['Update']
Deployability,"Update:; * Added `read_visium` function, based on [understanding output from 10x](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview); * addressed #1027 by changing requirements; * added features to import in `adata.obs`, in addition to coordinates in `adata.obsm`, in `scanpy.datasets`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1034:0,Update,Update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1034,2,"['Update', 'pipeline']","['Update', 'pipelines']"
Deployability,Updated /api and /tools to include sandbag and cyclone of the pypairs method. For more details see https://github.com/rfechtner/pypairs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/126:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/126,1,['Update'],['Updated']
Deployability,Updated PyPairs to v3.0.9,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457,1,['Update'],['Updated']
Deployability,Updated `sc.external.pp.bbknn()` to match current arguments and docstring.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1868:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868,1,['Update'],['Updated']
Deployability,Updated missing params in docstrings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2888:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2888,1,['Update'],['Updated']
Deployability,Updated plots. <details>; <summary> ranked_genes </summary>. Orig:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785867-1b087600-d1d5-11ea-85dc-e8ea31303ae7.png). Current:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785705-e0064280-d1d4-11ea-966e-f856ee2468f6.png). </details>. <details>; <summary> ranked_genes_sharey </summary>. Orig:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785853-13e16800-d1d5-11ea-997a-d72955c5b3ce.png). Current:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785819-06c47900-d1d5-11ea-8a74-22ef2b4803d0.png). </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920,1,['Update'],['Updated']
Deployability,Updated the PR:; - renamed argument in `visium_sge` to `include_hires_tiff`; - renamed argument in `read_visium` to `source_image_path` and removed guessing of image file if `source_image_path` is None,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299,1,['Update'],['Updated']
Deployability,Updated the notebook above with `to_adata_joint` example - this function returns the concatenation of `adata_ref` and `adata_new` with projected representations and mapped labels.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-517078460:0,Update,Updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-517078460,1,['Update'],['Updated']
Deployability,Updating release guide to contain current practices.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2184:9,release,release,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2184,1,['release'],['release']
Deployability,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697:0,Upgrade,Upgrade,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697,1,['Upgrade'],['Upgrade']
Deployability,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269:191,release,release,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314#issuecomment-431635269,1,['release'],['release']
Deployability,"Using a dict is an interesting idea. Right now I'd prefer that it matches with other ""vectorized"" arguments (like `vmin`, `vmax`) which take a list. I think for categorical values the continuous arguments are ignored (right @fidelram?). Using a `dict` would get rid of the ""sometimes arguments are ignored"" part of this, but I think consistency is more important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471:184,continuous,continuous,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471,1,['continuous'],['continuous']
Deployability,"Using the progress bar from tqdm.auto causes a `ImportError` when `ipywidgets` is not installed. ; The progressbar from the top level `tqdm` module does not have this dependency. . Repex: . ```python; import scanpy as sc; sc.datasets.moignard15(); ```. Output: ; ```; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols); 97 else: # No total? Show info style bar with no progress tqdm status; ---> 98 pbar = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filena",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:86,install,installed,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['install'],['installed']
Deployability,Utilize magic comma in new black update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1394:33,update,update,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1394,1,['update'],['update']
Deployability,"Vector images would be rasterized when `settings._vector_friendly` is True. This should be fixed so that when `settings._vector_friendly` is True, image is NOT rasterized, and when False, image should be rasterized. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #1405; - [x] Tests included or not required because: simple reversion of booleans; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: did not create a release on this",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3181:668,release,release,668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3181,3,"['Release', 'release']","['Release', 'release']"
Deployability,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,integrat,integration,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791,1,['integrat'],['integration']
Deployability,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370:146,update,updated,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370,1,['update'],['updated']
Deployability,"Warning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")). #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; bottleneck 1.3.2; cairo 1.20.0; cffi 1.14.3; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2.30.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; joblib 0.17.0; kiwisolver 1.3.0; legacy_api_wrap 0.0.0; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.1.3; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.1; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; traitlets 5.0.5; typing_extensions NA; umap 0.5.0; wcwidth 0.2.5; win32api NA; win32com NA; win32security NA; yaml 5.3.1; zmq 19.0.2; zope NA; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.6; notebook 6.1.4; -----; Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.17763-SP0; 12 logical CPU cores, Intel64 Family 6 Model 165 Stepping 2, GenuineIntel; -----; Session information updated at 2021-03-14 11:37; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1742:3725,update,updated,3725,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742,1,['update'],['updated']
Deployability,Was the first install also with `pip`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037626429:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037626429,1,['install'],['install']
Deployability,We are submitting PR for speed up of the filtering; | | Time |; | -- | -- |; | Original | 290.59 |; | Updated | 187.03 |; | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge; <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3330:102,Update,Updated,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330,3,"['Release', 'Update', 'release']","['Release', 'Updated', 'release']"
Deployability,We call out to `loompy` to read in loom files. I also am having trouble replicating using `loompy 3.0.6`. Could you try updating your loompy install and seeing if the problem persists?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056:141,install,install,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056,1,['install'],['install']
Deployability,"We currently distribute `scanpy` through `conda-forge`, so I would recommend using that for up to date versions. Will this still error if you've done that?. Also, it's a little unclear if your session info came from the conda environment you're running into issues with. Is this definitely the case? There shouldn't be that new of releases of scanpy or anndata available through bioconda as far as I know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063056234:331,release,releases,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172#issuecomment-1063056234,1,['release'],['releases']
Deployability,We discussed outside @flying-sheep - we will release with the bound then and just wait here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2186436517:45,release,release,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2186436517,1,['release'],['release']
Deployability,"We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers.; Hi,; We are submitting PR for speed up of the _get_mean_var function.; | | Time(sec) |; | -- | -- |; | Original | 18.49 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:263,Update,Updated,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['Update'],['Updated']
Deployability,We have SCVI working on anndata objects for data integration in our benchmarking study.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617:49,integrat,integration,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617,1,['integrat'],['integration']
Deployability,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,integrat,integration,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122,1,['integrat'],['integration']
Deployability,"We have the ![](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg) badge. I think we should remove that if we’re not actually recommending installation that way. If there’s a “[On ]bioconda” badge to put next to the PyPI badge, I’d prefer that. /edit: The badges are custom, we can control the text. I replaced it to match the PyPI badge, except that we can’t get a version, so I put a cute snake: ![](https://img.shields.io/pypi/v/scanpy.svg) ![](https://img.shields.io/badge/bioconda-🐍-blue.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-460560962:45,install,install,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-460560962,2,['install'],"['install', 'installation']"
Deployability,"We just merged an update on the `downsample_counts` function by @ivirshup; evidently, the data type shouldn't be changed by downsampling, should it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475782293:18,update,update,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475782293,1,['update'],['update']
Deployability,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:259,hotfix,hotfix,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['hotfix'],['hotfix']
Deployability,We recently made a release (1.8.2) which has a bug fix for an incompatibility with umap 0.5.2. The fix was actually contributed by the author of UMAP!. Upgrading should fix your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069:19,release,release,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069,1,['release'],['release']
Deployability,We recently transitioned from scvi to scvi-tools. Here I update the ecosystem docs to reflect this change. I decided to make a new section for it as we have a diversity of models in our package that don't fit any of the other categories well.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1421:57,update,update,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421,1,['update'],['update']
Deployability,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:258,release,release,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:126,install,installing,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565,1,['install'],['installing']
Deployability,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271:450,install,installed,450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271,1,['install'],['installed']
Deployability,We should mention that scanpy is now distributed through conda forge in the installation instructions,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1912:76,install,installation,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1912,1,['install'],['installation']
Deployability,"We should probably let users provide a set of clusters for the reference. . Until then, could you do:. ```python; groups_to_test = (; adata.obs[""clusters""]; .value_counts(); .loc[lambda x: x > 1]; .index; ); subset_adata = adata[adata.obs[""clusters""].isin(groups_to_test)].copy(); sc.tl.rank_genes_groups(subset_adata, ...); adata.uns[""rank_genes_groups""] = subset_adata.uns[""rank_genes_groups""]; ```. Or, if you want the singlets in the reference:. ```python; sc.tl.rank_genes_groups(adata, groups=list(groups_to_test), ...); ```. BTW, about the h5py bytes thing, things written as strings are read as strings with h5py 3 as of anndata 0.7.5, which just got released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542:659,release,released,659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542,1,['release'],['released']
Deployability,"We sped up our release process, which means it’s not a huge deal to make one. But it’s still more than just clicking a button, so since we just made a release, maybe in 1-2 weeks?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1814013706:15,release,release,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1814013706,2,['release'],['release']
Deployability,We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/939:220,release,release,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939,1,['release'],['release']
Deployability,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302:240,integrat,integrated,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302,1,['integrat'],['integrated']
Deployability,We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1570:66,release,releases,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570,2,['release'],"['release', 'releases']"
Deployability,"We'll have 1.5.0 in a couple of days, and so it's fine to make a few behavior changes. We should put a warning in the release notes, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/957#issuecomment-627228911:118,release,release,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-627228911,1,['release'],['release']
Deployability,"We're also hitting this. I didnt test this yet, but from our CI history it seems like this worked with seaborn-0.12.2, but broke when that updated to seaborn-0.13.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761944327:139,update,updated,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761944327,1,['update'],['updated']
Deployability,"We're having trouble installing louvain on CI due to a recent setuptools release (would have been nice if setuptools had more vocal warnings about this ahead of time, but alas). See: vtraag/louvain-igraph/issues/57. This PR makes louvain optional. This was done by:. ### Skip louvain dependent tests. While these largely were tests checking that louvain works, some of these are testing other things. The biggest example here is `test_paga_paul15_subsampled.py`, which is really a test of PAGA. This should be corrected. ### Remove louvain dependency from tests. Some tests, like those for `rank_genes_groups_logreg` used louvain, but really didn't have to. `test_pbmc3k` could just have `louvain` calls replaced with `leiden` with only one plot triggering an error. ### `louvain` is no longer installed on CI. This should get around the build issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2063:21,install,installing,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2063,3,"['install', 'release']","['installed', 'installing', 'release']"
Deployability,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:476,update,update,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487,1,['update'],['update']
Deployability,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:599,integrat,integration,599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,1,['integrat'],['integration']
Deployability,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:212,Install,Installing,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984,4,"['Install', 'install']","['Installing', 'installation', 'installations', 'installs']"
Deployability,"Well, I think it is better to be consistent with the paper [Diffusion pseudotime robustly reconstructs lineage branching](https://www.nature.com/nmeth/journal/v13/n10/full/nmeth.3971.html) since this paper first introduces the diffusion pseudotime concept. In Figure 1 (c) of this paper, there is a _DPT order_. It seems the dpt order in this paper is just a global rank for each individual cell according to their pseudotime. Therefore, I suggest that the adata.smp['dpt_order'] and the one in the figure should have the same meaning, though IMHO dpt_order only matters for cells on the same branch. If we extract cells by their dpt_group, then the dpt_order is still applicable even though it is not continuous now. . In short, I think a dpt_order defined as the global rank by pseudotime like the one in the original paper is more understandable. By the way, if there are multiple branches in the diffusion map, is there some way to assign the cells to a certain branch? That is to say, can we provide an _adata.smp['branch']_ field?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836:702,continuous,continuous,702,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/27#issuecomment-314766836,1,['continuous'],['continuous']
Deployability,"Well, it makes working with all this easier, specifically if you can just see your figures inline instead of popup windows. You can try out jupyterlab here to get a small tutorial: https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/try.jupyter.org?urlpath=lab. If you think you’d like it, just `pip install jupyterlab` and start it with `jupyter lab`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/850#issuecomment-532657402:302,install,install,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532657402,1,['install'],['install']
Deployability,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439747800:203,install,install,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439747800,1,['install'],['install']
Deployability,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now!. Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/888#issuecomment-547828011:35,release,release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888#issuecomment-547828011,1,['release'],['release']
Deployability,We’ll update this issue when we merge the PR. You can subscribe to scanpy releases on GitHub to be notified when we release something!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892350467:6,update,update,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892350467,3,"['release', 'update']","['release', 'releases', 'update']"
Deployability,"What command are you running?. You might be running into the fact that we no longer put scanpy on bioconda, but instead use conda-forge. So `conda install -c conda-forge scanpy` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580,1,['install'],['install']
Deployability,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:45,install,installed,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616,2,"['install', 'update']","['installed', 'update']"
Deployability,"What do you mean? Scanpy’s metadata only specifies a lower Python version bound:. https://github.com/scverse/scanpy/blob/d7e13025b931ad4afd03b4344ef5ff4a46f78b2b/pyproject.toml#L13. Which of the following is it?. - Does it refuse to install because some dependency is not Python 3.11 compatible?; - Does it crash when run there?; - Something else?. I assume it’s just that numba is incompatible still (which will always be a recurring problem as long as we depend on it), but please let me know. With installation issues, you can always run `pip -vv install scanpy` to get much more output that could be helpful. Please [use code blocks](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) when including it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462:233,install,install,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462,3,['install'],"['install', 'installation']"
Deployability,"What happens is . 1. `pip install -e` adds the repo directory to `sys.path`, since we don‘t use a `src` directory; 2. something imports the `data` directory instead of `scanpy.testing._helpers.data` and stores it in the wrong place in `sys.modules`:. ```; >>> sys.modules[""scanpy.testing._helpers.data""]; <module 'scanpy.testing._helpers.data' (namespace) from ['/home/phil/Dev/Python/Single Cell/scanpy/data']>; ```. I assume that some of the attempts to fix our doctest woes in Pytest 8.1 led to this bug being introduced in Pytest 8.1. But I’d like to figure out what exactly causes that bug before we do anything. PS: Of course things like this are exactly why we switched anndata to a `src` layout: that way `src` can be in `sys.path` instead of a kitchen sink directory, and `import <something>` will only see directories that are meant to contain exclusively python packages. Pytest shouldn’t `import data` and then store the resulting module as `sys.modules['scanpy.testing._helpers.data']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993#issuecomment-2045317672:26,install,install,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993#issuecomment-2045317672,1,['install'],['install']
Deployability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:28,update,update,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,1,['update'],['update']
Deployability,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:31,install,installation,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825,12,"['Install', 'install']","['Installing', 'install', 'installation', 'installations', 'installing']"
Deployability,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:58,update,update,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['update'],['update']
Deployability,"What stays the same:. - `pip install scanpy`; - `pip install . `; - `pip install git+https://...`; - you can install your deps with conda; - you can do a dev install. What changes:. - Please check the [install docs](https://scanpy.readthedocs.io/en/flit-for-isaac/installation.html#development-version), in short:; - `pip install -e .[every,single,extra]` → `flit install -s` for dev installs; - `beni pyproject.toml > environment.yml` for conda; - Extremely simple `flit build` and `flit publish`. Maybe install `keyring` to store your publish password, and you know everything you need to.; - `flit build` doesn’t clutter your dev directory with `build/` and `*.egg-info/` junk, it just creates `dist/scanpy-*{.whl,.tar.gz}`.; - No more obscure stuff nobody understands (MANIFEST.in, package_data, …); - Centralized setup configuration in pyproject.toml instead of spread over multiple files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527,12,"['configurat', 'install']","['configuration', 'install', 'installation', 'installs']"
Deployability,What version of scanpy are you using? I don't see a function called `_get_color_values` in `scatterplots.py` in the HEAD version. Can you try installing the development version and seeing if you get the same error? The issue might be that the version you are currently using does not include the fix I made above. ```; git clone https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451:142,install,installing,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451,2,['install'],"['install', 'installing']"
Deployability,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:246,install,install,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375,2,['install'],['install']
Deployability,"What would be a better name?. Also, believe this has been discussed before, but not sure where. I think we wanted to normalize on `frac` vs `pct` vs `percent` or something?. This was also based on `scater`'s qc metrics function, which now lives in `scuttle`. The argument there is named `percent.top`: https://bioconductor.org/packages/release/bioc/manuals/scuttle/man/scuttle.pdf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2842#issuecomment-1934570365:336,release,release,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2842#issuecomment-1934570365,1,['release'],['release']
Deployability,What you can do is. 1. go into the folder from the extracted zip; 2. `git init`; 3. `git tag v1.4.5.dev0`; 4. `pip install -e .`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-533015846:115,install,install,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533015846,1,['install'],['install']
Deployability,"What you describe doesn‘t need to happen, and you can fix this!. 1. go to https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/; 2. make a PR that patches conda’s dependency data to include this constraint; 3. the problem is gone",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158:126,patch,patches-feedstock,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158,2,['patch'],"['patches', 'patches-feedstock']"
Deployability,Whats the timeline here? When will there be a release that includes this fix? Or should I downgrade pandas in the meantime?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015:46,release,release,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015,1,['release'],['release']
Deployability,"When I call `sc.pp.normalize_total(adata)` on a system that does not have Dask installed, I get the following error:. ```; AnnData object with n_obs × n_vars = 710 × 33538; obs: 'filter_with_counts', 'scrublet_doublet_score', 'filter_with_scrublet'; var: 'gene_ids', 'feature_types', 'genome', 'filter_with_counts'. File ""/viash_automount/home/rcannood/workspace/viash_temp//viash-run-normalize_total-ppLnd4"", line 32, in <module>; sc.pp.normalize_total(; File ""/usr/local/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py"", line 200, in normalize_total; adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer; File ""/usr/local/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py"", line 25, in _normalize_data; if isinstance(counts, DaskArray):; TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union; ```. The error was introduced in [9cb915](https://github.com/scverse/scanpy/commit/9cb915bee5bfe11f62ffb37c0405656aae4574f2#diff-34d549afa2b23d0b2066964a51f698d918398edffd38379a73d02390e31ae5e8R24). . This PR solves the issue by checking that DaskArray is not None, though I'm sure alternative solutions are also possible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2209:79,install,installed,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2209,1,['install'],['installed']
Deployability,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340,7,['install'],['install']
Deployability,When I do this it tells me I am using version 1.4.1 but when I run sc.logging.print_versions() it comes up as version 1.01. I assumed this is somehow related to the version installed with scanpy that it uses by default maybe? But I feel I am not proficient enough in python to determine that,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683:173,install,installed,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683,1,['install'],['installed']
Deployability,"When I have again time I will try step by step my script and try to see what happens, Maybe it will be useful in future for someone else :); I will post an update here in a little while.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440#issuecomment-456702805:156,update,update,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440#issuecomment-456702805,1,['update'],['update']
Deployability,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:589,install,installed,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316,1,['install'],['installed']
Deployability,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:529,install,install,529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"When I tried to import scanpy into python 3.5.2, I got the following error message,. ```; >>> import scanpy as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>; from .utils import check_versions, annotate_doc_types; File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>; from ._settings import settings; File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351; f'{k} = {v!r}'; ^; SyntaxError: invalid syntax; ```; My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/855:617,install,installed,617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855,2,['install'],"['install', 'installed']"
Deployability,"When an old version of tqdm ([< 4.29.1](https://github.com/nteract/papermill/issues/287)) is installed, scanpy cannot be imported. . ```bash; conda create -n test_scanpy_tqdm scanpy=1.5.1 tqdm=4.19.4; conda activate test_scanpy_tqdm; python -c ""import scanpy as sc"" ; ```. ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/sturm/anaconda3/envs/test_scanpy_tqdm/lib/python3.8/site-packages/scanpy/__init__.py"", line 39, in <module>; from . import datasets, logging, queries, external, get; File ""/home/sturm/anaconda3/envs/test_scanpy_tqdm/lib/python3.8/site-packages/scanpy/datasets/__init__.py"", line 3, in <module>; from ._datasets import (; File ""/home/sturm/anaconda3/envs/test_scanpy_tqdm/lib/python3.8/site-packages/scanpy/datasets/_datasets.py"", line 8, in <module>; from tqdm.auto import tqdm; ModuleNotFoundError: No module named 'tqdm.auto'; ```. ### Suggested solution; Either ; * merge #1130 (which has the additional benefit that the [ipywidgets extension does not have to be installed when running in jupyterlab](https://stackoverflow.com/questions/53247985/tqdm-4-28-1-in-jupyter-notebook-intprogress-not-found-please-update-jupyter-an)) or ; * require a minimal version of `tqdm>=4.29.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1244:93,install,installed,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1244,3,"['install', 'update']","['installed', 'update-jupyter-an']"
Deployability,"When combined with https://github.com/theislab/anndata/pull/298, fixes #1000. Unfortunately, the tests here won't pass until the PR at anndata is in a release. * Adds tests checking for copying when plotting a view; * Prevents setting color palette when it doesn't need to change",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1002:151,release,release,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1002,1,['release'],['release']
Deployability,When data is continuous and legend is a continuous color bar a second legend could be added below it showing the categorical NaN (like the usual categorical legend).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540:13,continuous,continuous,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540,2,['continuous'],['continuous']
Deployability,"When less than 30 features are present in adata.X, pca_loadings will plot some components twice.; The patch solves the problem by adding an n_point parameter (default value 30, same as anndata.ranking())that is then passed to anndata.ranking(). . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2061:102,patch,patch,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061,1,['patch'],['patch']
Deployability,"When less than 30 features are present in adata.X, pca_loadings will plot some components twice.; The patch solves the problem by adding an n_point parameter (default value 30, same as anndata.ranking())that is then passed to anndata.ranking(). the patch also modifies rankings plot to account for remove the dots when all elements in order_scores are plotted; the patch replaces the previously submitted patch. ps: had to struggle a lot with gi. the contribution guide should be updated... I may work on it",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075:102,patch,patch,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075,5,"['patch', 'update']","['patch', 'updated']"
Deployability,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:41,release,released,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652,3,"['release', 'update']","['release', 'released', 'update']"
Deployability,"When would you want `na_as_category` off? Is it important enough to add a new parameter for?. Also, how should the continuous color bar show that there is a null value? (Suggestions with code very welcome)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327:115,continuous,continuous,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327,1,['continuous'],['continuous']
Deployability,"While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'.; Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually?. ```pytb; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-191-71b705e00011> in <module>; ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 112 directed = False; 113 if not directed: logg.debug(' using the undirected graph'); --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 115 if use_weights:; 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 379 def get_igraph_from_adjacency(adjacency, directed=None):; 380 """"""Get igraph graph from adjacency matrix.""""""; --> 381 import igraph as ig; 382 sources, targets = adjacency.nonzero(); 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>; 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807:1432,upgrade,upgrade,1432,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807,2,['upgrade'],['upgrade']
Deployability,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,integrat,integration,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['integrat'],['integration']
Deployability,Will tag a release on merge,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1785:11,release,release,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1785,1,['release'],['release']
Deployability,Will the api of “scanpy.read_visium” be upgraded to be compatible with the new space information file name tissue_positions.csv which was previously named tissue_positions_list.csv ，after Space Ranger 2.0?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2462:40,upgrade,upgraded,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2462,1,['upgrade'],['upgraded']
Deployability,Will there be an update on this issue?; It would be really helpful!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/984#issuecomment-627067415:17,update,update,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-627067415,1,['update'],['update']
Deployability,Wishbone integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1063:9,integrat,integration,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063,1,['integrat'],['integration']
Deployability,"With #3056 merged, this now says. > before | after | ratio | benchmark; > --- | --- | --- | ---; > 448±100ms | 381±100ms | ~0.85 | `preprocessing_counts.time_scrublet('pbmc68k_reduced')`. but ASV seems to think that’s not enough to report. Also unclear why it’s reported as taking 26 minutes by github:. > <img width=""252"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/291575/6117cece-a145-4b46-85a4-dd86a61819ef"">. I see in the server logs. > - May 14 10:15:57 scvbench benchmark[1462905]: 2024-05-14T10:15:57.547945Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting; > - […running benchmarks]; > - May 14 10:27:18 scvbench benchmark[1462905]: 2024-05-14T10:27:18.793352Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting. which means that between setting the check run to “running” and to “done”, 11m21s passed. Maybe GitHub counts the queue time?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339:567,PATCH,PATCH,567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339,2,['PATCH'],['PATCH']
Deployability,"With pandas 1.3.4 and 1.3.3. * I can't replicate the initial issue; * I can replicate @michalk8's example. This looks very upstream in pandas. I will try and submit an issue/ check that this hasn't been reported to pandas already tomorrow. This may be a kinda easy fix (e.g. check value shape better during column assignment in pandas), but it can take a bit to figure out how to fix things there. AFAIK, we removed calls in scanpy which assigned (n x 1) matrices to pandas because of related, non-formatting error. Is the current scanpy release assigning these matrices anywhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046:538,release,release,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046,1,['release'],['release']
Deployability,"With the new ebi single cell database release one of the files is a little different (col names/ obs_names). That part of the reader has been fixed, and I've updated the test for the reprocessed data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/741:38,release,release,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/741,2,"['release', 'update']","['release', 'updated']"
Deployability,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,integrat,integrated,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988,1,['integrat'],['integrated']
Deployability,"Would be good to know if there's any nice tooling around this, as this could make fixtures and parametrized tests a bit difficult. For example, not installing `dask` makes test collection fail in the `test_normalization.py` file. <details>; <summary> Error </summary>. ```pytb; _______________________ ERROR collecting scanpy/tests/test_normalization.py ________________________; ImportError while importing test module '/Users/isaac/github/scanpy/scanpy/tests/test_normalization.py'.; Hint: make sure your test modules/packages have valid Python names.; Traceback:; ../../miniconda3/envs/scanpy-minimal/lib/python3.9/importlib/__init__.py:127: in import_module; return _bootstrap._gcd_import(name[level:], package, level); scanpy/tests/test_normalization.py:5: in <module>; import dask.array as da; E ModuleNotFoundError: No module named 'dask'; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088611630:148,install,installing,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088611630,1,['install'],['installing']
Deployability,Would you also update the installation docs to show people how to call this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/180#issuecomment-398733707,2,"['install', 'update']","['installation', 'update']"
Deployability,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434087328:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434087328,1,['release'],['release']
Deployability,"Writing the example for `sc.queries.enrich(adata, ...)` made me realize I probably don't want to encourage that. Potentially could be fixed by a default p-value cutoff. As for the `gprofiler` version, there are two paths forward I think would work:. 1. Put a hold on the `enrich` function for a bit for the new API to be released; 2. If it'd be useful enough to include now, the docs could note current implementation is provisional and results will change pretty soon. Maybe @fidelram and @LuckyMD have thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-473615430:321,release,released,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-473615430,1,['release'],['released']
Deployability,Yeah noted there is an issue with scipy... Not related to this the single-line update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2739#issuecomment-1803816341:79,update,update,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2739#issuecomment-1803816341,1,['update'],['update']
Deployability,"Yeah, I decided just to go for it 🤠. I'd be happy to update the code if there ends up being a `gene_symbols` flag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/385#issuecomment-456026742:53,update,update,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/385#issuecomment-456026742,1,['update'],['update']
Deployability,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:224,Update,Updates,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522,1,['Update'],['Updates']
Deployability,"Yeah, please file your issue with [scanpy-scripts](https://github.com/ebi-gene-expression-group/scanpy-scripts) then, and ask them why they want an old scipy version and if they can upgrade their code to work with scipy 1.3+",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559:182,upgrade,upgrade,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559,1,['upgrade'],['upgrade']
Deployability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:51,release,release,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,"['release', 'update']","['release', 'updates']"
Deployability,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,configurat,configuration,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328,1,['configurat'],['configuration']
Deployability,"Yep, but still you need data passed to not only have the same dimensionality, you need dimensions to have the same meaning any time you want to project new data. If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. Using genes to fit a initial UMAP will ensure that you can transform new data, provided you have the same genes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205:174,integrat,integrated,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205,1,['integrat'],['integrated']
Deployability,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553415534:69,install,installed,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553415534,1,['install'],['installed']
Deployability,Yes! Please wait for the next release. Please also checkout squidpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2462#issuecomment-1503220199:30,release,release,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2462#issuecomment-1503220199,1,['release'],['release']
Deployability,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,integrat,integrate,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514,1,['integrat'],['integrate']
Deployability,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:463,update,updated,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291,1,['update'],['updated']
Deployability,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/906#issuecomment-551413589:7,install,installed,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906#issuecomment-551413589,1,['install'],['installed']
Deployability,"Yes, I know about an issue that is probably related to that: At some point in `add_or_update_graph_in_adata`, numpy takes more cores than it's supposed to - that's the only instance in the whole of Scanpy. Otherwise it's well-behaved. When Scanpy 1.0 will be released in the next few days, this will be resolved. Do you think this will do the job for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/101#issuecomment-371489944:259,release,released,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/101#issuecomment-371489944,1,['release'],['released']
Deployability,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699:222,update,updated,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699,1,['update'],['updated']
Deployability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:187,install,installation,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['install'],"['installation', 'installed']"
Deployability,"Yes, It helped. I updated the magic version and it seems to be working fine now. ; Thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/208#issuecomment-405897974:18,update,updated,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208#issuecomment-405897974,1,['update'],['updated']
Deployability,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169:116,release,release,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431635169,2,['release'],['release']
Deployability,"Yes, `n_jobs>2` will be faster as computations are done in parallel. But something in your installation doesn't seem to go well with this. I now set the default number of jobs to 1, so in the next Scanpy release, you won't have to set it manually anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032:91,install,installation,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194#issuecomment-404259032,2,"['install', 'release']","['installation', 'release']"
Deployability,"Yes, fine with me. I didn't do it for Scanpy in the beginning as the formatting I wanted requires quite a few lines. I then closely mimicked Scanpy's logging using Python's logging for another package here: https://github.com/NDKoehler/DataScienceBowl2017_7th_place/blob/638542c3cde5af45bf34d0391695ab0e54ce78b8/dsb3/pipeline.py#L373-L430. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035:317,pipeline,pipeline,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256#issuecomment-418711035,1,['pipeline'],['pipeline']
Deployability,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:61,install,installation,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231,1,['install'],['installation']
Deployability,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:29,update,update,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019,1,['update'],['update']
Deployability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,integrat,integrated,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,4,"['continuous', 'integrat']","['continuous', 'integrated', 'integration']"
Deployability,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615#issuecomment-489554643:62,release,release,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615#issuecomment-489554643,1,['release'],['release']
Deployability,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357:22,update,update,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357,1,['update'],['update']
Deployability,"Yes, the 'leiden_colors' field in `.uns` will only be updated if needed, i.e., if the number of categories in the `leiden` field in `.obs` exceeds the number of available colors. As Fidel mentions, passing `palette` will automatically trigger resetting the colors according to the chosen palette.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/420#issuecomment-453700295:54,update,updated,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/420#issuecomment-453700295,1,['update'],['updated']
Deployability,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124:209,install,installs,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124,1,['install'],['installs']
Deployability,"Yes, this is a duplicate of #1260. Please follow #1319 to track the next release",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267:73,release,release,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267,1,['release'],['release']
Deployability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,integrat,integration,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['integrat'],['integration']
Deployability,"Yes, we should as soon as many people report seemless installation of the leiden package. I'm still using louvain as I never had any problems with it, but I agree that we should migrate when leiden is stable, mature and easily-installable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-482500682:54,install,installation,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-482500682,2,['install'],"['installable', 'installation']"
Deployability,"Yes, we should definitely have the possibility of visualizing several observation annotations in the future. But Fidel's function is a very good start. I can release a new subversion of Scanpy anytime if you need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178#issuecomment-399889641:158,release,release,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178#issuecomment-399889641,1,['release'],['release']
Deployability,Yes. Do you think scanpy is quality-controlled enough that we can cut new releases whenever we please? Else I’m not comfortable to just create a new tag from master and release it by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460560176:74,release,releases,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460560176,2,['release'],"['release', 'releases']"
Deployability,You can also try creating a new conda environment and installing pytables there at first and try to import it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173:54,install,installing,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173,1,['install'],['installing']
Deployability,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-424700191:72,release,release,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-424700191,1,['release'],['release']
Deployability,You can just use scanpy master if you need this fix ASAP. Otherwise I think we should release a new version pretty soon after merging #1038.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986:86,release,release,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986,1,['release'],['release']
Deployability,"You can use the `categories_order` argument, though would have to check which version that was added in. Are you having issues with newer releases of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461:138,release,releases,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461,1,['release'],['releases']
Deployability,"You could also not make this breaking by checking if there are any `/` in the string and appending `umap` or `violin` after the last `/`? Would be a bit messy, but quicker to release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343:175,release,release,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343,1,['release'],['release']
Deployability,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396:77,install,installation,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396,2,['install'],['installation']
Deployability,"You have scanpy 1.7.3, not the newest version. This is fixed in #2434. I’ll release 1.9.3 soon with the fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564#issuecomment-1645368662:76,release,release,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564#issuecomment-1645368662,1,['release'],['release']
Deployability,You have to first uninstall the version you have already installed of numba.; pip uninstall numba; pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914:57,install,installed,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914,2,['install'],"['install', 'installed']"
Deployability,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/344#issuecomment-436069713:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344#issuecomment-436069713,1,['install'],['install']
Deployability,You should be able to install an experimental m1 native numba here:. https://numba.discourse.group/t/wheels-for-apple-silicon-m1/1282,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2237#issuecomment-1101038489:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2237#issuecomment-1101038489,1,['install'],['install']
Deployability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['integrat'],['integrate']
Deployability,"You’re right: They would be failing on master if master had been tested since anndata 0.7 was released earlier today. We have to merge #989 to get tests to pass again before we can merge this. /edit: done, should work now",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388:94,release,released,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388,1,['release'],['released']
Deployability,"Yup, not a bug with scanpy, but a problem with installing louvain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/786#issuecomment-522570296:47,install,installing,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522570296,1,['install'],['installing']
Deployability,"[ Yes] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ncont = ncont[ncont.obs.pct_counts_mt < 5, :]; ncont.raw = ncont; ```. ```pytb; [TypeError: cannot unpack non-iterable NoneType object]; ```. #### Versions. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.21.5 scipy==1.7.1 pandas==1.3.2 scikit-learn==0.24.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. <details>. [WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbform",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2188:1090,install,installs,1090,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2188,1,['install'],['installs']
Deployability,[Docker Image] scanpy Installation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:22,Install,Installation,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['Install'],['Installation']
Deployability,[Proposal] Integrate Marsilea to visualize AnnData,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:11,Integrat,Integrate,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['Integrat'],['Integrate']
Deployability,"\<!-- Please check (“- [x]”) and fill in the following two boxes --> ; - [x] Closes N/A; - [x] Tests included or not required because:. \<!-- Only check the following box if you did not include release nodes --> ; - [x] Release notes not necessary because: It’s a dev process PR. (made comments visible to show what PR authors see). Current design is that the jobs fail, respectively:; - If there’s no milestone; - If the “Development process” label isn’t there and the “Release notes …” checkbox is not checked, and there are not edits to the relevant release notes file. This is how it looks like if that check fails:. ![grafik](https://github.com/scverse/scanpy/assets/291575/08c324a5-a580-4b30-a838-b7f98fa9787e)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2569:194,release,release,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2569,4,"['Release', 'release']","['Release', 'release']"
Deployability,"\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/; ----------------------------------------; ERROR: Command errored out with exit status 1: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2' Check the logs for full command output",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:5256,install,install-,5256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,5,['install'],"['install', 'install-', 'install-headers', 'install-record']"
Deployability,"\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 601 pipeline = pipeline_class(typingctx, targetctx, library,; 602 args, return_type, flags, locals); --> 603 return pipeline.compile_extra(func); 604 ; 605 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 337 self.state.lifted = (); 338 self.state.lifted_from = None; --> 339 return self._compile_bytecode(); 340 ; 341 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 399 """"""; 400 assert self.state.func_ir is None; --> 401 return self._compile_core(); 402 ; 403 def _compile_ir(self):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 379 self.state.status.fail_reason = e; 380 if is_final_pipeline:; --> 381 raise e; 382 else:; 383 raise CompilerError(""All available pipelines exhausted""). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 370 res = None; 371 try:; --> 372 pm.run(self.state); 373 if self.state.cr is not None:; 374 break. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 339 (self.pipeline_name, pass_desc); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:9314,pipeline,pipelines,9314,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['pipeline'],['pipelines']
Deployability,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2413,install,install,2413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],['install']
Deployability,"\numba\core\compiler_machinery.py in run(self, state); 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 216 # Materialize LLVM Module; --> 217 self.library.add_ir_module(self.module); 218 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\codegen.py in add_ir_module(self, ir_module); 205 ir = cgutils.normalize_ir_text(str(ir_module)); --> 206 ll_module = ll.parse_assembly(ir); 207 ll_module.name = ir_module.name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\llvmlite\binding\module.py in parse_assembly(llvmir, context); 24 mod.close(); ---> 25 raise RuntimeError(""LLVM IR parsing error\n{0}"".format(errmsg)); 26 return mod. RuntimeError: Failed in nopython mode pipeline (step: nopython mode back",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:3802,pipeline,pipeline,3802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['pipeline'],['pipeline']
Deployability,] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33213,pipeline,pipeline,33213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"]; not reaching the requested tolerance 1e-08.; Use iteration 19 instead with accuracy ; 0.013873452618210833. eigenvalues, eigenvectors = scipy.sparse.linalg.lobpcg(; /data/home/ruanlab/huangxingyu/miniconda3/lib/python3.10/site-packages/umap/spectral.py:342: UserWarning: Exited postprocessing with accuracies ; [0.0114102 0.01466 0.01555016]; not reaching the requested tolerance 1e-08.; eigenvalues, eigenvectors = scipy.sparse.linalg.lobpcg(; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (1:07:21); ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.10.1; -----; Cython 0.29.33; IPython 8.13.2; PIL 9.4.0; annoy NA; asttokens NA; backcall 0.2.0; bbknn 1.6.0; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.0.4; cycler 0.10.0; cython 0.29.33; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; executing 1.2.0; fontTools 4.39.0; h5py 3.8.0; idna 3.4; igraph 0.10.5; jedi 0.18.2; joblib 1.3.1; kiwisolver 1.4.4; lazy_loader 0.4; legacy_api_wrap NA; llvmlite 0.40.1; matplotlib 3.9.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.23.0; packaging 23.0; pandas 2.0.1; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.0; plotly 5.13.1; pooch v1.7.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pycparser 2.21; pygments 2.14.0; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; requests 2.28.1; ruamel NA; scipy 1.10.1; seaborn 0.13.2; session_info 1.0.0; six 1.16.0; skimage 0.24.0; sklearn 1.3.0; socks 1.7.1; sphinxcontrib NA; stack_data 0.6.2; statsmodels 0.14.2; texttable 1.6.7; threadpoolctl 3.1.0; tqdm 4.64.1; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; urllib3 1.26.14; wcwidth 0.2.6; yaml 6.0; zstandard 0.18.0; -----; Python 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]; Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-07-03 12:41. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3139:2644,update,updated,2644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3139,1,['update'],['updated']
Deployability,"_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata, max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(); sc._settings.ScanpyConfig.n_jobs = os.cpu_count(); sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''; from sklearn.manifold import TSNE; from scanpy.tools._utils import _choose_representation; X = _choose_representation(adata, n_pcs=tsne_n_pcs); X_tsne = TSNE().fit_transform(X.astype(np.float32)); adata.obsm['X_tsne'] = X_tsne; '''; print(""Tsne time:"", time.time()-t0); ``` . ; <!-- Please check (“- [x]”) and fill in the following boxes ; - [ ] Closes #; - [ ] Tests included or not required because:; -->; <!-- Only check the following box if you did not include release notes ; - [ ] Release notes not necessary because:; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:4046,release,release,4046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,"['Release', 'release']","['Release', 'release']"
Deployability,"_No response_. ### Versions. <details>. ```; -----; anndata 0.8.0; scanpy 1.9.3; -----; OpenSSL 19.0.0; PIL 10.0.0; apport_python_hook NA; backcall 0.2.0; certifi 2019.11.28; cffi 1.15.0; chardet 3.0.4; cloudpickle 2.2.1; colorama 0.4.3; colorcet 3.0.1; cryptography 2.8; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; gseapy 1.0.5; h5py 3.7.0; idna 2.8; igraph 0.10.6; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.10.0; llvmlite 0.39.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.6.1; matplotlib_inline NA; more_itertools NA; mpl_toolkits NA; natsort 8.2.0; netifaces 0.10.4; numba 0.56.3; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 1.5.3; parso 0.8.2; patsy 0.5.3; pexpect 4.6.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.15.0; prompt_toolkit 3.0.20; psutil 5.9.4; ptyprocess 0.7.0; pyarrow 12.0.1; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 2.4.7; pytz 2022.4; requests 2.22.0; scipy 1.10.1; seaborn 0.12.0; session_info 1.0.0; setuptools 68.0.0; simplejson 3.16.0; sitecustomize NA; six 1.14.0; sklearn 1.3.0; socks 1.7.1; statsmodels 0.14.0; storemagic NA; tblib 2.0.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.1; toolz 0.12.0; tornado 6.1; traitlets 5.1.0; typing_extensions NA; urllib3 1.25.8; wcwidth 0.2.5; yaml 5.3.1; zipp NA; zmq 22.3.0; zope NA; -----; IPython 7.28.0; jupyter_client 7.0.6; jupyter_core 4.8.1; notebook 6.4.5; -----; Python 3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0]; Linux-5.15.0-1040-aws-x86_64-with-glibc2.29; -----; Session information updated at 2023-08-11 23:46; sc.pl.stacked_violin(adata,['GATA3','CD8A','CD4'],groupby='sample_id',cmap='PuRd', order=new_order); sc.pl.stacked_violin(adata,['GATA3','CD8A','CD; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2611:2927,update,updated,2927,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2611,1,['update'],['updated']
Deployability,"_Originally posted by @OnlyBelter in https://github.com/theislab/scanpy/issues/1850#issuecomment-863089065_. > But in the original jupyter notebook which I used to process this .h5ad file (also contains many other steps before this step), I cannot use some specific columns to set parameter color in this function (some columns can be used correctly). > I think this problem may cause by `seaborn`!; > ; > The following code should reproduce the error:. ```python; import scanpy as sc; import seaborn as sns; sns.set() # <--- here. pbmc = sc.datasets.pbmc68k_reduced(); sc.pl.umap(pbmc, color = 'phase'); ```. -------------------------------. On current release this errors:. <details>; <summary> </summary>. ```python; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); <ipython-input-1-4c43dbe94eaf> in <module>; 4 ; 5 pbmc = sc.datasets.pbmc68k_reduced(); ----> 6 sc.pl.umap(pbmc, color = 'phase'). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs); 601 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 602 """"""; --> 603 return embedding(adata, 'umap', **kwargs); 604 ; 605 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 244 groups=groups,; 245 ); --> 246 color_vector, categorical = _color_vector(; 247 adata,; 248 value_to_plot,. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:654,release,release,654,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['release'],['release']
Deployability,"_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1566,Pipeline,PipelineDevelope,1566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,_UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopyt,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31765,pipeline,pipeline,31765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32507,pipeline,pipeline,32507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pipelin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:3047,Pipeline,PipelineDevelope,3047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'); sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0-4, multi_panel=True); ```. ### Error output. ```pytb; Kernel Restarting; The kernel for Tests/scanpytutorial/Untitled.ipynb appears to have died. It will restart automatically.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 10.2.0; asttokens NA; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; exceptiongroup 1.2.0; executing 2.0.1; h5py 3.10.0; igraph 0.11.3; ipykernel 6.29.0; jedi 0.19.1; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.8.2; mpl_toolkits NA; natsort 8.4.0; nt NA; numba 0.59.0; numpy 1.26.3; packaging 23.2; pandas 2.2.0; parso 0.8.3; pickleshare 0.7.5; platformdirs 4.1.0; prompt_toolkit 3.0.42; psutil 5.9.8; pure_eval 0.2.2; pyarrow 15.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing 3.1.1; pythoncom NA; pytz 2023.4; pywin32_system32 NA; pywintypes NA; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; stack_data 0.6.2; texttable 1.7.0; threadpoolctl 3.2.0; tornado 6.3.3; traitlets 5.14.1; typing_extensions NA; wcwidth 0.2.13; win32api NA; win32com NA; zmq 25.1.2; zoneinfo NA; -----; IPython 8.20.0; jupyter_client 8.6.0; jupyter_core 5.7.1; -----; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:27:34) [MSC v.1937 64 bit (AMD64)]; Windows-10-10.0.22631-SP0; -----; Session information updated at 2024-02-05 18:01; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840:3340,update,updated,3340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840,1,['update'],['updated']
Deployability,"_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 323 # make the scatter plot; 324 if projection == '3d':; --> 325 cax = ax.scatter(; 326 coords[:, 0],; 327 coords[:, 1],; 328 coords[:, 2],; 329 marker=""."",; 330 c=color_vector,; 331 rasterized=settings._vector_friendly,; 332 norm=normalize,; 333 **kwargs,; 334 ); 335 else:; 336 scatter = (; 337 partial(ax.scatter, s=size, plotnonfinite=True); 338 if scale_factor is None; (...); 341 ) # size in circles is radius; 342 ). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:2417, in Axes3D.scatter(self, xs, ys, zs, zdir, s, c, depthshade, *args, **kwargs); 2414 zs = zs.copy(); 2416 patches = super().scatter(xs, ys, s=s, c=c, *args, **kwargs); -> 2417 art3d.patch_collection_2d_to_3d(patches, zs=zs, zdir=zdir,; 2418 depthshade=depthshade); 2420 if self._zmargin < 0.05 and xs.size > 0:; 2421 self.set_zmargin(0.05). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:674, in patch_collection_2d_to_3d(col, zs, zdir, depthshade); 672 col._depthshade = depthshade; 673 col._in_draw = False; --> 674 col.set_3d_properties(zs, zdir). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:542, in Path3DCollection.set_3d_properties(self, zs, zdir); 539 def set_3d_properties(self, zs, zdir):; 540 # Force the collection to initialize the face and edgecolors; 541 # just in case it is a scalarmappable with a colormap.; --> 542 self.update_scalarmappable(); 543 offsets = self.get_offsets(); 544 if len(offsets) > 0:. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/collections.py:926, in Collection.update_scalarmappable(self); 924 # pcolormesh, scatter, maybe others flatten their _A; 925 self._alpha = self._alpha.reshape(self._A.s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:3225,patch,patches,3225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['patch'],['patches']
Deployability,_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34144,pipeline,pipeline,34144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplemente,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34378,pipeline,pipeline,34378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILE,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20376,pipeline,pipeline,20376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"_genes_groups.py:93, in _RankGenes.__init__(self, adata, groups, groupby, reference, use_raw, layer, comp_pts); 82 def __init__(; 83 self,; 84 adata,; (...); 90 comp_pts=False,; 91 ):; ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:; 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])); 95 else:. KeyError: 'base'; ```. #### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.2.0; appnope 0.1.3; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 0.8.3; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.9.11; ipykernel 6.13.0; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.7.1; matplotlib 3.6.0; matplotlib_inline NA; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.56.2; numpy 1.23.0; packaging 21.3; pandas 1.4.3; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.7; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.9.1; seaborn 0.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.1.2; stack_data 0.2.0; statsmodels 0.13.2; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; typing_extensions NA; umap 0.5.3; wcwidth 0.2.5; yaml 5.3.1; zmq 22.3.0; -----; IPython 8.2.0; jupyter_client 7.2.2; jupyter_core 4.9.2; notebook 6.4.10; -----; Python 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]; macOS-13.3.1-x86_64-i386-64bit; -----; Session information updated at 2023-05-30 21:48; ```. </details>. Any help would be appreciated! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2497:4265,update,updated,4265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497,1,['update'],['updated']
Deployability,_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25807,pipeline,pipeline,25807,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:1946,install,installed,1946,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['install'],['installed']
Deployability,"_pass, internal_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53); ```. </details>. I am running scanpy using python v3.9 with numba v0.55.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:7604,pipeline,pipeline,7604,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['pipeline'],['pipeline']
Deployability,_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.fi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25594,pipeline,pipeline,25594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29619,pipeline,pipeline,29619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38973,pipeline,pipeline,38973,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21529,pipeline,pipeline,21529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19442,pipeline,pipeline,19442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19209,pipeline,pipeline,19209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17358,pipeline,pipeline,17358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18748,pipeline,pipeline,18748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"_symbols'].""; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; Levenshtein 0.21.0; PIL 9.5.0; adjustText NA; airr 1.4.1; asttokens NA; awkward 2.2.2; awkward_cpp NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; idna 3.4; igraph 0.10.4; invgauss_ufunc NA; ipykernel 6.22.0; ipywidgets 8.0.6; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mudata 0.2.3; muon 0.1.5; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; networkx 3.1; numba 0.56.4; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.2.0; pooch v1.7.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; rapidfuzz 2.15.1; requests 2.28.2; scipy 1.10.1; scirpy 0.13.1.dev2+g4ed908b; seaborn 0.12.2; session_info 1.0.0; setuptools 67.7.1; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; socks 1.7.1; squarify NA; stack_data 0.6.2; statsmodels 0.13.5; texttable 1.6.7; threadpoolctl 3.1.0; torch 1.12.1.post201; tornado 6.3; tqdm 4.65.0; tracerlib NA; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; urllib3 1.26.15; wcwidth 0.2.6; yaml 6.0; yamlordereddictloader NA; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.2; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2023-06-28 18:02; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2540:4713,update,updated,4713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540,1,['update'],['updated']
Deployability,"_top_genes` there is an indexing error when selecting the dispersion cutoff here https://github.com/scverse/scanpy/blob/9018e16cae6f3199f914f58841b00a00790cd494/scanpy/preprocessing/_highly_variable_genes.py#L268. There should probably be a check (with a warning) when this happens. ### Minimal code sample (that we can copy&paste without having any data). ```python; import anndata; import numpy as np; import scanpy as sc. adata = anndata.AnnData(np.random.poisson(2, (100, 30))); sc.pp.normalize_total(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, n_top_genes=1000, flavor=""cell_ranger""); ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 434, in highly_variable_genes; df = _highly_variable_genes_single_batch(; File ""/usr/local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 268, in _highly_variable_genes_single_batch; disp_cut_off = dispersion_norm[n_top_genes - 1]; IndexError: index 29 is out of bounds for axis 0 with size 21; ```. #### Versions. <details>. -----; anndata 0.7.8; scanpy 1.9.1; -----; PIL 9.1.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; google NA; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.9; joblib 1.1.0; kiwisolver 1.4.2; llvmlite 0.38.0; louvain 0.7.1; matplotlib 3.5.1; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.5; packaging 21.3; pandas 1.4.2; pkg_resources NA; psutil 5.9.0; pyparsing 3.0.8; pytz 2022.1; scipy 1.8.0; session_info 1.0.0; six 1.16.0; sklearn 1.0.2; statsmodels 0.13.2; texttable 1.6.4; threadpoolctl 3.1.0; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; -----; Python 3.8.13 (default, Apr 7 2022, 04:56:26) [GCC 10.2.1 20210110]; Linux-5.10.76-linuxkit-x86_64-with-glibc2.2.5; -----; Session information updated at 2022-04-11 12:44. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2230:2626,update,updated,2626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2230,1,['update'],['updated']
Deployability,`FutureWarning` for the next release to begin the process.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2865#issuecomment-2012549256:29,release,release,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2865#issuecomment-2012549256,1,['release'],['release']
Deployability,"`RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332466200:22,install,install,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332466200,1,['install'],['install']
Deployability,"```; from importlib.metadata import version as v. v(""anndata""); ```; returns Python `None`. ```anndata.__version__```. returns `'0.10.6'`. I had `anndata==0.8.0` in my conda environment, and then I did `pip install anndata -U` to get `0.10.6`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037490510:207,install,install,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2037490510,1,['install'],['install']
Deployability,```; sc.__version__; '1.8.0.dev78+gc488909a'; ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. ; In other words if adata.raw was missing the notation it failed for me (different error though). ; I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537:458,update,update,458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537,1,['update'],['update']
Deployability,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didn’t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think that’s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:112,install,install,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050,2,['install'],['install']
Deployability,"```pytb; package=scanpy; pversion=1.5.1 #found in docs/release-latest.rst after git download; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; cd /usr/common/src; git clone https://github.com/theislab/scanpy.git; cd scanpy; module load python3-libraries #for PYTHONPATH; python3 ./setup.py install \; --install-scripts=$TOPDIR/bin --prefix /usr/common \; 2>&1 | tee ../install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:55,release,release-latest,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,3,"['install', 'release']","['install', 'install-scripts', 'release-latest']"
Deployability,"```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}. dp = sc.pl.dotplot(pbmc, markers, 'bulk_labels', return_fig=True); dp.add_totals().style(dot_edge_color='black', dot_edge_lw=0.5).show(); ```. ### Error output. ```pytb; (Error output is a bad plot, included in the description above.); ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.1; -----; IPython 8.13.2; PIL 10.0.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.10.1; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.6; dot_parser NA; entrypoints 0.4; exceptiongroup 1.1.1; executing 1.2.0; fasteners 0.17.3; flytekitplugins NA; gmpy2 2.1.2; google NA; h5py 3.8.0; icu 2.11; igraph 0.11.2; jedi 0.19.1; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; lz4 4.3.2; markupsafe 2.1.2; matplotlib 3.8.3; mpl_toolkits NA; mpmath 1.3.0; msgpack 1.0.5; natsort 8.3.1; numba 0.59.1; numcodecs 0.11.0; numexpr 2.7.3; numpy 1.26.4; packaging 23.1; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; plotly 5.14.1; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 10.0.1; pydot 1.4.2; pygments 2.15.1; pyparsing 3.0.9; pyteomics NA; pytz 2023.3.post1; scipy 1.13.0; session_info 1.0.0; setuptools 67.7.2; setuptools_scm NA; six 1.16.0; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; tblib 1.7.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.11.2; torch 2.1.1; torchgen NA; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; wcwidth 0.2.6; xxhash NA; yaml 5.4.1; zarr 2.14.2; zc NA; zipp NA; zoneinfo NA; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]; macOS-14.4.1-x86_64-i386-64bit; -----; Session information updated at 2024-05-15 18:46; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062:3579,update,updated,3579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062,1,['update'],['updated']
Deployability,"```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4992,install,installed,4992,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['installed']
Deployability,"```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=0.5). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""FCGR3A""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. before:; ![image](https://user-images.githubusercontent.com/25887487/100453873-0e7d2700-30bc-11eb-9451-0240f6170e2f.png). after:; ![image](https://user-images.githubusercontent.com/25887487/100453885-15a43500-30bc-11eb-9b95-66d0881aad31.png). as you can see, heatmap colors for rows (genes) is consistent with columns (clusters) ; @wkopp is this helpful? notebook still to be updated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743:753,update,updated,753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743,1,['update'],['updated']
Deployability,"`adata.obsm['X_pca']` seems to have predicted better clustering. Thank you. I'm going to try that with the rest of my data and see what clustering it suggests. . As for `adata.uns['neighbors']['distances']`, shortly after my post I read that the `silhouette_score` function wasn't designed to handle spares matrices, so you're right with that. ; (source: https://books.google.com/books?id=skvZDQAAQBAJ&pg=PA786&lpg=PA786&dq=silhouette_score+precomputed+python3&source=bl&ots=YRC9VPTPPW&sig=7KPSQDWtZG6537-f_vZGwMpMvCc&hl=en&sa=X&ved=2ahUKEwjO3ruPqsXcAhWEg-AKHXD5BUAQ6AEwCXoECAMQAQ#v=onepage&q=silhouette_score%20precomputed%20python3&f=false). ; It suggested using todense() if the matrix is small, but to avoid this function all together if the matrix is large. When I trade toarray() for todense() it seems to produce similar results. Since single-cell datasets are likely always to be too big, it suggested using V-Measure or Adjusted Mutual Information as a way to evaluate sparse matrices instead. Just thought I'd update with my findings. Again, the `adata.obsm['X_pca']` suggestion seems to predict better clustering arrangements. Big ups for that! . Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788:1020,update,update,1020,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/222#issuecomment-408842788,1,['update'],['update']
Deployability,"`anndata.readwrite.read.read_excel` is using pandas when fetching bundled datasets like `moignard15()`. However since xlrd is not bundles with pandas, it throws an error. See https://stackoverflow.com/questions/17063458/reading-an-excel-file-in-python-using-pandas#comment83338990_17063653. xlrd should be listed in the installation instructions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/167:320,install,installation,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/167,1,['install'],['installation']
Deployability,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438,1,['install'],['install']
Deployability,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457,1,['install'],['install']
Deployability,"`flit install --deps none -s` breaks `conda list` for me.; using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847,1,['install'],['install']
Deployability,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947,3,['install'],"['install', 'installed']"
Deployability,"`matplotlib.colormaps` was introduced in v3.5.0, but scanpy currently specifies only v3.4+. Bump it and everything should be fine. - [x] Tests included or not required because: trivial change; - [x] Release notes not necessary because: trivial change. (this is just my own evaluation of triviality; if you want a release notes entry I am happy to add one)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2733:199,Release,Release,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2733,2,"['Release', 'release']","['Release', 'release']"
Deployability,`pip install -U pynndescent`; This fix my problems; I hope it's able to help others!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944,1,['install'],['install']
Deployability,`pip install -e .` raises LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['install'],['install']
Deployability,"`pip install -e .`; gives:; ```; Obtaining file:///apps/gau/scanpy; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/apps/gau/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/apps/gau/scanpy/scanpy/__init__.py"", line 31, in <module>; from . import tools as tl; File ""/apps/gau/scanpy/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing._simple import pca; File ""/apps/gau/scanpy/scanpy/preprocessing/__init__.py"", line 7, in <module>; from ._combat import combat; File ""/apps/gau/scanpy/scanpy/preprocessing/_combat.py"", line 111, in <module>; def combat(adata: AnnData, key: str = 'batch', inplace: bool = True):; NameError: name 'AnnData' is not defined; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/416:5,install,install,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/416,1,['install'],['install']
Deployability,`pip3 install scanpy` -> 'TypeError: Can't convert 'list' object to str implicitly',MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['install'],['install']
Deployability,"`print_versions` outputs more, updated GH issue template",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343:31,update,updated,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343,1,['update'],['updated']
Deployability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:103,install,install,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,5,['install'],"['install', 'installation', 'installing']"
Deployability,"`sc.datasets.moignard15()` results in a 404. The previous link http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx seems to not be working anymore. It looks like supp info is still there, so links probably just need to be updated: https://www.nature.com/articles/nbt.3154#Sec24. Alternatively, maybe springer needs the 10k in publishing fees to keep their servers running.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1515:235,update,updated,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1515,1,['update'],['updated']
Deployability,"`some_series[0]`: “`Series.__getitem__` treating keys as positions is deprecated”; - The default value of 'ignore' for the `na_action` parameter in pandas.Categorical.map is deprecated and will be changed to 'None' in a future version. Please set na_action to the desired value to avoid seeing this warning; - `df = df.groupby('gene').agg(` (in SeriesGroupBy.agg?) The provided callable <function nansum at 0x7f50a03cb7e0> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass 'sum' instead.; - same for nanmean/SeriesGroupBy.mean.; - scanpy/tests/test_highly_variable_genes.py:59: SettingWithCopyWarning (in `adata.obs.batch.loc[::2] = ""b""`) A value is trying to be set on a copy of a slice from a DataFrame. See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.10.0.dev178+ga073532a; -----; PIL 10.1.0; asciitree NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2023.12.1; dateutil 2.8.2; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jinja2 3.1.2; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.2; packaging 23.2; pandas 2.1.4; pluggy 1.3.0; psutil 5.9.7; py NA; pyparsing 3.1.1; pytest 7.4.3; pytz 2023.3.post1; scipy 1.11.4; session_info 1.0.0; setuptools 69.0.2; setuptools_scm NA; six 1.16.0; sklearn 1.3.2; sparse 0.14.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.0; toolz 0.12.0; typing_extensions NA; wcwidth 0.2.12; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; Python 3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]; Linux-6.6.7-zen1-1-zen-x86_64-with-glibc2.38; -----; Session information updated at 2023-12-19 16:08; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2788:2961,update,updated,2961,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2788,1,['update'],['updated']
Deployability,"`tbb` was installed via conda i guess, try to uninstall `tbb` with conda and then install `scanpy` again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706#issuecomment-1777066040:10,install,installed,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706#issuecomment-1777066040,2,['install'],"['install', 'installed']"
Deployability,"`twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1585:126,continuous,continuously,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585,1,['continuous'],['continuously']
Deployability,`wx` module missing after installing scanpy on macOS,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:26,install,installing,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['install'],['installing']
Deployability,"a can’t correctly detect when a threading backend is available. This was fine in the past, since `pynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:988,install,installed,988,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,2,['install'],"['install', 'installed']"
Deployability,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1723,integrat,integrate,1723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,2,['integrat'],"['integrate', 'integration']"
Deployability,a-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:37316,pipeline,pipeline,37316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"a/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:598) ); [599](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:599) if var_names == ""gene_symbols"":; [600](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:600) var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:8213,Pipeline,PipelineDevelope,8213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"a/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:5816,pipeline,pipelines,5816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['pipeline'],['pipelines']
Deployability,a2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; r,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:4080,Install,Installing,4080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,3,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,"a2; Running setup.py install for fa2 ... error. ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:3964,install,install,3964,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install']
Deployability,"a3/envs/scvelo/lib/python3.8/site-packages/umap/umap_.py in fuzzy_simplicial_set(X, n_neighbors, random_state, metric, metric_kwds, knn_indices, knn_dists, angular, set_op_mix_ratio, local_connectivity, apply_set_operations, verbose); 600 knn_dists = knn_dists.astype(np.float32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; six 1.16.0; sklearn 0.24.2; storemagic NA; tables 3.6.1; texttabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:4123,install,install,4123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['install'],['install']
Deployability,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4872,install,installed,4872,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['install'],['installed']
Deployability,"a\lib\site-packages (from fa2) (1.18.4); Requirement already satisfied: scipy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.4.1); Requirement already satisfied: tqdm in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (4.46.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\xkalaj\AppData\Local\Temp\pip-wheel-yjh93oit'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running bdist_wheel; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:1587,install,install-,1587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,2,"['Install', 'install']","['Installing', 'install-']"
Deployability,"aac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Version('0.10.6')>; ```. </details>. Interesting to see that this seems to work now!. I do think the recommended solution here is ""don't do this"", but I'm considering just using `anndata.__version__` here since that will work when the package metadata is broken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:1247,Install,Installing,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,3,"['Install', 'install']","['Installing', 'installation', 'installed']"
Deployability,able_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_vari,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23797,pipeline,pipeline,23797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"able_pyobject:. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); [669](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:18948,pipeline,pipeline,18948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"ack `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1536,install,install,1536,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['install'],['install']
Deployability,"acked, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 542 return key, value; 543 ; --> 544 key, value = postprocess_reading(key, value); 545 d[key_write] = value; 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value); 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))); 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]; --> 541 value = value.astype(new_dtype); 542 return key, value; 543 . ValueError: invalid shape in fixed-type tuple.; ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:3693,update,updated,3693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['update'],['updated']
Deployability,"acy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:3264,Pipeline,PipelineDevelope,3264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ad(""adata2.h5ad""); check_equal_adata_X(adata_run1_concat, adata_run2_concat); adata_run1_concat.X = adata_run1_concat.X.astype(np.float64); adata_run2_concat.X = adata_run2_concat.X.astype(np.float64); sc.pp.normalize_total(adata_run1_concat); sc.pp.normalize_total(adata_run2_concat); check_equal_adata_X(adata_run1_concat, adata_run2_concat); ```. Output:; ```pytb; True; True; True; True; True; True; ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python; sc.datasets.pbmc3k(); pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""); pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""); check_equal_adata_X(pbmc3k_1, pbmc3k_2); sc.pp.normalize_total(pbmc3k_1); sc.pp.normalize_total(pbmc3k_2); check_equal_adata_X(pbmc3k_1, pbmc3k_2); ```. ```pytb; True; True; True; True; True; True; ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.1; sinfo 0.3.1; -----; anndata 0.7.5; backcall 0.1.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; importlib_metadata 1.6.0; ipykernel 5.2.1; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 0.14.1; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.31.0; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.0.1; numba 0.48.0; numexpr 2.7.1; numpy 1.18.2; packaging 20.3; pandas 1.0.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2019.3; scanpy 1.6.1; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.14.0; sklearn 0.22.2.post1; storemagic NA; tables 3.6.1; tornado 6.0.4; tqdm 4.45.0; traitlets 4.3.3; wcwidth NA; zipp NA; zmq 19.0.0; -----; IPython 7.13.0; jupyter_client 6.1.3; jupyter_core 4.6.3; notebook 6.0.3; -----; Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]; Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core; 36 logical CPU cores; -----; Session information updated at 2021-01-27 17:16; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1612:3039,update,updated,3039,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612,1,['update'],['updated']
Deployability,adapted from https://github.com/scverse/anndata/blob/main/.azure-pipelines.yml. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:65,pipeline,pipelines,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['pipeline'],['pipelines']
Deployability,"adata = sc.read_visium(; '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',; count_file='filtered_feature_bc_matrix.h5',; source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',; ); ```. ### Error output. ```pytb; /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.; utils.warn_names_duplicates(""var""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium; raise OSError(f""Could not find '{f}'""); OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'; ```. ### Versions. <details>; <summary>Details</summary>. ```; >>> import scanpy; scanpy.logging.print_versions(); -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dot_parser NA; gmpy2 2.1.2; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; importlib_resources NA; invgauss_ufunc NA; joblib 1.1.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; matplotlib 3.7.1; mpl_toolkits NA; mpmath 1.2.1; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.57.0; numpy 1.24.3; nvfuser NA; opt_einsum v3.3.0; packaging 23.0; pandas 2.0.1; pkg_resources NA; pydot 1.4.2; pyparsing 3.0.9; pytz 2022.7; scipy 1.10.1; session_info 1.0.0; setuptools 66.0.0; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 2.2.0; torch 2.0.1; tqdm 4.65.0; typing_extensions NA; yaml 6.0; zipp NA; zoneinfo NA; -----; Python 3.9.16 (main, Mar 8 2023, 14:00:05) [GCC 11.2.0]; Linux-5.15.0-71-generic-x86_64-with-glibc2.35; -----; Session information updated at 2023-07-20 15:11; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565:2308,update,updated,2308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565,1,['update'],['updated']
Deployability,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.59.0; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.3; wcwidth 0.2.5; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.10; notebook 6.2.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-44-generic-x86_64-with-glibc2.10; 28 logical CPU cores; -----; Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:2491,update,updated,2491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,1,['update'],['updated']
Deployability,"add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1631,release,release,1631,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['release'],['release']
Deployability,add conda-forge installation instructions,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1974:16,install,installation,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974,1,['install'],['installation']
Deployability,add robust installation instructions again,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1140:11,install,installation,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1140,1,['install'],['installation']
Deployability,added integration tutorial spatial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1229:6,integrat,integration,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1229,1,['integrat'],['integration']
Deployability,adding release note in #1740,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441:7,release,release,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441,1,['release'],['release']
Deployability,"ady been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### Description. I was installing the dev version of scanpy following the instructions [here](https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#working-with-git) and [here](https://scanpy.readthedocs.io/en/stable/installation.html#development-version). However two tests : `test_paga_plots[master_paga_continuous_obs-func2]` and `test_pca_chunked` seem to fail on master. . ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```bash; git clone https://github.com/scverse/scanpy.git; cd scanpy; git submodule update --init --recursive; conda create --name scanpy-dev python=3.8; conda activate scanpy-dev; pip install -e '.[dev,doc,test]'; pytest; ```. ```pytb; =========================================================================================== FAILURES ============================================================================================; _______________________________________________________________________ test_paga_plots[master_paga_continuous_obs-func2] _______________________________________________________________________. image_comparer = <function image_comparer.<locals>.make_comparer at 0x7fbbf032dc10>; pbmc = AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score...roups', 'paga', 'bulk_labels_sizes'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'; test_id = 'master_paga_continuous_obs', func = functools.partial(<function paga at 0x7fc009276e50>, color='cool_feature'). @needs_igraph; @pytest.mark.parametrize(; ""test_id,func"",; [; (""master_paga"", sc.pl.paga),; (""master_pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2459:1060,install,install,1060,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2459,1,['install'],['install']
Deployability,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5555,install,install,5555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,"ah, and I forgot to add release notes. I need to get https://github.com/scverse/scanpy/pull/2569 done …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2672#issuecomment-1764686065:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2672#issuecomment-1764686065,1,['release'],['release']
Deployability,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513183516:92,update,update,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513183516,1,['update'],['update']
Deployability,"aha, sorry, I didn't install the 'fa2' package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411:21,install,install,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411,1,['install'],['install']
Deployability,"al branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata.; > ; > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change.; > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:1129,Update,Update,1129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['Update'],['Update']
Deployability,"al-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_house'],; jitter=0.4, multi_panel=True). ```. ```pytb; /Users/banana/opt/miniconda3/lib/python3.8/site-packages/anndata/_core/anndata.py:1192: FutureWarning:. is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead. /Users/banana/opt/miniconda3/lib/python3.8/site-packages/seaborn/_core.py:1303: UserWarning:. Vertical orientation ignored with only `x` specified. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.4; appnope 0.1.0; attr 20.2.0; backcall 0.2.0; cffi 1.14.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; idna 2.9; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.17.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.8; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; pandas 1.1.2; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 4.12.0; prompt_toolkit 3.0.8; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; retrying NA; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; statsmodels 0.12.1; storemagic NA; tables 3.6.1; tornado 6.0.4; traitlets 5.0.5; wcwidth 0.2.5; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.8.3 (default, May 19 2020, 13:54:14) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 4 logical CPU cores, i386; -----; Session information updated at 2020-11-05 17:47; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1486:2591,update,updated,2591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1486,1,['update'],['updated']
Deployability,"all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.7.0; seaborn 0.11.2; setuptools 62.1.0; simplejson 3.17.6; six 1.16.0; sklearn 1.0.1; statsmodels 0.13.2; storemagic NA; tables 3.7.0; threadpoolctl 3.1.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 19.0.1; -----; IPython 7.15.0; jupyter_client 6.1.3; jupyter_core 4.6.3; jupyterlab 2.1.4; notebook 6.1.5; -----; Python 3.7.13 (default, Aug 6 2022, 00:43:34) [GCC 9.2.0]; Linux-4.18.0-348.12.2.el8_5.x86_64-x86_64-with-centos-8.7-Green_Obsidian; 192 logical CPU cores, x86_64; -----; Session information updated at 2023-08-22 16:49; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:5056,update,updated,5056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['update'],['updated']
Deployability,allowed the function to use specified var_names. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] #1549; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2765:465,release,release,465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2765,2,"['Release', 'release']","['Release', 'release']"
Deployability,"already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:2645,Install,Installing,2645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['Install'],['Installing']
Deployability,als_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21300,pipeline,pipeline,21300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplement,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:34611,pipeline,pipeline,34611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED sca,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18287,pipeline,pipeline,18287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22230,pipeline,pipeline,22230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21069,pipeline,pipeline,21069,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,als_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22460,pipeline,pipeline,22460,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897,1,['integrat'],['integration']
Deployability,"aming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1887) errors=self.options.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:13793,Pipeline,PipelineDevelope,13793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"aming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-pack",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4270,Pipeline,PipelineDevelope,4270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"aming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:598) ); [599](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roamin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:7015,Pipeline,PipelineDevelope,7015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1259,update,update,1259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['update'],['update']
Deployability,"and errored out with exit status 1:; command: /usr/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/setup.py'""'""'; __file__='""'""'/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-j38v6hmh/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ube/.local/include/python3.10/llvmlite; cwd: /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/; Complete output (29 lines):; running install; running build; got version from file /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/llvmlite/_version.py {'version': '0.34.0', 'full': 'c5889c9e98c6b19d5d85ebdd982d64a03931f8e2'}; running build_ext; /usr/bin/python /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py; LLVM version... Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 105, in main_posix; out = subprocess.check_output([llvm_config, '--version']); File ""/usr/lib/python3.10/subprocess.py"", line 420, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; File ""/usr/lib/python3.10/subprocess.py"", line 501, in run; with Popen(*popenargs, **kwargs) as process:; File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__; self._execute_child(args, executable, preexec_fn, close_fds,; File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:1374,install,install-,1374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,1,['install'],['install-']
Deployability,"anges from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:1949,update,update,1949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061,1,['update'],['update']
Deployability,"anpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2258,update,update,2258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,2,['update'],['update']
Deployability,"anpy. ### What happened?. When using `sc.pl.pca` with a combination of:. - Enabled `annotate_var_explained`; - Enabled `return_fig`; - A colored-by column. The plotting function fails because it is trying to set the name on color bars. Specifically, [this loop](https://github.com/scverse/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L893) will encounter both, the actual plot as well as the colorbar. When it does hit the colorbar, it tries to look it up in the `label_dict`, which will fail (those axes are labeled `<colorbar>`). ### Minimal code sample. ```python; import scanpy as sc; import anndata as ad; import pandas as pd; import numpy as np. obs = pd.DataFrame(np.arange(100), ; columns=['a'], ; index=['cell{}'.format(i) for i in range(100)]). X = np.random.rand(100, 5); adata = ad.AnnData(X=X, obs=obs); sc.tl.pca(adata); sc.pl.pca(adata, color='a', return_fig=True, annotate_var_explained=True); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File "".../scanpy/plotting/_tools/scatterplots.py"", line 894, in pca; ax.set_xlabel(label_dict[ax.xaxis.get_label().get_text()]); KeyError: ''; ```. ### Versions. <details>. ```; -----; anndata 0.8.0; scanpy 1.10.0.dev128+g616d5803; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.2; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.56.3; numpy 1.23.4; packaging 21.3; pandas 1.5.1; pkg_resources NA; pynndescent 0.5.8; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.9.3; session_info 1.0.0; setuptools 65.5.1; six 1.16.0; sklearn 1.1.3; threadpoolctl 3.1.0; typing_extensions NA; zoneinfo NA; -----; Python 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]; macOS-<redacted>-arm64-arm-64bit; -----; Session information updated at 2023-10-11 14:45; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2681:2221,update,updated,2221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2681,1,['update'],['updated']
Deployability,"anpy. ---; Hello, . I would like to derive the transition matrix with the following input:. ```python; sc.Neighbors.compute_transitions(adata); ```. The error below was produced. The anndata object was fine with sc.pp.neighbors(adata) and sc.tl.umap(adata) for UMAP plotting. Thanks. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/root/miniconda3/envs/scVI/lib/python3.8/site-packages/scanpy/neighbors/__init__.py"", line 911, in compute_transitions; W = self._connectivities; AttributeError: 'AnnData' object has no attribute '_connectivities'; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.5; attr 20.3.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; deprecate 0.3.0; docrep 0.3.2; fsspec 2022.01.0; google NA; h5py 3.1.0; igraph 0.9.1; joblib 0.17.0; kiwisolver 1.3.1; leidenalg 0.8.3; llvmlite 0.37.0; matplotlib 3.3.3; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.0; numba 0.54.1; numexpr 2.7.1; numpy 1.19.2; opt_einsum v3.3.0; packaging 20.7; pandas 1.1.5; pkg_resources NA; pycparser 2.20; pygments 2.7.3; pynndescent 0.5.5; pyparsing 2.4.7; pyro 1.8.0+0ec1e87; pytorch_lightning 1.3.8; pytz 2020.4; rich NA; scanpy 1.8.2; scipy 1.5.2; scvi 0.14.5; setuptools 49.6.0.post20201009; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 1.0.1; sphinxcontrib NA; tables 3.6.1; tensorboard 2.7.0; texttable 1.6.3; threadpoolctl 2.1.0; torch 1.9.0; torchmetrics 0.6.2; tqdm 4.62.3; typing_extensions NA; umap 0.5.2; wcwidth 0.2.5; yaml 5.4.1; -----; Python 3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) [GCC 9.3.0]; Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10; 24 logical CPU cores, x86_64; -----; Session information updated at 2022-01-14 13:58. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2109:2179,update,updated,2179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2109,1,['update'],['updated']
Deployability,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1362,install,installation,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['install'],['installation']
Deployability,"anpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:598) ); [599](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:599) if var_names == ""gene_symbols"":; [600](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:600) var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:7456,Pipeline,PipelineDevelope,7456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"anpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:598) ); [599](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:599) if var_names == ""gene_symbols"":; [600](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:600) var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:7644,Pipeline,PipelineDevelope,7644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,anpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[pca] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[spatial] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_dimen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:44872,pipeline,pipeline,44872,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1871205457:4,update,update,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1871205457,1,['update'],['update']
Deployability,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package?. Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763250072:4,update,updates,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763250072,1,['update'],['updates']
Deployability,"apper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad..callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\methods.py:500, in read_sparse(elem, _reader); 495 @_REGISTRY.register_read(H5Group, IOSpec(""csc_matrix"", ""0.1.0"")); 496 @_REGISTRY.register_read(H5Group, IOSpec(""csr_matrix"", ""0.1.0"")); 497 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csc_matrix"", ""0.1.0"")); 498 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csr_matrix"", ""0.1.0"")); 499 def read_sparse(elem, _reader):; --> 500 return SparseDataset(elem).to_memory(). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_core\sparse_dataset.py:379, in SparseDataset.to_memory(self); ...; 189 f""Above error raised while reading key {elem.name!r} of ""; 190 f""type {type(elem)} from {parent}.""; 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type from /.; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.3; -----; PIL 10.0.0; PyQt5 NA; anyio NA; arrow 1.2.3; asttokens NA; attr 23.1.0; attrs 23.1.0; babel 2.12.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.7; brotli 1.0.9; certifi 2023.07.22; cffi 1.15.1; charset_normalizer 3.2.0; colorama 0.4.6; comm 0.1.4; cvxopt 1.3.1; cycler 0.10.0; cython_runtime NA; ...; Python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]; Windows-10-10.0.19045-SP0; -----; Session information updated at 2023-08-04 10:17; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2592:2668,update,updated,2668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592,1,['update'],['updated']
Deployability,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1573,pipeline,pipeline,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,"ards compat; # groups=None, # backwards compat; # plot: bool = True,; # show: Optional[bool] = None,; save=""/reg_label_full_nonan.pdf""; # ax: Optional[Axes] = None,; ). ```; Plot output showing lack of nodes with dpt_pseudotime:; [reg_label_full_nonan.pdf](https://github.com/scverse/scanpy/files/9073420/reg_label_full_nonan.pdf); ![reg_label_full_nonan](https://user-images.githubusercontent.com/64251655/178032248-88717c04-ae02-4a8f-b13b-b8a679d9c6c2.png). #### Versions. <details>. ```; -----; anndata 0.8.0; scanpy 1.9.1. -----; PIL 9.0.1; appnope 0.1.3; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 0.8.3; fontTools 4.25.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.10; ipykernel 6.13.1; ipython_genutils 0.2.0; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.17.1; kiwisolver 1.3.2; leidenalg 0.8.10; llvmlite 0.38.0; louvain 0.7.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.7.1; numba 0.55.1; numpy 1.21.6; packaging 21.3; pandas 1.4.2; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.4; pytz 2021.3; scipy 1.8.0; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.0.2; sphinxcontrib NA; stack_data 0.2.0; texttable 1.6.4; threadpoolctl 2.2.0; tornado 6.1; traitlets 5.2.2; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 23.1.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; jupyterlab 3.4.3; notebook 6.4.12; -----; Python 3.8.13 (default, Mar 28 2022, 06:16:26) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2022-07-08 11:58; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2292:4575,update,updated,4575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2292,1,['update'],['updated']
Deployability,ariable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23564,pipeline,pipeline,23564,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ariable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24245,pipeline,pipeline,24245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,arson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29384,pipeline,pipeline,29384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"as soon as `0.1` is ready, this will be released and the version of the release will simply be `0.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896:40,release,released,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/15#issuecomment-298314896,2,['release'],"['release', 'released']"
Deployability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['integrat'],['integrate']
Deployability,"ass 'h5py._hl.files.File'> from /. ```. Am I doing something wrong with this data? Or should I be using a different command?. #### Versions. <details>. anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.2.0; anndata2ri 1.0.6; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; backports NA; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; dunamai 1.6.0; fsspec 0.9.0; get_version 3.5; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.35.0; markupsafe 1.1.1; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.52.0; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; rpy2 3.4.5; samalg 0.8.6; scipy 1.6.2; seaborn 0.11.1; send2trash NA; six 1.15.0; sklearn 0.24.1; skmisc 0.1.4; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.9.4; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; tzlocal NA; umap 0.5.1; urllib3 1.26.4; wcwidth 0.2.5; yaml 5.4.1; zmq 20.0.0; zope NA; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.3.0; -----; Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-08-16 12:25; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:8183,update,updated,8183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['update'],['updated']
Deployability,"ast NA; genericpath NA; google NA; gprofiler 1.0.0; h5py 3.3.0; idna 3.1; igraph 0.9.6; imagecodecs 2021.6.8; imageio 2.9.0; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.36.0; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.5; ntpath NA; numba 0.53.1; numexpr 2.7.3; numpy 1.21.1; opcode NA; openpyxl 3.0.7; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pooch v1.4.0; posixpath NA; prompt_toolkit 3.0.19; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pydoc_data NA; pyexpat NA; pygments 2.9.0; pynndescent 0.5.4; pyparsing 2.4.7; pytz 2021.1; requests 2.26.0; scanpy 1.8.1; scipy 1.7.0; seaborn 0.11.1; sinfo 0.3.1; sip NA; six 1.16.0; skimage 0.18.2; sklearn 0.24.2; socks 1.7.1; soupsieve 2.0.1; sphinxcontrib NA; spyder 5.0.5; spyder_kernels 2.0.5; spydercustomize NA; sre_compile NA; sre_constants NA; sre_parse NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tensorboard 2.5.0; tensorflow 2.5.0; termcolor 1.1.0; texttable 1.6.4; tifffile 2021.7.2; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.61.2; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; urllib3 1.26.6; wcwidth 0.2.5; wrapt 1.12.1; wurlitzer 2.1.0; xlsxwriter 1.4.4; yaml 5.4.1; zmq 22.1.0; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; -----; Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) [GCC 9.3.0]; Linux-5.4.0-72-generic-x86_64-with-glibc2.27; 40 logical CPU cores, x86_64; -----; Session information updated at 2021-07-29 21:02; </details>. PS - As a side note, another issue that pandas 1.3 introduced is that adata.write() no longer works unless I manually convert every column in adata.obs back to its original data type (e.g. adata.obs['leiden']=adata.obs['leiden'].astype('int')), because I get a TypeError from h5py until I do that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:6021,update,updated,6021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['update'],['updated']
Deployability,"at this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I tried to run the code below on a Windows laptop and received the error (also below). I've tried uninstalling and reinstalling igraph, leidenalg, and scanpy. I tried running the code with flavor=""leidenalg"" and got the same/basically the same error. ### Minimal code sample. ```python; sc.tl.leiden(; adata, ; resolution=0.9,; random_state=0,; flavor=""igraph"", ; n_iterations=2,; directed=False,; ); ```. ### Error output. ```pytb; TypeError Traceback (most recent call last); Cell In[159], line 1; ----> 1 sc.tl.leiden( #So leidan is identifying and coloring clusters for you, but not changing the shape of the graph.; 2 adata, #lets just pretend that I understand what each of those things mean; 3 resolution=0.9,; 4 random_state=0,; 5 flavor=""igraph"", #did pip install leidenalg and started receiving the no flavor keyword error; https://github.com/scverse/scanpy/issues/350 indicates this is to be expected, but https://scanpy.readthedocs.io/en/latest/generated/scanpy.tl.leiden.html indicates it should have it ; 6 n_iterations=2,; 7 directed=False,; 8 ). File ~\miniconda3\Lib\site-packages\scanpy\tools\_leiden.py:144, in leiden(adata, resolution, restrict_to, random_state, key_added, adjacency, directed, use_weights, n_iterations, partition_type, neighbors_key, obsp, copy, **partition_kwargs); 142 msg = 'In the future, the default backend for leiden will be igraph instead of leidenalg.\n\n To achieve the future defaults please pass: flavor=""igraph"" and n_iterations=2. directed must also be False to work with igraph\'s implementation.'; 143 _utils.warn_once(msg, FutureWarning, stacklevel=3); --> 144 except ImportError:; 145 raise ImportError(; 146 ""Please install the leiden algorithm: `conda install -c conda-forge leidenalg` or `pip3 install leidenalg`.""; 147 );",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2981:1054,install,install,1054,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2981,1,['install'],['install']
Deployability,"ata is None:; -> 1473 return func(; 1474 ax,; 1475 *map(sanitize_sequence, args),; 1476 **{k: sanitize_sequence(v) for k, v in kwargs.items()}); 1478 bound = new_sig.bind(ax, *args, **kwargs); 1479 auto_label = (bound.arguments.get(label_namer); 1480 or bound.kwargs.get(label_namer)). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5662, in Axes.fill_betweenx(self, y, x1, x2, where, step, interpolate, **kwargs); 5660 def fill_betweenx(self, y, x1, x2=0, where=None,; 5661 step=None, interpolate=False, **kwargs):; -> 5662 return self._fill_between_x_or_y(; 5663 ""y"", y, x1, x2,; 5664 where=where, interpolate=interpolate, step=step, **kwargs). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5629, in Axes._fill_between_x_or_y(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs); 5625 pts = pts[:, ::-1]; 5627 polys.append(pts); -> 5629 collection = mcoll.PolyCollection(polys, **kwargs); 5631 # now update the datalim and autoscale; 5632 pts = np.vstack([np.hstack([ind[where, None], dep1[where, None]]),; 5633 np.hstack([ind[where, None], dep2[where, None]])]). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/collections.py:1198, in PolyCollection.__init__(self, verts, sizes, closed, **kwargs); 1178 def __init__(self, verts, sizes=None, *, closed=True, **kwargs):; 1179 """"""; 1180 Parameters; 1181 ----------; (...); 1196 Forwarded to `.Collection`.; 1197 """"""; -> 1198 super().__init__(**kwargs); 1199 self.set_sizes(sizes); 1200 self.set_verts(verts, closed). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/collections.py:206, in Collection.__init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, offset_transform, norm, cmap, pickradius, hatch, urls, zorder, **kwargs); 203 self._offset_transform = offset_transform; 205 self._path_effects = None; --> 206 self._internal_update(kwargs); 207 sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:6510,update,update,6510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,1,['update'],['update']
Deployability,"ata' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post3.dev17+g09b9856 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:1233,install,install,1233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['install'],['install']
Deployability,"ata. ```pycon; >>> adata.X.shape; Out[21]: (3433, 16836). >>> adata.X; array([[0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; ...,; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.]], dtype=float32); ```. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; AlexFunctions NA; JonFunctions NA; PIL 9.1.0; PyQt5 NA; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bs4 4.11.1; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.1; import_all NA; ipykernel 6.13.0; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.2; leidenalg 0.8.4; llvmlite 0.38.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.6; packaging 21.3; pandas 1.4.2; params NA; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.7.0; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.8; pytz 2022.1; scipy 1.8.0; seaborn 0.11.2; session_info 1.0.0; setuptools 62.1.0; sip NA; six 1.16.0; sklearn 1.0.2; soupsieve 2.3.1; sphinxcontrib NA; spyder 5.3.0; spyder_kernels 2.3.0; spydercustomize NA; statsmodels 0.13.2; storemagic NA; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.64.0; traitlets 5.1.1; typing_extensions NA; umap 0.5.3; wcwidth 0.2.5; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 7.32.0; jupyter_client 7.2.2; jupyter_core 4.9.2; -----; Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:59) [GCC 10.3.0]; Linux-5.4.0-100-generic-x86_64-with-glibc2.31; -----; Session information updated at 2022-04-14 16:33. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2236:2684,update,updated,2684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2236,1,['update'],['updated']
Deployability,"ata[:, adata.var.highly_variable]. sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']). sc.pp.scale(adata, max_value=10). sc.tl.pca(adata, svd_solver='arpack'). sc.pl.pca(adata, color='CST3'). sc.pl.pca_variance_ratio(adata, log=True). adata.write(results_file). sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40). sc.tl.umap(adata). sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP']). sc.tl.tsne(adata). sc.pl.tsne(adata, color=['CST3', 'NKG7', 'PPBP']). ```. ![image](https://user-images.githubusercontent.com/5612966/158490622-dae4e20c-1e60-4f27-a844-9235fd77c0ea.png). I would expect a umap like the sample on the website:; ![image](https://user-images.githubusercontent.com/5612966/158491669-e4e256c9-edfd-4cd5-9390-3c09cac540bc.png). I was having this problem with my own SC data:; ![image](https://user-images.githubusercontent.com/5612966/158490800-bd710bb1-4fba-4e4f-be2d-d878ebb76d4a.png). Therefore I reproduced the problem using the tutorial. I installed scanpy yesterday using:; ```sh; conda install -c bioconda scanpy; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 9.0.1; anndata 0.7.8; annoy NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.14.5; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; deepMNN NA; defusedxml 0.7.1; dunamai 1.10.0; entrypoints 0.4; executing 0.8.3; fbpca NA; get_version 3.5.4; h5py 3.6.0; hypergeom_ufunc NA; intervaltree NA; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; llvmlite 0.38.0; matplotlib 3.3.2; matplotlib_inline NA; metrics NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; packaging 21.3; pandas 1.4.1; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:3202,install,installed,3202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,2,['install'],"['install', 'installed']"
Deployability,"aving any data); (this cannot be run without any data, because the problem is that it fails with a big dataset. I included an aws download command to the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory); ```pytb; Traceback (most recent call last):; File ""scanpy_test.py"", line 9, in <module>; adata = sc.read_h5ad(PATH, backed=True); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 202, in read_h5ad; return read_h5ad_backed(filename, mode); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 146, in read_h5ad_backed; d.update({k: read_elem(f[k]) for k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 146, in <dictcomp>; d.update({k: read_elem(f[k]) for k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:2344,update,update,2344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['update'],['update']
Deployability,"b/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; start, end = indptr[i], indptr[i + 1]; sums[i] = np.sum(data[start:end]); ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ```; numba is 0.47.0 but 0.43.1 gave the same error.; It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:4713,release,release,4713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['release'],['release']
Deployability,"b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot_ng 2.0.0; pygments 2.6.1; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2018.9; requests 2.23.0; rsa 4.8; scipy 1.4.1; seaborn 0.11.2; session_info 1.0.0; simplegeneric NA; sitecustomize NA; six 1.15.0; sklearn 1.0.2; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.2; storemagic NA; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; tensorflow_probability 0.16.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.11.2; tornado 5.1.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.6; typing_extensions NA; umap 0.5.2; uritemplate 3.0.1; urllib3 1.24.3; wcwidth 0.2.5; wrapt 1.14.0; yaml 3.13; zipp NA; zmq 22.3.0. IPython 5.5.0; jupyter_client 5.3.5; jupyter_core 4.9.2; notebook 5.3.1. Python 3.7.13 (default, Mar 16 2022, 17:37:17) [GCC 7.5.0]; Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic. Session information updated at 2022-04-04 17:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:5616,update,updated,5616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['update'],['updated']
Deployability,bbknn integrates multiple variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:6,integrat,integrates,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['integrat'],['integrates']
Deployability,bbknn wrapper needs an update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/635:23,update,update,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635,1,['update'],['update']
Deployability,bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 pypi_0 pypi; pandas 1.2.4 py38h2531618_0 ; pandoc 2.12 h06a4308_0 ; pandocfilters 1.4.3 py38h06a4308_1 ; pango 1.42.4 h049681c_0 ; parso 0.7.0 py_0 ; partd 1.2.0 pyhd3eb1b0_0 ; patchelf 0.12 h2531618_1 ; path 15.1.2 py38h06a4308_0 ; path.py 12.5.0 0 ; pathlib2 2.3.5 py38h06a4308_2 ; pathspec 0.7.0 py_0 ; pathtools 0.1.2 py_1 ; patsy 0.5.1 py38_0 ; pcre 8.44 he6710b0_0 ; pep8 1.7.1 py38_0 ; pexpect 4.8.0 pyhd3eb1b0_3 ; phenograph 1.5.7 pypi_0 pypi; pickleshare 0.7.5 pyhd3eb1b0_1003 ; pillow 8.2.0 py38he98fc37_0 ; pip 21.1.1 py38h06a4308_0 ; pixman 0.40.0 h7b6447c_0 ; pkginfo 1.7.0 py38h06a4308_0 ; pluggy 0.13.1 py38h06a4308_0 ; ply 3.11 py38_0 ; progeny-py 1.0.3 pypi_0 pypi; progressbar2 3.37.1 py38h06a4308_0 ; prometheus_client 0.10.1 pyhd3eb1b0_0 ; prompt-toolkit 3.0.17 pyh06a4308_0 ; prompt_toolkit 3.0.17 hd3eb1b0_0 ; protobuf 3.17.0 pypi_0 pypi; psutil 5.8.0 py38h27cfd23_1 ; ptyprocess 0.7.0 pyhd3eb1b0_2 ; py 1.10.0 pyhd3eb1b0_0 ; py-lief 0.10.1 py38h403a769_0 ; py-spy 0.3.7 pypi_0 pypi; pyasn1 0.4.8 pypi_0 pypi; pyasn1-modules 0.2.8 pypi_0 pypi; pycairo 1.19.1 py38h708ec4a_0 ; pycodestyle 2.6.0 pyhd3eb1b0_0 ; pycosat 0.6.3 py38h7b6447c_1 ; pycparser 2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:12434,patch,patchelf,12434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['patch'],['patchelf']
Deployability,"been added.; * **update**: method chaining was added to avoid explosion of parameters as suggested in #956. The caveat is that a different name is needed to keep the current functionality of `sc.pl.dotplot`. Now is possible to do `sc.pl.DotPlot(adata, ....).add_dendrogram(size=1.2).swap_axes().show()`. ![image](https://user-images.githubusercontent.com/4964309/77526545-13ce0600-6e8b-11ea-996a-3d548f52f200.png). ![image](https://user-images.githubusercontent.com/4964309/77529251-8c36c600-6e8f-11ea-8446-c1dd18374cd4.png). Open issues:; * we need a better way to filter the output from rank_genes_groups (and I would also consider a better name). One option is to filter while plotting.; * to correctly plot the results from `sc.tl.rank_genes_groups` is better to report the results for all genes (`n_genes`). The default is to keep the top 100 genes per group. Thus, for many genes, the pvalue, logfoldchange etc in other groups is not saved.; * added new parameters, for some I would be happy to have some feedback. One is `style` that has two options `square color` and `dot color`. **Update**; The following is now possible (notice the method chaining):. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}; sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \; .add_totals(sort='descending', size=1.2) \; .legend(color_title='log(UMI count+1)', width=1.6)\; .style(color_map='RdBu_r', dot_min=0.3)\; .show(); ```; ![image](https://user-images.githubusercontent.com/4964309/77777939-889e6d00-7050-11ea-842e-cf2a11739df6.png). TODO:; - [x] Remove small secondary ticks; - [x] Adapt matrixplot to new object model; - [ ] Adapt heatmap to object model; - [x] Adapt stackedviolin to object model; - [ ] Incorporate modifications from #1116 ; - [x] Fix call from rank_genes_groups; - [x] 'black' code; - [x] Update docstrings; - [ ] Update tests; - [ ] Update tutorial and readthedocs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127:1793,Update,Update,1793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127,4,['Update'],['Update']
Deployability,"bel_namer, func.__name__),; 1804 RuntimeWarning, stacklevel=2); -> 1805 return func(ax, *args, **kwargs); 1806 ; 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs); 5762 kwargs.setdefault('snap', False); 5763 ; -> 5764 collection = mcoll.PolyCollection(verts, **kwargs); 5765 ; 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs); 931 %(Collection)s; 932 """"""; --> 933 Collection.__init__(self, **kwargs); 934 self.set_sizes(sizes); 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs); 164 ; 165 self._path_effects = None; --> 166 self.update(kwargs); 167 self._paths = None; 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0); 914 ; 915 with cbook._setattr_cm(self, eventson=False):; --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]; 917 ; 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v); 910 func = getattr(self, 'set_' + k, None); 911 if not callable(func):; --> 912 raise AttributeError('Unknown property %s' % k); 913 return func(v); 914 . AttributeError: Unknown property standard_scale; ```; Any idea of why I'm getting this? . Package info:. ```; scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ```. Th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/559:2261,update,update,2261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559,1,['update'],['update']
Deployability,"bject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17186,install,install,17186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,"ble types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:17472,Pipeline,PipelineDevelope,17472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,ble_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24456,pipeline,pipeline,24456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1430,continuous,continuous,1430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,1,['continuous'],['continuous']
Deployability,"bmc3k(). sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata); adata = adata[:, adata.var.highly_variable].copy(). sc.pp.scale(adata); sc.pp.pca(adata); sc.pp.neighbors(adata). sc.tl.leiden(adata); sc.tl.umap(adata). sc.pl.umap(adata, color=""leiden"", legend_loc=""on data""); ```. #### Versions. <details>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; google NA; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.4; invgauss_ufunc NA; ipykernel 6.22.0; ipywidgets 8.0.6; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; setuptools_scm NA; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; yaml 6.0; zmq 25.0.2; zoneinfo NA; -----; IPython 8.12.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-02 16:27. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480:2629,update,updated,2629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480,1,['update'],['updated']
Deployability,"box_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs); 525 ; 526 else:; --> 527 FigureCanvasAgg.draw(self); 528 renderer = self.get_renderer(); 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self); 386 self.renderer = self.get_renderer(cleared=True); 387 with RendererAgg.lock:; --> 388 self.figure.draw(self.renderer); 389 # A GUI class may be need to update a window using this draw, so; 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer); 290 sorted(self.collections,; 291 key=lambda col: col.do_3d_projection(renderer),; --> 292 reverse=True)):; 293 col.zorder = zorder_offset + i; 294 for i, patch in enumerate(. ~/.local/lib/python3.7/si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/787:2468,patch,patch,2468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787,1,['patch'],['patch']
Deployability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['integrat'],['integrating']
Deployability,bset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32978,pipeline,pipeline,32978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1411,install,install-,1411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,"c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:776) **compression_args,; [777](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); [190](file:///C:/Program%20Files/Python312/Lib/gzip.py:190) mode += 'b'; [191](file:///C:/Program%20Files/Python312/Lib/gzip.py:191) if fileobj",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:18224,Pipeline,PipelineDevelope,18224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"c.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 422, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. #### Versions. <details>; scanpy 1.9.1; [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>; I don't know how to deal with this problem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:2500,install,install,2500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,2,['install'],['install']
Deployability,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918:8,install,install,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918,2,['install'],['install']
Deployability,"can you link a doc build failure, please? Then I can check out why it fails. I assume they changed the documented location of the class. We have it in `qualname_overrides` and should probably just update the location there (or remove it that line if the new documented location now matches the qualified name). https://github.com/theislab/scanpy/blob/e5d246aacc71fb9ed71d49e2e7d5e26743fd4acb/docs/conf.py#L136-L140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781:197,update,update,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781,1,['update'],['update']
Deployability,can you please update your code sample so we can just copy and paste it? There’s an `import scanpy as ac` and a `adata = ???` missing,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2540#issuecomment-1612612204:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2540#issuecomment-1612612204,1,['update'],['update']
Deployability,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965:15,update,update,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965,1,['update'],['update']
Deployability,"canpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appdirs 1.4.4; attr 20.1.0; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.3; colorlog NA; cupy 7.8.0; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.1; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; fa2 NA; fastrlock 0.5; fsspec 0.8.7; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; iniconfig NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.1; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; py 1.9.0; pyarrow 0.16.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytest 6.1.2; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; sca NA; scanpy 1.7.0; scipy 1.6.1; scprep 1.0.5.post2; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; skmisc 0.1.3; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; tblib 1.7.0; texttable 1.6.2; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; tqdm 4.48.2; traitlets 4.3.3; typing_extensions NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-05-03 16:30; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1827:3141,update,updated,3141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1827,1,['update'],['updated']
Deployability,"ch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_genes,; 669 min_disp=min_disp,; (...); 672 max_mean=max_mean,; 673 ); 674 del min_disp, max_disp, min_mean, max_mean, n_top_genes. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:68, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 66 from skmisc.loess import loess; 67 except ImportError:; ---> 68 raise ImportError(; 69 ""Please install skmisc package via `pip install --user scikit-misc""; 70 ); 71 df = pd.DataFrame(index=adata.var_names); 72 data = _get_obs_rep(adata, layer=layer). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. error when attempting install w/ conda; ```python. (scanpy_env) user@Mac ~ % conda install conda-forge::scikit-misc; Channels:; - defaults; - conda-forge; Platform: osx-arm64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::scikit-misc. Current channels:. - defaults; - https://conda.anaconda.org/conda-forge. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org; ```. Im having trouble reproducing the pip3 install scikit-misc error, which i believe is due to me switching between versions of numpy, heres what I had previously copy pasted. ```python; error: subprocess-exited-with-error; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [32 lines of output]; + meson setup /private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c6914ced8cef4877a07998e75e28ca3e /private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c69",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:4492,install,install,4492,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['install'],['install']
Deployability,ching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_spatial_external_img - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching typ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:8978,pipeline,pipeline,8978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"cipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.ScanpyConfig.N_PCS'):. Extension error (sphinx.ext.autosummary):; Handler <function process_generate_options at 0x139c4a940> for event 'builder-inited' threw an exception (exception: type object 'ScanpyConfig' has no attribute 'N_PCS'); make: *** [html] Error 2; ```. </details>. However, I'm entirely sure if this is Sphinx's fault, or our own. Currently the [N_PCS parameter isn't in the rendered documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy._settings.ScanpyConfig.html#scanpy._settings.ScanpyConfig). I think it should be, and am not sure why it's not showing up here. To summarize:. * Previous versions of our doc builds didn't seem to be including attribute docstrings for `ScanpyConfig`.; * Sphinx 4.1.0 raises an error when it hits this a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:2617,release,release-notes,2617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['release'],['release-notes']
Deployability,"ckages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:622) if chunksize or iterator:; [623](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:623) return parser. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds); [1617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1617) self.options[""has_index_names""] = kwds[""has_index_names""]; [1619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/Ap",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:12192,Pipeline,PipelineDevelope,12192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"cksSM.connect('changed', cb.update_normal); 1734 mappable.colorbar = cb. File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/colorbar.py:1225, in Colorbar.__init__(self, ax, mappable, **kwargs); 1223 if isinstance(mappable, martist.Artist):; 1224 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1225 ColorbarBase.__init__(self, ax, **kwargs). File ~/miniconda3/envs/scvi10j/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451, in _make_keyword_only.<locals>.wrapper(*args, **kwargs); 445 if len(args) > idx:; 446 warn_deprecated(; 447 since, message=""Passing the %(name)s %(obj_type)s ""; 448 ""positionally is deprecated since Matplotlib %(since)s; the ""; 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs). TypeError: __init__() got an unexpected keyword argument 'location'; ```; I was having this problem with scanpy 1.9.1 and matplotlib 3.3.2 I just updated to 1.9.2 and confirm the issue is unchanged; ```; scanpy==1.9.2 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==0.23.2 statsmodels==0.13.2 python-igraph==0.9.9 louvain==0.7.1 pynndescent==0.5.6; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.0.1; absl NA; asttokens NA; attr 21.4.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.06.15; cffi 1.14.5; charset_normalizer 2.0.12; chex 0.1.5; cloudpickle 2.2.0; colorama 0.4.4; contextlib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:3618,update,updated,3618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1362,patch,patch,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,4,['patch'],['patch']
Deployability,"colorama 0.4.6; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; exceptiongroup 1.2.2; fastjsonschema NA; fqdn NA; google NA; h5py 3.11.0; idna 3.3; importlib_resources NA; ipykernel 6.5.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; isoduration NA; jedi 0.18.0; jinja2 3.0.3; joblib 1.4.2; json5 0.9.25; jsonpointer 3.0.0; jsonschema 4.23.0; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.2; jupyterlab_server 2.27.3; kiwisolver 1.4.7; legacy_api_wrap NA; llvmlite 0.43.0; markupsafe 2.0.1; matplotlib 3.9.2; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; nt NA; ntsecuritycon NA; numba 0.60.0; numpy 1.26.4; overrides NA; packaging 24.1; pandas 2.2.2; parso 0.8.2; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.1; pooch v1.7.0; prometheus_client NA; prompt_toolkit 3.0.22; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 3.1.4; pythoncom NA; pythonjsonlogger NA; pytz 2024.1; pywin32_bootstrap NA; pywin32_system32 NA; pywintypes NA; referencing NA; requests 2.32.3; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.13.1; send2trash NA; session_info 1.0.0; setuptools_scm NA; simplejson 3.19.2; six 1.16.0; sklearn 1.5.1; sniffio 1.3.1; storemagic NA; threadpoolctl 3.5.0; torch 1.10.1+cpu; tornado 6.4.1; tqdm 4.66.5; traitlets 5.14.3; typing_extensions NA; uri_template NA; urllib3 1.26.7; wcwidth 0.2.5; webcolors 24.8.0; websocket 1.8.0; win32api NA; win32com NA; win32con NA; win32security NA; win32trace NA; winerror NA; yaml 6.0; zipp NA; zmq 26.2.0; zoneinfo NA; -----; IPython 7.29.0; jupyter_client 7.4.9; jupyter_core 5.7.2; jupyterlab 4.2.5; notebook 7.2.2; -----; Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]; Windows-10-10.0.22621-SP0; -----; Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:10360,update,updated,10360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['update'],['updated']
Deployability,"columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__; filename=filename, filemode=filemode); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual; X = X.astype(dtype, copy=False); ValueError: setting an array element with a sequence.; ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:2411,release,release,2411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['release'],['release']
Deployability,"com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(None); E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); E non-precise type pyobject; E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); E ; E File ""scanpy/preprocessing/_simple.py"", line 763:; E def do_scale(X, maxv, nthr):; E <source elided>; E # t0= time.time(); E s = np.zeros((nthr, X.shape[1])); E ^ ; E ; E This error may have been caused by the following argumen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:1445,pipeline,pipeline,1445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,1,['pipeline'],['pipeline']
Deployability,"compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 289 mutated |= check(pss.run_initialization, internal_state); 290 with SimpleTimer() as pass_time:; --> 291 mutated |= check(pss.run_pass, internal_state); 292 with SimpleTimer() as finalize_time:; 293 mutated |= check(pss.run_finalizer, internal_state). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 440 ; 441 # TODO: Pull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']; 444 signature = typing.signature(state.return_type, *state.args). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 368 lower = lowering.Lower(targetctx, library, fndesc, interp,; 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:; 372 lower.create_cpython_wrapper(flags.release_gil). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 177 if self.generator_info is None:; 178 self.genlower = None; --> 179 self.lower_normal_function(self.fndesc); 180 else:; 181 self.genlower = self.GeneratorLower(self). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc); 231 # Init argument values; 232 self.extract_function_arguments(); --> 233 entry_block_tail = self.lower_function_body(); 234 ; 235 # Close tail of e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:11229,pipeline,pipeline,11229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['pipeline'],['pipeline']
Deployability,conda install -c bioconda anndata2ri breaks import of scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172,1,['install'],['install']
Deployability,conda install -c conda-forge pynndescent. This also fixed my problem. Thanks @FlyPythons for the hint.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075,1,['install'],['install']
Deployability,conda install pytables. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. But there is still ImportError.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063703998:6,install,install,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063703998,2,['install'],"['install', 'installed']"
Deployability,conda installation?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/29:6,install,installation,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29,1,['install'],['installation']
Deployability,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5876,pipeline,pipelines,5876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipelines']
Deployability,"construction.py in init_dict(data, index, columns, dtype); 237 else:; 238 nan_dtype = dtype; --> 239 val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype); 240 arrays.loc[missing] = [val] * missing.sum(); 241 . /usr/local/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in construct_1d_arraylike_from_scalar(value, length, dtype); 1438 else:; 1439 if not isinstance(dtype, (np.dtype, type(np.dtype))):; -> 1440 dtype = dtype.dtype; 1441 ; 1442 if length and is_integer_dtype(dtype) and isna(value):. AttributeError: type object 'object' has no attribute 'dtype'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.4; -----; MulticoreTSNE NA; PIL 8.0.1; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; cffi 1.14.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.01.0; dateutil 2.8.1; decorator 4.4.2; dunamai 1.7.0; fsspec 2022.01.0; get_version 3.5.3; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; jsonschema 3.2.0; jupyter_server 1.13.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.38.0; loompy 3.0.6; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.1; nbformat 5.0.8; numba 0.55.0; numexpr 2.7.3; numpy 1.21.5; numpy_groupies 0.9.13; packaging 21.3; pandas 1.0.4; parso 0.7.1; patsy 0.5.2; pex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:4438,install,install,4438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,1,['install'],['install']
Deployability,continuous color map,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550:0,continuous,continuous,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550,1,['continuous'],['continuous']
Deployability,correlation between cell types and continuous variables stored in .obs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:35,continuous,continuous,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['continuous'],['continuous']
Deployability,could not install louvain via conda,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/143:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143,1,['install'],['install']
Deployability,"count after normalization. Similar functions are used, for │ │ │ │ │; │ │ │ │ │ │ example, by Seurat [Satija15], Cell Ranger [Zheng17] or SPRING [Weinreb17]. Parameters: data AnnData | ndarray | spmatrixThe (annotated) data matrix of shape n_obs × n_vars. Rows correspond to cells and columns to genes. counts_per_cell_after │ │ │ │ │; │ │ │ │ │ │ Optional[float] (default: None)If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization. counts_per_cell Optional[ndarray] (default: None)Precomputed counts per cell. key_n_counts │ │ │ │ │; │ │ │ │ │ │ str (default: 'n_counts')Name of the field in adata.obs where the total counts per cell are stored. copy bool (default: False)If an AnnData is passed, determines whether a copy is returned. min_counts int (default: 1)Cells with counts less than │ │ │ │ │; │ │ │ │ │ │ min_counts are filtered out during normalization. Return type: UnionType[AnnData, ndarray, spmatrix, None] Returns: Returns None if copy=False, else returns an updated AnnData object. Sets the following fields: adata.Xnumpy.ndarray | │ │ │ │ │; │ │ │ │ │ │ scipy.sparse._csr.csr_matrix (dtype float)Normalized count data matrix. Examples >>> import scanpy as sc >>> adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]], dtype=np.float32)) >>> print(adata.X.sum(axis=1)) [ 1. 3. 11.] >>> │ │ │ │ │; │ │ │ │ │ │ sc.pp.normalize_per_cell(adata) >>> print(adata.obs) n_counts 0 1.0 1 3.0 2 11.0 >>> print(adata.X.sum(axis=1)) [3. 3. 3.] >>> sc.pp.normalize_per_cell( ... adata, counts_per_cell_after=1, ... key_n_counts='n_counts2', ... ) >>> print(adata.obs) │ │ │ │ │; │ │ │ │ │ │ n_counts n_counts2 0 1.0 3.0 1 3.0 3.0 2 11.0 3.0 >>> print(adata.X.sum(axis=1)) [1. 1. 1.] │ │ │ │ │; │ │ │ │ ╰───┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805:6169,update,updated,6169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805,1,['update'],['updated']
Deployability,cov.io/gh/scverse/scanpy/pull/3191?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `66.66667%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.61%. Comparing base [(`243a46e`)](https://app.codecov.io/gh/scverse/scanpy/commit/243a46e674f97f04c835893320dfd21543f89827?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`deb9f38`)](https://app.codecov.io/gh/scverse/scanpy/commit/deb9f3876ec6848562b1f5f3e5ce9f9a8551eead?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 49 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3191?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3191?src=pr&el=tree&filepath=src%2Fscanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9nZXQvZ2V0LnB5) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3191?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3191 +/- ##; ==========================================; - Coverage 76.61% 76.61% -0.01% ; ==========================================; Files 109 109 ; Lines 12529 12532 +3 ; ==========================================; + Hits 9599 9601 +2 ; - Misses 2930 2931 +1 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3191#issuecomment-2263391738:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3191#issuecomment-2263391738,1,['Patch'],['Patch']
Deployability,cov.io/gh/scverse/scanpy/pull/3264?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `92.00000%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.95%. Comparing base [(`d998742`)](https://app.codecov.io/gh/scverse/scanpy/commit/d9987426be03f9ef1bdab065f50959d046734ea4?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`9d0ffa5`)](https://app.codecov.io/gh/scverse/scanpy/commit/9d0ffa5b52b7311999bd52f7c856096a2b3d7653?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3264?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3264?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 81.81% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3264?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3264 +/- ##; ==========================================; - Coverage 76.96% 76.95% -0.02% ; ==========================================; Files 109 109 ; Lines 12469 12466 -3 ; ==========================================; - Hits 9597 9593 -4 ; - Misses 2872 2873 +1 ; ```. | [Flag](https://app.codecov.io/gh/scverse/scanpy/pull/3264/flags?src,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3264#issuecomment-2376822003:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264#issuecomment-2376822003,1,['Patch'],['Patch']
Deployability,cov.io/gh/scverse/scanpy/pull/3335?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `90.54054%` with `7 lines` in your changes missing coverage. Please review.; > Project coverage is 76.56%. Comparing base [(`6440515`)](https://app.codecov.io/gh/scverse/scanpy/commit/6440515ebce6e38b62bac5bce6d656f71fbeaa5b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`b426035`)](https://app.codecov.io/gh/scverse/scanpy/commit/b4260358866324a1097cdce17315ceebfe0cef0b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3335?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 87.23% | [6 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 87.50% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3335?src=pr&el=tree&utm_medium,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2450235904:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2450235904,1,['Patch'],['Patch']
Deployability,"ct(; 205 X=_normalize_data(X, counts_per_cell, target_sum, copy=True),; 206 norm_factor=counts_per_cell,; 207 ). File ~/my-conda-envs/sc2022-multiomics/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:25, in _normalize_data(X, counts, after, copy); 23 if issubclass(X.dtype.type, (int, np.integer)):; 24 X = X.astype(np.float32) # TODO: Check if float64 should be used; ---> 25 if isinstance(counts, DaskArray):; 26 counts_greater_than_zero = counts[counts > 0].compute_chunk_sizes(); 27 else:. TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union; ```. I've checked that obs_names, var_names, obs columns names are all unique. Any clue how to solve?. Thanks!. #### Versions. <details>. -----; anndata 0.7.8; scanpy 1.9.0; -----; PIL 9.1.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; entrypoints 0.4; executing 0.8.3; google NA; h5py 3.6.0; hypergeom_ufunc NA; ipykernel 6.12.1; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.2; llvmlite 0.38.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.5; packaging 21.3; pandas 1.4.2; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.7; pytz 2022.1; scipy 1.8.0; session_info 1.0.0; setuptools 62.0.0; six 1.16.0; sklearn 1.0.2; stack_data 0.2.0; threadpoolctl 3.1.0; tornado 6.1; traitlets 5.1.1; wcwidth 0.2.5; zmq 22.3.0; -----; IPython 8.2.0; jupyter_client 7.2.1; jupyter_core 4.9.2; -----; Python 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]; Linux-4.15.0-112-generic-x86_64-with-glibc2.31; -----; Session information updated at 2022-04-05 09:40. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2210:2926,update,updated,2926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2210,1,['update'],['updated']
Deployability,"cvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:981,install,install,981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,2,['install'],['install']
Deployability,cverse/scanpy/pull/3115?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `57.14286%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.50%. Comparing base [(`8d9a5f0`)](https://app.codecov.io/gh/scverse/scanpy/commit/8d9a5f0d2b303abeb42f7e4c9252d505000fd05c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`8fb2a0e`)](https://app.codecov.io/gh/scverse/scanpy/commit/8fb2a0eac8778931a22d70c16e76b9f516f9ef78?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 48 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3115?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3115?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3115?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3115 +/- ##; ==========================================; + Coverage 76.31% 76.50% +0.18% ; ==========================================; Files 109 109 ; Lines 12515 12474 -41 ; ==========================================; - Hits 9551 9543 -8 ; + Misses 2964 2931 -33 ; ```. | [Fil,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2181074546:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2181074546,1,['Patch'],['Patch']
Deployability,cverse/scanpy/pull/3246?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `25.00000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.71%. Comparing base [(`2553c67`)](https://app.codecov.io/gh/scverse/scanpy/commit/2553c67af6e47992abde5cb13e4c9deb82a3adbc?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`2e3ca25`)](https://app.codecov.io/gh/scverse/scanpy/commit/2e3ca25422b317736d49b2b14a61683cb9bfa98b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3246?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_stacked\_violin.py](https://app.codecov.io/gh/scverse/scanpy/pull/3246?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_stacked_violin.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fc3RhY2tlZF92aW9saW4ucHk=) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3246?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3246 +/- ##; ==========================================; - Coverage 76.76% 76.71% -0.05% ; ==========================================; Files 109 109 ; Lines 12529 12533 +4 ; ==========================================; - Hits 9618 9615 -3 ; - Misses 2911 2918 +7 ; ```. | [Files with m,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3246#issuecomment-2363411554:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3246#issuecomment-2363411554,1,['Patch'],['Patch']
Deployability,cverse/scanpy/pull/3258?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.24528%` with `22 lines` in your changes missing coverage. Please review.; > Project coverage is 76.96%. Comparing base [(`8b2088d`)](https://app.codecov.io/gh/scverse/scanpy/commit/8b2088de18452ff11e555bac0c147eaf15cf27f4?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`1b1fbc9`)](https://app.codecov.io/gh/scverse/scanpy/commit/1b1fbc93200bdbf7b69d0dd87f01a5c0ede2bdc1?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3258?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_tools/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_tools%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdG9vbHMvX19pbml0X18ucHk=) | 44.44% | [5 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_sim.py](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_sim.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fc2ltLnB5) | 58.33% | [5 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3258?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3258#issuecomment-2371725350:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3258#issuecomment-2371725350,1,['Patch'],['Patch']
Deployability,"d breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging 21.3; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pyexpat NA; pygments 2.10.0; pynndescent 0.5.6; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.26.0; scipy 1.7.1; seaborn 0.11.2; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; statsmodels 0.13.2; storemagic NA; tables 3.7.0; terminado 0.11.1; texttable 1.6.4; tornado 6.1; tqdm 4.63.0; traitlets 5.0.5; umap 0.5.2; urllib3 1.26.6; wcwidth 0.2.5; websocket 1.2.1; zmq 22.2.1; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.1.7; notebook 6.4.3; -----; Python 3.9.6 (default, Aug 18 2021, 11:08:34) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]; Linux-3.10.0-1160.41.1.el7.x86_64-x86_64-with-glibc2.17; 36 logical CPU cores, x86_64; -----; Session information updated at 2022-03-21 23:04]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2188:3035,update,updated,3035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2188,1,['update'],['updated']
Deployability,"d, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1732,install,install,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['install'],['install']
Deployability,"d-bb115cd014aa.png); Fig. 3; ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png); Fig. 4; ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png); Fig. 5; ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png); Fig. 6; ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>; -----; anndata 0.7.5; scanpy 1.7.0rc2.dev1+g2a123065; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; coverage 5.3; cycler 0.10.0; cython_runtime NA; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.6.1; matplotlib 3.3.3; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.51.2; numcodecs 0.7.2; numexpr 2.7.1; numpy 1.19.4; packaging 20.8; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pygments 2.7.3; pyparsing 2.4.7; pytz 2020.5; ruamel NA; scanpy 1.7.0rc2.dev1+g2a123065; scipy 1.5.4; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sparse 0.11.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.1; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.6.1; zmq 20.0.0; zope NA; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.7.0; notebook 6.1.5; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.10.0-1-amd64-x86_64-with-glibc2.10; 8 logical CPU cores; -----; Session information updated at 2021-01-16 21:28; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1591:5747,update,updated,5747,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591,1,['update'],['updated']
Deployability,"d.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3390,install,install,3390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:37546,pipeline,pipeline,37546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"de a directory like `scanpy/tests/_images`. Tests generate a new file with the same name as the reference image inside `scanpy/tests/figures`. These files are compared. If they're not similar enough, a file with a similar name, appended with `failed-diff` is written to the figures folder. There are a few annoyances with this setup. ### Opening the relevant files to investigate a failed test. Its not hard to open up the generated file, reference, and diff to see what changed, but it's harder than it should be. You'd have to write a function to make this at all easy. ### It's not obvious from the paths which file is the reference and which file was generated. E.g. given the paths:. ```; scanpy/tests/_images/pca.png; scanpy/tests/figures/pca.png; ```. which one is the reference?. ### Shared file names cause ambiguity. <img width=""771"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113106711-cda24400-924e-11eb-8e00-743f178e40b8.png"">. ~~This is more of a hunch.~~ There should be three images here, but only two show up. I suspect this has to do with the missing image having a duplicated name. Update: Tested this in #1773, all images show up now (@gokceneraslan):. <img width=""364"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113120191-0f39eb80-925d-11eb-9c99-b2630e829ecf.png"">. ## This PR. This PR addresses the three points from above. Now each reference image gets its own directory. The reference image is named `expected.png`, and is stored in git. When a test runs the generated image is stored as `actual.png`, and compared with `expected.png`. A failing diff is also written to this directory if the comparison fails. * Opening all images is now as simple as `open scanpy/tests/_images/{fig_name}/*`; * Naming conventions are obvious; * Per test, each file has a unique name. ## TODO. - [x] Dev docs; - [ ] Decide on fixture api; - [ ] Decide how generated outputs are handled (`.gitignore`-d, deleted each time that test is run, both?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1772:1348,Update,Update,1348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1772,1,['Update'],['Update']
Deployability,"de counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1158,install,installing,1158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['install'],['installing']
Deployability,"de is used; 111 if cres.typing_error is not None and not flags.enable_pyobject:. ~\anaconda3\lib\site-packages\numba\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 549 pipeline = pipeline_class(typingctx, targetctx, library,; 550 args, return_type, flags, locals); --> 551 return pipeline.compile_extra(func); 552 ; 553 . ~\anaconda3\lib\site-packages\numba\compiler.py in compile_extra(self, func); 329 self.state.lifted = (); 330 self.state.lifted_from = None; --> 331 return self._compile_bytecode(); 332 ; 333 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_bytecode(self); 391 """"""; 392 assert self.state.func_ir is None; --> 393 return self._compile_core(); 394 ; 395 def _compile_ir(self):. ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_core(self); 371 self.state.status.fail_reason = e; 372 if is_final_pipeline:; --> 373 raise e; 374 else:; 375 raise CompilerError(""All available pipelines exhausted""). ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_core(self); 362 res = None; 363 try:; --> 364 pm.run(self.state); 365 if self.state.cr is not None:; 366 break. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 345 (self.pipeline_name, pass_desc); 346 patched_exception = self._patch_error(msg, e); --> 347 raise patched_exception; 348 ; 349 def dependency_analysis(self):. ~\anaconda3\lib\site-packages\numba\compiler_machinery.py in run(self, state); 336 pass_inst = _pass_registry.get(pss).pass_inst; 337 if isinstance(pass_inst, CompilerPass):; --> 338 self._runPass(idx, pass_inst, state); 339 else:; 340 raise BaseException(""Legacy pass in use""). ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\ana",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:9150,pipeline,pipelines,9150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['pipeline'],['pipelines']
Deployability,diffxpy integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955,1,['integrat'],['integration']
Deployability,"directed'; Traceback (most recent call last):; File ""/data/home/ruanlab/huangxingyu/miniconda3/lib/python3.10/site-packages/scanpy/neighbors/__init__.py"", line 406, in __init__; self._connected_components = connected_components(self._connectivities); ValueError: Buffer dtype mismatch, expected 'ITYPE_t' but got 'long'; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:29:05); ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.10.1; -----; Cython 0.29.33; IPython 8.13.2; PIL 9.4.0; annoy NA; asttokens NA; backcall 0.2.0; bbknn 1.6.0; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.0.4; cycler 0.10.0; cython 0.29.33; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; executing 1.2.0; fontTools 4.39.0; h5py 3.8.0; idna 3.4; igraph 0.10.5; jedi 0.18.2; joblib 1.3.1; kiwisolver 1.4.4; lazy_loader 0.4; legacy_api_wrap NA; llvmlite 0.40.1; matplotlib 3.9.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.23.0; packaging 23.0; pandas 2.0.1; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.0; plotly 5.13.1; pooch v1.7.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pycparser 2.21; pygments 2.14.0; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; requests 2.28.1; ruamel NA; scipy 1.10.1; seaborn 0.13.2; session_info 1.0.0; six 1.16.0; skimage 0.24.0; sklearn 1.3.0; socks 1.7.1; sphinxcontrib NA; stack_data 0.6.2; statsmodels 0.14.2; texttable 1.6.7; threadpoolctl 3.1.0; tqdm 4.64.1; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; urllib3 1.26.14; wcwidth 0.2.6; yaml 6.0; zstandard 0.18.0; -----; Python 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]; Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-07-03 12:41. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3141:2669,update,updated,2669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3141,1,['update'],['updated']
Deployability,"ditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Dear scanpy teams, research fellows,; I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,; and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!!. ### Minimal code sample. ```python; all_adata = anndata.AnnData.concatenate(train_adata,test_adata); ```. ### Error output. ```pytb; File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate; raise AttributeError(""Can only use .str accessor with string values!""); AttributeError: Can only use .str accessor with string values!; ```. ### Versions. <details>. ```; >>> scanpy.logging.print_versions(); -----; anndata 0.8.0; scanpy 1.9.3; -----; CIForm NA; PIL 9.1.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; google NA; h5py 3.11.0; igraph 0.10.4; joblib 1.2.0; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.5.2; mpl_toolkits NA; natsort 8.3.1; nt NA; numba 0.56.4; numpy 1.23.5; opt_einsum v3.3.0; packaging 21.3; pandas 2.2.3; plotly 5.13.1; psutil 5.9.4; pyparsing 3.0.9; pytz 2022.1; scipy 1.10.0; session_info 1.0.0; six 1.16.0; sklearn 1.2.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 1.13.1+cpu; tqdm 4.64.1; typing_extensions NA; yaml 6.0; zoneinfo NA; zope NA; -----; Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]; Windows-10-10.0.19041-SP0; -----; Session information updated at 2024-09-26 11:05; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3261:1986,update,updated,1986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261,1,['update'],['updated']
Deployability,"dn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:776) **compression_args,; [777](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:18025,Pipeline,PipelineDevelope,18025,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"dn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:5798,Pipeline,PipelineDevelope,5798,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.ScanpyConfig.N_PCS'):. Extension error (sphinx.ext.autosummary):; Handler <function process_generate_options at 0x139c4a940> for event 'builder-inited' threw an exception (exception: type object 'ScanpyConfig' has no attribute 'N_PCS'); make: *** [html] Error 2; ```. </details>. However, I'm entirely sure if this is Sphinx's fault, or our own. Currently the [N_PCS parameter isn't in the rendered documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy._settings.ScanpyConfig.html#scanpy._settings.ScanpyConfig). I think it should be, and am not sure why it's not showing up here. To summarize:. * Previous versions of our doc builds didn't seem to be including attribute docstrings for `ScanpyConfig`.; * Sphinx 4.1.0 raises an error when it hits this attribute",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:2642,release,release-notes,2642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,6,['release'],"['release-latest', 'release-notes']"
Deployability,documentation and small plot updates,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1316:29,update,updates,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1316,1,['update'],['updates']
Deployability,"ds, init_pos='paga'),**; **drawing single-cell graph using layout 'fa'; WARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).**. installation error. Collecting fa2; Using cached fa2-0.3.5.tar.gz (435 kB); Requirement already satisfied: numpy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.18.4); Requirement already satisfied: scipy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.4.1); Requirement already satisfied: tqdm in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (4.46.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\xkalaj\AppData\Local\Temp\pip-wheel-yjh93oit'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running bdist_wheel; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.eg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:1176,install,install-,1176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install-']
Deployability,duals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18516,pipeline,pipeline,18516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18056,pipeline,pipeline,18056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17594,pipeline,pipeline,17594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19676,pipeline,pipeline,19676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,duals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17823,pipeline,pipeline,17823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"dularity is Q = 0.796318; Louvain completed 36 runs in 2.867233991622925 seconds. `Louvain` in the above example should be changed to `Leiden`. . ### Minimal code sample. ```python; from scanpy import external. df = pd.DataFrame(np.zeros(1000,10)); phenograph = external.tl.phenograph ; cluster_ph = phenograph(df.values, k=60, method='leiden')[0]; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.4; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 1.0.0; functions NA; google NA; h5py 3.9.0; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.3.2; jupyter_server 1.18.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.1rc1; markupsafe 2.1.1; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; plotly 5.14.1; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 9.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.11.2; seaborn 0.12.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; stack_data 0.5.0; statsmodels 0.14.0; tblib 1.7.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; tornado 6.2; traitlets 5.3.0; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zipp NA; zmq 23.2.1; zope NA; -----; IPython 8.4.0; jupyter_client 7.3.5; jupyter_core 4.11.1; jupyterlab 3.4.6; notebook 6.4.12; -----; Python 3.10.6 (main, Aug 9 2022, 08:40:02) [GCC 11.2.0]; Linux-6.2.0-1010-aws-x86_64-with-glibc2.35; -----; Session information updated at 2023-09-05 09:34. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2653:2472,update,updated,2472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653,1,['update'],['updated']
Deployability,"duplicate of #2345, #2565, and so on. we’ll do a release soon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488#issuecomment-1653487974:49,release,release,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488#issuecomment-1653487974,1,['release'],['release']
Deployability,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2032,update,update,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['update']
Deployability,e matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImple,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:13745,pipeline,pipeline,13745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,integrat,integration,2121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"e output for PAGA looks weird. . Please see below. Is this something to do with figure margins/axes?. Thanks in advance. ![Screenshot 2023-09-13 at 20 48 43](https://github.com/scverse/scanpy/assets/40166451/c7313231-a2dc-49cb-a3df-21e97d86d278). ### Minimal code sample. ```python; adata2 = sc.datasets.pbmc3k_processed(); sc.tl.paga(adata2, groups='louvain'); sc.pl.paga(adata2); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.5; -----; PIL 8.0.1; backcall 0.2.0; bottleneck 1.3.7; cellrank 1.5.1; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; decorator 5.1.1; docrep 0.3.2; google NA; h5py 3.8.0; igraph 0.10.4; importlib_resources NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 3.0.3; joblib 1.2.0; kiwisolver 1.3.0; leidenalg 0.9.1; llvmlite 0.34.0; lz4 3.1.10; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; numba 0.51.2; numexpr 2.8.5; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 4.2.0; prompt_toolkit 3.0.8; psutil 5.7.2; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pygpcca 1.0.4; pyparsing 2.4.7; python_utils NA; pytz 2020.1; ruamel NA; scipy 1.10.1; scvelo 0.2.5; seaborn 0.11.0; session_info 1.0.0; six 1.15.0; sklearn 1.2.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.1.0; tlz 0.12.1; toolz 0.11.1; tornado 6.1; tqdm 4.50.2; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; wrapt 1.15.0; yaml 5.3.1; zipp NA; zmq 19.0.2; zope NA; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.4.0-137-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-09-15 09:42; Running scvelo 0.2.5 (python 3.8.5) on 2023-09-15 09:42. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2665:2312,update,updated,2312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2665,1,['update'],['updated']
Deployability,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1491,Install,Installing,1491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"e sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.2; pkg_resources NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; tables 3.6.1; tqdm 4.62.1; typing_extensions NA; yaml 5.4.1; zipp NA; -----; Python 3.7.11 (default, Jul 27 2021, 07:03:16) [Clang 10.0.0 ]; Darwin-20.4.0-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-09-15 10:41. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:2455,update,updated,2455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['update'],['updated']
Deployability,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2450,pipeline,pipeline,2450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['pipeline'],['pipeline']
Deployability,"e to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scipy 1.6.3; send2trash NA; six 1.16.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.9.4; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; yaml 5.4.1; zmq 20.0.0; zope NA; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.3.0; -----; Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 4 logical CPU cores, i386; -----; Session information updated at 2021-06-04 10:04. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:8164,update,updated,8164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['update'],['updated']
Deployability,"e, False):. ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 406 # TODO: Pull this out into the pipeline; --> 407 NativeLowering().run_pass(state); 408 lowered = state['cr']. ~\anaconda3\lib\site-packages\numba\typed_passes.py in run_pass(self, state); 348 metadata=metadata); --> 349 lower.lower(); 350 if not flags.no_cpython_wrapper:. ~\anaconda3\lib\site-packages\numba\lowering.py in lower(self); 231 # Materialize LLVM Module; --> 232 self.library.add_ir_module(self.module); 233 . ~\anaconda3\lib\site-packages\numba\targets\codegen.py in add_ir_module(self, ir_module); 200 ir = cgutils.normalize_ir_text(str(ir_module)); --> 201 ll_module = ll.parse_assembly(ir); 202 ll_module.name = ir_module.name. ~\anaconda3\lib\site-packages\llvmlite\binding\module.py in parse_assembly(llvmir, context); 25 mod.close(); ---> 26 raise RuntimeError(""LLVM IR parsing error\n{0}"".format(errmsg)); 27 return mod. RuntimeError: Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-21-b19e785cf655> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata_10x, inplace = True). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel); 281 percent_top=percent_top,; 282 inplace=inplace,; --> 283 X=X,; 284 ); 285 var_metrics = describe_var(. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel); 107 if percent_top:; 108 percent_top = sorted(percent_top); --> 109 proportions = top_segment_proportions(X, percent_top); 110 for i, n in enumerate(percent_top):; 111 obs_metr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:4993,pipeline,pipeline,4993,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['pipeline'],['pipeline']
Deployability,"e-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:10163,Pipeline,PipelineDevelope,10163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"e.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:600) var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pipelin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:9136,Pipeline,PipelineDevelope,9136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"e/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance(val, pd.DataFrame):; 92 name = self.attrname.title().rstrip(""s""). ValueError: Value passed for key 'mean' is of incorrect shape. Values of layers must match dimensions ('obs', 'var') of parent. Value had shape (11, 2) while it should have had (11, 765).; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0rc2.dev19+ga6126980; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.3.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; h5py 3.10.0; igraph 0.11.3; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.3; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.1; parso 0.8.3; pexpect 4.9.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 15.0.1; pygments 2.17.2; pyparsing 3.1.1; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.1; toolz 0.12.1; traitlets 5.14.1; typing_extensions NA; wcwidth 0.2.13; yaml 6.0.1; zarr 2.17.1; zipp NA; -----; Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]; Linux-5.15.0-101-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-03-19 13:05; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:4918,update,updated,4918,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['update'],['updated']
Deployability,"e/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:776) **compression_args,; [777](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); [190](file:///C:/Program%20Files/Python312/Lib/gzip.py:190) mode += 'b'; [191](file:///C:/Program%20Files/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:18458,Pipeline,PipelineDevelope,18458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"e/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4860,Pipeline,PipelineDevelope,4860,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"e/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:5056,Pipeline,PipelineDevelope,5056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"e169-11ea-86f9-597bb1d5da01.png). ### Master. ![output_6_0](https://user-images.githubusercontent.com/8238804/90475475-da17d780-e16a-11ea-9ae9-6fb61fd6357b.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""]); ```. ### Current. ![output_8_0](https://user-images.githubusercontent.com/8238804/90474984-ec454600-e169-11ea-935c-cb094f520a1a.png). ### Master. ![output_8_0](https://user-images.githubusercontent.com/8238804/90475481-de43f500-e16a-11ea-80da-2636b1b6dddf.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""], groups=[""0"", ""1""]); ```. ### Current. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475011-fb2bf880-e169-11ea-90a6-17e13388672e.png). ### Master. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475498-e69c3000-e16a-11ea-876c-747ec02fae6d.png). </details>. ## Continuous values. <details>; <summary> Spatial </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475149-4219ee00-e16a-11ea-9b42-aea86a8883a4.png). ### Master. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475522-eef46b00-e16a-11ea-8d88-4ede97e16252.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""], vmin=""p10"", vmax=""p90""); ```. ### Current. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475174-5100a080-e16a-11ea-91ae-d3c883829b9a.png). ### Master. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475536-f3b91f00-e16a-11ea-8ceb-fc954a9601e6.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![outpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322:2800,Continuous,Continuous,2800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322,1,['Continuous'],['Continuous']
Deployability,"e: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex. ```. </details>. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; backcall 0.2.0; bbknn NA; bottleneck 1.3.2; cairo 1.19.1; cffi 1.14.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.2.0; legacy_api_wrap 0.0.0; leidenalg 0.8.2; llvmlite 0.34.0; lxml 4.6.1; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.0.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; palantir 1.0.0; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.2; scipy 1.5.2; scvelo 0.2.3; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tornado 6.0.4; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; yaml 5.3.1; zipp NA; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-24 12:06. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:7080,update,updated,7080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['update'],['updated']
Deployability,"e:; 138 self.genlower = self.GeneratorLower(self); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/contextlib.py in __exit__(self, type, value, traceback); 129 value = type(); 130 try:; --> 131 self.gen.throw(type, value, traceback); 132 except StopIteration as exc:; 133 # Suppress StopIteration *unless* it's the same exception that; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 ; ; LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32; ; File ""../../../../miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^; ; During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /Users/depretis.stefano/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py (52); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:9918,pipeline,pipeline,9918,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['pipeline'],['pipeline']
Deployability,"e; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\_typing.py in <module>; 82 # array-like; 83 ; ---> 84 ArrayLike = Union[""ExtensionArray"", np.ndarray]; 85 AnyArrayLike = Union[ArrayLike, ""Index"", ""Series""]; 86 . AttributeError: module 'numpy' has no attribute 'ndarray'; ```; It looks like an endless error. What's wrong with the `!pip install scanpy[leiden]`? It installed so many incompatible packages which never happened before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:8526,install,install,8526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,2,['install'],"['install', 'installed']"
Deployability,"e\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=349) msg = ""Failed in %s mode pipeline (step: %s)"" % \; [351](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=350) (self.pipeline_name, pass_desc); [352](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=351) patched_exception = self._patch_error(msg, e); --> [353](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=352) raise patched_exception. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:341, in PassManager.run(self, state); [339](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=338) pass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:22478,pipeline,pipeline,22478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,e_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:30527,pipeline,pipeline,30527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"e_linked_requirement(self._ireq, parallel_builds=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 491, in prepare_linked_requirement; return self._prepare_linked_requirement(req, parallel_builds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 577, in _prepare_linked_requirement; dist = _get_prepared_distribution(; ^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/prepare.py"", line 69, in _get_prepared_distribution; abstract_dist.prepare_distribution_metadata(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/distributions/sdist.py"", line 61, in prepare_distribution_metadata; self.req.prepare_metadata(); File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/req/req_install.py"", line 541, in prepare_metadata; self.metadata_directory = generate_metadata_legacy(; ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 71, in generate_metadata; raise MetadataGenerationFailed(package_details=details) from error; pip._internal.exceptions.MetadataGenerationFailed: metadata generation failed; Remote version of pip: 22.3.1; Local version of pip: 22.3.1; Was pip installed by pip? False; Removed numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) from build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Removed build tracker: '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:10719,install,installed,10719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,"each plot like colormap, edge color, linewidth. `swap_axes` to transpose the figure, `add_dendrogram` with options to change the with of the dendrogran and `add_total` tho show a bar plot of the total number of cells per category. Also includes options to sort the categories. ![image](https://user-images.githubusercontent.com/4964309/81702505-a9d5e400-946b-11ea-823b-018f5dadac84.png). Here is description of changes:; **all figures**:; * Set a title to the image. ; * Pass an `axe` where to plot the image.; * Return a dictionary of axes for further manipulation; * using `return_fig` the plot object can be used to adjust the proportions to the legend and other visual aspects can be fine tuned.; * a bar plot with the totals per category can be added. This will align at the left of the image if the categories are plotted in rows or at the top if they are plotted in columns.; * legend can be removed; * `groupby` can be a list of categories. . **dotplot**; * Improved the colorbar and size legend for dotplots. Now the colorbar and size have titles, which can be modified using the `colorbar_title` and `size_title` arguments. They also align at the bottom of the image and do not shrink if the dotplot image is smaller.; * Plot genes in rows and categories in columns (swap_axes).; * Using the DotPlot object the dot_edge_color and line width can be set up, a grid added as well as several other features; * `sc.pl.rank_genes_groups_dotplot` can now plot `pvals` and `log fold changes`. **matrixplot**; * added title for colorbar and positioned as in dotplot; * `sc.pl.rank_genes_groups_matrixplot` can now plot `pvals` and `log fold changes`. **stacked_violin**; * [update] violin colors can be colored based on average gene expression as in dotplots; * made the linewidth of the violin plots smaller.; * removed the tics for the y axis as they tend to overlap with each other. Using the `style` method they can be visualized. TODO:; - [x] Update tests; - [x] Update tutorial and readthedocs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210:3808,update,update,3808,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210,3,"['Update', 'update']","['Update', 'update']"
Deployability,"ealised that when I used the groups separation the legend has 2 elements: the selected sample and NA. In the older version it was only the name selected in groups. I also tried to get rid afterwards, but it modifies the whole legend... ### Code:. ```python; scanpy.pl.umap(adata,color='Sample_ID',groups='1M_curiox'); ```. ```pytb; ![out](https://user-images.githubusercontent.com/59560120/131350267-f6c2f0b2-209a-4c3c-8f88-6df0ab001333.png); ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; attr 20.3.0; bbknn NA; cached_property 1.5.1; cairo 1.20.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; joblib 0.17.0; jsonschema 3.2.0; kaleido 0.2.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.34.0; louvain 0.6.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.1.3; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; packaging 20.4; pandas 1.1.4; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 4.14.3; prompt_toolkit 1.0.15; psutil 5.8.0; ptyprocess 0.6.0; pvectorc NA; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pyrsistent NA; pytz 2020.4; retrying NA; scanpy 1.7.2; scipy 1.5.3; scvi 0.6.8; seaborn 0.11.0; setuptools_scm NA; simplegeneric NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; solo 0.1; sphinxcontrib NA; statsmodels 0.12.1; storemagic NA; tables 3.6.1; texttable 1.6.3; torch 1.8.1+cu102; tornado 6.1; tqdm 4.54.0; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; zipp NA; zmq 20.0.0; -----; IPython 5.8.0; jupyter_client 6.1.7; jupyter_core 4.7.0; -----; Python 3.7.8 | packaged by conda-forge | (default, Nov 27 2020, 19:24:58) [GCC 9.3.0]; Linux-4.9.0-16-amd64-x86_64-with-debian-9.13; 8 logical CPU cores; -----; Session information updated at 2021-08-30 15:50. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988:2306,update,updated,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988,1,['update'],['updated']
Deployability,"eapdict NA; idna 2.10; igraph 0.9.1; ipykernel 5.4.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.4.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; netifaces 0.10.9; networkx 2.5.1; numba 0.53.1; numexpr 2.7.3; numpy 1.19.5; nvtx NA; opt_einsum v3.3.0; packaging 20.8; pandas 1.2.4; parso 0.8.1; petsc4py 3.14.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 3.53.1; prometheus_client NA; prompt_toolkit 3.0.10; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 1.0.1; pycparser 2.20; pygam 0.8.0; pygments 2.7.4; pygpcca 1.0.2; pynndescent 0.5.2; pynvml 8.0.4; pyparsing 2.4.7; pyrsistent NA; python_utils NA; pytz 2021.1; requests 2.25.1; rmm 0.20.0a+28.g7768d4d; scanpy 1.7.2; scanpy_gpu_funcs NA; scipy 1.6.3; scvelo 0.2.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.2; slepc4py 3.14.0; sniffio 1.2.0; socks 1.7.1; sortedcontainers 2.3.0; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; tensorboard 2.6.0a20210510; tensorflow 2.6.0-dev20210510; termcolor 1.1.0; texttable 1.6.3; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; treelite 1.1.0; treelite_runtime 1.1.0; typing_extensions NA; ucp 0.20.0a+30.g2aa87da; umap 0.5.1; urllib3 1.26.4; virtualenvwrapper NA; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zict 2.0.0; zipp NA; zmq 21.0.1; -----; IPython 7.19.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.5; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.8.0-50-generic-x86_64-with-glibc2.10; 32 logical CPU cores, x86_64; -----; Session information updated at 2021-05-12 13:23. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:4719,update,updated,4719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['update'],['updated']
Deployability,earson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29153,pipeline,pipeline,29153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"eature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc_clust[i], 0.6, bo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:954,integrat,integrated,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['integrat'],['integrated']
Deployability,"ecause I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1051,update,update,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458,1,['update'],['update']
Deployability,"ed, there were issues on how to concatenate objects, but they did not seem to answer my question here. . ### Minimal code sample (that we can copy&paste without having any data). ```python; # Concatenate to main adata object; adata = adata.concatenate(adata_tmp, batch_key='sample_id'); ```. ```pytb; InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; anndata2ri 1.0.4; attr 20.2.0; backcall 0.2.0; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.1; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; gprofiler 1.0.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; louvain 0.6.1; markupsafe 1.1.1; matplotlib 3.3.1; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; pandas 1.1.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.6.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; seaborn 0.10.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; texttable 1.6.3; tornado 6.0.4; traitlets 4.3.3; tzlocal NA; urllib3 1.25.10; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.13.0-36-generic-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2020-09-08 15:06. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409:2591,update,updated,2591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409,1,['update'],['updated']
Deployability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4727,Install,Installing,4727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,3,"['Install', 'install']","['Installing', 'installation', 'installed']"
Deployability,"elope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:622) if chunksize or iterator:; [623](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:623) return parser. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds); [1617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roami",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:10952,Pipeline,PipelineDevelope,10952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"else:; 852 # Make sure it is a list; 853 node_sizes = list(node_sizes); → 855 self._partition = _c_leiden._new_RBConfigurationVertexPartition(pygraph_t,; 856 initial_membership, weights, node_sizes, resolution_parameter); 857 self._update_internal_membership(). BaseException: Could not construct partition: Weight vector not the same size as the number of edges.; ```. #### Versions. <details>. ```; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; appnope 0.1.3; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 1.1.0; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.9.11; ipykernel 6.16.0; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; joblib 1.2.0; jupyter_server 1.19.1; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.7.1; matplotlib 3.6.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.56.2; numpy 1.23.3; packaging 21.3; pandas 1.5.0; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.9.1; session_info 1.0.0; setuptools 65.4.0; six 1.16.0; sklearn 1.1.2; sphinxcontrib NA; stack_data 0.5.1; statsmodels 0.13.2; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.1; traitlets 5.4.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.5; zipp NA; zmq 24.0.1; zoneinfo NA; -----; IPython 8.5.0; jupyter_client 7.3.5; jupyter_core 4.11.1; jupyterlab 3.4.7; notebook 6.4.12; -----; Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) [Clang 13.0.1 ]; macOS-12.6-arm64-arm-64bit; -----; Session information updated at 2022-09-29 11:08; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2341:3957,update,updated,3957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2341,1,['update'],['updated']
Deployability,ementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16433,pipeline,pipeline,16433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16204,pipeline,pipeline,16204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,end.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31266,pipeline,pipeline,31266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,eneral[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20609,pipeline,pipeline,20609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 213, in _attempt_to_pin_criterion; criteria = self._get_updated_criteria(candidate); ^^^^^^^^^",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5910,install,install,5910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install']
Deployability,ents&utm_term=scverse). <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3044 +/- ##; =======================================; Coverage 76.27% 76.28% ; =======================================; Files 117 117 ; Lines 12803 12802 -1 ; =======================================; Hits 9766 9766 ; + Misses 3037 3036 -1 ; ```. | [Files](https://app.codecov.io/gh/scverse/scanpy/pull/3044?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Coverage Δ | |; |---|---|---|; | [scanpy/preprocessing/\_scrublet/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L19faW5pdF9fLnB5) | `96.80% <100.00%> (+0.10%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/pipeline.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fpipeline.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3BpcGVsaW5lLnB5) | `94.59% <100.00%> (+0.30%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/sparse\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3044?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fsparse_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3NwYXJzZV91dGlscy5weQ==) | `89.28% <71.42%> (-1.90%)` | :arrow_down: |. ... and [1 file with indirect coverage changes](https://app.codecov.io/gh/scverse/scanpy/pull/3044/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scv,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2096282888:1797,pipeline,pipeline,1797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2096282888,1,['pipeline'],['pipeline']
Deployability,"equirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); tsundoku@tsundoku-OptiPlex-7070:~/networkanalyst/src/main/webapp/resources/data$ pip install scanpy; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:6079,install,install,6079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,eral[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:29851,pipeline,pipeline,29851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"erator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-pack",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:9760,Pipeline,PipelineDevelope,9760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"erefore I reproduced the problem using the tutorial. I installed scanpy yesterday using:; ```sh; conda install -c bioconda scanpy; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 9.0.1; anndata 0.7.8; annoy NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.14.5; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; deepMNN NA; defusedxml 0.7.1; dunamai 1.10.0; entrypoints 0.4; executing 0.8.3; fbpca NA; get_version 3.5.4; h5py 3.6.0; hypergeom_ufunc NA; intervaltree NA; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; llvmlite 0.38.0; matplotlib 3.3.2; matplotlib_inline NA; metrics NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; packaging 21.3; pandas 1.4.1; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2021.3; scanpy 1.7.2; scipy 1.8.0; seaborn 0.11.2; setuptools 60.9.3; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.23.2; sortedcontainers 2.4.0; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.13.2; tables 3.7.0; torch 1.11.0; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; umap 0.5.2; wcwidth 0.2.5; zipp NA; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.9; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.4.0-104-generic-x86_64-with-glibc2.10; 24 logical CPU cores, x86_64; -----; Session information updated at 2022-03-15 16:53. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:5108,update,updated,5108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,1,['update'],['updated']
Deployability,"ernel_3816/1026803476.py in <module>; ----> 1 sc.settings.set_figure_params(dpi=80, facecolor='white'). NameError: name 'sc' is not defined; ```. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.4; -----; PIL 9.2.0; PyObjCTools NA; appnope 0.1.2; astunparse 1.6.3; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.7; entrypoints 0.4; fsspec 2022.7.1; gmpy2 2.1.2; google NA; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.1; ipykernel 6.15.2; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; mpl_toolkits NA; mpmath 1.2.1; natsort 7.1.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.1; numexpr 2.8.4; numpy 1.21.6; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pyarrow 13.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.9.1; session_info 1.0.0; setuptools 63.4.1; six 1.16.0; sklearn 1.0.2; snappy NA; sphinxcontrib NA; storemagic NA; sympy 1.10.1; tblib 1.7.0; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.2; torch 2.0.1; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; zstandard 0.19.0; -----; IPython 7.31.1; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2023-09-30 11:34. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675:2997,update,updated,2997,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675,1,['update'],['updated']
Deployability,"error message:. ```; computing UMAP; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap; verbose=settings.verbosity > 3,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding; verbose=verbose,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please rep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:1130,pipeline,pipeline,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['pipeline'],['pipeline']
Deployability,"ers[""n_top_genes""].default); --> 655 return _highly_variable_genes_seurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_genes,; 669 min_disp=min_disp,; (...); 672 max_mean=max_mean,; 673 ); 674 del min_disp, max_disp, min_mean, max_mean, n_top_genes. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:68, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 66 from skmisc.loess import loess; 67 except ImportError:; ---> 68 raise ImportError(; 69 ""Please install skmisc package via `pip install --user scikit-misc""; 70 ); 71 df = pd.DataFrame(index=adata.var_names); 72 data = _get_obs_rep(adata, layer=layer). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. error when attempting install w/ conda; ```python. (scanpy_env) user@Mac ~ % conda install conda-forge::scikit-misc; Channels:; - defaults; - conda-forge; Platform: osx-arm64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::scikit-misc. Current channels:. - defaults; - https://conda.anaconda.org/conda-forge. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org; ```. Im having trouble reproducing the pip3 install scikit-misc error, which i believe is due to me switching between versions of numpy, heres what I had previously copy pasted. ```python; error: subprocess-exited-with-error; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [32 lines of output]; + meson setup /private/var/folders/76/3kg7fkgs3_d2gnkhx",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:4344,install,install,4344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,2,['install'],['install']
Deployability,erse/scanpy/pull/3017?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 75.86%. Comparing base [(`3ba3f46`)](https://app.codecov.io/gh/scverse/scanpy/commit/3ba3f46b4e6e77e8c6f0551db9663822097b486a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`277c1bf`)](https://app.codecov.io/gh/scverse/scanpy/commit/277c1bfb0885234aa757d0fdaeaa9103eb8568e2?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 47 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3017?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/preprocessing/\_highly\_variable\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3017?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_highly_variable_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2hpZ2hseV92YXJpYWJsZV9nZW5lcy5weQ==) | 85.71% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3017?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3017 +/- ##; ==========================================; - Coverage 75.87% 75.86% -0.01% ; ==========================================; Files 110 110 ; Lines 12533 12533 ; ==========================================; - Hits 9509 9508 -1 ; - Misses 3024 3025 +1 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3017#issuecomment-2069245430:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3017#issuecomment-2069245430,1,['Patch'],['Patch']
Deployability,erse/scanpy/pull/3134?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `57.14286%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.50%. Comparing base [(`a5eadd5`)](https://app.codecov.io/gh/scverse/scanpy/commit/a5eadd5b723799105d724b5e9f80b711e0be87ca?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`25f0c97`)](https://app.codecov.io/gh/scverse/scanpy/commit/25f0c97e81e9143bece1ded7c4838964ed7d3866?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 43 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3134?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_utils/compute/is\_constant.py](https://app.codecov.io/gh/scverse/scanpy/pull/3134?src=pr&el=tree&filepath=src%2Fscanpy%2F_utils%2Fcompute%2Fis_constant.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fdXRpbHMvY29tcHV0ZS9pc19jb25zdGFudC5weQ==) | 25.00% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3134?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3134 +/- ##; ==========================================; + Coverage 76.31% 76.50% +0.18% ; ==========================================; Files 109 109 ; Lines 12515 12474 -41 ; ==========================================; - Hits 9551 9543 -8 ; + Misses 2964 2931 -33 ; ```. | [F,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3134#issuecomment-2202709708:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3134#issuecomment-2202709708,1,['Patch'],['Patch']
Deployability,ertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:43786,pipeline,pipeline,43786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6771,update,updates,6771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['update'],['updates']
Deployability,"es not support python 3.10](https://github.com/numba/llvmlite/issues/804#issuecomment-1002971267). Is there a way I can install scanpy with an older version of llvmlite? . ```python; pip install --user scanpy; ```. ```pytb. ERROR: Command errored out with exit status 1:; command: /usr/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/setup.py'""'""'; __file__='""'""'/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-j38v6hmh/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/ube/.local/include/python3.10/llvmlite; cwd: /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/; Complete output (29 lines):; running install; running build; got version from file /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/llvmlite/_version.py {'version': '0.34.0', 'full': 'c5889c9e98c6b19d5d85ebdd982d64a03931f8e2'}; running build_ext; /usr/bin/python /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py; LLVM version... Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 105, in main_posix; out = subprocess.check_output([llvm_config, '--version']); File ""/usr/lib/python3.10/subprocess.py"", line 420, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; File ""/usr/lib/python3.10/subprocess.py"", line 501, in run; with Popen(*popenargs, **kwargs) as process:; File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__; self._execute_child(args, executable, preexec_fn, close_fds,; File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child; raise child_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:1022,install,install-,1022,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,3,['install'],"['install', 'install-']"
Deployability,es.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25144,pipeline,pipeline,25144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"escent(self, distances); 284 ; 285 first_col = np.arange(distances.shape[0])[:, None]; --> 286 init_indices = np.hstack((first_col, np.stack(distances.tolil().rows))); 287 ; 288 self._nnd_idx = NNDescent(. <__array_function__ internals> in stack(*args, **kwargs). ~/.local/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.7.0; seaborn 0.11.2; setuptools 62.1.0; simplejson 3.17.6; six 1.16.0; sklearn 1.0.1; statsmodels 0.13.2; storemagic NA; tables 3.7.0;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:3645,install,install,3645,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['install'],['install']
Deployability,"escr.target_context,; 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 625 pipeline = pipeline_class(typingctx, targetctx, library,; 626 args, return_type, flags, locals); --> 627 return pipeline.compile_extra(func); 628 ; 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 361 self.state.lifted = (); 362 self.state.lifted_from = None; --> 363 return self._compile_bytecode(); 364 ; 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 423 """"""; 424 assert self.state.func_ir is None; --> 425 return self._compile_core(); 426 ; 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 403 self.state.status.fail_reason = e; 404 if is_final_pipeline:; --> 405 raise e; 406 else:; 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 394 res = None; 395 try:; --> 396 pm.run(self.state); 397 if self.state.cr is not None:; 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 339 (self.pipeline_name, pass_desc); 340 patched_exception = self._patch_error(msg, e); --> 341 raise patched_exception; 342 ; 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 330 pass_inst = _pass_registry.get(pss).pass_inst; 331 if isinstance(pass_inst, CompilerPass):; --> 332 self._runPass(idx, pass_inst, state); 333 else:; 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:5661,pipeline,pipelines,5661,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['pipeline'],['pipelines']
Deployability,esiduals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAI,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:18979,pipeline,pipeline,18979,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,est_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:27322,pipeline,pipeline,27322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,eta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython m,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:36346,pipeline,pipeline,36346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"eturn_type, flags, locals, library, pipeline_class); 669 """"""Compiler entry point; 670 ; 671 Parameter; (...); 689 compiler pipeline; 690 """"""; 691 pipeline = pipeline_class(typingctx, targetctx, library,; 692 args, return_type, flags, locals); --> 693 return pipeline.compile_extra(func). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:429, in CompilerBase.compile_extra(self, func); 427 self.state.lifted = (); 428 self.state.lifted_from = None; --> 429 return self._compile_bytecode(). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:497, in CompilerBase._compile_bytecode(self); 493 """"""; 494 Populate and run pipeline for bytecode input; 495 """"""; 496 assert self.state.func_ir is None; --> 497 return self._compile_core(). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:476, in CompilerBase._compile_core(self); 474 self.state.status.fail_reason = e; 475 if is_final_pipeline:; --> 476 raise e; 477 else:; 478 raise CompilerError(""All available pipelines exhausted""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:463, in CompilerBase._compile_core(self); 461 res = None; 462 try:; --> 463 pm.run(self.state); 464 if self.state.cr is not None:; 465 break. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:353, in PassManager.run(self, state); 350 msg = ""Failed in %s mode pipeline (step: %s)"" % \; 351 (self.pipeline_name, pass_desc); 352 patched_exception = self._patch_error(msg, e); --> 353 raise patched_exception. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341, in PassManager.run(self, state); 339 pass_inst = _pass_registry.get(pss).pass_inst; 340 if isinstance(pass_inst, CompilerPass):; --> 341 self._runPass(idx, pass_inst, state); 342 else:; 343 raise BaseException(""Legacy pass in use""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:6679,pipeline,pipelines,6679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['pipeline'],['pipelines']
Deployability,"eurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_genes,; 669 min_disp=min_disp,; (...); 672 max_mean=max_mean,; 673 ); 674 del min_disp, max_disp, min_mean, max_mean, n_top_genes. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:68, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 66 from skmisc.loess import loess; 67 except ImportError:; ---> 68 raise ImportError(; 69 ""Please install skmisc package via `pip install --user scikit-misc""; 70 ); 71 df = pd.DataFrame(index=adata.var_names); 72 data = _get_obs_rep(adata, layer=layer). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. error when attempting install w/ conda; ```python. (scanpy_env) user@Mac ~ % conda install conda-forge::scikit-misc; Channels:; - defaults; - conda-forge; Platform: osx-arm64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::scikit-misc. Current channels:. - defaults; - https://conda.anaconda.org/conda-forge. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org; ```. Im having trouble reproducing the pip3 install scikit-misc error, which i believe is due to me switching between versions of numpy, heres what I had previously copy pasted. ```python; error: subprocess-exited-with-error; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [32 lines of output]; + meson setup /private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c6914ced8cef4877a07",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:4431,install,install,4431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['install'],['install']
Deployability,"evelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:5438,Pipeline,PipelineDevelope,5438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:415,pipeline,pipeline,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"expect 4.8.0; pickleshare 0.7.5; platformdirs 4.1.0; prometheus_client NA; prompt_toolkit 3.0.42; psutil 5.9.7; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing 3.1.1; pythonjsonlogger NA; pytz 2023.3.post1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.11.4; send2trash NA; session_info 1.0.0; six 1.16.0; sklearn 1.3.2; sniffio 1.3.0; socks 1.7.1; stack_data 0.6.2; threadpoolctl 3.2.0; tornado 6.3.3; traitlets 5.14.1; typing_extensions NA; uri_template NA; urllib3 2.1.0; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; yaml 6.0.1; zmq 25.1.2; zoneinfo NA; -----; IPython 8.20.0; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyterlab 4.0.10; -----; Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]; Linux-6.6.9-200.fc39.x86_64-x86_64-with-glibc2.38; -----; Session information updated at 2024-01-14 04:38; ```; </Details>. # **working version**. <Details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.2.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; brotli 1.1.0; certifi 2023.11.17; cffi 1.16.0; charset_normalizer 3.3.2; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.10.0; idna 3.6; ipykernel 6.28.0; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.20.0; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.12.4; jupyterlab_server 2.25.2; kiwisolver 1.4.5; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; numba 0.58.1; numpy 1.26.3; overrides NA; packaging 23.2; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; platformdirs 4.1.0; prometheus_client NA; prom",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:10368,update,updated,10368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['update'],['updated']
Deployability,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5006,pipeline,pipeline,5006,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,2,['pipeline'],['pipeline']
Deployability,"f scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When using sc.pl.highest_expr_genes, seaborn throws a FutureWarning. Specifically:. envs/scanpy/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead. ### Minimal code sample. ```python; sc.pl.highest_expr_genes(adata, n_top=20, ); ```. ### Error output. ```pytb; envs/scanpy/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead; ```. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.1.0; asttokens NA; backcall 0.2.0; comm 0.1.2; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 0.8.3; h5py 3.10.0; ipykernel 6.25.0; jedi 0.18.1; joblib 1.3.2; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numpy 1.26.2; packaging 23.1; pandas 2.1.3; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.1.1; pytz 2023.3.post1; scipy 1.11.3; seaborn 0.12.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.2; stack_data 0.2.0; statsmodels 0.14.0; threadpoolctl 3.2.0; tornado 6.3.3; tqdm 4.66.1; traitlets 5.7.1; umap 0.5.4; wcwidth 0.2.5; zmq 25.1.0; -----; IPython 8.15.0; jupyter_client 8.6.0; jupyter_core 5.5.0; -----; Python 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]; Linux-6.2.0-1018-gcp-x86_64-with-glibc2.37; -----; Session information updated at 2023-11-15 20:32; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2755:2134,update,updated,2134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2755,1,['update'],['updated']
Deployability,faiss was reasonably easy to install via conda and has fairly easy to use gpu support which is nice,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1602758898:29,install,install,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1602758898,1,['install'],['install']
Deployability,"figure_params(; ......etc.....; if self._is_run_from_ipython():; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; ```; The ""try:"" ; ```; >>> import IPython; ```; doesn't throw an exception, as IPython is installed:; ```; $ pip show IPython; Name: ipython; Version: 7.18.1; Summary: IPython: Productive Interactive Computing; Home-page: https://ipython.org; Author: The IPython Development Team; Author-email: ipython-dev@python.org; License: BSD; Location: /slipstream/home/sbridgett/miniconda3/lib/python3.8/site-packages; Requires: pygments, jedi, prompt-toolkit, backcall, pexpect, setuptools, decorator, traitlets, pickleshare; Required-by: ipywidgets, ipykernel; ```. whereas the _is_run_from_ipython() will correctly ``return False``, as ``__IPYTHON__`` gives an exception:; ```; def _is_run_from_ipython():; """"""Determines whether run from Ipython.; Only affects progress bars.; """"""; try:; __IPYTHON__; return True; except NameError:; return False; ```. #### Versions. <details>. scanpy.logging.print_versions(); -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.0; anndata 0.7.4; cairo 1.20.0; cffi 1.14.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; get_version 2.1; h5py 2.10.0; igraph 0.8.3; joblib 0.17.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.2; llvmlite 0.34.0; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.0.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.1.3; pkg_resources NA; pyparsing 2.4.7; pytz 2020.1; scanpy 1.6.0; scipy 1.5.2; setuptools_scm NA; sinfo 0.3.1; six 1.14.0; sklearn 0.23.2; tables 3.6.1; texttable 1.6.3; typing_extensions NA; wcwidth 0.2.5; -----; Python 3.8.3 (default, May 19 2020, 18:47:26) [GCC 7.3.0]; Linux-3.13.0-143-generic-x86_64-with-glibc2.10; 32 logical CPU cores, x86_64; -----; Session information updated at 2020-11-01 11:37. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477:3444,update,updated,3444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477,1,['update'],['updated']
Deployability,"file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:10275,update,update,10275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['update'],['update']
Deployability,"fixed in #2424, reported many times. please use the search function. we’ll do a release soon!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2598#issuecomment-1667789925:80,release,release,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2598#issuecomment-1667789925,1,['release'],['release']
Deployability,fixed in https://github.com/theislab/anndata/commit/555c15c8a170944b762ba7ce1d8c0b41f4e4dfbe. the fix should be in the next anndata release (0.6.2 or 0.7),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-391984289:132,release,release,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-391984289,1,['release'],['release']
Deployability,"flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-normalizer=3.3.2; - colorama=0.4.6; - colorcet=3.1.0; - colorful=0.5.6; - comm=0.2.2; - contourpy=1.2.1; - cycler=0.12.1; - debugpy=1.8.1; - decorator=5.1.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2399,pipeline,pipeline,2399,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"follow the normal pipeline and only meet problems when i use the function ""sc.pp.regress"". it works well on R; system is windows 8.1; and use annaconda to manage environment; python 3.6.6. OverflowError Traceback (most recent call last); <ipython-input-21-c0d016811ded> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 777 import multiprocessing; 778 pool = multiprocessing.Pool(n_jobs); --> 779 res = pool.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:18,pipeline,pipeline,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,1,['pipeline'],['pipeline']
Deployability,"for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:1469,release,release,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['release'],['release']
Deployability,for the record: @odorea had the distribution “igraph” installed which contains the “jgraph” package. Scanpy needs the distribution “python-igraph” containing hte package “igraph”. … which is all confusing and annoying so I appreciate that “jgraph” has changed its name away from “igraph”.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-527179520:54,install,installed,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-527179520,1,['install'],['installed']
Deployability,"for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:976,install,install-,976,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],['install-']
Deployability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:2297,pipeline,pipelines,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipelines']
Deployability,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1971,update,updated,1971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['update'],['updated']
Deployability,"formation for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.tl.umap(adata, init_pos='paga', method='rapids'); ```. ```pytb; WARNING: .obsp[""connectivities""] have not been computed using umap. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed eval> in <module>. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 221 ) # 0 is not a valid value for rapids, unlike original umap; 222 X_contiguous = np.ascontiguousarray(X, dtype=np.float32); --> 223 umap = UMAP(; 224 n_neighbors=n_neighbors,; 225 n_components=n_components,. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/cuml/internals/api_decorators.py in inner_f(*args, **kwargs); 792 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); 793 ; --> 794 return func(**kwargs); 795 ; 796 # Set this flag to prevent auto adding this decorator twice. cuml/manifold/umap.pyx in cuml.manifold.umap.UMAP.__init__(). TypeError: %d format: a number is required, not str; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; 2f7ece400a652629565c523b34ee61b04afa385c NA; PIL 8.1.2; absl NA; anndata 0.7.6; anyio NA; astunparse 1.6.3; attr 20.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; cachetools 4.2.2; cellrank 1.3.1; certifi 2020.12.05; cffi 1.14.4; chardet 4.0.0; click 7.1.2; cloudpickle 1.6.0; colorama 0.4.4; cudf 0.20.0a+294.gfbb9a988fa; cugraph 0.20.0a+65.g924f6782.dirty; cuml 0.20.0a+110.gab47f2e11; cupy 9.0.0; cupy_backends NA; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dask_cuda 0+unknown; dask_cudf 0.20.0a+294.gfbb9a988fa; dateutil 2.8.1; decorator 4.4.2; distributed 2021.04.0; docrep 0.3.2; fastrlock 0.5; flatbuffers NA;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:1650,update,update,1650,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['update'],['update']
Deployability,"g exists on the main branch of scanpy. ### What happened?. Python compiler of Pandas is giving Future Warning that in line:; disp_grouped = df.groupby(""mean_bin"")[""dispersions""] ; at \preprocessing\_highly_variable_genes.py:226; Gives this warning:; ""The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning."". ### Minimal code sample. ```python; disp_grouped = df.groupby(""mean_bin"", observed=False)[""dispersions""]; ```. ### Error output. ```pytb; \scanpy\preprocessing\_highly_variable_genes.py:226: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 10.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; google NA; h5py 3.9.0; hypergeom_ufunc NA; igraph 0.11.4; invgauss_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.8.0; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; nt NA; numba 0.59.0; numpy 1.26.4; packaging 23.1; pandas 2.2.1; psutil 5.9.8; pyparsing 3.0.9; pythoncom NA; pytz 2023.3.post1; pywin32_system32 NA; pywintypes NA; scipy 1.11.4; session_info 1.0.0; setuptools 68.2.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; texttable 1.7.0; threadpoolctl 2.2.0; torch 2.2.1; torchgen NA; tqdm 4.65.0; typing_extensions NA; wcwidth 0.2.13; win32api NA; win32com NA; yaml 6.0.1; zoneinfo NA; -----; Python 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.22621-SP0; -----; Session information updated at 2024-03-26 18:50; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2967:2186,update,updated,2186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2967,1,['update'],['updated']
Deployability,"g telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata = AnnData(random(5000, 2000, density=0.6, format='csr')); adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1); multi = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=8, copy=True); ```. I've gotten the same results on master and the current releases.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:2316,release,releases,2316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['release'],['releases']
Deployability,gend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding.py::test_umap_init_paga[fr] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._e,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:11666,pipeline,pipeline,11666,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,gend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38737,pipeline,pipeline,38737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signa,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33909,pipeline,pipeline,33909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_hi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23329,pipeline,pipeline,23329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17357,Install,Installing,17357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,4,"['Install', 'install']","['Installing', 'install', 'installed']"
Deployability,getting an error in scanpy after 'successfully' installing it through anaconda prompt,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587:48,install,installing,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587,1,['install'],['installing']
Deployability,gh/scverse/scanpy/pull/3098?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.16667%` with `5 lines` in your changes missing coverage. Please review.; > Project coverage is 76.35%. Comparing base [(`d34e575`)](https://app.codecov.io/gh/scverse/scanpy/commit/d34e5756aa6a6f763e06d48c060efdd0a94fa468?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`a01cf79`)](https://app.codecov.io/gh/scverse/scanpy/commit/a01cf79bf6dc5809357d037d17bece02d92616f5?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 35 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3098?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_score\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&filepath=scanpy%2Ftools%2F_score_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL19zY29yZV9nZW5lcy5weQ==) | 82.35% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&filepath=scanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2dldC9nZXQucHk=) | 66.66% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3098?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3098#issuecomment-2147840847:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3098#issuecomment-2147840847,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3230?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `33.33333%` with `30 lines` in your changes missing coverage. Please review.; > Project coverage is 76.72%. Comparing base [(`0f6acdf`)](https://app.codecov.io/gh/scverse/scanpy/commit/0f6acdf5dda52b698fbf3e675d018ef75806115c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`a98157b`)](https://app.codecov.io/gh/scverse/scanpy/commit/a98157b52c1b8fa5108a348a5dcf33bd123cc5e6?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3230?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 25.00% | [15 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_draw\_graph.py](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_draw_graph.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fZHJhd19ncmFwaC5weQ==) | 31.57% | [13 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3230?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3230#issuecomment-2348590448:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3230#issuecomment-2348590448,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3248?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `87.87879%` with `4 lines` in your changes missing coverage. Please review.; > Project coverage is 76.75%. Comparing base [(`b0597a9`)](https://app.codecov.io/gh/scverse/scanpy/commit/b0597a9f6f114a1aee6737e0acae6b1ca403e1b8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`3cce3f2`)](https://app.codecov.io/gh/scverse/scanpy/commit/3cce3f28e94d29dc907f94056bd6995f197d9f93?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3248?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/external/tl/\_wishbone.py](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&filepath=src%2Fscanpy%2Fexternal%2Ftl%2F_wishbone.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9leHRlcm5hbC90bC9fd2lzaGJvbmUucHk=) | 50.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 75.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3248?src=pr&el=tre,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3248#issuecomment-2363527588:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3248#issuecomment-2363527588,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3251?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `94.59459%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.71%. Comparing base [(`b325b50`)](https://app.codecov.io/gh/scverse/scanpy/commit/b325b50f942ba75d77e1a4caa181d67f83d0a057?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`408a7b5`)](https://app.codecov.io/gh/scverse/scanpy/commit/408a7b58758a609147276eea01e9f86ae16855ee?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3251?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3251?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 93.10% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3251?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3251 +/- ##; ==========================================; - Coverage 76.72% 76.71% -0.01% ; ==========================================; Files 109 109 ; Lines 12536 12541 +5 ; ==========================================; + Hits 9618 9621 +3 ; - Misses 2918 2920 +2 ; ```. | [Flag](https://app.codecov.io/gh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3251#issuecomment-2363756462:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3251#issuecomment-2363756462,1,['Patch'],['Patch']
Deployability,gh/scverse/scanpy/pull/3316?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 77.21%. Comparing base [(`3d220a9`)](https://app.codecov.io/gh/scverse/scanpy/commit/3d220a93c83fdd60ee3220c94db3dd8d5533c60d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`f5f2775`)](https://app.codecov.io/gh/scverse/scanpy/commit/f5f27756930da430c3f6d803800076e8501952e6?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3316?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3316?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 85.71% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3316?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3316 +/- ##; ==========================================; - Coverage 77.23% 77.21% -0.02% ; ==========================================; Files 111 111 ; Lines 12605 12597 -8 ; ==========================================; - Hits 9735 9727 -8 ; Misses 2870 2870 ; ```. | [Files with missing lines](ht,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316#issuecomment-2435332939:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316#issuecomment-2435332939,1,['Patch'],['Patch']
Deployability,"ghts the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this param",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1195,install,install,1195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['install']
Deployability,"h of scanpy. ### What happened?. Hello scanpy!; First time, please let me know what to fix about my question asking!; When running sc.pp.highly_variable_genes I get this error; ""ImportError: Please install skmisc package via `pip install --user scikit-misc ""; I've tried numerous times to install the package through pip and conda, pip gives me a metadata error and conda can't find the package even using the command the website gives at [https://anaconda.org/conda-forge/scikit-misc](url). I checked around online and there was a two year old thread on the subject (https://github.com/scverse/scanpy/issues/2073), but none of the solutions worked on my machine. I tried version control of all the packages, most importantly numpy as it was mentioned as a problem. I was wondering if this you all have any newer solutions for this issue. Due to my inexperience im really not sure what is causing this issue and therefore what to provide you all with. I also just realized that after a clean install trying to pip3 install scikit-misc returns this . (scanpy_env) user@Mac ~ % pip3 install scikit-misc ; Requirement already satisfied: scikit-misc in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (0.3.1); Requirement already satisfied: numpy>=1.22.3 in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (from scikit-misc) (1.26.4). Still getting the error though... Hardware: M2 max mac, macos 15 beta (could this be it somehow?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. Module",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:1248,install,install,1248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,2,['install'],['install']
Deployability,h/scverse/scanpy/pull/3314?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `88.88889%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 76.95%. Comparing base [(`947afa1`)](https://app.codecov.io/gh/scverse/scanpy/commit/947afa157474130bd94b5130dd2de433692e06ff?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`ca18a68`)](https://app.codecov.io/gh/scverse/scanpy/commit/ca18a68e60d72a0b16263ea43d5b950fcf9b0c44?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3314?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3314?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 88.88% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3314?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3314 +/- ##; =======================================; Coverage 76.94% 76.95% ; =======================================; Files 109 109 ; Lines 12462 12467 +5 ; =======================================; + Hits 9589 9594 +5 ; Misses 2873 2873 ; ```. | [Files with missing lines](https://app.codeco,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3314#issuecomment-2434892685:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3314#issuecomment-2434892685,1,['Patch'],['Patch']
Deployability,"h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17005,Install,Installing,17005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:223,continuous,continuous,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544,1,['continuous'],['continuous']
Deployability,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739#issuecomment-513181427:62,update,update,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513181427,1,['update'],['update']
Deployability,"handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3171,install,install,3171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,['install'],['install']
Deployability,"hat each of those things mean; 3 resolution=0.9,; 4 random_state=0,; 5 flavor=""igraph"", #did pip install leidenalg and started receiving the no flavor keyword error; https://github.com/scverse/scanpy/issues/350 indicates this is to be expected, but https://scanpy.readthedocs.io/en/latest/generated/scanpy.tl.leiden.html indicates it should have it ; 6 n_iterations=2,; 7 directed=False,; 8 ). File ~\miniconda3\Lib\site-packages\scanpy\tools\_leiden.py:144, in leiden(adata, resolution, restrict_to, random_state, key_added, adjacency, directed, use_weights, n_iterations, partition_type, neighbors_key, obsp, copy, **partition_kwargs); 142 msg = 'In the future, the default backend for leiden will be igraph instead of leidenalg.\n\n To achieve the future defaults please pass: flavor=""igraph"" and n_iterations=2. directed must also be False to work with igraph\'s implementation.'; 143 _utils.warn_once(msg, FutureWarning, stacklevel=3); --> 144 except ImportError:; 145 raise ImportError(; 146 ""Please install the leiden algorithm: `conda install -c conda-forge leidenalg` or `pip3 install leidenalg`.""; 147 ); 148 clustering_args = dict(clustering_args). File ~\miniconda3\Lib\site-packages\leidenalg\functions.py:81, in find_partition(graph, partition_type, initial_membership, weights, n_iterations, max_comm_size, seed, **kwargs); 79 if not weights is None:; 80 kwargs['weights'] = weights; ---> 81 partition = partition_type(graph,; 82 initial_membership=initial_membership,; 83 **kwargs); 84 optimiser = Optimiser(); 86 optimiser.max_comm_size = max_comm_size. TypeError: RBConfigurationVertexPartition.__init__() got an unexpected keyword argument 'flavor'; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.9.8; -----; PIL 10.2.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; brotli 1.0.9; certifi 2023.11.17; cffi 1.16.0; charset_normalizer 2.0.4; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; debugp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2981:1963,install,install,1963,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2981,3,['install'],['install']
Deployability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Integrat,Integration,966,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,3,"['Integrat', 'integrat']","['Integration', 'integrate', 'integration']"
Deployability,"he latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Installation using pip & installation from github repository. ### Minimal code sample. ```python; pip install scanpy; ```. ### Error output. ```pytb; Installing collected packages: tbb, distlib, asciitree, stdlib-list, setuptools-scm, pbr, numcodecs, nodeenv, natsort, igraph, identify, filelock, fasteners, docutils, cfgv, array-api-compat, accessible-pygments, zarr, virtualenv, sphinx, session-info, pytest-nunit, pytest-mock, profimp, mdit-py-plugins, leidenalg, sphinxext-opengraph, sphinx-design, sphinx-copybutton, sphinx-autodoc-typehints, scanpydoc, pynndescent, pydata-sphinx-theme, pre-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:1105,install,installed,1105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['install'],['installed']
Deployability,"heck_bool_indexer(self.index, key); 910 ; --> 911 return self._get_with(key); 912 ; 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 return self.loc[key]; 952 ; --> 953 return self.reindex(key); 954 except Exception:; 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs); 3732 @Appender(generic.NDFrame.reindex.__doc__); 3733 def reindex(self, index=None, **kwargs):; -> 3734 return super(Series, self).reindex(index=index, **kwargs); 3735 ; 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4354 # perform the reindex on the axes; 4355 return self._reindex_axes(axes, level, limit, tolerance, method,; -> 4356 fill_value, copy).__finalize__(self); 4357 ; 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4367 ax = self._get_axis(a); 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,; -> 4369 tolerance=tolerance, method=method); 4370 ; 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 else:; 502 if not target.is_unique:; --> 503 raise ValueError(""cannot reindex with a non-unique indexer""); 504 ; 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer; ```. These the [packages](https://gist.github.com/helios/a8c2f0f74cb9cc26097a0cdf1aed08e9) I have installed for this analysis both conda and pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264:2852,install,installed,2852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264,1,['install'],['installed']
Deployability,"hecked that this issue has not already been reported.; - [ Yes] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(; ncase,; n_top_genes=3000,; # subset=True, # to automatically subset to the 4000 genes; layer=""counts"",; flavor=""seurat""; ); ```. ```pytb; ValueError: cannot specify integer `bins` when input data contains infinity; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:1033,install,installs,1033,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['install'],['installs']
Deployability,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1566:92,install,install,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566,1,['install'],['install']
Deployability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['integrat'],['integration']
Deployability,hi @dm8000 did you install it with pip or conda? I would try to re installl with either of the two methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210554228:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2306#issuecomment-1210554228,2,['install'],"['install', 'installl']"
Deployability,"hi @ktpolanski ,thank you for the heads up. > A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument spaceranger_image_path to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. > The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location. @LucaMarconato do we have any datasets that test for spaceranger 3.0.1 ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2332223204:48,hotfix,hotfix,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2332223204,1,['hotfix'],['hotfix']
Deployability,"hi @pacificoceanmist , which scanpy version are you using, and could you update to latest version ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305#issuecomment-1210560162:73,update,update,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305#issuecomment-1210560162,1,['update'],['update']
Deployability,"hi @yotamcons ,. thanks a lot for the feedback, we'd really appreciate if you could submit a PR fixing these parts of the documentations that needs to be updated. Happy to support if you need any help,. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561:154,update,updated,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2301#issuecomment-1210561561,1,['update'],['updated']
Deployability,"hi, ; I installed the scanpy-master , but when I type `scanpy --help` in bash the error occurred. I noticed that diffrank was replaced by rank_genes_groups, but I don't know how to fix it. ; ```; Traceback (most recent call last):; File ""/public/bioapps/ana/anaconda3/envs/python35/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==0+unknown', 'console_scripts', 'scanpy')(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 278, in main; init_main_parser().print_help(); File ""/public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/scanpy-0+unknown-py3.5-linux-x86_64.egg/scanpy/__main__.py"", line 117, in init_main_parser; descr = 78*'-' + '\n' + getattr(tools, key).__doc__; AttributeError: module 'scanpy.api.tools' has no attribute 'diffrank'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30:8,install,installed,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30,1,['install'],['installed']
Deployability,"hi, sorry for the confusion. i removed the comments not relevant to the issue. for the record:. - continuum inc. is responsible for providing packages installable with `conda`, we can’t influence that IIRC. they will probably eventually include scanpy.; - python comes with `pip` by default, and `pip` can be used to install all packages on [the python package index](https://pypi.python.org); - the only advantage of anaconda is that it comes with preinstalled packages, but installing scanpy via `pip` will always be just as fast as it will eventually be with `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798:151,install,installable,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/29#issuecomment-321782798,3,['install'],"['install', 'installable', 'installing']"
Deployability,highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24030,pipeline,pipeline,24030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,hly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:26231,pipeline,pipeline,26231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"hon-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyparsing 3.0.7; pytz 2022.1; scipy 1.8.0; session_info 1.0.0; sitecustomize NA; six 1.16.0; sklearn 1.1.2; threadpoolctl 3.1.0; -----; Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]; Windows-10-10.0.22000-SP0; -----; Session information updated at 2022-10-26 15:35. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:4832,update,updated,4832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['update'],['updated']
Deployability,"hon37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError(""minvalue must be positive""); 1182 ; 1183 def __call__(self, value, clip=None):; ValueError: minvalue must be positive",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3599,patch,patch,3599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['patch'],['patch']
Deployability,"how = False); ```; The empty subplot axes are plotted first: ; ![image](https://user-images.githubusercontent.com/57301867/198739563-e1e2faea-b2af-43d5-bd38-2fee9aca57ae.png); then each sc.pl.rank_genes_groups is plotted correctly, but as a separate figure. The documentation states that `ax` only works if you are plotting a single component, so I think it should work in this case, but it's also possible I'm doing something incorrectly. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 1.0.0; google NA; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.9.11; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.56.2; numexpr 2.8.3; numpy 1.23.3; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.9.1; seaborn 0.12.0; session_info 1.0.0; setuptools 63.4.1; six 1.16.0; sklearn 1.1.2; stack_data 0.5.0; statsmodels 0.13.2; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.1; traitlets 5.3.0; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 23.2.1; -----; IPython 8.5.0; jupyter_client 7.3.5; jupyter_core 4.11.1; jupyterlab 3.4.6; notebook 6.4.12; -----; Python 3.8.0 (default, Nov 6 2019, 21:49:08) [GCC 7.3.0]; Linux-4.15.0-192-generic-x86_64-with-glibc2.10; -----; Session information updated at 2022-10-28 15:05. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2366:2993,update,updated,2993,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2366,1,['update'],['updated']
Deployability,"how can it been installed if linux env dose not have a GUI and Tkinner, and I only have a user account",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/595:16,install,installed,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595,1,['install'],['installed']
Deployability,https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/pyproject.toml#L57. Colab has 3.2.2 installed and installing scanpy requires restarting the runtime. I also can't find a great reason why 3.4 was chosen. https://github.com/scverse/scanpy/pull/2212/. based on this 3.2.2 should be fine?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2413:116,install,installed,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413,2,['install'],"['installed', 'installing']"
Deployability,"https://github.com/scverse/scanpy/blob/master/.azure-pipelines.yml#L14. https://learn.microsoft.com/en-us/answers/questions/1181262/what-happens-to-azure-vms-with-ubuntu-18-04-lts-af. and our CI already cries about it. Let's up this to a more recent version, maybe even 22.04?. While we're at it, I'd also remove 3.7 from the CI because it'll lose support very soon. We could add one of the more recent Python versions instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2446:53,pipeline,pipelines,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2446,1,['pipeline'],['pipelines']
Deployability,"https://github.com/theislab/scanpy/blob/0fde3f6cf93806bb1be1ac5fa4449da8a670fcf3/scanpy/tests/test_plotting.py#L1504. should it be moved in tests externals? if not, then `flit install -s --deps=develop` doesn't install scrublet whereas it should imho otherwise test fails for fail import",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2158:176,install,install,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2158,2,['install'],['install']
Deployability,"https://github.com/theislab/scanpy/blob/63b42e4bff46a9e1386abfede4adeef3a8100c7b/pyproject.toml#L65. > The sinfo package has changed name and is now called session_info to become more discoverable and self-explanatory. The sinfo PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install session_info instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1996:284,install,installs,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1996,2,['install'],"['install', 'installs']"
Deployability,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,integrat,integrating-data-using-ingest,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561,1,['integrat'],['integrating-data-using-ingest']
Deployability,"i 2023.11.17; cffi 1.16.0; charset_normalizer 2.0.4; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.10.0; idna 3.4; igraph 0.11.4; ipykernel 6.29.3; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; json5 0.9.22; jsonpointer 2.1; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.9.1; jupyter_server 2.13.0; jupyterlab_server 2.25.4; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.8.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.2; numba 0.59.0; numpy 1.26.4; overrides NA; packaging 23.1; pandas 2.2.1; parso 0.8.3; patsy 0.5.6; platformdirs 3.10.0; prometheus_client NA; prompt_toolkit 3.0.43; psutil 5.9.8; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pynndescent 0.5.11; pyparsing 3.1.2; pythoncom NA; pythonjsonlogger NA; pytz 2024.1; pywin32_bootstrap NA; pywin32_system32 NA; pywintypes NA; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; ruamel NA; scipy 1.12.0; seaborn 0.13.2; send2trash NA; session_info 1.0.0; six 1.16.0; sklearn 1.4.1.post1; sniffio 1.3.1; socks 1.7.1; stack_data 0.6.3; statsmodels 0.14.1; texttable 1.7.0; threadpoolctl 3.3.0; tornado 6.4; tqdm 4.65.0; traitlets 5.14.2; umap 0.5.5; uri_template NA; urllib3 1.26.18; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; win32api NA; win32com NA; win32con NA; win32trace NA; winerror NA; yaml 6.0.1; zmq 25.1.2; zstandard 0.19.0; -----; IPython 8.22.2; jupyter_client 8.6.1; jupyter_core 5.7.2; jupyterlab 4.1.4; notebook 7.1.1; -----; Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.22631-SP0; -----; Session information updated at 2024-04-05 10:44; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2981:4771,update,updated,4771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2981,1,['update'],['updated']
Deployability,"i encountered this error when using a new conda env in pycharm after install scannpy in cmd line according to scannpy manual. . I don;t know why but I didn't experience the error any longer if I set up new conda env and install scannpy in cmd line, and call spyder to run the same codes to import python packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-966679910:69,install,install,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-966679910,2,['install'],['install']
Deployability,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:417,update,update,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015,1,['update'],['update']
Deployability,"i! I am wondering why `loompy` and `pybiomart` are not in the list of dependencies. As of now ; the package (at least in bioconda) is not fully functional and requieres some extra installation; steps if one wants to use certain functionalities provided. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### Minimal code sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:1037,install,installed,1037,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['install'],['installed']
Deployability,"ib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1805,update,updated,1805,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['update'],['updated']
Deployability,"ib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 306, in calculate_qc_metrics; obs_metrics = describe_obs(; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 401, in top_segment_proportions; return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(ns)); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/numba/core/dispatcher.py"", line 468, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/numba/core/dispatcher.py"", line 409, in error_rewrite; raise e.with_traceback(None); numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<built-in function iadd>) found for signature:; ; >>> iadd(array(bool, 1d, C), array(int64, 1d, C)); ; There are 18 candidate implementations:; - Of which 14 did not match due to:; Overload of function 'iadd': File: <numerous>: Line N/A.; With argument(s): '(array(bool, 1d, C), array(int64, 1d, C))':; No match.; - Of which 2 did not match due to:; Overload in function 'NumpyRulesInplaceArrayOperator.generic': File: numba/core/typing/npydecl.py: Line 243.; With argument(s): '(array(bool, 1d, C), array(int64, 1d, C))':; Rejected as the implementation raised a specific error:; AttributeError: 'NoneType' object has no attribute 'args'; raised from /home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/numba/core/typing/npydecl.py:254; - Of which 2 did not match due to:; Operator Overload in function 'iadd': File: unknown: Line unknown.; With argument(s): '(array(bool, 1d, C), array(int64, 1d, C))':; No match for registered cases:; * (int64, int64) -> int64; * (int64, uint64) -> int64; * (uint64, int64) -> int64; * (uint64, uint64) -> uint64; * (flo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2758:2388,pipeline,pipeline,2388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2758,1,['pipeline'],['pipeline']
Deployability,iduals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILE,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28683,pipeline,pipeline,28683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,iduals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21998,pipeline,pipeline,21998,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ighly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_hi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24692,pipeline,pipeline,24692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4227,install,install,4227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['install']
Deployability,iled in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28003,pipeline,pipeline,28003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6599,Pipeline,PipelineDevelope,6599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>); During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:; def array_sort_impl(arr):; <source elided>; # Note we clobber the return value; sort_func(arr); ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017); D",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8973,pipeline,pipeline,8973,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['pipeline'],['pipeline']
Deployability,"ine 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 20.0.2 . ubuntu; 20.04. pip list; Package Version Location ; ------------------------- -------------------- -----------------------------; analysaurus 0.0.1 /home/ubuntu/code/analysaurus; anndata 0.7.5 ; ansi2html 1.5.2 ; appdirs 1.4.4 ; argon2-cffi 20.1.0 ; astroid 2.4.2 ; async-generator 1.10 ; attrs 20.3.0 ; autopep8 1.5.4 ; backcall 0.2.0 ; biopython 1.78 ; black 20.8b1 ; bleach 3.2.1 ; bokeh 2.2.3 ; botocore 1.19.18 ; Brotli 1.0.9 ; cellforest 0.0.2 /home/ubuntu/code/cellforest ; certifi 2019.11.28 ; cffi 1.14.3 ; chardet 3.0.4 ; click 7.1.2 ; colorama 0.4.4 ; commonmark 0.9.1 ; cycler 0.10.0 ; dash 1.17.0 ; dash-building-blocks 0.1.2 ; dash-core-components 1.13.0 ; dash-html-components 1.1.1 ; dash-renderer 1.8.3 ; dash-table 4.11.0 ; dataclasses 0.6 ; dataforest 0.0.2 /home/ubuntu/code/dataforest ; dbus-python 1.2.16 ; decorator 4.4.2 ; defusedxml 0.6.0 ; distr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:3115,install,installation,3115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['install'],['installation']
Deployability,"ing up 98% of my CPU while running `filter_rank_genes_groups`. #### Versions. <details>. `scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.19.1 scipy==1.5.2 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1`. and. ```; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; Bio 1.77; PIL 7.2.0; adjustText NA; anndata 0.7.4; annoy NA; backcall 0.2.0; bbknn NA; brotli NA; cachecontrol 0.12.6; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.1; changeo 1.0.0; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dandelion 0.0.15; dateutil 2.8.0; decorator 4.4.2; descartes NA; distance NA; get_version 2.1; h5py 2.10.0; hdmedians NA; idna 2.10; igraph 0.8.2; importlib_metadata 1.7.0; ipykernel 5.3.3; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; markupsafe 1.1.1; matplotlib 3.3.0; mizani 0.7.1; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; networkx 2.4; numba 0.50.1; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; palettable 3.3.0; pandas 1.0.5; parso 0.7.1; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotnine 0.6.0; polyleven NA; presto 0.6.1; prompt_toolkit 3.0.6; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pyparsing 2.4.7; pytz 2019.2; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; scrublet NA; seaborn 0.10.1; setuptools_scm NA; sinfo 0.3.1; six 1.12.0; skbio 0.5.6; sklearn 0.23.1; socks 1.7.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; texttable 1.6.2; tools NA; tornado 6.0.4; tqdm 4.48.0; traitlets 4.3.3; tzlocal NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; yaml 5.1.2; zipp NA; zmq 19.0.1; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; -----; Python 3.7.6 | packaged by conda-forge | (default, Jun 1 2020, 18:57:50) [GCC 7.5.0]; Linux-4.4.0-189-generic-x86_64-with-debian-buster-sid; 24 logical CPU cores, x86_64; -----; Session information updated at 2020-10-08 16:18; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1449:3341,update,updated,3341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1449,1,['update'],['updated']
Deployability,"ing up your package via pip, as instructed. This crashed out very quickly:. 	mib111492i:~ kp9$ pip install scanpy; 	Collecting scanpy; 	 Downloading scanpy-0.2.9.1.tar.gz (208kB); 		100% |################################| 215kB 2.8MB/s ; 		Complete output from command python setup.py egg_info:; 		Traceback (most recent call last):; 		 File ""<string>"", line 1, in <module>; 		 File ""/private/tmp/pip-build-cx2i4lbu/scanpy/setup.py"", line 39, in <module>; 			readme = readme_f.read(); 		 File ""/Users/kp9/anaconda3/lib/python3.6/encodings/ascii.py"", line 26, in decode; 			return codecs.ascii_decode(input, self.errors)[0]; 		UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 296: ordinal not in range(128); 	; 		----------------------------------------; 	Command ""python setup.py egg_info"" failed with error code 1 in /private/tmp/pip-build-cx2i4lbu/scanpy/. The offender seems to be the stylised README file, so I downloaded the source code, got rid of it, and proceeded with the installation. I'm unsure how representative the following encountered issues are of an ideal pip installation, but I figured I'd bring them to your attention anyway just in case they're relevant:. - h5py crashed out on account of not having hdf5 available. This was remedied via `brew install hdf5`, and it seems like the most likely of these issues to affect other users.; - The installer ignored my 2.1.0 setup of matplotlib, tried to install 2.0.0 in some weird way and crashed out. Installing 2.0.0 via pip (absolutely painlessly, mind you - what was that weird installer that crashed the thing out?) allowed the setup to proceed past this point. This might be an isolated incident, but it seemed weird enough to alert you of just in case.; - As is, louvain crashes immediately and uninformatively when attempts are made to pip it in. Installing from the GitHub source code still works fine though. I'll notify the louvain team of this situation, but this may be of relevance to you too. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/49:1121,install,installation,1121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/49,7,"['Install', 'install']","['Installing', 'install', 'installation', 'installer']"
Deployability,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1214,install,install,1214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"iniconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 17, in <module>; from ._sim import sim. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/tools/_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/readwrite.py"", line 10, in <module>; import tables. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/__init__.py"", line 62, in <module>; from .file import File, open_file, copy_file. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/file.py"", line 33, in <module>; from . import hdf5extension. ImportError: /home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/hdf5extension.cpython-37m-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_direct; ```; Removing anndata2ri with `conda remove anndata2ri` fixes the issue. Installing with `pip install anndata2ri` prevents the issue. #### Versions; After fixing the issue.; <details>; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 9.0.1; PyQt5 NA; anndata 0.7.8; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.10.0; fsspec 2022.01.0; get_version 3.5.4; h5py 2.10.0; igraph 0.9.9; ipykernel 6.4.1; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; leidenalg 0.8.9; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; packaging 21.3; pandas 1.3.4; parso 0.8.3; pexpect 4.8.0; picklesha",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172:1743,Install,Installing,1743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"iniconda3/envs/py48/lib/contextlib.py?line=131) except StopIteration as exc:; [133](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=132) # Suppress StopIteration *unless* it's the same exception that; [134](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=133) # was passed to throw(). This prevents a StopIteration; [135](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=134) # raised inside the ""with"" statement from being suppressed.; [136](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/contextlib.py?line=135) return exc is not value. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\errors.py:837, in new_error_context(fmt_, *args, **kwargs); [835](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=834) else:; [836](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=835) tb = None; --> [837](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=836) raise newerr.with_traceback(tb); [838](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=837) elif use_new_style_errors():; [839](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/errors.py?line=838) raise e. LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x00000242239BD700> (trying to write member #1). File ""D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py (53); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:33208,pipeline,pipeline,33208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:0,install,install,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692,7,['install'],"['install', 'installing']"
Deployability,install of scanpy dredges up anndata's scipy issue,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:0,install,install,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,1,['install'],['install']
Deployability,integrate with CCA and pyscenic,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265:0,integrat,integrate,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265,1,['integrat'],['integrate']
Deployability,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself 😃), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1122,install,installation,1122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,1,['install'],['installation']
Deployability,"io.microsoft.com/downloads/; ----------------------------------------; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Running setup.py install for fa2 ... error. ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:3669,install,install-headers,3669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,3,"['Install', 'install']","['Installing', 'install-', 'install-headers']"
Deployability,io/gh/scverse/scanpy/pull/3084?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `62.50000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 75.85%. Comparing base [(`5dc489d`)](https://app.codecov.io/gh/scverse/scanpy/commit/5dc489d5c71fa91fd0cabe6adb363172119ce5eb?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`6ba3676`)](https://app.codecov.io/gh/scverse/scanpy/commit/6ba36767cbb6fe59524c1ac54347b3a36de41a28?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 46 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3084?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/external/exporting.py](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&filepath=scanpy%2Fexternal%2Fexporting.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2V4dGVybmFsL2V4cG9ydGluZy5weQ==) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/preprocessing/\_combat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_combat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2NvbWJhdC5weQ==) | 0.00% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3084?src=pr&el=tree&utm_medium=referral&utm_s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3084#issuecomment-2141838152:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3084#issuecomment-2141838152,1,['Patch'],['Patch']
Deployability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9850,pipeline,pipeline,9850,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,ionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_pl,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:44347,pipeline,pipeline,44347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.black_tup-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding.py::test_umap_init_paga[fr] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' d,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:11435,pipeline,pipeline,11435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing byteco,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:32744,pipeline,pipeline,32744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:622) if chunksize or iterator:; [623](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:623) return parser. File ~\AppData\Roaming\Python\Pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:10727,Pipeline,PipelineDevelope,10727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3258,install,install,3258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,2,['install'],['install']
Deployability,"is in a `results/` directory, with each step having its own subdirectory. Currently, this doesn't seem possible. If I set `sc.settings.figdir = ./results`, then try to save to, say ""qcmetrics/total_counts.png"", the code fails as ""violinqcmetrics/total_counts.png"" does not exist. A partial work around is to set `sc.settings.figdir` every time I want to save in a different directory, but this feels quite clunky to me. And it will still result in all my files being prepended with ""violin"" (or ""scatter"", etc.), which I'd like to avoid. Is there any way to disable this prefix on save behaviour?. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.settings.figdir = ""./""; sc.pl.violin(adata, [""total_counts""], save=""results/test_plot.png""); ```. ```pytb; FileNotFoundError: [Errno 2] No such file or directory: 'violinresults/test_plot.png'; ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; IPython 7.19.0; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; flufl NA; get_version 2.1; h5py 3.1.0; igraph 0.8.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0rc3; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; packaging 20.4; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.6.0; pygments 2.7.2; pyparsing 2.4.7; pytz 2020.4; ruamel NA; scanpy 1.6.0; scipy 1.5.4; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.3; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zc NA; -----; Python 3.8.4 (default, Jul 16 2020, 19:35:12) [GCC 9.3.0]; Linux-5.4.0-54-generic-x86_64-with-glibc2.29; 8 logical CPU cores, x86_64; -----; Session information updated at 2020-11-25 11:03. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1508:2542,update,updated,2542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508,1,['update'],['updated']
Deployability,"ite-packages/matplotlib/collections.py:1198, in PolyCollection.__init__(self, verts, sizes, closed, **kwargs); 1178 def __init__(self, verts, sizes=None, *, closed=True, **kwargs):; 1179 """"""; 1180 Parameters; 1181 ----------; (...); 1196 Forwarded to `.Collection`.; 1197 """"""; -> 1198 super().__init__(**kwargs); 1199 self.set_sizes(sizes); 1200 self.set_verts(verts, closed). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/collections.py:206, in Collection.__init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, offset_transform, norm, cmap, pickradius, hatch, urls, zorder, **kwargs); 203 self._offset_transform = offset_transform; 205 self._path_effects = None; --> 206 self._internal_update(kwargs); 207 self._paths = None. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/artist.py:1216, in Artist._internal_update(self, kwargs); 1209 def _internal_update(self, kwargs):; 1210 """"""; 1211 Update artist properties without prenormalizing them, but generating; 1212 errors as if calling `set`.; 1213 ; 1214 The lack of prenormalization is to maintain backcompatibility.; 1215 """"""; -> 1216 return self._update_props(; 1217 kwargs, ""{cls.__name__}.set() got an unexpected keyword argument ""; 1218 ""{prop_name!r}""). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/artist.py:1190, in Artist._update_props(self, props, errfmt); 1188 func = getattr(self, f""set_{k}"", None); 1189 if not callable(func):; -> 1190 raise AttributeError(; 1191 errfmt.format(cls=type(self), prop_name=k)); 1192 ret.append(func(v)); 1193 if ret:. AttributeError: PolyCollection.set() got an unexpected keyword argument 'rotation'; ```. ### Versions. <details>. ```; numpy 1.26.4; pandas 2.2.2; scanpy 1.10.2; session_info 1.0.0; -----. PIL 10.3.0; anndata 0.10.8; anyio NA; arrow 1.3.0; asciitree NA; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.15.0; certifi 2024.06.02; cffi 1.16.0; charset_norma",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:7717,Update,Update,7717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,1,['Update'],['Update']
Deployability,"ite; cwd: /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/; Complete output (29 lines):; running install; running build; got version from file /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/llvmlite/_version.py {'version': '0.34.0', 'full': 'c5889c9e98c6b19d5d85ebdd982d64a03931f8e2'}; running build_ext; /usr/bin/python /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py; LLVM version... Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 105, in main_posix; out = subprocess.check_output([llvm_config, '--version']); File ""/usr/lib/python3.10/subprocess.py"", line 420, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; File ""/usr/lib/python3.10/subprocess.py"", line 501, in run; with Popen(*popenargs, **kwargs) as process:; File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__; self._execute_child(args, executable, preexec_fn, close_fds,; File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 107, in main_posix; raise RuntimeError(""%s failed executing, please point LLVM_CONFIG ""; RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config; error: command '/usr/bin/python' failed with exit code 1; ----------------------------------------; ```. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:2369,install,install-,2369,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,3,['install'],['install-']
Deployability,"ith this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1391,Install,Installing,1391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['Install'],['Installing']
Deployability,"ither provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in adata.var.keys() or y in adata.obs.index); 170 and (color is None or color in adata.var.keys() or color in adata.obs.index); 171 ):; 172 adata_T = adata.T. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:521, in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 517 legend = axs[ikey].legend(; 518 frameon=False, loc=legend_loc, fontsize=legend_fontsize; 519 ); 520 if legend is not None:; --> 521 for handle in legend.legendHandles:; 522 handle.set_sizes([300.0]); 524 # draw a frame around the scatter. AttributeError: 'Legend' object has no attribute 'legendHandles'; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; cffi 1.16.0; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; defusedxml 0.7.1; dill 0.3.8; h5py 3.11.0; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; llvmlite 0.42.0; matplotlib 3.9.0; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numexpr 2.10.0; numpy 1.26.4; packaging 24.0; pandas 2.2.2; psutil 5.9.8; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; six 1.16.0; sklearn 1.5.0; threadpoolctl 3.5.0; torch 2.3.1+cu121; torchgen NA; tqdm 4.66.4; typing_extensions NA; yaml 6.0.1; -----; Python 3.11.7 (main, Jan 17 2024, 16:00:28) [GCC 8.5.0]; Linux-3.10.0-1160.114.2.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-06-06 11:18; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:3837,update,updated,3837,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['update'],['updated']
Deployability,"joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; lxml 4.6.3; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 21.0; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.9.0; pylab NA; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; requests_cache 0.6.4; scipy 1.6.2; seaborn 0.11.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; soupsieve 2.2.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; url_normalize 1.4.3; urllib3 1.26.6; wcwidth 0.2.5; webencodings 0.5.1; wrapt 1.12.1; yaml 5.4.1; zmq 20.0.0; zope NA; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.4.0; -----; Python 3.8.11 (default, Aug 3 2021, 05:10:14) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-08-16 12:24",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:5721,update,updated,5721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['update'],['updated']
Deployability,"just mention this PR with brief description and your name here: https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst. nice! only one PR to go, thank you @hspitzer @ivirshup !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015:116,release,release-latest,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015,1,['release'],['release-latest']
Deployability,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:14,install,install,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898,5,['install'],['install']
Deployability,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420:35,release,release,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420,1,['release'],['release']
Deployability,"kages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:2603,install,installed,2603,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['install'],['installed']
Deployability,"kwds); 577 edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist]); 578 ; --> 579 if not cb.iterable(width):; 580 lw = (width,); 581 else:. AttributeError: module 'matplotlib.cbook' has no attribute 'iterable'; ```; ![Screenshot 2020-10-16 at 16 10 20](https://user-images.githubusercontent.com/32264060/96275744-ff3d9080-0fc9-11eb-8706-d398e3b08c79.png). #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; atomicwrites 1.3.0; attr 20.2.0; backcall 0.2.0; cffi 1.14.3; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; defusedxml 0.6.0; future_fstrings NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; importlib_metadata 0.23; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.14.0; kiwisolver 1.1.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.6.1+2.g8073db7; matplotlib 3.3.2; more_itertools NA; mpl_toolkits NA; natsort 6.0.0; networkx 2.3; numba 0.51.2; numexpr 2.7.0; numpy 1.19.1; packaging 20.4; pandas 1.1.2; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.0; prompt_toolkit 3.0.7; psutil 5.7.2; ptyprocess 0.6.0; py 1.8.0; pycparser 2.20; pygments 2.7.1; pyparsing 2.4.2; pytest 5.2.1; pytz 2019.2; scanpy 1.6.0; scipy 1.5.2; setuptools_scm NA; simplejson 3.17.2; sinfo 0.3.1; six 1.15.0; sklearn 0.21.3; sphinxcontrib NA; storemagic NA; tables 3.5.2; texttable 1.6.3; tornado 6.0.4; tqdm 4.36.1; traitlets 4.3.3; typing_extensions NA; umap 0.3.10; wcwidth 0.2.5; xlrd 1.2.0; yaml 5.1.2; zipp NA; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) [GCC 7.5.0]; Linux-4.4.0-190-generic-x86_64-with-debian-buster-sid; 24 logical CPU cores, x86_64; -----; Session information updated at 2020-10-16 15:10. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1459:3837,update,updated,3837,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1459,1,['update'],['updated']
Deployability,"l 0.12.10; certifi 2022.12.07; cffi 1.15.0; charset_normalizer 2.0.11; colorama 0.4.4; comm 0.1.1; compositional 2022.8.31; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.4; decorator 5.1.1; defusedxml 0.7.1; ensemble_networkx 2023.1.23; entrypoints 0.4; ete3 3.1.2; executing 0.8.2; fastcluster 1.1.26; fontTools 4.29.1; h5py 3.7.0; hdmedians NA; hive_networkx 2021.05.18; hypergeom_ufunc NA; idna 3.3; igraph 0.10.2; ipykernel 6.19.4; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.23.4; kiwisolver 1.3.2; leidenalg 0.9.1; llvmlite 0.38.0; lxml 4.7.1; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; matplotlib_venn 0.11.6; mpl_toolkits NA; msgpack 1.0.3; natsort 8.1.0; nbinom_ufunc NA; networkx 2.6.3; numba 0.55.1; numpy 1.21.5; packaging 21.3; palettable 3.3.0; pandas 1.4.0; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.3.0; prompt_toolkit 3.0.26; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.1; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2021.3; pytz_deprecation_shim NA; requests 2.27.1; rpy2 3.5.6; scipy 1.8.0; seaborn 0.11.2; session_info 1.0.0; setuptools 60.7.1; sip NA; six 1.16.0; skbio 0.5.6; sklearn 1.0.2; socks 1.7.1; soothsayer 2022.8.31; soothsayer_utils 2022.6.24; stack_data 0.1.4; statsmodels 0.13.1; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.62.3; traitlets 5.1.1; typing_extensions NA; tzlocal NA; umap 0.5.2; unicodedata2 NA; urllib3 1.26.8; wcwidth 0.2.5; xarray 2023.1.0; zmq 24.0.1; zoneinfo NA; -----; IPython 8.0.1; jupyter_client 7.3.4; jupyter_core 5.0.0; jupyterlab 3.5.2; notebook 6.5.2; -----; Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37) [Clang 14.0.6 ]; macOS-12.6-x86_64-i386-64bit; -----; Session information updated at 2023-04-27 15:56; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2477:3119,update,updated,3119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2477,1,['update'],['updated']
Deployability,"l 0.2.0; brotli NA; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; click 8.1.3; cloudpickle 2.2.1; colorama 0.4.6; colorful 0.5.5; colorful_orig 0.5.5; comm 0.1.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.5.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; distributed 2023.5.0; entrypoints 0.4; executing 1.2.0; fasteners 0.17.3; filelock 3.12.0; fsspec 2023.5.0; google NA; grpc 1.43.0; h5py 3.8.0; idna 3.4; igraph 0.10.4; ipykernel 6.23.1; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jsonschema 4.17.3; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; locket NA; lz4 4.3.2; markupsafe 2.1.2; matplotlib 3.6.3; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.5; natsort 8.3.1; numba 0.57.0; numcodecs 0.11.0; numpy 1.24.3; packaging 23.1; pandas 2.0.1; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.1; prometheus_client NA; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pyarrow 9.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pynvml NA; pyparsing 3.0.9; pyrsistent NA; pytz 2023.3; ray 2.3.0; rb_analysis NA; requests 2.29.0; scipy 1.10.1; seaborn 0.12.2; session_info 1.0.0; setproctitle 1.2.2; setuptools 67.7.2; six 1.16.0; sklearn 1.2.2; socks 1.7.1; sortedcontainers 2.4.0; stack_data 0.6.2; statsmodels 0.14.0; tblib 1.7.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; urllib3 1.26.15; wcwidth 0.2.6; yaml 6.0; zarr 2.14.2; zict 3.0.0; zipp NA; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.2; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]; macOS-13.3.1-arm64-arm-64bit; -----; Session information updated at 2023-05-18 14:00. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:6605,update,updated,6605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['update'],['updated']
Deployability,"l scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copying scanpy/tools/draw_graph.py -> build/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148:1014,install,install,1014,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148,1,['install'],['install']
Deployability,"l) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(; ncase,; n_top_genes=3000,; # subset=True, # to automatically subset to the 4000 genes; layer=""counts"",; flavor=""seurat""; ); ```. ```pytb; ValueError: cannot specify integer `bins` when input data contains infinity; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:1170,install,install,1170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['install'],['install']
Deployability,"l/venv/xomx/lib/python3.9/site-packages/scanpy/neighbors/__init__.py"", line 791, in compute_neighbors; knn_indices, knn_distances, forest = compute_neighbors_umap(; File ""/home/paul/venv/xomx/lib/python3.9/site-packages/scanpy/neighbors/__init__.py"", line 305, in compute_neighbors_umap; knn_indices, knn_dists, forest = nearest_neighbors(; File ""/home/paul/venv/xomx/lib/python3.9/site-packages/umap/umap_.py"", line 328, in nearest_neighbors; knn_search_index = NNDescent(; File ""/home/paul/venv/xomx/lib/python3.9/site-packages/pynndescent/pynndescent_.py"", line 875, in __init__; self._neighbor_graph = nn_descent(; File ""/home/paul/venv/xomx/lib/python3.9/site-packages/numba/core/dispatcher.py"", line 468, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/paul/venv/xomx/lib/python3.9/site-packages/numba/core/dispatcher.py"", line 409, in error_rewrite; raise e.with_traceback(None); numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); non-precise type pyobject; During: typing of argument at /home/paul/venv/xomx/lib/python3.9/site-packages/pynndescent/pynndescent_.py (330). File ""../pynndescent/pynndescent_.py"", line 330:; def nn_descent(; <source elided>. if init_graph[0].shape[0] == 1: # EMPTY_GRAPH; ^ . This error may have been caused by the following argument(s):; - argument 4: Cannot determine Numba type of <class 'function'>; ```. ### Minimal code sample to reproduce the error. ```python; import scanpy as sc; import numpy as np. def custom_distance(x1, x2):; return dist_mat[int(x1), int(x2)]. n = 4096. # generate a fake distance matrix for n elements; dist_mat = np.random.rand(n, n); # make it symmetrical; dist_mat= np.tril(dist_mat) + np.tril(dist_mat, -1).T; # zeros on the diagonal; for i in range(len(dist_mat)):; dist_mat[i][i] = 0. xd = sc.AnnData(shape=(n, 1)); xd.obs_names = [i for i in range(n)]; xd.X = np.empty((xd.n_obs, xd.n_vars)); for i in range(xd.n_obs):; xd.X[i, 0] = i; print(""computing connectivity graph",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2139:1695,pipeline,pipeline,1695,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2139,1,['pipeline'],['pipeline']
Deployability,l[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33678,pipeline,pipeline,33678,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,latest install AnnData is not defined,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/416:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/416,1,['install'],['install']
Deployability,"leReader.__init__(self, f, engine, **kwds); [1617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1617) self.options[""has_index_names""] = kwds[""has_index_names""]; [1619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:12792,Pipeline,PipelineDevelope,12792,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"le__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1547,install,install-,1547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,leiden install instructions are wrong,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243,1,['install'],['install']
Deployability,leiden install via conda code is wrong in the current page. it should be:. ```; conda install -c conda-forge leidenalg ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015,2,['install'],['install']
Deployability,leiden install via conda code is wrong in the current page. it should be:. ```; conda install -c conda-forge leidenalg ; ```. _Originally posted by @YubinXie in https://github.com/theislab/scanpy/pull/1216#issuecomment-632506015_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1243:7,install,install,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243,2,['install'],['install']
Deployability,"ler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.1.3; executing 1.2.0; fastjsonschema NA; fqdn NA; gmpy2 2.1.2; h5py 3.10.0; idna 3.4; igraph 0.11.2; importlib_resources NA; ipykernel 6.25.2; isoduration NA; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.17.3; jupyter_events 0.6.3; jupyter_server 2.7.3; jupyterlab_server 2.24.0; kiwisolver 1.4.5; leidenalg 0.10.1; llvmlite 0.40.1; markupsafe 2.1.3; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numpy 1.24.4; opt_einsum v3.3.0; overrides NA; packaging 23.2; pandas 2.1.1; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.11.0; prometheus_client NA; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.16.1; pynndescent 0.5.10; pyparsing 3.1.1; pyrsistent NA; pythonjsonlogger NA; pytz 2023.3; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpy2 3.5.11; scipy 1.11.3; seaborn 0.13.0; send2trash NA; session_info 1.0.0; six 1.16.0; sklearn 1.3.1; sniffio 1.3.0; socks 1.7.1; sparse 0.14.0; stack_data 0.6.2; statsmodels 0.14.0; sympy 1.12; texttable 1.7.0; threadpoolctl 3.2.0; torch 2.0.0; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; typing_extensions NA; tzdata 2023.3; tzlocal NA; umap 0.5.4; uri_template NA; urllib3 2.0.6; wcwidth 0.2.8; webcolors 1.13; websocket 1.6.4; yaml 6.0.1; zipp NA; zmq 25.1.1; zoneinfo NA; -----; IPython 8.16.1; jupyter_client 8.4.0; jupyter_core 5.4.0; jupyterlab 4.0.7; notebook 7.0.5; -----; Python 3.9.18 | packaged by conda-forge | (main, Aug 30 2023, 03:49:32) [GCC 12.3.0]; Linux-3.10.0-1160.95.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2023-10-16 08:14; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685:4909,update,updated,4909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685,1,['update'],['updated']
Deployability,"ler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self); 134 if self.generator_info is None:; 135 self.genlower = None; --> 136 self.lower_normal_function(self.fndesc); 137 else:; 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7735,pipeline,pipeline,7735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['pipeline'],['pipeline']
Deployability,"lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 90 % (state.func_id.func_name,)):; 91 # Type inference; ---> 92 typemap, return_type, calltypes = type_inference_stage(; 93 state.typingctx,; 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors); 68 ; 69 infer.build_constraint(); ---> 70 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/b",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8178,pipeline,pipeline,8178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,5,['pipeline'],['pipeline']
Deployability,"lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1518, in _format_strings; return list(self.get_result_as_array()); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1482, in get_result_as_array; formatted_values = format_values_with(float_format); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1456, in format_values_with; values = format_with_na_rep(values, formatter, na_rep); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1427, in format_with_na_rep; [; File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1428, in <listcomp>; formatter(val) if not m else na_rep; ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.2; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; cffi 1.14.6; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; encodings NA; genericpath NA; get_version 3.5; h5py 3.4.0; joblib 1.0.1; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; llvmlite 0.37.0; matplotlib 3.4.3; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.0; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.3; pkg_resources NA; posixpath NA; pycparser 2.20; pyexpat NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 1.0; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tables 3.6.1; -----; Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) [GCC 9.4.0]; Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.31; 24 logical CPU cores, x86_64; -----; Session information updated at 2021-10-01 14:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008:5263,update,updated,5263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008,1,['update'],['updated']
Deployability,"lib2 NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.11.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; executing 0.8.3; flax 0.6.1; fsspec 2022.11.0; google NA; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.9; ipykernel 6.9.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.24; jaxlib 0.3.24; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; kiwisolver 1.3.2; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.3.2; matplotlib_inline NA; ml_collections NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.1; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2021.3; requests 2.27.1; rich NA; scipy 1.8.0; scrublet NA; scvi 0.19.0; seaborn 0.11.2; session_info 1.0.0; setuptools 60.9.3; setuptools_scm NA; six 1.16.0; sklearn 0.23.2; socks 1.7.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.13.2; tensorboard 2.10.1; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 1.11.0; torchmetrics 0.10.2; torchvision 0.12.0; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; umap 0.5.2; urllib3 1.26.8; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.9; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.4.0-139-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-02-26 19:13; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483:6085,update,updated,6085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318#issuecomment-1445561483,1,['update'],['updated']
Deployability,lite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-server 0.4.0 ; python-language-server 0.36.1 ; pytoml 0.1.21 ; pytz 2020.4 ; PyYAML 5.3.1 ; pyzmq 20.0.0 ; regex 2020.11.13 ; requests 2.22.0 ; requests-unixsocket 0.2.0 ; retrying 1.3.3 ; rich 9.2.0 ; rope 0.18.0 ; scikit-learn 0.23.2 ; scikit-misc 0.1.3 ; scipy 1.5.4 ; scriptedforms 0.10.1 ; scvi-tools 0.7.1 ; seaborn 0.11.0 ; Send2Trash 1.5.0 ; setuptools 50.3.2 ; setuptools-scm 4.1.2 ; sinfo 0.3.1 ; six 1.14.0 ; smmap 3.0.4 ; snowballstemmer 2.0.0 ; SQLAlchemy 1.3.20 ; statsmodels 0.12.1 ; stdlib-list 0.7.0 ; tables 3.6.1 ; termcolor 1.1.0 ; terminado 0.9.1 ; testpath 0.4.4 ; threadpoolctl 2.1.0 ; toml 0.10.2 ; torch 1.7.0 ; tornado 6.1 ; tqdm 4.51.0 ; traitlets 5.0.5 ; typed-ast 1.4.1 ; typeguard 2.10.0 ; typing-extensions 3.7.4.3 ; ujson 4.0.1 ; umap-learn 0.4.6 ; unattended-upgrades 0.1 ; urllib3 1.25.8 ; watchdog 0.10.3 ; wcwidth 0.2.5 ; webencodings 0.5.1 ; Werkzeug 1.0.1 ; wheel 0.35.1 ; widgetsnbextension 3.5.1 ; wrapt 1.12.1 ; xeus-python 0.8.3 ; xlrd 1.2.0 ; yapf 0.30.0 ; zipp 3.4.0. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:6767,upgrade,upgrades,6767,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['upgrade'],['upgrades']
Deployability,"lled from conda-forge scanpy (1.9.3) and leidenalg. Now the plots are reproducible. Installed package versions bellow. My guess now is that there is some unexpected change in the random number generator caused by some other package? Perhaps it has to do something with pytorch 2.x? This one is installed because I have in the same environment scvi-tools. But if we pass the random_state, as umap and leiden methods do, shouldn't that work anyway?. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-03 21:49. </p>; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542:2026,update,updated,2026,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1532980542,1,['update'],['updated']
Deployability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integration']
Deployability,"loat_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:9975,Pipeline,PipelineDevelope,9975,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"log a short version of the sinfo header:. ```; -----; anndata 0.7.4; scanpy 1.5.2.dev106+gd355654f; sinfo 0.3.1; -----; Python 3.8.5 (default, Jul 27 2020, 08:42:51) [GCC 10.1.0]; Linux-5.7.12-arch1-1-x86_64-with-glibc2.2.5; 16 logical CPU cores; -----; Session information updated at 2020-08-16 21:09; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1373:274,update,updated,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1373,1,['update'],['updated']
Deployability,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,integrat,integrating,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182,1,['integrat'],['integrating']
Deployability,"lope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:776) **compression_args,; [777](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:17844,Pipeline,PipelineDevelope,17844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"lor_map)). File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:1214, in _get_palette(adata, values_key, palette); 1212 _utils._set_default_colors_for_categorical_obs(adata, values_key); 1213 else:; -> 1214 _utils._validate_palette(adata, values_key); 1215 return dict(zip(values.categories, adata.uns[color_key])). File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_utils.py:357, in _validate_palette(adata, key); 355 _palette.append(color); 356 # Don't modify if nothing changed; --> 357 if _palette is not None and list(_palette) != list(adata.uns[color_key]):; 358 adata.uns[color_key] = _palette. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.1.0; appnope 0.1.3; asttokens NA; comm 0.1.4; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; executing 2.0.1; h5py 3.10.0; ipykernel 6.26.0; jedi 0.19.1; joblib 1.3.2; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.1; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numpy 1.26.1; packaging 23.2; pandas 2.1.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; platformdirs 3.11.0; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.16.1; pyparsing 3.1.1; pytz 2023.3.post1; scipy 1.11.3; session_info 1.0.0; six 1.16.0; sklearn 1.3.2; stack_data 0.6.2; threadpoolctl 3.2.0; tornado 6.3.3; tqdm 4.66.1; traitlets 5.13.0; typing_extensions NA; wcwidth 0.2.9; zmq 25.1.1; -----; IPython 8.17.2; jupyter_client 8.5.0; jupyter_core 5.5.0; -----; Python 3.11.6 | packaged by conda-forge | (main, Oct 3 2023, 10:40:37) [Clang 15.0.7 ]; macOS-14.0-x86_64-i386-64bit; -----; Session information updated at 2023-11-03 17:57; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2730:5244,update,updated,5244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2730,1,['update'],['updated']
Deployability,"ls; writing scanpy.egg-info/PKG-INFO; writing entry points to scanpy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"",",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:6640,install,install,6640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['install'],['install']
Deployability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:5293,Install,Installing,5293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,4,"['Install', 'install', 'update']","['Installing', 'installation', 'installed', 'updated']"
Deployability,ls_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_hig,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22691,pipeline,pipeline,22691,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1735,install,installs,1735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,2,['install'],['installs']
Deployability,"m of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; anndata2ri 1.1; annoy NA; backcall 0.2.0; backports NA; bbknn NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; entrypoints 0.4; fsspec 2022.11.0; future_fstrings NA; google NA; h5py 3.7.0; igraph 0.9.1; ipykernel 6.14.0; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.7.2; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.3; numpy 1.21.6; packaging 21.3; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.31; psutil 5.9.3; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.5; pytz_deprecation_shim NA; rpy2 3.5.1; scib 1.0.4; scipy 1.7.3; seaborn 0.12.1; session_info 1.0.0; six 1.16.0; sklearn 1.0.2; statsmodels 0.13.2; storemagic NA; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; tornado 6.2; tqdm 4.64.1; traitlets 5.5.0; typing_extensions NA; tzlocal NA; umap 0.5.3; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 24.0.1; zope NA; -----; IPython 7.33.0; jupyter_client 7.4.4; jupyter_core 4.11.1; notebook 6.5.1; -----; Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]; Linux-5.4.0-131-generic-x86_64-with-debian-buster-sid; -----; Session information updated at 2022-12-28 13:52; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:5796,update,updated,5796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['update'],['updated']
Deployability,"m, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance(val, pd.DataFrame):; 92 name = self.attrname.title().rstrip(""s""). ValueError: Value passed for key 'mean' is of incorrect shape. Values of layers mus",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2621,update,update,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['update'],['update']
Deployability,"map; copying scanpy/neighbors/umap/distances.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/umap_.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/__init__.py -> build/lib/scanpy/neighbors/umap; copying scanpy/neighbors/umap/sparse.py -> build/lib/scanpy/neighbors/umap; creating build/lib/scanpy/plotting/tools; copying scanpy/plotting/tools/paga.py -> build/lib/scanpy/plotting/tools; copying scanpy/plotting/tools/__init__.py -> build/lib/scanpy/plotting/tools; running egg_info; writing scanpy.egg-info/PKG-INFO; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing requirements to scanpy.egg-info/requires.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found. reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py"", line 50, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/install.py"", line 61, in run; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/command/install.py"", line 545, in run; self.run_command('build'); File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/distutils/cmd.py"", line 31",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148:5034,install,install-,5034,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148,1,['install'],['install-']
Deployability,"max(0)).fillna(0); ```; and that is when using `standard_scale` which doesn't seem to have an effect for `logfoldchanges` (naturally). I'm just not sure where or why my nan takes the color of values >0 in my matrix plot. #### Versions; `sc.__version__`; '1.9.1'. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; <details>. ```; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; certifi 2021.10.08; cffi 1.15.0; charset_normalizer 2.0.12; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 0.8.3; gprofiler 1.0.0; h5py 3.6.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.10; ipykernel 6.12.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.16.0; kiwisolver 1.4.2; leidenalg 0.8.9; llvmlite 0.38.0; matplotlib 3.6.2; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.4; packaging 21.3; pandas 1.5.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pytz 2022.1; requests 2.27.1; scipy 1.8.1; seaborn 0.12.1; session_info 1.0.0; setuptools 62.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.0.2; socks 1.7.1; stack_data 0.2.0; statsmodels 0.13.2; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; traitlets 5.1.1; typing_extensions NA; urllib3 1.26.9; wcwidth 0.2.5; zmq 22.3.0; zoneinfo NA; -----; IPython 8.2.0; jupyter_client 7.2.2; jupyter_core 4.9.2; jupyterlab 3.3.3; notebook 6.4.10; -----; Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55) [GCC 10.3.0]; Linux-5.15.0-52-generic-x86_64-with-glibc2.31; -----; Session information updated at 2022-11-11 15:54; ```; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2368:2562,update,updated,2562,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2368,1,['update'],['updated']
Deployability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3312,install,installed,3312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['installed']
Deployability,mentedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: u,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:35609,pipeline,pipeline,35609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"mmand errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/; ------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:3982,install,install,3982,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install']
Deployability,mmh can I help you with how to go wrt to update?. @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129:41,update,update,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129,1,['update'],['update']
Deployability,"mmmm #2...; The previous email regarding decorator issue, may have been a problem from; namespace shadowing in a jupyter notebook. I think the previous pull; request works--but suggest testing with and without vmin vmax arguments.; I'll update you as I do more testing myself.; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/388#issuecomment-444388428>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACez5ZEF7goRe3PYEixKaLT4f0cNthGks5u13gdgaJpZM4ZB23Z>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444661453:237,update,update,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444661453,2,"['patch', 'update']","['patch', 'update']"
Deployability,"mmmm...; looks like there are some difficulties here.; The decorator sitting ontop of dotplot() causes a weird error for kwds; dictionary lookups. If I leave the decorator in place, then I get a; keywords error when, vmin is left out as a parameter. If I take the; decorator off the method, it works. error is coming from the. @doc_params(). decorator. But 1. it looks like this function only purpose in life it to; ensure that the __doc__ string starts with a '\' character. And in the case; of dotplot() it already does. When I comment out the decorator, the code; works. This error is too strange for me to understand. I don't often use; decorators, and it seems to be the problem here.; Tim. On Tue, Dec 4, 2018 at 11:39 PM Fidel Ramirez <notifications@github.com>; wrote:. > The change is quite useful. Please go ahead and add a PR.; >; > On Wed, Dec 5, 2018 at 3:52 AM Tim Rand <notifications@github.com> wrote:; >; > > Here is a patch that fixes the above problem...; > >; > > import matplotlib.colors; > >; > > #if user defined, then use the vmax, vmin keywords, else use data to; > generate them...; > > if ('vmax' in kwds) and ('vmin' in kwds):; > > _vmax = kwds['vmax']; > > _vmin = kwds['vmin']; > > else:; > > _vmax = max(mean_flat); > > _vmin = min(mean_flat); > >; > > #normalize = matplotlib.colors.Normalize(vmin=min(mean_flat),; > vmax=max(mean_flat)); > > normalize = matplotlib.colors.Normalize(vmin=_vmin, vmax=_vmax); > >; > > I'll submit a pull request.; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub; > > <https://github.com/theislab/scanpy/issues/388#issuecomment-444339817>,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AEu_1WglYAlmHO-3DyNHUCRJwBtAOfskks5u1zT6gaJpZM4ZB23Z; > >; > > .; > >; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388#issuecomment-444632665:936,patch,patch,936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388#issuecomment-444632665,1,['patch'],['patch']
Deployability,mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40573,continuous,continuous-,40573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['continuous'],['continuous-']
Deployability,mplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:27769,pipeline,pipeline,27769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"n directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preprocessing/recipes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/highly_variable_genes.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/__init__.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/magic.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/dca.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/mnn_correct.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/qc.py -> build/lib/scanpy/preprocessing; copying scanpy/preprocessing/simple.py -> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:3872,install,install,3872,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['install'],['install']
Deployability,"n reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed at nopython (nopython frontend); Unknown attribute 'partition' of type Module(<module 'numpy' from '/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/__init__.py'>). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. [1] During: typing of get attribute at /home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ... [Version](url) of the packages in path : ; scanpy 1.4.4.post1; anndata 0.6.22.post1; anndata2ri 1.0.1; umap-learn 0.3.10; numpy 1.16.5; scipy 1.3.1; pandas 1.0.1; scikit-learn 0.21.3; statsmodels 0.10.1; python-igraph 0.7.1.post6; louvain 0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:3932,release,release,3932,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['release'],['release']
Deployability,"n running sc.pp.highly_variable_genes I get this error; ""ImportError: Please install skmisc package via `pip install --user scikit-misc ""; I've tried numerous times to install the package through pip and conda, pip gives me a metadata error and conda can't find the package even using the command the website gives at [https://anaconda.org/conda-forge/scikit-misc](url). I checked around online and there was a two year old thread on the subject (https://github.com/scverse/scanpy/issues/2073), but none of the solutions worked on my machine. I tried version control of all the packages, most importantly numpy as it was mentioned as a problem. I was wondering if this you all have any newer solutions for this issue. Due to my inexperience im really not sure what is causing this issue and therefore what to provide you all with. I also just realized that after a clean install trying to pip3 install scikit-misc returns this . (scanpy_env) user@Mac ~ % pip3 install scikit-misc ; Requirement already satisfied: scikit-misc in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (0.3.1); Requirement already satisfied: numpy>=1.22.3 in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (from scikit-misc) (1.26.4). Still getting the error though... Hardware: M2 max mac, macos 15 beta (could this be it somehow?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:1337,install,install,1337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['install'],['install']
Deployability,"n them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_k",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3372,install,installed,3372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['install'],['installed']
Deployability,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3879,pipeline,pipeline,3879,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,3,['pipeline'],['pipeline']
Deployability,"n_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:2426,Pipeline,PipelineDevelope,2426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"n_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; six 1.16.0; sklearn 0.24.2; storemagic NA; tables 3.6.1; texttable 1.6.4; tornado 6.1; traitlets 5.0.5; wcwidth 0.2.5; zipp NA; zmq 22.1.0; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) [GCC 9.3.0]; Linux-4.18.0-240.22.1.el8_3.x86_64-x86_64-with-glibc2.10; 96 logical CPU cores, x86_64; -----; Session information updated at 2021-08-17 16:58. time: 402 ms (started: 2021-08-17 16:58:46 +01:00); ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:5482,update,updated,5482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['update'],['updated']
Deployability,"n_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; openpyxl 3.0.9; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pyexpat NA; pygments 2.10.0; pynndescent 0.5.6; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.26.0; scipy 1.7.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; statsmodels 0.13.2; storemagic NA; tables 3.7.0; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; terminado 0.11.1; texttable 1.6.4; tornado 6.1; tqdm 4.63.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.2; urllib3 1.26.6; wcwidth 0.2.5; websocket 1.2.1; wrapt 1.14.0; xlrd 1.2.0; zmq 22.2.1; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.1.7; notebook 6.4.3; -----; Python 3.9.6 (default, Aug 18 2021, 11:08:34) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]; Linux-3.10.0-1160.41.1.el7.x86_64-x86_64-with-glibc2.17; 36 logical CPU cores, x86_64; -----; Session information updated at 2022-03-26 18:52. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:3284,update,updated,3284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['update'],['updated']
Deployability,n_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_gen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39422,pipeline,pipeline,39422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,n_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:21763,pipeline,pipeline,21763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"naconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10787,install,install,10787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,ncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signa,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:36852,pipeline,pipeline,36852,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"nd we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1247,pipeline,pipeline,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['pipeline'],['pipeline']
Deployability,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:3086,Install,Installing,3086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['Install'],['Installing']
Deployability,"ne uses an unsupported extension; ; #### Versions. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.0; PyQt5 NA; appnope 0.1.3; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; bs4 4.11.1; certifi 2022.05.18.1; cffi 1.15.0; chardet 4.0.0; charset_normalizer 2.0.12; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.05.0; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fastcluster 1.1.26; fsspec 2022.3.0; gprofiler 1.0.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; ipykernel 6.13.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; jupyter_server 1.17.0; kiwisolver 1.4.2; leidenalg 0.8.8; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.1.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; networkx 2.6.3; numba 0.55.0; numpy 1.21.5; packaging 21.3; pandas 1.4.0; parso 0.8.3; pexpect 4.8.0; phyloproc NA; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pyparsing 3.0.9; pytz 2022.1; requests 2.27.1; scipy 1.7.3; seaborn 0.11.2; session_info 1.0.0; setuptools 62.2.0; sip NA; six 1.16.0; sklearn 1.0.2; socks 1.7.1; soupsieve 2.3.1; sphinxcontrib NA; spyder 5.2.2; spyder_kernels 2.2.1; spydercustomize NA; statsmodels 0.13.2; storemagic NA; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.2.0; typing_extensions NA; unicodedata2 NA; urllib3 1.26.9; wcwidth 0.2.5; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 7.33.0; jupyter_client 7.3.1; jupyter_core 4.10.0; jupyterlab 3.2.8; notebook 6.4.11; -----; Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:27:05) [Clang 12.0.1 ]; macOS-12.4-x86_64-i386-64bit; -----; Session information updated at 2022-07-03 15:14",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2288:2921,update,updated,2921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2288,1,['update'],['updated']
Deployability,"ne); assert out is None; ```. Should return an axis, as stated in the docs. ```; Returns; -------; A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.; ```. Temporary workaround for those wanting a solution: . ```; ax = sc.pl.violin(adata, keys=['CD8A', 'CD8B'], ax=None, show=False); ```. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.4.0; anndata 0.7.8; asttokens NA; backcall 0.2.0; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.0; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; django 4.0; executing 0.8.2; google NA; h5py 2.10.0; igraph 0.9.8; ipykernel 6.7.0; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.13.3; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.37.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.0.2; numba 0.54.1; numexpr 2.8.0; numpy 1.19.5; packaging 21.3; pandas 1.1.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.24; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.1; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 3.0.6; pytz 2021.3; scanpy 1.8.2; scipy 1.5.3; scprep 1.1.0; seaborn 0.11.2; setuptools 58.0.4; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; stack_data 0.1.4; statsmodels 0.13.1; tables 3.6.1; texttable 1.6.4; tornado 6.1; tqdm 4.62.3; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zmq 22.3.0; -----; IPython 8.0.0; jupyter_client 6.1.12; jupyter_core 4.9.1; jupyterlab 3.2.8; notebook 6.4.7; -----; Python 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) [GCC 9.4.0]; Linux-5.4.0-1064-gcp-x86_64-with-glibc2.10; 16 logical CPU cores; -----; Session information updated at 2022-02-10 16:29; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2135:2375,update,updated,2375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2135,1,['update'],['updated']
Deployability,"neDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4670,Pipeline,PipelineDevelope,4670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,networkx update breaking CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1959:9,update,update,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1959,1,['update'],['update']
Deployability,"nexpected keyword argument 'rotation'; ```. ### Versions. <details>. ```; numpy 1.26.4; pandas 2.2.2; scanpy 1.10.2; session_info 1.0.0; -----. PIL 10.3.0; anndata 0.10.8; anyio NA; arrow 1.3.0; asciitree NA; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.15.0; certifi 2024.06.02; cffi 1.16.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.11.0; idna 3.7; igraph 0.11.5; ipykernel 6.29.4; isoduration NA; jedi 0.19.1; jinja2 3.1.4; joblib 1.4.2; json5 0.9.25; jsonpointer 2.4; jsonschema 4.22.0; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.1; jupyterlab_server 2.27.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.9.0; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; numba 0.59.1; numcodecs 0.12.1; overrides NA; packaging 24.0; parso 0.8.4; platformdirs 4.2.2; prometheus_client NA; prompt_toolkit 3.0.46; psutil 5.9.8; pure_eval 0.2.2; pyarrow 16.1.0; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.18.0; pyparsing 3.1.2; pythonjsonlogger NA; pytz 2024.1; referencing NA; requests 2.32.3; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.11.0; send2trash NA; six 1.16.0; sklearn 1.5.0; sniffio 1.3.1; stack_data 0.6.3; texttable 1.7.0; threadpoolctl 3.5.0; tlz 0.12.1; toolz 0.12.1; tornado 6.4.1; traitlets 5.14.3; uri_template NA; urllib3 2.2.1; wcwidth 0.2.13; webcolors 24.6.0; websocket 1.8.0; yaml 6.0.1; zarr 2.18.2; zmq 26.0.3. -----; IPython 8.25.0; jupyter_client 8.6.2; jupyter_core 5.7.2; jupyterlab 4.2.1; -----; Python 3.12.3 | packaged by Anaconda, Inc. | (main, May 6 2024, 19:46:43) [GCC 11.2.0]; Linux-5.4.0-84-generic-x86_64-with-glibc2.27; -----; Session information updated at 2024-07-03 15:12. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:10370,update,updated,10370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,1,['update'],['updated']
Deployability,ng bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40486,pipeline,pipeline,40486,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,ng/_utils.py; scanpy-1.8.0/scanpy/plotting/palettes.py; scanpy-1.8.0/scanpy/preprocessing/__init__.py; scanpy-1.8.0/scanpy/preprocessing/_combat.py; scanpy-1.8.0/scanpy/preprocessing/_deprecated/__init__.py; scanpy-1.8.0/scanpy/preprocessing/_deprecated/highly_variable_genes.py; scanpy-1.8.0/scanpy/preprocessing/_distributed.py; scanpy-1.8.0/scanpy/preprocessing/_docs.py; scanpy-1.8.0/scanpy/preprocessing/_highly_variable_genes.py; scanpy-1.8.0/scanpy/preprocessing/_normalization.py; scanpy-1.8.0/scanpy/preprocessing/_pca.py; scanpy-1.8.0/scanpy/preprocessing/_qc.py; scanpy-1.8.0/scanpy/preprocessing/_recipes.py; scanpy-1.8.0/scanpy/preprocessing/_simple.py; scanpy-1.8.0/scanpy/preprocessing/_utils.py; scanpy-1.8.0/scanpy/queries/__init__.py; scanpy-1.8.0/scanpy/queries/_queries.py; scanpy-1.8.0/scanpy/readwrite.py; scanpy-1.8.0/scanpy/sim_models/__init__.py; scanpy-1.8.0/scanpy/sim_models/krumsiek11.txt; scanpy-1.8.0/scanpy/sim_models/krumsiek11_params.txt; scanpy-1.8.0/scanpy/sim_models/toggleswitch.txt; scanpy-1.8.0/scanpy/sim_models/toggleswitch_params.txt; scanpy-1.8.0/scanpy/tools/__init__.py; scanpy-1.8.0/scanpy/tools/_dendrogram.py; scanpy-1.8.0/scanpy/tools/_diffmap.py; scanpy-1.8.0/scanpy/tools/_dpt.py; scanpy-1.8.0/scanpy/tools/_draw_graph.py; scanpy-1.8.0/scanpy/tools/_embedding_density.py; scanpy-1.8.0/scanpy/tools/_ingest.py; scanpy-1.8.0/scanpy/tools/_leiden.py; scanpy-1.8.0/scanpy/tools/_louvain.py; scanpy-1.8.0/scanpy/tools/_marker_gene_overlap.py; scanpy-1.8.0/scanpy/tools/_paga.py; scanpy-1.8.0/scanpy/tools/_pca.py; scanpy-1.8.0/scanpy/tools/_rank_genes_groups.py; scanpy-1.8.0/scanpy/tools/_score_genes.py; scanpy-1.8.0/scanpy/tools/_sim.py; scanpy-1.8.0/scanpy/tools/_top_genes.py; scanpy-1.8.0/scanpy/tools/_tsne.py; scanpy-1.8.0/scanpy/tools/_umap.py; scanpy-1.8.0/scanpy/tools/_utils.py; scanpy-1.8.0/scanpy/tools/_utils_clustering.py; scanpy-1.8.0/PKG-INFO; ```. </details>. ## Flit build. Using flit to build it includes all the files I would expect,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909:3873,toggle,toggleswitch,3873,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909,1,['toggle'],['toggleswitch']
Deployability,"niconda3/envs/uhler/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 383 rasterized=settings._vector_friendly,; 384 norm=normalize,; --> 385 **kwargs,; 386 ); 387 . TypeError: functools.partial object got multiple values for keyword argument 'marker'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cached_property 1.5.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.09.1; dateutil 2.8.2; debugpy 1.5.0; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fsspec 2021.10.0; google NA; h5py 3.4.0; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.2; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; mudata 0.1.0; muon 0.1.1; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2122:2705,install,installs,2705,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2122,1,['install'],['installs']
Deployability,"nit__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:2624,Pipeline,PipelineDevelope,2624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:216,continuous,continuous,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['continuous'],['continuous']
Deployability,"nocytes': {'CD14', 'LYZ'},; 'B cells': {'MS4A1'},; 'CD8 T cells': {'CD8A'},; 'NK cells': {'GNLY', 'NKG7'},; 'FCGR3A+ Monocytes': {'FCGR3A', 'MS4A7'},; 'Dendritic Cells': {'FCER1A', 'CST3'},; 'Megakaryocytes': {'PPBP'}}; marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes); ```. ```pytb; 	0	1	2	3	4	5	6; CD4 T cells	1.0	1.0	1.0	1.0	1.0	1.0	1.0; CD14+ Monocytes	1.0	1.0	1.0	1.0	1.0	1.0	1.0; B cells	1.0	1.0	1.0	1.0	1.0	1.0	1.0; CD8 T cells	1.0	1.0	1.0	1.0	1.0	1.0	1.0; NK cells	2.0	2.0	2.0	2.0	2.0	2.0	2.0; FCGR3A+ Monocytes	1.0	1.0	1.0	1.0	1.0	1.0	1.0; Dendritic Cells	2.0	2.0	2.0	2.0	2.0	2.0	2.0; Megakaryocytes	0.0	0.0	0.0	0.0	0.0	0.0	0.0; ![image](https://user-images.githubusercontent.com/57720451/92608724-029c7880-f2b6-11ea-9860-59c705183fb2.png). ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cffi 1.14.1; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.15.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mpl_toolkits NA; natsort 7.0.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; pandas 1.1.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.6.0; scipy 1.5.2; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; statsmodels 0.12.0; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.0.4; traitlets 4.3.3; umap 0.4.6; wcwidth 0.2.5; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-3.10.0-1062.9.1.el7.x86_64-x86_64-with-glibc2.10; 128 logical CPU cores, x86_64; -----; Session information updated at 2020-09-09 15:47. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1411:2446,update,updated,2446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411,1,['update'],['updated']
Deployability,"nored. - [x] `…/scanpy/plotting/_tools/__init__.py:1269:`. > FutureWarning: The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect. ; > `_ax = sns.violinplot(`. - [x] `…/scanpy/preprocessing/_simple.py:274:`. > ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; > `adata.var[""n_cells""] = number`. - [x] `…/scanpy/plotting/_stacked_violin.py:503: FutureWarning:`. > Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect. ; > `row_ax = sns.violinplot(`. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.10.0.dev191+gf7f5d5c6; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.1.4; parso 0.8.3; pexpect 4.9.0; pluggy 1.3.0; prompt_toolkit 3.0.43; psutil 5.9.7; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pygments 2.17.2; pyparsing 3.1.1; pytest 7.4.4; pytz 2023.3.post1; scipy 1.11.4; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; sitecustomize NA; six 1.16.0; sklearn 1.3.2; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.0; toolz 0.12.0; traitlets 5.14.1; typing_extensions NA; wcwidth 0.2.13; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; Python 3.11.6 (main, Nov 2 2023, 04:39:43) [Clang 14.0.3 (clang-1403.0.22.14.1)]; macOS-13.6.1-arm64-arm-64bit; -----; Session information updated at 2024-01-30 16:40; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839:2904,update,updated,2904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839,1,['update'],['updated']
Deployability,"npointer 2.4; jsonschema 4.19.1; jsonschema_specifications NA; jupyter_events 0.8.0; jupyter_server 2.8.0; jupyterlab_server 2.25.0; kiwisolver 1.4.5; kneed 0.8.5; legacy_api_wrap NA; leidenalg 0.9.1; llvmlite 0.40.1; louvain 0.8.0; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.6.2; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.6; mudata 0.2.3; muon 0.1.6; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numcodecs 0.12.1; numexpr 2.8.7; numpy 1.24.4; overrides NA; packaging 23.2; pandas 2.2.1; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.11.0; plotly 5.17.0; prometheus_client NA; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 10.0.1; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.16.1; pynndescent 0.5.10; pyparsing 3.1.1; pysam 0.20.0; pythonjsonlogger NA; pytz 2023.3.post1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; rpds NA; scipy 1.11.3; seaborn 0.13.0; send2trash NA; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; simplejson 3.19.2; six 1.16.0; sklearn 1.3.1; sniffio 1.3.0; socks 1.7.1; sortedcontainers 2.4.0; sphinxcontrib NA; stack_data 0.6.2; statsmodels 0.14.0; tblib 2.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.2; tomli 2.0.1; toolz 0.12.0; tornado 6.3.3; tqdm 4.66.1; traitlets 5.9.0; typing_extensions NA; umap 0.5.4; upsetplot 0.8.0; uri_template NA; urllib3 1.26.18; wcwidth 0.2.8; webcolors 1.13; websocket 1.6.4; xxhash NA; yaml 6.0.1; zarr 2.17.2; zipp NA; zmq 25.1.1; zoneinfo NA; zope NA; zstandard 0.21.0; -----; IPython 8.16.1; jupyter_client 8.4.0; jupyter_core 5.4.0; jupyterlab 4.1.4; notebook 7.1.1; -----; Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]; Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28; -----; Session information updated at 2024-06-13 10:40; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3106:4391,update,updated,4391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3106,1,['update'],['updated']
Deployability,"npy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:6715,install,install,6715,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['install'],['install']
Deployability,npy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/preprocessing/_simple.py::scanpy.preprocessing._simple.filter_cells; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/plotting/_baseplot_class.py::scanpy.plotting._baseplot_class.BasePlot.add_totals; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:26462,pipeline,pipeline,26462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"nstall the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1029,install,install,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027,1,['install'],['install']
Deployability,"nstalls and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.4; -----; MulticoreTSNE NA; PIL 8.0.1; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; cffi 1.14.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.01.0; dateutil 2.8.1; decorator 4.4.2; dunamai 1.7.0; fsspec 2022.01.0; get_version 3.5.3; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; jsonschema 3.2.0; jupyter_server 1.13.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.38.0; loompy 3.0.6; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.1; nbformat 5.0.8; numba 0.55.0; numexpr 2.7.3; numpy 1.21.5; numpy_groupies 0.9.13; packaging 21.3; pandas 1.0.4; parso 0.7.1; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.9.0; ptyprocess 0.6.0; pvectorc NA; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pyrsistent NA; pytz 2020.4; scipy 1.4.1; seaborn 0.11.2; send2trash NA; setuptools_scm NA; sitecustomize NA; six 1.14.0; sklearn 0.22.2.post1; statsmodels 0.13.0rc0; storemagic NA; tables 3.7.0; tblib 1.7.0; terminado 0.9.1; texttable 1.6.4; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.3.10; wcwidth 0.2.5; yaml 5.3.1; zmq 20.0.0; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.7.0; jupyterlab 3.2.8; notebook 6.1.5; -----; Python 3.8.12 (default, Oct 22 2021, 18:39:35) [Clang 13.0.0 (clang-1300.0.29.3)]; macOS-12.1-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2022-01-25 07:11. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:6263,update,updated,6263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,1,['update'],['updated']
Deployability,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1599,update,updates,1599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['update'],['updates']
Deployability,"nt, despite the fact that it is more accurate. And then downstream analyses can change quite a bit. Thus we should remedy this for the next minor release as it is a breaking change to e.g., https://dev.azure.com/scverse/scanpy/_build/results?buildId=7094&view=logs&j=5ea502cf-d418-510c-3b5f-c4ba606ae534&t=534778bb-2f86-5739-7d3c-59518f7b5a2b&l=2171; <img width=""731"" alt=""Screenshot 2024-06-26 at 16 21 27"" src=""https://github.com/scverse/scanpy/assets/43999641/6c58d0a1-aa44-4f42-87ca-16250d3de641"">. ### Minimal code sample. ```python; import numpy as np; arr = np.random.random((10, 10_000)).astype(""float32""). print(np.multiply(arr, arr, dtype=""float64"")); print(np.multiply(arr, arr)); ```. ### Error output. ```pytb; N/A; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.0rc2.dev74+g1c98fd19; -----; IPython 8.24.0; PIL 10.3.0; asciitree NA; asttokens NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.5.1; dateutil 2.9.0.post0; decorator 5.1.1; defusedxml 0.7.1; distutils 3.12.3; executing 2.0.1; h5py 3.11.0; igraph 0.11.5; jedi 0.19.1; jinja2 3.1.4; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; louvain 0.8.2; markupsafe 2.1.5; matplotlib 3.9.0; mpl_toolkits NA; msgpack 1.0.8; natsort 8.4.0; numba 0.59.1; numcodecs 0.12.1; numpy 1.26.4; packaging 24.0; pandas 2.2.2; parso 0.8.4; pkg_resources NA; prompt_toolkit 3.0.45; psutil 5.9.8; pure_eval 0.2.2; pyarrow 16.1.0; pygments 2.18.0; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; setuptools 70.0.0; setuptools_scm NA; sitecustomize NA; six 1.16.0; sklearn 1.5.0; sparse 0.15.4; sphinxcontrib NA; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.5.0; tlz 0.12.1; toolz 0.12.1; traitlets 5.14.3; wcwidth 0.2.13; yaml 6.0.1; zarr 2.18.2; -----; Python 3.12.3 (main, Apr 9 2024, 08:09:14) [Clang 15.0.0 (clang-1500.1.0.2.5)]; macOS-13.6.1-arm64-arm-64bit; -----; Session information updated at 2024-06-26 16:19. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3127:2986,update,updated,2986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3127,1,['update'],['updated']
Deployability,ntedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_var,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:41894,pipeline,pipeline,41894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ntime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; etils 0.6.0; executing 0.8.3; flatbuffers 2.0; flax 0.5.2; fsspec 2022.5.0; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.11; iniconfig NA; ipykernel 6.15.1; ipython_genutils 0.2.0; ipywidgets 7.7.1; jax 0.3.14; jaxlib 0.3.14; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.3; leidenalg 0.8.10; llvmlite 0.38.1; louvain 0.7.1; matplotlib 3.5.2; matplotlib_inline NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; newick 1.0.0; numba 0.55.2; numpy 1.22.4; numpyro 0.10.0; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pluggy 1.0.0; prompt_toolkit 3.0.30; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; py 1.11.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pyparsing 3.0.9; pyro 1.8.1; pytest 7.1.2; pytorch_lightning 1.6.5; pytz 2022.1; requests 2.28.1; rich NA; scHPL NA; scarches 0.5.3; scipy 1.8.1; scvi 0.17.1; seaborn 0.11.2; session_info 1.0.0; setuptools 63.1.0; six 1.16.0; sklearn 1.1.1; socks 1.7.1; stack_data 0.3.0; statsmodels 0.13.2; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.12.0; torch 1.12.0+cu102; torchmetrics 0.9.2; tornado 6.2; tqdm 4.64.0; traitlets 5.3.0; tree 0.1.7; typing_extensions NA; urllib3 1.26.10; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zmq 23.2.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; notebook 6.4.12; -----; Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]; Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2022-09-09 14:21; combined_data.write(f""{workspace}Data/Models/Healthy/combined_hvgs_recalculated.h5ad""); combined_data.write(f""{workspace}Data/Models/H",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2321:3460,update,updated,3460,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2321,1,['update'],['updated']
Deployability,"ntory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.ScanpyConfig.N_PCS'):. Extension error (sphinx.ext.autosummary):; Handler <function process_generate_options at 0x139c4a940> for event 'builder-inited' threw an exception (exception: type object 'ScanpyConfig' has no attribute 'N_PCS'); make: *** [html] Error 2; ```. </details>. However, I'm entirely sure if this is Sphinx's fault, or our own. Currently the [N_PCS parameter isn't in the rendered documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy._settings.ScanpyConfig.html#scanpy._settings.ScanpyConfig). I think it should be, and am not sure why it's not showing up here. To summarize:. * Previous versions of our doc builds didn't seem to be including attribute docstrings for `ScanpyConfig`.; * Sphinx 4.1.0 raises an ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:2592,release,release-notes,2592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['release'],['release-notes']
Deployability,"numba 0.51.2; numexpr 2.7.1; numpy 1.20.3; packaging 20.4; pandas 1.3.2; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pyarrow 0.16.0; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.2; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.8.1; scipy 1.5.2; sinfo 0.3.1; sip NA; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; sphinxcontrib NA; spyder 4.1.5; spyder_kernels 1.9.4; spydercustomize NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; webencodings 0.5.1; win32api NA; win32com NA; win32security NA; yaml 5.3.1; zmq 19.0.2; zope NA; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.6; notebook 6.1.4; -----; Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.19041-SP0; 4 logical CPU cores, Intel64 Family 6 Model 78 Stepping 3, GenuineIntel; -----; Session information updated at 2021-09-09 14:23. </details>\. Reading this stackoverflow (https://stackoverflow.com/questions/58518554/attributeerror-graph-object-has-no-attribute-node) I tried downgrading to networkx version 2.3 instead of 2.6.2 and it fixed this issue, but raised a separate incompatibility between networkx and matplotlib. ```pytb; AttributeError: module 'matplotlib.cbook' has no attribute 'iterable'; ```; ; which traces back to an issue in networkx rather than scanpy.; The following stackoverflow suggests that the older networkx2.3 requires matplotlib2.2.3 which is quite a large downgrade from my current matplatlib3.4.3 (current as of this date), so downgrading both networkx and matplotlib is not a great solution: https://stackoverflow.com/questions/63198347/attributeerror-module-matplotlib-cbook-has-no-attribute-iterable,. How i fixed it: ; After all that I went to line 860-862 of plotting/_tools/paga.py in my own scanpy installation and simply ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1997:2590,update,updated,2590,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1997,1,['update'],['updated']
Deployability,"numba\lowering.py in lower_block(self, block); 286 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block); 290 . ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 723 from numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (399)>, 400: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (405)>, 402: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (406)>, 276: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (403)>, 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:12951,pipeline,pipeline,12951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,2,['pipeline'],['pipeline']
Deployability,"o do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram inte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1267,integrat,integrated,1267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['integrat'],['integrated']
Deployability,"o the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory); ```pytb; Traceback (most recent call last):; File ""scanpy_test.py"", line 9, in <module>; adata = sc.read_h5ad(PATH, backed=True); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 202, in read_h5ad; return read_h5ad_backed(filename, mode); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 146, in read_h5ad_backed; d.update({k: read_elem(f[k]) for k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\h5ad.py"", line 146, in <dictcomp>; d.update({k: read_elem(f[k]) for k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:2487,update,update,2487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['update'],['update']
Deployability,o/gh/scverse/scanpy/pull/3097?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `79.16667%` with `5 lines` in your changes missing coverage. Please review.; > Project coverage is 76.35%. Comparing base [(`4f40d68`)](https://app.codecov.io/gh/scverse/scanpy/commit/4f40d68c8958ef74fd8abe5f97601c40ffee9337?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`e89ebac`)](https://app.codecov.io/gh/scverse/scanpy/commit/e89ebaceddc37589fe22bfe32b1e8a9f1b5746f9?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 41 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3097?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_score\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&filepath=scanpy%2Ftools%2F_score_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL19zY29yZV9nZW5lcy5weQ==) | 82.35% | [3 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&filepath=scanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L2dldC9nZXQucHk=) | 66.66% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3097?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3097#issuecomment-2147774048:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3097#issuecomment-2147774048,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3220?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `33.33333%` with `30 lines` in your changes missing coverage. Please review.; > Project coverage is 76.73%. Comparing base [(`d4e1fb4`)](https://app.codecov.io/gh/scverse/scanpy/commit/d4e1fb4cb290d9835710fba2b5b9594d97176601?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`403dd30`)](https://app.codecov.io/gh/scverse/scanpy/commit/403dd30f9e523ae84eba4ac239c6fb72fb439585?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3220?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 25.00% | [15 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/tools/\_draw\_graph.py](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&filepath=src%2Fscanpy%2Ftools%2F_draw_graph.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS90b29scy9fZHJhd19ncmFwaC5weQ==) | 31.57% | [13 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3220?src=pr&el=tree&utm_medium=ref,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3220#issuecomment-2323386009:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220#issuecomment-2323386009,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3249?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `94.59459%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.75%. Comparing base [(`1650aed`)](https://app.codecov.io/gh/scverse/scanpy/commit/1650aed30fd0141a97c01a6a6b19c2735e058c77?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`b3fa09b`)](https://app.codecov.io/gh/scverse/scanpy/commit/b3fa09ba950806f9e2a2c5060b32d3d768f0f14e?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3249?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/plotting/\_anndata.py](https://app.codecov.io/gh/scverse/scanpy/pull/3249?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_anndata.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fYW5uZGF0YS5weQ==) | 93.10% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3249?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3249 +/- ##; ==========================================; - Coverage 76.75% 76.75% -0.01% ; ==========================================; Files 109 109 ; Lines 12551 12556 +5 ; ==========================================; + Hits 9634 9637 +3 ; - Misses 2917 2919 +2 ; ```. | [Files with missing lines](https:/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3249#issuecomment-2363632936:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3249#issuecomment-2363632936,1,['Patch'],['Patch']
Deployability,o/gh/scverse/scanpy/pull/3307?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `95.45455%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 77.23%. Comparing base [(`2f0afac`)](https://app.codecov.io/gh/scverse/scanpy/commit/2f0afac72be3644624cf996323197239580f14f9?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`65c74cf`)](https://app.codecov.io/gh/scverse/scanpy/commit/65c74cfa67b2f9d9493b9cd2384685245ecacc2c?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 2 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3307?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_qc.py](https://app.codecov.io/gh/scverse/scanpy/pull/3307?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_qc.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19xYy5weQ==) | 94.28% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3307?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3307 +/- ##; ==========================================; + Coverage 77.21% 77.23% +0.02% ; ==========================================; Files 111 111 ; Lines 12597 12618 +21 ; ==========================================; + Hits 9727 9746 +19 ; - Misses 2870 2872 +2 ; ```. | [Files with missing lines](https,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3307#issuecomment-2429279414:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307#issuecomment-2429279414,1,['Patch'],['Patch']
Deployability,"oading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.ScanpyConfig.N_PCS'):. Extension error (sphinx.ext.autosummary):; Handler <function process_generate_options at 0x139c4a940> for event 'builder-inited' threw an exception (exception: type object 'ScanpyConfig' has no attribute 'N_PCS'); make: *** [html] Error 2; ```. </details>. However, I'm entirely sure if this is Sphinx's fault, or our own. Currently the [N_PCS parameter isn't in the rendered documentation](https://scanpy.readthedocs.io/en/stable/generated/scanpy._settings.ScanpyConfig.html#scanpy._settings.ScanpyConfig). I think it should be, and am not sure why it's not showing up here. To summarize:. * Previous versions of our doc builds didn't seem to be including attribute docstrings for `ScanpyConfig`.; *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:2567,release,release-notes,2567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['release'],['release-notes']
Deployability,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:2180,update,update,2180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,1,['update'],['update']
Deployability,"oc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block); 275 . ~\AppData\Local\Continuum\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 750 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 751 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 752 reraise(type(newerr), newerr, tb); 753 ; 754 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\utils.py in reraise(tp, value, tb); 79 if value.__traceback__ is not tb:; 80 raise value.with_traceback(tb); ---> 81 raise value; 82 ; 83 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4079:36: error: '%.2747' defined with type 'i64' but expected 'i32'; %"".2748"" = icmp eq i32 %"".2746"", %"".2747""; ^. File ""..\..\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 412:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. During: lowering ""id=13[LoopNest(index_variable = parfor_index.264, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (412)>, 400: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (418)>, 402: <ir.Block at C:\Users\tpeng\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (419)>, 276: <ir.Block at C:\Users\tpeng\AppData\L",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:13607,pipeline,pipeline,13607,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,2,['pipeline'],['pipeline']
Deployability,"of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ncont = ncont[ncont.obs.pct_counts_mt < 5, :]; ncont.raw = ncont; ```. ```pytb; [TypeError: cannot unpack non-iterable NoneType object]; ```. #### Versions. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.21.5 scipy==1.7.1 pandas==1.3.2 scikit-learn==0.24.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. <details>. [WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging 21.3; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2188:1227,install,install,1227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2188,1,['install'],['install']
Deployability,"oftware/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session information updated at 2022-05-17 14:56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:2694,update,updated,2694,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['update'],['updated']
Deployability,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:188,release,release,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112,2,['release'],['release']
Deployability,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:48,install,installing,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006,1,['install'],['installing']
Deployability,"ol.map_async(_regress_out_chunk, tasks).get(9999999); 780 pool.close(); 781 . ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in get(self, timeout); 636 ; 637 def get(self, timeout=None):; --> 638 self.wait(timeout); 639 if not self.ready():; 640 raise TimeoutError. ~\AppData\Local\conda\conda\envs\scanpy\lib\multiprocessing\pool.py in wait(self, timeout); 633 ; 634 def wait(self, timeout=None):; --> 635 self._event.wait(timeout); 636 ; 637 def get(self, timeout=None):. ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 549 signaled = self._flag; 550 if not signaled:; --> 551 signaled = self._cond.wait(timeout); 552 return signaled; 553 . ~\AppData\Local\conda\conda\envs\scanpy\lib\threading.py in wait(self, timeout); 297 else:; 298 if timeout > 0:; --> 299 gotit = waiter.acquire(True, timeout); 300 else:; 301 gotit = waiter.acquire(False). OverflowError: timeout value is too large. update:-----------------------------------------------------------------------------------------------------. when i used the n_jobs= 1 as a paramters ,seems like i lack a module named patsy; oduleNotFoundError Traceback (most recent call last); <ipython-input-22-6ea7e0dee435> in <module>(); ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito'],n_jobs = 1). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in regress_out(adata, keys, n_jobs, copy); 781 ; 782 else:; --> 783 res = list(map(_regress_out_chunk, tasks)); 784 ; 785 # res is a list of vectors (each corresponding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:1512,update,update,1512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,1,['update'],['update']
Deployability,"om numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (399)>, 400: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (405)>, 402: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (406)>, 276: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (403)>, 318: <ir.Block at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (404)>}Var(parfor_index.260, _qc.py:399)"" at C:\Users\lyciansarpedon\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py (399); ```. Unlikely to be related, but this was after I had issues installing scanpy from conda (as in #1142), which I got around by installing through pip. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:14299,install,installing,14299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,2,['install'],['installing']
Deployability,"on3.11/site-packages/anndata/_core/anndata.py"", line 362, in __init__; self._init_as_actual(; File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py"", line 574, in _init_as_actual; self._check_uniqueness(); File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py"", line 1906, in _check_uniqueness; utils.warn_names_duplicates(""obs""); File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/utils.py"", line 275, in warn_names_duplicates; warnings.warn(; UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; /mnt/workspace/repos/scanpy/scanpy/preprocessing/_simple.py:90: UnexpectedException; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.10.0.dev197+g96e19540; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.1; dateutil 2.8.2; decorator 5.1.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.11.3; iniconfig NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.0; parso 0.8.3; pexpect 4.9.0; pluggy 1.4.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pygments 2.17.2; pyparsing 3.1.1; pytest 8.0.0; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.1; toolz 0.12.1; traitlets 5.14.1; wcwidth 0.2.13; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]; Linux-5.15.0-87-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-01-29 14:16; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2836:5297,update,updated,5297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2836,1,['update'],['updated']
Deployability,"on312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:10379,Pipeline,PipelineDevelope,10379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"on312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:620, in _read(filepath_or_buffer, kwds); [617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:617) _validate_names(kwds.get(""names"", None)); [619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:622) if chunksize or iterator:; [623](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:623) return parser. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds); [1617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1617) self.options[""has_index_names""] = kwds[""has_index_names""]; [1619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDeve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:11397,Pipeline,PipelineDevelope,11397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"on312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4070,Pipeline,PipelineDevelope,4070,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"on5 0.9.24; jsonpointer 2.4; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.13.0; jupyterlab_server 2.25.4; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.6.2; mpl_toolkits NA; msgpack 1.0.8; mudata 0.2.3; muon 0.1.5; mygene 3.2.2; natsort 8.4.0; nbformat 5.10.3; networkx 3.2.1; numba 0.59.1; numexpr 2.9.0; numpy 1.26.4; optree 0.10.0; optuna 3.6.0; overrides NA; packaging 24.0; pandas 1.5.3; pandas_flavor NA; parso 0.8.3; patsy 0.5.6; pingouin 0.5.4; pkg_resources NA; platformdirs 4.2.0; plotly 5.20.0; prometheus_client NA; prompt_toolkit 3.0.43; psutil 5.9.8; pure_eval 0.2.2; pyBigWig 0.3.22; pyarrow 15.0.2; pychromvar 0.0.4; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 2.0.0; pyfaidx 0.8.1.1; pygments 2.17.2; pyjaspar 3.0.0; pynndescent 0.5.11; pyparsing 3.1.2; pysam 0.22.0; pythonjsonlogger NA; pytz 2024.1; ray 2.10.0; referencing NA; requests 2.31.0; requests_cache 1.2.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; rpds NA; scipy 1.12.0; seaborn 0.13.2; send2trash NA; session_info 1.0.0; setproctitle 1.2.2; simplejson 3.19.2; sitecustomize NA; six 1.16.0; sklearn 1.4.1.post1; sniffio 1.3.1; stack_data 0.6.3; statsmodels 0.14.1; swig_runtime_data4 NA; tabulate 0.9.0; tensorboard 2.16.2; texttable 1.7.0; threadpoolctl 3.4.0; torch 2.2.1+cu121; torchgen NA; tornado 6.4; tqdm 4.66.2; traitlets 5.14.2; typing_extensions NA; umap 0.5.5; uri_template NA; url_normalize 1.4.3; urllib3 2.2.1; uvloop 0.19.0; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; wget 3.2; xarray 2024.2.0; yaml 6.0.1; zmq 25.1.2; zoneinfo NA; -----; IPython 8.22.2; jupyter_client 8.6.1; jupyter_core 5.7.2; jupyterlab 4.1.5; notebook 7.1.2; -----; Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]; Linux-6.5.0-27-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-04-18 18:58; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3014:4447,update,updated,4447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3014,1,['update'],['updated']
Deployability,on_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedErro,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:15440,pipeline,pipeline,15440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"onal) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample . ```python; import scanpy as sc; import numpy as np; import pandas as pd; adata = sc.datasets.pbmc68k_reduced(); sc.pl.pca_loadings(adata, components = '1,2,3'). ```. ```pytb; ValueError: Axis limits cannot be NaN or Inf; ```; **Note**: recalculating pca solves the problem . #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.8.0.dev78+gc488909a; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appnope 0.1.0; attr 19.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2.20.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.33.0+1.g022ab0f; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.2.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.7; numba 0.50.1; numexpr 2.7.1; numpy 1.18.5; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pvectorc NA; pygments 2.6.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; scanpy 1.8.0.dev78+gc488909a; scipy 1.5.0; seaborn 0.10.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.3; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 2.1.5; notebook 6.0.3; -----; Python 3.8.3 (default, Jul 2 2020, 11:26:31) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 4 logical CPU cores, i386; -----; Session information updated at 2021-04-27 11:22. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1816:2114,update,updated,2114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1816,1,['update'],['updated']
Deployability,onda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge; h5py 2.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1573,patch,patch,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684,1,['patch'],['patch']
Deployability,one vs. one logreg update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/213:19,update,update,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/213,1,['update'],['update']
Deployability,one vs. one logreg update (fix),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/214:19,update,update,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/214,1,['update'],['update']
Deployability,"ong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=668) """"""Compiler entry point; [670](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=669) ; [671](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=670) Parameter; (...); [689](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=688) compiler pipeline; [690](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=689) """"""; [691](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=690) pipeline = pipeline_class(typingctx, targetctx, library,; [692](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=691) args, return_type, flags, locals); --> [693](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=692) return pipeline.compile_extra(func). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:429, in CompilerBase.compile_extra(self, func); [427](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=426) self.state.lifted = (); [428](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=427) self.state.lifted_from = None; --> [429](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=428) return self._compile_bytecode(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:497, in CompilerBase._compile_bytecode(self); [493](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=492) """"""; [494](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=493) Populate and run pipeline for bytecode input; [495](file:///",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:19268,pipeline,pipeline,19268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipeline']
Deployability,"op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17271,install,install,17271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['install'],['install']
Deployability,"opying scanpy/neighbors/umap/utils.py -> build/lib/scanpy/neighbors/umap; running egg_info; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing scanpy.egg-info/PKG-INFO; writing top-level names to scanpy.egg-info/top_level.txt; writing requirements to scanpy.egg-info/requires.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/usr/lib/python3.5/distutils/command/install.py"", line 583, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for packa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:9717,install,install,9717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['install'],['install']
Deployability,"or 5.1.1; defusedxml 0.7.1; docrep 0.3.2; executing 0.8.3; flax 0.6.1; fsspec 2023.4.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; idna 3.4; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.19.2; ipython_genutils 0.2.0; ipywidgets 8.0.4; jax 0.4.8; jaxlib 0.4.7; jedi 0.18.1; joblib 1.2.0; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.9.1; lightning_fabric 1.9.4; lightning_utilities 0.8.0; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mkl 2.4.0; ml_collections NA; ml_dtypes 0.1.0; mpl_toolkits NA; mpmath 1.2.1; msgpack 1.0.5; mudata 0.2.2; multipledispatch 0.6.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; numpyro 0.11.0; nvfuser NA; opt_einsum v3.3.0; optax 0.1.5; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.2.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pyparsing 3.0.9; pyro 1.8.4+9ed468d; pytorch_lightning 1.9.4; pytz 2023.3; requests 2.28.1; rich NA; scipy 1.10.1; scvi 0.20.3; seaborn 0.12.2; session_info 1.0.0; setuptools 66.0.0; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; skmisc 0.1.4; socks 1.7.1; stack_data 0.2.0; statsmodels 0.13.5; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; toolz 0.12.0; torch 2.0.0; torchmetrics 0.11.4; torchvision 0.15.0; tornado 6.2; tqdm 4.65.0; traitlets 5.7.1; tree 0.1.7; typing_extensions NA; unicodedata2 NA; urllib3 1.26.15; wcwidth 0.2.5; yaml 6.0; zmq 23.2.0; zoneinfo NA; -----; IPython 8.12.0; jupyter_client 8.1.0; jupyter_core 5.3.0; jupyterlab 3.5.3; notebook 6.5.4; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-40-generic-x86_64-with-glibc2.35; -----; Session information updated at 2023-04-26 05:02. </details>; Thanks for your help",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2474:5512,update,updated,5512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2474,1,['update'],['updated']
Deployability,"or in this step: sc.pp.scrublet(adata, batch_key=""sample""); AttributeError: module 'scanpy.preprocessing' has no attribute 'scrublet'. Can you help with this? Thanks!. Best,; B. ### Minimal code sample. ```python; sc.pp.scrublet(adata, batch_key=""lib_prep""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); /tmp/ipykernel_54187/3500521297.py in <module>; ----> 1 sc.pp.scrublet(adata, batch_key=""lib_prep""). AttributeError: module 'scanpy.preprocessing' has no attribute 'scrublet'; ```. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.1.0; backcall 0.2.0; cffi 1.14.6; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; exceptiongroup 1.2.0; get_annotations NA; google NA; h5py 3.10.0; importlib_resources NA; ipykernel 6.2.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.3.2; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.2; matplotlib_inline NA; mpl_toolkits NA; mudata 0.2.3; muon 0.1.6; natsort 8.4.0; numba 0.58.1; numpy 1.26.2; packaging 21.0; pandas 2.1.3; parso 0.8.2; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pyexpat NA; pygments 2.10.0; pynndescent 0.5.11; pyparsing 2.4.7; pytz 2023.3.post1; scipy 1.11.4; scrublet NA; seaborn 0.12.2; session_info 1.0.0; setuptools 65.5.1; six 1.16.0; sklearn 1.3.2; statsmodels 0.14.0; storemagic NA; threadpoolctl 3.2.0; torch 1.12.1+cu102; tornado 6.1; tqdm 4.66.1; traitlets 5.0.5; typing_extensions NA; umap 0.5.5; wcwidth 0.2.5; yaml 6.0.1; zipp NA; zmq 22.2.1; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.3; -----; Python 3.9.0 (default, Oct 6 2020, 11:01:41) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]; Linux-3.10.0-1160.108.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-04-23 14:43; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3026:2397,update,updated,2397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3026,1,['update'],['updated']
Deployability,"orceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1630,install,install,1630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,2,"['install', 'release']","['install', 'release']"
Deployability,"ore\lowering.py in lower_block(self, block); 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 485 if isinstance(inst, _class):; --> 486 func(self, inst); 487 return. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _lower_parfor_parallel(lowerer, parfor); 239 lowerer, parfor, typemap, typingctx, targetctx, flags, {},; --> 240 bool(alias_map), index_var_typ, parfor.races); 241 finally:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _create_gufunc_for_parfor_body(lowerer, parfor, typemap, typingctx, targetctx, flags, locals, has_aliases, index_var_typ, races); 1326 flags,; -> 1327 locals); 1328 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(typingctx, targetctx, func_ir, args, return_type, flags, locals, lifted, lifted_from, is_lifted_loop, library, pipeline_class); 666 return pipeline.compile_ir(func_ir=func_ir, lifted=lifted,; --> 667 lifted_from=lifted_from); 668 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(self, func_ir, lifted, lifted_from); 348 FixupArgs().run_pass(self.state); --> 349 return self._compile_ir(); 350 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_ir(self); 407 assert self.state.func_ir is not None; --> 408 return self._compile_core(); 409 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 380 if is_final_pipeline:; --> 381 raise e; 382 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 371 try:; --> 372 pm.run(self.state); 373 if self.state.cr is not None:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 340 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:1721,pipeline,pipeline,1721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['pipeline'],['pipeline']
Deployability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:1715,update,update,1715,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['update'],['update']
Deployability,"orm; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1275,release,release,1275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['release'],['release']
Deployability,"ort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 383 rasterized=settings._vector_friendly,; 384 norm=normalize,; --> 385 **kwargs,; 386 ); 387 . TypeError: functools.partial object got multiple values for keyword argument 'marker'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cached_property 1.5.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.09.1; dateutil 2.8.2; debugpy 1.5.0; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fsspec 2021.10.0; google NA; h5py 3.4.0; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.2; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; mudata 0.1.0; muon 0.1.1; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 2.0.10; psutil 5.9.0; ptyprocess 0.7.0; pycparser ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2122:2842,install,install,2842,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2122,1,['install'],['install']
Deployability,"oundError: The following packages are not available from current channels:. - conda-forge::scikit-misc. Current channels:. - defaults; - https://conda.anaconda.org/conda-forge. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org; ```. Im having trouble reproducing the pip3 install scikit-misc error, which i believe is due to me switching between versions of numpy, heres what I had previously copy pasted. ```python; error: subprocess-exited-with-error; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [32 lines of output]; + meson setup /private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c6914ced8cef4877a07998e75e28ca3e /private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c6914ced8cef4877a07998e75e28ca3e/.mesonpy-ykxq6c1e -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/76/3kg7fkgs3_d2gnkhxh5npt280000gn/T/pip-install-jl3ciqs5/scikit-misc_c6914ced8cef4877a07998e75e28ca3e/.mesonpy-ykxq6c1e/meson-python-native-file.ini; Preparing metadata (pyproject.toml) did not run successfully.; ```. ### Error output. _No response_. ### Versions. <details>. ```; # Name Version Build Channel; absl-py 2.1.0 pyhd8ed1ab_0 conda-forge; anndata 0.10.8 pypi_0 pypi; anyio 4.2.0 py39hca03da5_0 ; appnope 0.1.2 py39hca03da5_1001 ; argon2-cffi 21.3.0 pyhd3eb1b0_0 ; argon2-cffi-bindings 21.2.0 py39h1a28f6b_0 ; arpack 3.9.1 nompi_h593882a_101 conda-forge; array-api-compat 1.7.1 pyhd8ed1ab_0 conda-forge; asttokens 2.0.5 pyhd3eb1b0_0 ; async-lru 2.0.4 py39hca03da5_0 ; attrs 23.1.0 py39hca03da5_0 ; babel 2.11.0 py39hca03da5_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; beautifulsoup4 4.12.3 py39hca03da5_0 ; blas 1.0 openblas ; bleach 4.1.0 pyhd3eb1b0_0 ; blosc2 2.0.0 pypi_0 pypi; brotli 1.1.0 hb547adb_1 conda-forge; brotli-bin 1.1.0 hb547adb_1 conda-forge; brotli-python 1.0.9 py39h313beb8_8 ; b",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:5601,release,release,5601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,3,"['install', 'release']","['install-', 'release']"
Deployability,ov.io/gh/scverse/scanpy/pull/2684?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `83.33333%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 71.83%. Comparing base [(`42e3c2a`)](https://app.codecov.io/gh/scverse/scanpy/commit/42e3c2a04e2bee8431da4e831abd16d198bc8323?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`7b6a875`)](https://app.codecov.io/gh/scverse/scanpy/commit/7b6a875a75529e41e4a53993a0e04a4ad04a9d78?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 223 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/2684?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/tools/\_umap.py](https://app.codecov.io/gh/scverse/scanpy/pull/2684?src=pr&el=tree&filepath=scanpy%2Ftools%2F_umap.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3Rvb2xzL191bWFwLnB5) | 83.33% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/2684?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #2684 +/- ##; ==========================================; - Coverage 71.98% 71.83% -0.16% ; ==========================================; Files 108 108 ; Lines 11920 11921 +1 ; ==========================================; - Hits 8581 8563 -18 ; - Misses 3339 3358 +19 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2684#issuecomment-1763195554:1085,Patch,Patch,1085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2684#issuecomment-1763195554,1,['Patch'],['Patch']
Deployability,ov.io/gh/scverse/scanpy/pull/3283?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `87.50000%` with `3 lines` in your changes missing coverage. Please review.; > Project coverage is 76.94%. Comparing base [(`be99b23`)](https://app.codecov.io/gh/scverse/scanpy/commit/be99b230fa84e077f5167979bc9f6dacc4ad0d41?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`8268e54`)](https://app.codecov.io/gh/scverse/scanpy/commit/8268e543090721ae9b056355c0cbb8d2ac742d13?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3283?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/external/pl.py](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&filepath=src%2Fscanpy%2Fexternal%2Fpl.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9leHRlcm5hbC9wbC5weQ==) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |; | [src/scanpy/plotting/\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&filepath=src%2Fscanpy%2Fplotting%2F_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wbG90dGluZy9fdXRpbHMucHk=) | 83.33% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3283?src=pr&el=tree&utm_medium=referral&utm_source=github,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3283#issuecomment-2411417242:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283#issuecomment-2411417242,1,['Patch'],['Patch']
Deployability,"ows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:1254,patch,patches,1254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['patch'],['patches']
Deployability,"p.normalize_per_cell(adata); sc.pp.log1p(adata); sc.tl.pca(adata, n_comps=300); sc.pp.neighbors(adata, knn=30); sce.tl.palantir(adata, n_components=5, knn=30) # error occurs here; ```. ### Error output. ```pytb; RuntimeError Traceback (most recent call last); RuntimeError: module compiled against API version 0xf but this version of numpy is 0xe; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/ipykernel_5717/3469234522.py in <cell line: 7>(); 5 sc.tl.pca(adata, n_comps=300); 6 sc.pp.neighbors(adata, knn=30); ----> 7 sce.tl.palantir(adata, n_components=5, knn=30). /p/project/hai_microbio/sb/miniconda3/envs/cellrank2/lib/python3.9/site-packages/scanpy/external/tl/_palantir.py in palantir(adata, n_components, knn, alpha, use_adjacency_matrix, distances_key, n_eigs, impute_data, n_steps, copy); 207 ; 208 # Diffusion maps; --> 209 dm_res = run_diffusion_maps(; 210 data_df=df,; 211 n_components=n_components,. TypeError: run_diffusion_maps() got an unexpected keyword argument 'data_df'; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.3; -----; PIL 10.0.0; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; gmpy2 2.1.2; h5py 3.9.0; igraph 0.10.6; importlib_resources NA; joblib 1.3.2; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.40.1; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.57.1; numexpr 2.8.5; numpy 1.24.4; opt_einsum v3.3.0; packaging 23.1; pandas 1.5.3; psutil 5.9.5; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.1; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; sympy 1.12; texttable 1.6.7; threadpoolctl 3.2.0; torch 2.0.0; tqdm 4.65.2; typing_extensions NA; wcwidth 0.2.6; yaml 6.0; zipp NA; zoneinfo NA; -----; Python 3.9.16 | packaged by conda-forge | (main, Feb 1 2023, 21:39:03) [GCC 11.3.0]; Linux-4.18.0-477.15.1.el8_8.x86_64-x86_64-with-glibc2.28; -----; Session information updated at 2023-08-09 17:08; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2608:3190,update,updated,3190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2608,1,['update'],['updated']
Deployability,"p.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1291,install,install-,1291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['install'],['install-']
Deployability,"pData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:13412,Pipeline,PipelineDevelope,13412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"pData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1889) ); [1890](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1890) assert self.handles is not None; [1891](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1891) f = self.handles.handle. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:761) if compression == ""gzip"":; [762](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:16183,Pipeline,PipelineDevelope,16183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"p_pts=False,; 91 ):; ---> 93 if 'log1p' in adata.uns_keys() and adata.uns['log1p']['base'] is not None:; 94 self.expm1_func = lambda x: np.expm1(x * np.log(adata.uns['log1p']['base'])); 95 else:. KeyError: 'base'; ```. #### Versions. <details>. -----; anndata 0.8.0rc1; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 9.0.1; anndata2ri 1.0.6; annoy NA; asttokens NA; backcall 0.2.0; backports NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.11.0; entrypoints 0.4; executing 0.8.3; get_version 3.5.4; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.9; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.13.5; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.1.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.0; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.28; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2021.3; pytz_deprecation_shim NA; rpy2 3.4.2; scipy 1.8.0; scrublet NA; seaborn 0.11.2; sitecustomize NA; six 1.14.0; skimage 0.19.2; sklearn 1.0.2; stack_data 0.2.0; statsmodels 0.13.2; tables 3.7.0; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; tzlocal NA; umap 0.5.2; wcwidth 0.2.5; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; jupyterlab 3.3.1; notebook 6.4.8; -----; Python 3.8.10 (default, Nov 26 2021, 20:14:08) [GCC 9.3.0]; Linux-5.10.76-linuxkit-x86_64-with-glibc2.29; 8 logical CPU cores, x86_64; -----; Session information updated at 2022-03-17 17:22. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2181:6633,update,updated,6633,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2181,1,['update'],['updated']
Deployability,"packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; Linux-5.19.0-41-generic-x86_64-with-glibc2.36; -----; Session information updated at 2023-05-04 01:16. </p>; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:2108,update,updated,2108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,1,['update'],['updated']
Deployability,paga test fails on master on a dev install,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2459:35,install,install,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2459,1,['install'],['install']
Deployability,"pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pipel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:17667,Pipeline,PipelineDevelope,17667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:771) handle = gzip.GzipFile(; [772](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:772) # No overload variant of ""GzipFile"" matches argument types; [773](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:773) # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); [776](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:776) **compression_args,; [777](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); [190](file:///C:/Program%20Files/Python312/Lib/gzip.py:190) mode += 'b'; [191](file:///C:/Program%20Files/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PD",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:18887,Pipeline,PipelineDevelope,18887,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"pbmc.uns[""rank_genes_groups""][""names""][0]). In [4]: sc.pl.stacked_violin(pbmc, var_names=genes, groupby='louvain', swap_axes=True); ```. <details>; <summary> Versions </summary>. ```; -----; scanpy 1.9.0.dev63+gb69015e9; session_info 1.0.0; -----; PIL 9.0.0; anndata 0.8.0rc1; appnope 0.1.2; asciitree NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.0.0; cycler 0.10.0; cython_runtime NA; dask 2021.12.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; executing 0.8.2; fasteners NA; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 6.7.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.13.1; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; msgpack 1.0.3; natsort 8.0.2; nbinom_ufunc NA; numba 0.55.0; numcodecs 0.9.1; numexpr 2.7.3; numpy 1.21.5; packaging 21.3; pandas 1.4.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.22; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.1; pyarrow 6.0.1; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyexpat NA; pygments 2.10.0; pyparsing 3.0.6; pytz 2021.3; ruamel NA; scipy 1.7.3; setuptools 60.5.0; setuptools_scm NA; sinfo 0.3.4; sitecustomize NA; six 1.16.0; sklearn 1.0.2; sparse 0.13.0; sphinxcontrib NA; stack_data 0.1.4; tables 3.7.0; tblib 1.7.0; texttable 1.6.4; threadpoolctl 3.0.0; tlz 0.11.2; toolz 0.11.2; tornado 6.1; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zappy NA; zarr 2.10.3; zmq 22.3.0; -----; IPython 8.0.0; jupyter_client 7.1.0; jupyter_core 4.9.1; jupyterlab 3.2.6; notebook 6.4.7; -----; Python 3.9.9 (main, Nov 21 2021, 03:23:42) [Clang 13.0.0 (clang-1300.0.29.3)]; macOS-11.6.2-x86_64-i386-64bit; -----; Session information updated at 2022-01-24 16:14; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020206162:2136,update,updated,2136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2118#issuecomment-1020206162,1,['update'],['updated']
Deployability,"pdated at 2024-01-14 04:38; ```; </Details>. # **working version**. <Details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.2.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; brotli 1.1.0; certifi 2023.11.17; cffi 1.16.0; charset_normalizer 3.3.2; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.10.0; idna 3.6; ipykernel 6.28.0; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.20.0; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.12.4; jupyterlab_server 2.25.2; kiwisolver 1.4.5; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; numba 0.58.1; numpy 1.26.3; overrides NA; packaging 23.2; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; platformdirs 4.1.0; prometheus_client NA; prompt_toolkit 3.0.42; psutil 5.9.7; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing 3.1.1; pythonjsonlogger NA; pytz 2023.3.post1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.11.4; seaborn 0.13.1; send2trash NA; session_info 1.0.0; six 1.16.0; sklearn 1.3.2; sniffio 1.3.0; socks 1.7.1; stack_data 0.6.2; statsmodels 0.14.1; threadpoolctl 3.2.0; tornado 6.3.3; traitlets 5.14.1; typing_extensions NA; uri_template NA; urllib3 2.1.0; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; yaml 6.0.1; zmq 25.1.2; zoneinfo NA; -----; IPython 8.20.0; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyterlab 4.0.10; -----; Python 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]; Linux-6.6.9-200.fc39.x86_64-x86_64-with-glibc2.38; -----; Session information updated at 2024-01-14 04:32; ```; </Details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:12325,update,updated,12325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['update'],['updated']
Deployability,pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/te,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28471,pipeline,pipeline,28471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28235,pipeline,pipeline,28235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[csr_matrix-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecod,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:28917,pipeline,pipeline,28917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"pelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6411,Pipeline,PipelineDevelope,6411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"pile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. ""; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower(self); 134 if self.generator_info is None:; 135 self.genlower = None; --> 136 self.lower_normal_function(self.fndesc); 137 else:; 138 self.genlower = self.GeneratorLower(self); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:7738,pipeline,pipeline,7738,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['pipeline'],['pipeline']
Deployability,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:4,install,install,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,pip install scanpy[rapids] fails as cuML requirement cannot be met,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1280:4,install,install,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1280,1,['install'],['install']
Deployability,pip installation - missing tools?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/560:4,install,installation,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560,1,['install'],['installation']
Deployability,"please check if my edits made some part worse in your opinion. if you want, you can add some release notes for this :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3122#issuecomment-2188843289:93,release,release,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3122#issuecomment-2188843289,1,['release'],['release']
Deployability,"plotlib/figure.py in draw(self, renderer); 1707 self.patch.draw(renderer); 1708 mimage._draw_list_compositing_images(; -> 1709 renderer, self, artists, self.suppressComposite); 1710 ; 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite); 133 if not_composite or not has_images:; 134 for a in artists:; --> 135 a.draw(renderer); 136 else:; 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs); 36 renderer.start_filter(); 37 ; ---> 38 return draw(artist, renderer, *args, **kwargs); 39 finally:; 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer); 290 sorted(self.collections,; 291 key=lambda col: col.do_3d_projection(renderer),; --> 292 reverse=True)):; 293 col.zorder = zorder_offset + i; 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col); 289 for i, col in enumerate(; 290 sorted(self.collections,; --> 291 key=lambda col: col.do_3d_projection(renderer),; 292 reverse=True)):; 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer); 543 self.set_facecolors(fcs); 544 ; --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else; 546 self._edgecolor3d); 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs); 845 norm = Normalize(min(zs), max(zs)); 846 sats = 1 - norm(zs) * 0.7; --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)); 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]); 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok); 180 [1, 2, 3]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/787:3425,patch,patch,3425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787,1,['patch'],['patch']
Deployability,"plotting/_tools/scatterplots.py:392, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 389 # if user did not set alpha, set alpha to 0.7; 390 kwargs['alpha'] = 0.7 if alpha is None else alpha; --> 392 cax = scatter(; 393 coords[:, 0],; 394 coords[:, 1],; 395 marker=""."",; 396 c=color_vector,; 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; 399 **kwargs,; 400 ); 402 # remove y and x ticks; 403 ax.set_yticks([]). File scanpy/plotting/_utils.py:1111, in circles(x, y, s, ax, marker, c, vmin, vmax, scale_factor, **kwargs); 1105 # You can set `facecolor` with an array for each patch,; 1106 # while you can only set `facecolors` with a value for all.; 1107 if scale_factor != 1.0:; -> 1108 x = x * scale_factor; 1109 y = y * scale_factor; 1110 zipped = np.broadcast(x, y, s). TypeError: can't multiply sequence by non-int of type 'float'; ```. #### Solution. This issue was caused by non-integer values in `adata.obsm['spatial']` after reading using `sc.read_visium`. Either of the following two solutions works:. 1. `adata.obsm['spatial'] = adata.obsm['spatial'].astype(int)`; 2. Add the following line 1108 - 1110 in `scanpy/plotting/_utils.py`:. ```python; 1107 if scale_factor != 1.0:; 1108 if not isinstance(x[0], int) or not isinstance(y[0], int):; 1109 x = x.astype(int); 1110 y = y.astype(int); 1111 x = x * scale_factor; 1112 y = y * scale_factor; ```. Personally, I suggest the second solution. Also, there is a post talking about not maintaining the spatial module in `scanpy`. . #### Versions; * `scanpy==1.9.1`; * `spaceranger==2.0.0`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2345:2205,patch,patch,2205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2345,1,['patch'],['patch']
Deployability,"portError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 269 ); 270 gprofiler = GProfiler(user_agent=""scanpy"", return_dataframe=True). ImportError: This method requires the `gprofiler-official` module to be installed.; ```. #### Versions. gprofiler-official bioconda/noarch::gprofiler-official-1.0.0-py_0. scanpy==1.7.1 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.3 pandas==1.1.5 scikit-learn==0.19.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:2315,install,installed,2315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,2,['install'],['installed']
Deployability,pre-release: AttributeError: type object 'StackedViolin' has no attribute 'DEFAULT_SCALE',MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2899:4,release,release,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2899,1,['release'],['release']
Deployability,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/727#issuecomment-508692089:21,update,update,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508692089,1,['update'],['update']
Deployability,"produce your bug.; scvi does not have models class anymore to access VAE and LDVAE, please change it to use scvi.module. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; sc.external.pp.scvi(adata, train_size=0.8, batch_key='batch'); ```. ```pytb; File ""/usr/local/apps/python/3.9.4/lib/python3.9/site-packages/scanpy/external/pp/_scvi.py"", line 116, in scvi; from scvi.models import VAE, LDVAE; ModuleNotFoundError: No module named 'scvi.models'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/data/lab/MCF7/for_PAGA/MCF7_batch_attempt1.py"", line 140, in <module>; sc.external.pp.scvi(adata, train_size=0.8, batch_key='batch'); File ""/usr/local/apps/python/3.9.4/lib/python3.9/site-packages/scanpy/external/pp/_scvi.py"", line 120, in scvi; raise ImportError(; ImportError: Please install scvi package from https://github.com/YosefLab/scVI. ```. #### Versions; >>> scanpy.logging.print_versions() ; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.5; cffi 1.14.5; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; get_version 2.1; google NA; h5py 3.2.1; igraph 0.9.1; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.36.0; matplotlib 3.4.1; mpl_toolkits NA; natsort 7.1.1; numba 0.53.1; numexpr 2.7.3; numpy 1.20.2; packaging 20.9; pandas 1.2.3; pkg_resources NA; pyexpat NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.1; scipy 1.6.2; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sphinxcontrib NA; tables 3.6.1; texttable 1.6.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; -----; Python 3.9.4 (default, Apr 5 2021, 15:43:56) [GCC 7.3.1 20180303 (Red Hat 7.3.1-5)]; Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.17; 56 logical CPU cores, x86_64; -----; Session information updated at 2021-04-06 12:14. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781:2359,update,updated,2359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781,1,['update'],['updated']
Deployability,prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python-editor 1.0.4 py_0 conda-forge; python-igraph 0.10.6 py310h33b8572_0 conda-forge; python-installer 0.7.0 pyhd8ed1ab_0 conda-forge; python-multipart 0.0.6 pyhd8ed1ab_0 conda-forge; python-tzdata 2023.3 pyhd8ed1ab_0 conda-forge; python_abi 3.10 3_cp310 conda-forge; pytorch 2.0.1 py3.10_cuda11.8_cudnn8.7.0_0 pytorch; pytorch-cuda 11.8 h7e8668a_5 pytorch; pytorch-lightning 2.0.4 pyhd8ed1ab_0 conda-forge; pytorch-mutex 1.0 cuda pytorch; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyyaml 6.0 py310h5764c6d_5 conda-forge; pyzmq 25.1.0 py310h5bbb5d0_0 conda-forge; rapidfuzz 2.15.1 py310heca2aa9_0 conda-forge; re2 2023.03.02 h8c504da_0 conda-forge; readchar 4.0.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; referencing 0.30.0 pyhd8ed1ab_0 conda-forge; requests 2.31.0 pyhd8ed1ab_0 conda-forge; requests-toolbelt 1.0.0 pyhd8ed1ab_0 conda-forge; rich 13.4.2 pyhd8ed1ab_0 conda-forge; rpds-py 0.9.2 py310hcb5633a_0 conda-forge; scanpy 1.9.3 pyhd8ed1ab_0 conda-forge; scikit-learn 1.3.0 py310hf7d194e_0 conda-forge; scipy 1.11.1 py310ha4c1d20_0 conda-forge; scvi-tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:18493,install,installer,18493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['install'],['installer']
Deployability,"protip: when wanting to link to lines, press the <kbd>y</kbd> key to change the url from `…/master/…` to `…/<sha1sum>/…`. that way the copied URL will stay valid and not break once the file is modified on the master branch. no objections to the release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298897726:245,release,release,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298897726,1,['release'],['release']
Deployability,ptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:16665,pipeline,pipeline,16665,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/; ----------------------------------------; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Running setup.py install for fa2 ... error. ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running insta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:2980,install,install,2980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install']
Deployability,"py.; - [ ✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; I installed Scanpy, scVelo, CellRank, bbknn 2 months ago and never upgrade the packages. They were running very smoothly until I reimage my PC and reinstall Scanpy in anaconda today (Anaconda3-2021.05-Windows-x86_64, python3.8.12).; I tired `pip install scanpy[leiden]`. Tried `conda install seaborn scikit-learn statsmodels numba pytables`, `conda install -c conda-forge python-igraph leidenalg`. Tried installing `Java` and `visual C++ 2012-2022 redistributable`. Also tried rebuilding a new environment and reinstalled everything. Whatever I try, this bug still exists when I import Scanpy.; I guess it may be the incompatibility issue of packages. Some dependency packages which were upgraded by the developer in these months caused this incompatibility issue. Could you please help me with this bug?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import matplotlib.pyplot as pl; from matplotlib import rcParams; ```. ```pytb; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_7844/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:1119,upgrade,upgraded,1119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['upgrade'],['upgraded']
Deployability,"py/tools/louvain.py -> build/lib.linux-x86_64-3.6/scanpy/tools; writing scanpy.egg-info/PKG-INFO; writing entry points to scanpy.egg-info/entry_points.txt; writing top-level names to scanpy.egg-info/top_level.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; ```; ```pytb; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py"", line 83, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/cluster/software/lib/python3.6/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/cluster/software/lib/python3.6/distutils/command/install.py"", line 539, in run; self.run_command('build'); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:6598,install,install,6598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['install'],['install']
Deployability,"pytables 3.7.0 is incompatible with Windows. Installing pytables 3.6.1 solves the problem.; For the UMAP, it's quite weird. Whether I restart anaconda between installing individual packages plays the trick.; It should be done in this way.; ```python; !pip install tables==3.6.1. !pip install scanpy[leiden]; !pip install -U scvelo; !pip install tqdm; !pip install ipywidgets; !pip install cellrank. close Edge. run anaconda powershell prompt (anaconda3) as admin; conda activate HYJ_py38; conda install -y -c anaconda cytoolz; pip install pyscenic. restart anaconda3. !pip install adjustText openpyxl; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391:45,Install,Installing,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391,11,"['Install', 'install']","['Installing', 'install', 'installing']"
Deployability,python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplemented,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40894,pipeline,pipeline,40894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6174,install,installs,6174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['install'],['installs']
Deployability,"python_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; lifelines 0.28.0; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.59.1; numexpr 2.8.4; numpy 1.26.4; packaging 23.1; pandas 2.2.2; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prometheus_client NA; prompt_toolkit 3.0.36; psutil 5.9.0; pure_eval 0.2.2; pvectorc NA; pyarrow 11.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pyrsistent NA; pythoncom NA; pytz 2023.3.post1; pywintypes NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; ruamel NA; scipy 1.11.1; seaborn 0.13.2; send2trash NA; session_info 1.0.0; setuptools 69.5.1; six 1.16.0; skewnorm_ufunc NA; sklearn 1.3.0; sniffio 1.2.0; socks 1.7.1; sparse 0.15.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 1.7.0; terminado 0.17.1; texttable 1.7.0; threadpoolctl 2.2.0; tlz 0.12.0; toolz 0.12.0; torch 2.2.1+cpu; torchgen NA; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; urllib3 1.26.16; wcwidth 0.2.5; websocket 0.58.0; win32api NA; win32com NA; win32con NA; win32trace NA; winerror NA; winpty 2.0.10; wrapt 1.14.1; xxhash 2.0.2; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 8.15.0; jupyter_client 7.4.9; jupyter_core 5.3.0; jupyterlab 3.6.3; notebook 6.5.4; -----; Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.22631-SP0; -----; Session information updated at 2024-06-02 17:20. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:8949,update,updated,8949,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['update'],['updated']
Deployability,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2150,install,installation,2150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,2,['install'],"['install', 'installation']"
Deployability,qc_metrics update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358:11,update,update,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358,1,['update'],['update']
Deployability,"r.compile(args, return_type); 820 except errors.ForceLiteralArg as e:; 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 80 return retval; 81 else:; ---> 82 raise retval; 83 ; 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 90 ; 91 try:; ---> 92 retval = self._compile_core(args, return_type); 93 except errors.TypingError as e:; 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 103 ; 104 impl = self._get_implementation(args, {}); --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,; 106 self.targetdescr.target_context,; 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 625 pipeline = pipeline_class(typingctx, targetctx, library,; 626 args, return_type, flags, locals); --> 627 return pipeline.compile_extra(func); 628 ; 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 361 self.state.lifted = (); 362 self.state.lifted_from = None; --> 363 return self._compile_bytecode(); 364 ; 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 423 """"""; 424 assert self.state.func_ir is None; --> 425 return self._compile_core(); 426 ; 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 403 self.state.status.fail_reason = e; 404 if is_final_pipeline:; --> 405 raise e; 406 else:; 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 394 res = None; 395 try:; --> 396 pm.run(self.state); 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:4818,pipeline,pipeline,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,2,['pipeline'],['pipeline']
Deployability,"r=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1737,integrat,integration,1737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['integrat'],['integration']
Deployability,r_matrix-float32-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode);,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:33446,pipeline,pipeline,33446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"r_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:3874,Pipeline,PipelineDevelope,3874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"rage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:761) if compression == ""gzip"":; [762](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:16861,Pipeline,PipelineDevelope,16861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ray([0.9270073, 0.4379562, 1.5985402, ..., 0.8540146, 1.5109489,; 0.8978102], dtype=float32), axis = 0, op = <built-in function truediv>. @singledispatch; def axis_mul_or_truediv(; X: sparse.spmatrix,; scaling_array,; axis: Literal[0, 1],; op: Callable[[Any, Any], Any],; *,; allow_divide_by_zero: bool = True,; out: sparse.spmatrix | None = None,; ) -> sparse.spmatrix:; check_op(op); if out is not None:; if X.data is not out.data:; raise ValueError(; ""`out` argument provided but not equal to X. This behavior is not supported for sparse matrix scaling.""; ); if not allow_divide_by_zero and op is truediv:; scaling_array = scaling_array.copy() + (scaling_array == 0); ; row_scale = axis == 0; column_scale = axis == 1; if row_scale:; ; def new_data_op(x):; return op(x.data, np.repeat(scaling_array, np.diff(x.indptr))); ; elif column_scale:; ; def new_data_op(x):; return op(x.data, scaling_array.take(x.indices, mode=""clip"")); ; > if X.format == ""csr"":; E AttributeError: 'DirectZappyArray' object has no attribute 'format'. scanpy/_utils/__init__.py:612: AttributeError; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.0rc2.dev61+g874d99b3; -----; PIL 10.3.0; asciitree NA; cloudpickle 3.0.0; coverage 7.5.3; cycler 0.12.1; cython_runtime NA; dask 2024.5.2; dateutil 2.9.0.post0; distutils 3.12.3; h5py 3.11.0; igraph 0.11.5; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.9.0; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numcodecs 0.12.1; numpy 1.26.4; packaging 24.0; pandas 2.2.2; psutil 5.9.8; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; setuptools 70.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.5.0; texttable 1.7.0; threadpoolctl 3.5.0; tlz 0.12.1; toolz 0.12.1; yaml 6.0.1; zappy NA; zarr 2.18.2; -----; Python 3.12.3 (main, Apr 23 2024, 09:16:07) [GCC 13.2.1 20240417]; Linux-6.9.3-zen1-1-zen-x86_64-with-glibc2.39; -----; Session information updated at 2024-06-03 11:27; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3087:4389,update,updated,4389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3087,1,['update'],['updated']
Deployability,"re version. Use .loc with labels or .iloc with positions instead.; df.loc[: int(n_top_genes), 'highly_variable'] = True; /gpfs01/berens/user/jlause/libs/scanpy/scanpy/preprocessing/_highly_variable_genes.py:146: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version. Use .loc with labels or .iloc with positions instead.; df.loc[: int(n_top_genes), 'highly_variable'] = True. ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.1.dev2+g8c469411; sinfo 0.3.1; -----; PIL 8.1.0; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; certifi 2020.12.05; cffi 1.14.4; chardet 4.0.0; constants NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; google NA; h5py 2.10.0; highs_wrapper NA; idna 2.10; ipykernel 5.4.2; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.1; jupyterlab_server 2.1.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbclassic NA; nbformat 5.0.8; numba 0.52.0; numexpr 2.7.2; numpy 1.19.5; packaging 20.8; pandas 1.2.0; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.10; ptyprocess 0.7.0; pvectorc NA; pygments 2.7.3; pyparsing 2.4.7; pyrsistent NA; pytz 2020.5; requests 2.25.1; scanpy 1.7.1.dev2+g8c469411; scipy 1.6.0; send2trash NA; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.24.0; sniffio 1.2.0; storemagic NA; tables 3.6.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.2; wcwidth 0.2.5; yaml 5.3.1; zmq 20.0.0; -----; IPython 7.19.0; jupyter_client 6.1.11; jupyter_core 4.7.0; jupyterlab 3.0.3; notebook 6.1.6; -----; Python 3.8.0 (default, Oct 28 2019, 16:14:01) [GCC 8.3.0]; Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.27; 40 logical CPU cores, x86_64; -----; Session information updated at 2021-03-09 19:18. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1725:4744,update,updated,4744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1725,1,['update'],['updated']
Deployability,re-order and rephrase some release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2872:27,release,release,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2872,1,['release'],['release']
Deployability,"release done, we can wait here …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2188807240:0,release,release,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2188807240,1,['release'],['release']
Deployability,remove colorbar if legend_loc=None for continuous colorbars,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821:39,continuous,continuous,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821,1,['continuous'],['continuous']
Deployability,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1269,install,installation,1269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['install'],['installation']
Deployability,resolved via . pip install 'matplotlib<3.7',MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462121858:19,install,install,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2445#issuecomment-1462121858,1,['install'],['install']
Deployability,"resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1887) errors=self.options.get(""encoding_errors"", ""strict""),; [1888](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1888) storage_options=self.options.get(""storage_options"", None),; [1889](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1889) ); [1890](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1890) assert self.handles is not None; [1891](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1891) f = self.handles.handle. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:14940,Pipeline,PipelineDevelope,14940,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"rg as e:; 770 def folded(args, kws):. ~\anaconda3\lib\site-packages\numba\dispatcher.py in compile(self, args, return_type); 75 ; 76 def compile(self, args, return_type):; ---> 77 status, retval = self._compile_cached(args, return_type); 78 if status:; 79 return retval. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_cached(self, args, return_type); 89 ; 90 try:; ---> 91 retval = self._compile_core(args, return_type); 92 except errors.TypingError as e:; 93 self._failed_cache[key] = e. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_core(self, args, return_type); 107 args=args, return_type=return_type,; 108 flags=flags, locals=self.locals,; --> 109 pipeline_class=self.pipeline_class); 110 # Check typing error if object mode is used; 111 if cres.typing_error is not None and not flags.enable_pyobject:. ~\anaconda3\lib\site-packages\numba\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 549 pipeline = pipeline_class(typingctx, targetctx, library,; 550 args, return_type, flags, locals); --> 551 return pipeline.compile_extra(func); 552 ; 553 . ~\anaconda3\lib\site-packages\numba\compiler.py in compile_extra(self, func); 329 self.state.lifted = (); 330 self.state.lifted_from = None; --> 331 return self._compile_bytecode(); 332 ; 333 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_bytecode(self); 391 """"""; 392 assert self.state.func_ir is None; --> 393 return self._compile_core(); 394 ; 395 def _compile_ir(self):. ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_core(self); 371 self.state.status.fail_reason = e; 372 if is_final_pipeline:; --> 373 raise e; 374 else:; 375 raise CompilerError(""All available pipelines exhausted""). ~\anaconda3\lib\site-packages\numba\compiler.py in _compile_core(self); 362 res = None; 363 try:; --> 364 pm.run(self.state); 365 if self.state.cr is not None:; 366 break. ~\a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:8343,pipeline,pipeline,8343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,2,['pipeline'],['pipeline']
Deployability,rge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare-func4] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_pie - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:42128,pipeline,pipeline,42128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,riable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39638,pipeline,pipeline,39638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:15209,pipeline,pipeline,15209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"rite_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; nu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6311,install,install,6311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['install'],['install']
Deployability,"rn_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:152, in _FunctionCompiler._compile_core(self, args, return_type); 149 flags = self._customize_flags(flags); 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,; 155 args=args, return_type=return_type,; 156 flags=flags, locals=self.locals,; 157 pipeline_class=self.pipeline_class); 158 # Check typing error if object mode is used; 159 if cres.typing_error is not None and not flags.enable_pyobject:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:693, in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 669 """"""Compiler entry point; 670 ; 671 Parameter; (...); 689 compiler pipeline; 690 """"""; 691 pipeline = pipeline_class(typingctx, targetctx, library,; 692 args, return_type, flags, locals); --> 693 return pipeline.compile_extra(func). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:429, in CompilerBase.compile_extra(self, func); 427 self.state.lifted = (); 428 self.state.lifted_from = None; --> 429 return self._compile_bytecode(). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:497, in CompilerBase._compile_bytecode(self); 493 """"""; 494 Populate and run pipeline for bytecode input; 495 """"""; 496 assert self.state.func_ir is None; --> 497 return self._compile_core(). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:476, in CompilerBase._compile_core(self); 474 self.state.status.fail_reason = e; 475 if is_final_pipeline:; --> 476 raise e; 477 else:; 478 raise CompilerError(""All available pipelines exhausted""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler.py:463, in CompilerBase._compile_core(self",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:5757,pipeline,pipeline,5757,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,3,['pipeline'],['pipeline']
Deployability,"rom you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1723,install,install,1723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['install'],['install']
Deployability,"ror output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.6; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; get_annotations NA; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 1.5.3; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-08-29 16:55. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215:2537,update,updated,2537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215,1,['update'],['updated']
Deployability,ror: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_highly_variable_genes.py::test_keep_layer[cell_ranger-None] - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_highly_variable_genes.py::test_keep_layer[cell_ranger-10] - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_paga.py::test_paga_path - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_sparse_nanmean - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_sparse_vs_dense - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_deplete - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_npnanmean_vs_sparsemean - AttributeError: 'csr_matrix' object has no attribute 'A'; ```. ### Minimal code sample. ```python; pip install scipy==1.14.0rc1; pytest; ```. ### Error output. _No response_. ### Versions. <details>. ```; + anndata==0.11.0.dev116+g1bff5fb (from git+https://github.com/scverse/anndata@1bff5fbf0894185c0759b61d78c6df66d6dfeeba); + annoy==1.17.3; + anyio==4.4.0; + array-api-compat==1.7.1; + pillow==10.3.0; + platformdirs==4.2.2; + pluggy==1.5.0; + pre-commit==3.7.1; + profimp==0.1.0; + psutil==5.9.8; + pyarrow==16.1.0; + pygments==2.18.0; + pygsp==0.5.1; + pynndescent==0.5.12; + pyparsing==3.1.2; + pytest==8.2.1; + pytest-cov==5.0.0; + pytest-memray==1.6.0; + pytest-mock==3.14.0; + pytest-nunit==1.0.7; + pytest-xdist==3.6.1; + python-dateutil==2.9.0.post0; + pytz==2024.1; + pyyaml==6.0.1; + rich==13.7.1; + scanorama==1.7.4; + scanpy==1.10.2.dev25+gf5a62eee (from file:///home/vsts/work/1/s); + scikit-image==0.23.2; + scikit-learn==1.5.0; + scikit-misc==0.3.1; + scipy==1.14.0rc1; + scprep==1.1.0; + seaborn==0.13.2; + session-info==1.0.0; + setuptools==70.0.0; + setuptools-scm==8.1.0; + six==1.1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3083:1682,install,install,1682,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3083,1,['install'],['install']
Deployability,ror: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matchi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:14749,pipeline,pipeline,14749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"rs/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6009,Pipeline,PipelineDevelope,6009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"rs_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 323 # make the scatter plot; 324 if projection == '3d':; --> 325 cax = ax.scatter(; 326 coords[:, 0],; 327 coords[:, 1],; 328 coords[:, 2],; 329 marker=""."",; 330 c=color_vector,; 331 rasterized=settings._vector_friendly,; 332 norm=normalize,; 333 **kwargs,; 334 ); 335 else:; 336 scatter = (; 337 partial(ax.scatter, s=size, plotnonfinite=True); 338 if scale_factor is None; (...); 341 ) # size in circles is radius; 342 ). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:2417, in Axes3D.scatter(self, xs, ys, zs, zdir, s, c, depthshade, *args, **kwargs); 2414 zs = zs.copy(); 2416 patches = super().scatter(xs, ys, s=s, c=c, *args, **kwargs); -> 2417 art3d.patch_collection_2d_to_3d(patches, zs=zs, zdir=zdir,; 2418 depthshade=depthshade); 2420 if self._zmargin < 0.05 and xs.size > 0:; 2421 self.set_zmargin(0.05). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:674, in patch_collection_2d_to_3d(col, zs, zdir, depthshade); 672 col._depthshade = depthshade; 673 col._in_draw = False; --> 674 col.set_3d_properties(zs, zdir). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:542, in Path3DCollection.set_3d_properties(self, zs, zdir); 539 def set_3d_properties(self, zs, zdir):; 540 # Force the collection to initialize the face and edgecolors; 541 # just in case it is a scalarmappable with a colormap.; --> 542 self.update_scalarmappable(); 543 offsets = self.get_offsets(); 544 if len(offsets) > 0:. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/collections.py:926, in Collection.update_scalar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:3123,patch,patches,3123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['patch'],['patches']
Deployability,"rsers/readers.py:1617) self.options[""has_index_names""] = kwds[""has_index_names""]; [1619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pipeline",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:12998,Pipeline,PipelineDevelope,12998,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,rson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:22904,pipeline,pipeline,22904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"rt preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```. The messages when updating anndata:; ```; The following packages will be REMOVED:. pytables-3.6.1-py38h9f153d1_1. The following packages will be UPDATED:. anndata 0.7.6-py38h578d9bd_0 --> 0.8.0-py38h578d9bd_0; ca-certificates pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0; h5py 2.10.0-nompi_py38h513d04c_102 --> 3.6.0-nompi_py38hfbb2109_100; hdf5 1.10.5-nompi_h5b725eb_1114 --> 1.12.1-nompi_h2750804_100. The following packages will be SUPERSEDED by a higher-priority channel:. certifi pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5.18.1-py38h578d9bd_0; openssl pkgs/main::openssl-1.1.1o-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0. Proceed ([y]/n)? y. Downloading and Extracting Packages; keyutils-1.6.1 | 115 KB | ############################################################################# | 100% ; h5py-3.6.0 | 1.4 MB | ############################################################################# | 100% ; cached_property-1.5. | 11 KB | ############################################################################# | 100% ; c-ares-1.18.1 | 113 K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:1541,UPDATE,UPDATED,1541,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['UPDATE'],['UPDATED']
Deployability,"running egg_info; writing dependency_links to scanpy.egg-info/dependency_links.txt; writing scanpy.egg-info/PKG-INFO; writing top-level names to scanpy.egg-info/top_level.txt; writing requirements to scanpy.egg-info/requires.txt; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run; return orig.install.run(self); File ""/usr/lib/python3.5/distutils/command/install.py"", line 583, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:9759,install,install,9759,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['install'],['install']
Deployability,"ry familiar with Scanpy (which seems like a fantastic library!), so please bear with me if some of the things I mention are not relevant to Scanpy. PyMDE (documentation here: https://pymde.org/) has a few benefits:; - as @adamgayoso mentioned, PyMDE supports computing embeddings on GPU. This makes it possible to compute very large embeddings quickly (often 4-10x faster than CPU).; - PyMDE is a very general embedding library. It is based on a general framework for embedding, and this framework includes many well-known methods --- such as UMAP, PCA, Laplacian embedding, multi-dimensional scaling, and more --- as special cases. This makes it easy to compare different methods using a single framework.; - PyMDE also supports creating entirely new types of embeddings, as custom instances of our framework.; - PyMDE provides ways to reason about how much an embedding distorts the original neighborhood graph. There are some comparisons to UMAP & openTSNE in the third part of our manuscript, which has been published in Foundations & Trends in Machine Learning and is available here: https://web.stanford.edu/~boyd/papers/pdf/min_dist_emb.pdf; - on CPU, UMAP and PyMDE are comparable in speed, with UMAP often having a slight edge; on GPU PyMDE can be much faster; - unlike UMAP/openTSNE, PyMDE allows users to fit constrained embeddings. Right now the supported constraints are standardization (zero mean, unit covariance; this forces embeddings to spread out, but not too much, and as a result standardized embeddings are typically similarly scaled), centering, and anchoring (pre-specifying the coordinates of a subset of the items); - PyMDE allows for more types of embeddings, in addition to UMAP-style embeddings. On the other hand, PyMDE is young software. If you do depend on it, I would recommend including it as an optional dependency, not a required one. Happy to chat more, to answer any questions, and to help with integration, if that is something you are ultimately interested in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627:2057,integrat,integration,2057,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627,1,['integrat'],['integration']
Deployability,"s per cell before normalization (adata.obs); normalizing by total count per cell; filtered out 1 cells that have less than 1 counts; finished (0:00:00): normalized adata.X and added 'n_counts', counts per cell before normalization (adata.obs); =================================================================================================== short test summary info ====================================================================================================; FAILED scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] - AssertionError: ; ======================================================================================== 1 failed, 985 deselected, 11 warnings in 3.74s ========================================================================================; ```. ### Versions. <details>. ```; -----; anndata 0.9.0rc2.dev43+g21a76088; scanpy 1.10.0.dev117+g6b9e734f; -----; PIL 9.1.1; asciitree NA; awkward 2.2.1; awkward_cpp NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.2.1; cycler 0.10.0; cython_runtime NA; dask 2023.5.0; dateutil 2.8.2; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.17.3; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; importlib_resources NA; jinja2 3.1.2; joblib 1.1.0; kiwisolver 1.4.3; leidenalg 0.9.1; llvmlite 0.38.1; markupsafe 2.1.1; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numcodecs 0.10.2; numpy 1.22.4; packaging 21.3; pandas 2.0.2; pkg_resources NA; psutil 5.9.1; pyparsing 3.0.9; pytz 2022.1; scipy 1.8.1; session_info 1.0.0; setuptools 67.8.0; setuptools_scm NA; six 1.16.0; sklearn 1.1.1; sphinxcontrib NA; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zarr 2.12.0; zipp NA; -----; Python 3.8.17 (default, Jun 17 2023, 20:09:37) [GCC 13.1.1 20230429]; Linux-6.3.8-zen1-1-zen-x86_64-with-glibc2.34; -----; Session information updated at 2023-06-23 14:29. ```; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:5720,update,updated,5720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['update'],['updated']
Deployability,"s.; 53 ; (...); 118 >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True); 119 """"""; --> 121 raise_not_implemented_error_if_backed_type(adata.X, ""dendrogram""); 122 if isinstance(groupby, str):; 123 # if not a list, turn into a list; 124 groupby = [groupby]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/_utils/__init__.py:1100, in raise_not_implemented_error_if_backed_type(X, method_name); 1098 def raise_not_implemented_error_if_backed_type(X: object, method_name: str) -> None:; 1099 if is_backed_type(X):; -> 1100 raise NotImplementedError(; 1101 f""{method_name} is not implemented for matrices of type {type(X)}""; 1102 ). NotImplementedError: dendrogram is not implemented for matrices of type <class 'anndata._core.sparse_dataset.CSRDataset'>; ```. ### Versions. <details>. ```; -----; anndata 0.10.8; scanpy 1.10.2; -----; PIL 10.3.0; asttokens NA; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; debugpy 1.8.1; decorator 5.1.1; executing 2.0.1; h5py 3.11.0; igraph 0.11.4; ipykernel 6.29.4; jedi 0.19.1; joblib 1.4.0; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.8.4; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numpy 1.26.4; packaging 24.0; pandas 2.2.2; parso 0.8.4; platformdirs 4.2.1; prompt_toolkit 3.0.43; psutil 5.9.8; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.2; stack_data 0.6.3; texttable 1.7.0; threadpoolctl 3.5.0; tornado 6.4; traitlets 5.14.3; vscode NA; wcwidth 0.2.13; zmq 26.0.3; -----; IPython 8.24.0; jupyter_client 8.6.1; jupyter_core 5.7.2; -----; Python 3.12.4 (main, Jun 7 2024, 06:33:07) [GCC 14.1.1 20240522]; Linux-6.10.3-zen1-1-zen-x86_64-with-glibc2.40; -----; Session information updated at 2024-08-06 12:18; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:7042,update,updated,7042,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['update'],['updated']
Deployability,s._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_right-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not co,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:14980,pipeline,pipeline,14980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"s.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1887) errors=self.options.get(""encoding_errors"", ""strict""),; [1888](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1888) storage_options=self.options.get(""storage_options"", None),; [1889](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1889) ); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:14261,Pipeline,PipelineDevelope,14261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,s/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous-func1] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_multiple-func3] - ImportError: cannot,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:40070,pipeline,pipeline,40070,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"s://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:769) ); [770](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:770) else:; [771](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:17087,Pipeline,PipelineDevelope,17087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"sError: axis 1 is out of bounds for array of dimension 0; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; certifi 2024.02.02; cffi 1.16.0; charset_normalizer 3.3.2; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.11.0; idna 3.7; igraph 0.11.4; ipykernel 6.29.4; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.4.0; json5 0.9.24; jsonpointer 2.4; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.0; jupyterlab_server 2.26.0; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.8.4; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; numba 0.59.1; numpy 1.26.4; overrides NA; packaging 24.0; pandas 2.2.2; parso 0.8.4; patsy 0.5.6; platformdirs 4.2.0; prometheus_client NA; prompt_toolkit 3.0.43; psutil 5.9.8; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing 3.1.2; pythonjsonlogger NA; pytz 2024.1; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rpds NA; scipy 1.13.0; seaborn 0.13.2; send2trash NA; session_info 1.0.0; six 1.16.0; sklearn 1.4.1.post1; sniffio 1.3.1; stack_data 0.6.3; statsmodels 0.14.1; texttable 1.7.0; threadpoolctl 3.4.0; tornado 6.4; traitlets 5.14.2; uri_template NA; urllib3 2.2.1; wcwidth 0.2.13; webcolors 1.13; websocket 1.7.0; yaml 6.0.1; zmq 25.1.2; -----; IPython 8.23.0; jupyter_client 8.6.1; jupyter_core 5.7.2; jupyterlab 4.1.6; -----; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]; Linux-5.14.0-362.8.1.el9_3.x86_64-x86_64-with-glibc2.34; -----; Session information updated at 2024-04-12 13:17; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3004:4879,update,updated,4879,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3004,1,['update'],['updated']
Deployability,"s\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:13601,Pipeline,PipelineDevelope,13601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,s_general[csr_matrix-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:30083,pipeline,pipeline,30083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20838,pipeline,pipeline,20838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,s_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[continuous_obs-func2] - ImportError: cannot import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_paga.py::test_paga_plots[-paga] ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:39187,pipeline,pipeline,39187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"same here, wish I tried to install earlier....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955:27,install,install,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955,1,['install'],['install']
Deployability,"satisfied: tqdm in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (4.46.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\xkalaj\AppData\Local\Temp\pip-wheel-yjh93oit'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running bdist_wheel; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:1756,install,install,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install']
Deployability,"sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", n_top_genes=3000); error:Please install skmisc package via `pip install --user scikit-misc; I have the same problem as the blogger, so how should I solve it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1518455042:87,install,install,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1518455042,4,['install'],['install']
Deployability,"sc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): #",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5102,install,installed,5102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['installed']
Deployability,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:30,update,updated,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,2,['update'],['updated']
Deployability,scanpy 1.10.0rc1 breaks anndata pre-release tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2878:36,release,release,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2878,1,['release'],['release']
Deployability,scanpy conda installation error,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990:13,install,installation,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990,1,['install'],['installation']
Deployability,"scanpy could just check on import if one of them is installed instead of the package that should be installed, and raise a nicely phrased ImportError.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181:52,install,installed,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181,2,['install'],['installed']
Deployability,scanpy fails to install with python 3.10,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:16,install,install,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,1,['install'],['install']
Deployability,scanpy.pp.neighbors won't recognize an installation of 'pynndescent',MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:39,install,installation,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['install'],['installation']
Deployability,"scanpy/plotting/_tools/scatterplots.py:1197, in _get_palette(adata, values_key, palette); 1195 _utils._set_default_colors_for_categorical_obs(adata, values_key); 1196 else:; -> 1197 _utils._validate_palette(adata, values_key); 1198 return dict(zip(values.categories, adata.uns[color_key])). File ~/mambaforge/envs/new/lib/python3.10/site-packages/scanpy/plotting/_utils.py:357, in _validate_palette(adata, key); 355 _palette.append(color); 356 # Don't modify if nothing changed; --> 357 if _palette is None or np.equal(_palette, adata.uns[color_key]).all():; 358 return; 359 adata.uns[color_key] = _palette. UFuncTypeError: ufunc 'equal' did not contain a loop with signature matching types (<class 'numpy.dtype[str_]'>, <class 'numpy.dtype[str_]'>) -> None; ```. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.9.7; -----; PIL 10.2.0; appnope 0.1.3; asttokens NA; comm 0.2.1; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; exceptiongroup 1.2.0; executing 2.0.1; h5py 3.10.0; ipykernel 6.29.0; jedi 0.19.1; joblib 1.3.2; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numpy 1.23.0; packaging 23.2; pandas 2.2.0; parso 0.8.3; pexpect 4.9.0; platformdirs 4.1.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pynndescent 0.5.11; pyparsing 3.1.1; pytz 2023.3.post1; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; stack_data 0.6.3; threadpoolctl 3.2.0; tornado 6.4; tqdm 4.66.1; traitlets 5.14.1; umap 0.5.5; wcwidth 0.2.13; zmq 25.1.2; zoneinfo NA; -----; IPython 8.20.0; jupyter_client 8.6.0; jupyter_core 5.7.1; -----; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:35:25) [Clang 16.0.6 ]; macOS-14.1.1-arm64-arm-64bit; -----; Session information updated at 2024-01-25 16:44; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830:5643,update,updated,5643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830,1,['update'],['updated']
Deployability,"scent(neighbors['distances']); 352 ; 353 def _init_pca(self, adata):. ~/.local/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_col = np.arange(distances.shape[0])[:, None]; --> 286 init_indices = np.hstack((first_col, np.stack(distances.tolil().rows))); 287 ; 288 self._nnd_idx = NNDescent(. <__array_function__ internals> in stack(*args, **kwargs). ~/.local/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:3508,install,installs,3508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['install'],['installs']
Deployability,scipy mann-whitney U update breaks tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1892:21,update,update,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892,1,['update'],['update']
Deployability,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/927:55,update,updates,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927,1,['update'],['updates']
Deployability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2771,install,installed,2771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['install'],['installed']
Deployability,scverse/scanpy/pull/3330?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `22.44898%` with `38 lines` in your changes missing coverage. Please review.; > Project coverage is 72.25%. Comparing base [(`a70582e`)](https://app.codecov.io/gh/scverse/scanpy/commit/a70582ee03556cf6821eb45148560cb259a5fb34?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`25e7cd4`)](https://app.codecov.io/gh/scverse/scanpy/commit/25e7cd418ac258329944ced2b4ba443d5d06865b?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 103 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3330?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_simple.py](https://app.codecov.io/gh/scverse/scanpy/pull/3330?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_simple.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19zaW1wbGUucHk=) | 22.44% | [38 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3330?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3330 +/- ##; ==========================================; - Coverage 76.27% 72.25% -4.03% ; ==========================================; Files 117 111 -6 ; Lines 12795 12639 -156 ; ==========================================; - Hits 9760 9132 -628 ; - Misses 3035 3507 +472 ; ```. | [Files with mi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3330#issuecomment-2443557729:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330#issuecomment-2443557729,1,['Patch'],['Patch']
Deployability,scvi integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520:5,integrat,integration,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520,1,['integrat'],['integration']
Deployability,"se KeyError(; 1322 ""Passing list-likes to .loc or [] with any missing labels ""; 1323 ""is no longer supported. "". KeyError: ""Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: CategoricalIndex(['1Ery'], categories=['1Ery', '2Ery', '3Ery', '4Ery', '5Ery', '6Ery', '7MEP', '8Mk', ...], ordered=False, name='paul15_clusters', dtype='category'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike""; ```. #### Versions. <details>. sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.6; autoreload NA; backcall 0.2.0; cffi 1.14.5; configobj 5.0.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; git 3.1.14; gitdb 4.0.7; google NA; gpytorch 1.4.1; h5py 3.2.1; igraph 0.9.6; inferelator NA; ipykernel 5.5.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; matplotlib 3.4.1; mpl_toolkits NA; natsort 7.1.1; numba 0.53.1; numexpr 2.7.3; numpy 1.20.2; packaging 20.9; pandas 1.2.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.18; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pytz 2021.1; scanpy 1.8.2; scipy 1.6.3; seaborn 0.11.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.24.2; smmap 4.0.0; statsmodels 0.12.2; storemagic NA; supirfactor NA; tables 3.6.1; texttable 1.6.3; torch 1.9.0+cu102; tornado 6.1; tqdm 4.60.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; wcwidth 0.2.5; zmq 22.0.3; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.3.0; -----; Python 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0]; Linux-5.4.0-91-generic-x86_64-with-glibc2.29; 12 logical CPU cores, x86_64; -----; Session information updated at 2021-12-10 17:16. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2078:7408,update,updated,7408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2078,1,['update'],['updated']
Deployability,se/scanpy/pull/3082?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `85.71429%` with `1 line` in your changes missing coverage. Please review.; > Project coverage is 75.80%. Comparing base [(`b3b9d05`)](https://app.codecov.io/gh/scverse/scanpy/commit/b3b9d0576897a8da5a4ae765b4b0b5609cebc890?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`2709e08`)](https://app.codecov.io/gh/scverse/scanpy/commit/2709e08fe39d3440c904b3cfcb1913611d9bc672?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 39 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3082?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [scanpy/preprocessing/\_highly\_variable\_genes.py](https://app.codecov.io/gh/scverse/scanpy/pull/3082?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_highly_variable_genes.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX2hpZ2hseV92YXJpYWJsZV9nZW5lcy5weQ==) | 85.71% | [1 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3082?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3082 +/- ##; =======================================; Coverage 75.80% 75.80% ; =======================================; Files 110 110 ; Lines 12500 12501 +1 ; =======================================; + Hits 9475 9476 +1 ; Misses 3025 3025 ; ```. | [Files with mi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3082#issuecomment-2141612509:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3082#issuecomment-2141612509,1,['Patch'],['Patch']
Deployability,"sed https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue after pynndescent is imported, but make pynndescent import fast. A kinda simpler solution would be to require pynndescent 0.5.3, import it at the top level (boosts total import time by about ~0.2 seconds out of 3.3 total), and set the threading layer right there. This also has the advantage of letting users specify the threading layer after import scanpy and not having us interfere. This also requires that pynndescent is installed, which hits the reproducibility issues again.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:2529,install,installed,2529,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['install'],['installed']
Deployability,"self._compute_connectivities_v1_2(); 129 elif self._model == 'v1.0':; 130 return self._compute_connectivities_v1_0(). C:\Anaconda\lib\site-packages\scanpy\tools\_paga.py in _compute_connectivities_v1_2(self); 141 g = utils.get_igraph_from_adjacency(ones, directed=True); 142 vc = igraph.VertexClustering(; --> 143 g, membership=self._adata.obs[self._groups_key].cat.codes.values); 144 ns = vc.sizes(); 145 n = sum(ns). C:\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key); 2925 if self.columns.nlevels > 1:; 2926 return self._getitem_multilevel(key); -> 2927 indexer = self.columns.get_loc(key); 2928 if is_integer(indexer):; 2929 indexer = [indexer]. C:\Anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 2657 return self._engine.get_loc(key); 2658 except KeyError:; -> 2659 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2660 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2661 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'louvain'; ```. </details>. I am a beginner in Python, so I fully realise that the background information may not be complete. Below is my system info:; ```; {'commit_hash': 'd774f565b',; 'commit_source': 'installation',; 'default_encoding': 'cp1252',; 'ipython_path': 'C:\\Anaconda\\lib\\site-packages\\IPython',; 'ipython_version': '7.4.0',; 'os_name': 'nt',; 'platform': 'Windows-10-10.0.18362-SP0',; 'sys_executable': 'C:\\Anaconda\\python.exe',; 'sys_platform': 'win32',; 'sys_version': '3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit '; '(AMD64)]'}; ```. Thank you very much for your response. Best regards,; Mikhael",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/755:3214,install,installation,3214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/755,1,['install'],['installation']
Deployability,"sers\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra; return self._compile_bytecode(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode; return self._compile_core(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core; raise e; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core; pm.run(self.state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run; raise patched_exception; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run; self._runPass(idx, pass_inst, state); File ""C:\Us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:4933,pipeline,pipeline,4933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['pipeline'],['pipeline']
Deployability,"sers\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra; return self._compile_bytecode(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode; return self._compile_core(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core; raise e; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core; pm.run(self.state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run; raise patched_exception; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run; self._runPass(idx, pass_inst, state); File ""C:\Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:5002,pipeline,pipeline,5002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['pipeline'],['pipeline']
Deployability,"set_figure_params(figsize=(4, 4), ipython_format=None); ```; then this continues without any ""In :"", and I can continue with script. . I think the reason for this is in the scanpy ""_settings.py"" file:; ```; def set_figure_params(; ......etc.....; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; from matplotlib import rcParams; .....etc......; ```; where the:; ```; IPython.display.set_matplotlib_formats(*ipython_format); ```; produces this ""In :"" message loop. I think this bug could be fixed by adding ""if self._is_run_from_ipython():"" before the ""try: import IPython"": ; ```; def set_figure_params(; ......etc.....; if self._is_run_from_ipython():; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; ```; The ""try:"" ; ```; >>> import IPython; ```; doesn't throw an exception, as IPython is installed:; ```; $ pip show IPython; Name: ipython; Version: 7.18.1; Summary: IPython: Productive Interactive Computing; Home-page: https://ipython.org; Author: The IPython Development Team; Author-email: ipython-dev@python.org; License: BSD; Location: /slipstream/home/sbridgett/miniconda3/lib/python3.8/site-packages; Requires: pygments, jedi, prompt-toolkit, backcall, pexpect, setuptools, decorator, traitlets, pickleshare; Required-by: ipywidgets, ipykernel; ```. whereas the _is_run_from_ipython() will correctly ``return False``, as ``__IPYTHON__`` gives an exception:; ```; def _is_run_from_ipython():; """"""Determines whether run from Ipython.; Only affects progress bars.; """"""; try:; __IPYTHON__; return True; except NameError:; return False; ```. #### Versions. <details>. scanpy.logging.print_versions(); -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.0; anndata 0.7.4; cairo 1.20.0; cffi 1.14.0; colorama 0.4.4; cycler 0.10.0;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477:1814,install,installed,1814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477,1,['install'],['installed']
Deployability,"should not be) in my `adata_sub` object. Thank you for any help/clarification as to what's going on!. #### Versions. <details>; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; certifi 2019.11.28; cffi 1.14.6; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.4.3; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; gi 3.36.0; gio NA; glib NA; gobject NA; gtk NA; h5py 3.4.0; idna 2.8; igraph 0.9.6; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.4; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.11.0; jupyterlab_server 2.8.1; kiwisolver 1.3.2; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; networkx 2.6.3; numba 0.54.0; numexpr 2.7.3; numpy 1.20.0; packaging 21.0; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.22.0; scipy 1.7.1; seaborn 0.11.2; send2trash NA; sitecustomize NA; six 1.14.0; sklearn 0.24.2; sniffio 1.2.0; statsmodels 0.13.0rc0; storemagic NA; tables 3.6.1; terminado 0.12.1; texttable 1.6.4; tornado 6.1; traitlets 5.1.0; typing_extensions NA; umap 0.5.1; urllib3 1.25.8; wcwidth 0.2.5; websocket 1.2.1; zmq 22.3.0; -----; IPython 7.27.0; jupyter_client 7.0.2; jupyter_core 4.7.1; jupyterlab 3.1.11; notebook 6.4.3; -----; Python 3.8.10 (default, Jun 2 2021, 10:49:15) [GCC 9.4.0]; Linux-5.10.25-linuxkit-x86_64-with-glibc2.29; 6 logical CPU cores, x86_64; -----; Session information updated at 2021-09-30 18:02. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2007:3919,update,updated,3919,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2007,1,['update'],['updated']
Deployability,siduals_general[toarray-float32-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:17126,pipeline,pipeline,17126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"sion of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When we calculate `X*X` for variance, we preserve the data type of the incoming `X`, but this actually can cause downstream inaccuracies from overflow differences. This has been the [case for many years](https://github.com/scverse/scanpy/blame/27e5f1fa62ec05ee9db0dfa7a9decbf8f25caa31/scanpy/preprocessing/_utils.py#L10). Really we should do something like `np.multiply(X, X, dtype=""float64)`. This would be more accurate/sensible. This came up in the context of https://github.com/scverse/scanpy/pull/3099/files#diff-afb2fb35cbde7ff5e7d9b79874ede22605918cdba923250dd554f23353702e45R65-R67 where @Intron7 was casting first, and then multiplying (because it should be more accurate), but this revealed that we are _not_ doing this at the moment, despite the fact that it is more accurate. And then downstream analyses can change quite a bit. Thus we should remedy this for the next minor release as it is a breaking change to e.g., https://dev.azure.com/scverse/scanpy/_build/results?buildId=7094&view=logs&j=5ea502cf-d418-510c-3b5f-c4ba606ae534&t=534778bb-2f86-5739-7d3c-59518f7b5a2b&l=2171; <img width=""731"" alt=""Screenshot 2024-06-26 at 16 21 27"" src=""https://github.com/scverse/scanpy/assets/43999641/6c58d0a1-aa44-4f42-87ca-16250d3de641"">. ### Minimal code sample. ```python; import numpy as np; arr = np.random.random((10, 10_000)).astype(""float32""). print(np.multiply(arr, arr, dtype=""float64"")); print(np.multiply(arr, arr)); ```. ### Error output. ```pytb; N/A; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.0rc2.dev74+g1c98fd19; -----; IPython 8.24.0; PIL 10.3.0; asciitree NA; asttokens NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.5.1; dateutil 2.9.0.post0; decorator 5.1.1; defusedxml 0.7.1; distutils 3.12.3; executing 2.0.1; h5py 3.11.0; igraph 0.11.5; jedi 0.19.1; jinja2 3.1.4; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3127:1176,release,release,1176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3127,1,['release'],['release']
Deployability,"sk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `sc.ex.neighbors`, which should return an instance of `sc.Neighbors` (which can then disappear from the root API). Similarly, when `sc.pp.neighbors` is called with `inplace=False`, one should directly get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:2948,pipeline,pipeline,2948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['pipeline'],['pipeline']
Deployability,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1346,install,install,1346,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027,3,['install'],['install']
Deployability,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we don’t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:24,install,installations,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416,3,['install'],"['installation', 'installations', 'installing']"
Deployability,"sort_values(""dispersions_norm"", ascending=False).iloc[:10, :]; print(top10_scanpy[[""means"", ""dispersions"", ""dispersions_norm""]]); ```. ```; mvp.mean mvp.dispersion mvp.dispersion.scaled; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242922 5.806298 7.895355; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598463; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; means dispersions dispersions_norm; index ; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242923 5.806298 7.895356; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598464; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; ```. To generate seurat_hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:3415,patch,patchwork,3415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,1,['patch'],['patchwork']
Deployability,"space, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1026) return _read(filepath_or_buffer, kwds). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:9556,Pipeline,PipelineDevelope,9556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"st file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools for Visual Studio"": https://visualstudio.microsoft.com/downloads/; ----------------------------------------; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Running setup.py install for fa2 ... error. ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:3330,install,install-,3330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['install-']
Deployability,start 0.11 release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2877:11,release,release,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2877,1,['release'],['release']
Deployability,"ster branch of scanpy. ---. I noticed that running the same single-cell analyses on different nodes of our HPC produces different results. ; Starting from the same anndata object with a precomputed `X_scVI` latent representation, the UMAP and leiden-clustering looks different. . On ; * Intel(R) Xeon(R) CPU E5-2699A v4 @ 2.40GHz; * AMD EPYC 7352 24-Core Processor; * Intel(R) Xeon(R) CPU E7-4850 v4 @ 2.10GHz. ![image](https://user-images.githubusercontent.com/7051479/137452257-b88f24fc-bb08-4620-9c1a-98d865ae5956.png); ```python; adata.obs[""leiden""].value_counts(); ```; ```console; 0 4268; 1 2132; 2 1691; 3 1662; 4 1659; 5 1563; ...; ```. On ; * Intel(R) Xeon(R) CPU E7- 4870 @ 2.40GHz. ![image](https://user-images.githubusercontent.com/7051479/137452439-7a094705-6473-4d22-8916-da3139273c6c.png); ```console; 0 3856; 1 2168; 2 2029; 3 1659; 4 1636; 5 1536; ...; ```. ### Minimal code sample (that we can copy&paste without having any data). A git repository with example data, notebook and a nextflow pipeline is available here:; https://github.com/grst/scanpy_reproducibility. A report of the analysis executed on four different CPU architectures is available here:; https://grst.github.io/scanpy_reproducibility/. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; cairo 1.20.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.7; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pytz 2020",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014:1210,pipeline,pipeline,1210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014,1,['pipeline'],['pipeline']
Deployability,"suddenly I have this problem, maybe related to an anndata upgrade. pip says all requirements are satisfied:. scanpy==1.4.3 anndata==0.6.22rc1 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1 ; ```; ; adata ; AnnData object with n_obs × n_vars = 466 × 28685 ; obs: 'GEO_Sample_age', 'age', 'age_unit', 'biosample_source_life_stage', 'biosample_source_gender', 'sample_category', 'biosample_cell_type', 'n_genes', 'n_counts', 'percent_mito'; var: 'n_cells'. fig1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728:58,upgrade,upgrade,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728,1,['upgrade'],['upgrade']
Deployability,"sue has not already been reported.; - [x ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello. I installed scanpy with no errors or warnings. When I tried to import it, I get the error. ```python; import scanpy as sc; ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. path/lib/python3.9/site-packages/scanpy/__init__.py in <module>; 6 from ._utils import check_versions; 7 ; ----> 8 check_versions(); 9 del check_versions, within_flit; 10 . path/lib/python3.9/site-packages/scanpy/_utils/__init__.py in check_versions(); 47 umap_version = pkg_version(""umap-learn""); 48 ; ---> 49 if version.parse(anndata_version) < version.parse('0.6.10'):; 50 from .. import __version__; 51 . path/lib/python3.9/site-packages/packaging/version.py in parse(version); 47 """"""; 48 try:; ---> 49 return Version(version); 50 except InvalidVersion:; 51 return LegacyVersion(version). path/lib/python3.9/site-packages/packaging/version.py in __init__(self, version); 262 ; 263 # Validate the version and parse it into pieces; --> 264 match = self._regex.search(version); 265 if not match:; 266 raise InvalidVersion(f""Invalid version: '{version}'""). TypeError: expected string or bytes-like object. ```. #### Versions. linux. ```. ! cat /etc/os-release. NAME=""CentOS Linux""; VERSION=""7 (Core)""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""7""; PRETTY_NAME=""CentOS Linux 7 (Core)""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:7""; HOME_URL=""https://www.centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-7""; CENTOS_MANTISBT_PROJECT_VERSION=""7""; REDHAT_SUPPORT_PRODUCT=""centos""; REDHAT_SUPPORT_PRODUCT_VERSION=""7"". ```. python. ```; sys.version_info(major=3, minor=9, micro=2, releaselevel='final', serial=0); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2306:1522,release,release,1522,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2306,2,['release'],"['release', 'releaselevel']"
Deployability,"sure these conditions are met. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(; raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True; ). ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.9; scanpy 1.10.3; -----; PIL 10.4.0; asttokens NA; charset_normalizer 3.4.0; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0; debugpy 1.8.5; decorator 5.1.1; executing 2.1.0; h5py 3.11.0; ipykernel 6.29.5; jedi 0.19.1; joblib 1.4.2; kiwisolver 1.4.7; legacy_api_wrap NA; llvmlite 0.43.0; matplotlib 3.9.2; matplotlib_inline 0.1.7; mpl_toolkits NA; natsort 8.4.0; numba 0.60.0; numpy 2.0.2; packaging 24.1; pandas 2.2.3; parso 0.8.4; pickleshare 0.7.5; platformdirs 4.3.6; prompt_toolkit 3.0.47; psutil 6.0.0; pure_eval 0.2.3; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.18.0; pyparsing 3.1.4; pytz 2024.1; scipy 1.14.1; session_info 1.0.0; six 1.16.0; sklearn 1.5.2; stack_data 0.6.2; threadpoolctl 3.5.0; tornado 6.4.1; traitlets 5.14.3; vscode NA; wcwidth 0.2.13; yaml 6.0.2; zmq 26.2.0; -----; IPython 8.27.0; jupyter_client 8.6.3; jupyter_core 5.7.2; -----; Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]; Linux-5.15.0-122-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-10-30 08:33. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3331:1972,update,updated,1972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331,1,['update'],['updated']
Deployability,"t 10 02 44](https://user-images.githubusercontent.com/3297906/109481911-47d78100-7a75-11eb-96c0-b24c1487d0a7.png). I have tried this in a MacOS and two Linux machines with the same error. I have downgraded to `scanpy==1.6` and the error seems to persist. . Any help would be appreciated. . <details>. -----; anndata 0.7.4; scanpy 1.7.1; sinfo 0.3.1; -----; OpenSSL 20.0.1; PIL 8.1.0; anndata 0.7.4; annoy NA; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bbknn NA; brotli NA; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; cryptography 3.3.1; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.0; ipykernel 5.4.2; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.32.1; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.49.1; numexpr 2.7.2; numpy 1.18.2; packaging 20.8; pandas 1.0.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.14; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.7.4; pyparsing 2.4.7; pyrsistent NA; pytz 2020.5; requests 2.23.0; ruamel NA; scanpy 1.7.1; scipy 1.4.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.14.0; sklearn 0.22.2.post1; sniffio 1.2.0; socks 1.7.1; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.3.10; urllib3 1.25.8; wcwidth 0.2.5; yaml 5.3.1; zmq 21.0.1; -----; IPython 7.19.0; jupyter_client 6.1.11; jupyter_core 4.7.0; jupyterlab 3.0.5; notebook 6.2.0; -----; Python 3.8.6 | packaged by conda-forge | (default, Dec 26 2020, 05:05:16) [GCC 9.3.0]; Linux-4.15.0-112-generic-x86_64-with-glibc2.10; 60 logical CPU cores, x86_64; -----; Session information updated at 2021-03-01 09:45. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701:6375,update,updated,6375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701,1,['update'],['updated']
Deployability,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6011,release,release,6011,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,3,"['patch', 'release']","['patch', 'release']"
Deployability,"t force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1099,install,install,1099,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['install'],['install']
Deployability,t import name 'gcd' from 'fractions' (/mnt/workspace/mambaforge/envs/anndata-min-deps-test/lib/python...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.default-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[spatial-na_color.default-na_in_legend.True-leg,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:12168,pipeline,pipeline,12168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"t it keeps saying : ""Please install skmisc package via `pip install --user scikit-misc"", but I have checked pip/conda and it is already installed. . I followed the instructions here:; [#2073](https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340). checked the ouput of ""from skmisc.loess import loess"". . Does anyone know how to solve this problem?. Thanks!. ### Minimal code sample (that we can copy&paste without having any data). ```python; from skmisc.loess import loess; ```. ```pytb; sc.pp.highly_variable_genes(all_exp_merged_bins_ad, n_top_genes=2000, flavor='seurat_v3', span=1.0); File ""/ldfssz1/ST_BI/USER/ywu/.conda/envs/scs/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 419, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""/ldfssz1/ST_BI/USER/ywu/.conda/envs/scs/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip install --user scikit-misc. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/wyu/.local/lib/python3.9/site-packages/skmisc/loess/__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; ImportError: /home/wyu/.local/lib/python3.9/site-packages/skmisc/loess/../../scikit_misc.libs/libopenblasp-r0-085ca80a.3.9.so: cannot read file data; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 9.4.0; anndata 0.7.5; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; google NA; h5py 3.8.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.5.0; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; numba 0.56.4; numexpr 2.8.3; numpy 1.22.4; packaging 23.0; pandas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2441:1284,install,install,1284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2441,2,['install'],['install']
Deployability,"t); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ; ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]); 30 ; 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'; ```. I used Jupyter Notebook. Should I update any package? Thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:2197,update,update,2197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622,1,['update'],['update']
Deployability,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:11658,install,install,11658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,4,"['install', 'upgrade']","['install', 'install-record', 'upgrade']"
Deployability,"t; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cairo 1.23.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; colorlog NA; comm 0.1.4; cupy 12.2.0; cupy_backends NA; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.7.1; dateutil 2.8.2; debugpy 1.6.6; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.7; dot_parser NA; exceptiongroup 1.1.1; executing 1.2.0; fastrlock 0.8.2; google NA; greenlet 2.0.2; h5py 3.10.0; hypergeom_ufunc NA; igraph 0.10.8; importlib_resources NA; iniconfig NA; ipykernel 6.25.2; ipywidgets 8.1.1; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.9.1; llvmlite 0.42.0; markupsafe 2.1.2; matplotlib 3.7.1; mmh3 NA; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.59.1; numexpr 2.8.3; numpy 1.24.4; optuna 3.5.0; packaging 23.0; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.1.1; pluggy 1.4.0; prompt_toolkit 3.0.38; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pyarrow 11.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.14.0; pynvml 11.4.1; pyparsing 3.0.9; pytest 8.1.1; pytz 2022.7.1; scipy 1.9.1; session_info 1.0.0; setuptools 67.6.0; six 1.16.0; sklearn 1.2.2; sparse 0.14.0; sqlalchemy 1.4.46; stack_data 0.6.2; tblib 1.7.0; texttable 1.6.7; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; torch 2.2.2+cu121; torchgen NA; tornado 6.2; tqdm 4.66.1; traitlets 5.9.0; typing_extensions NA; wcwidth 0.2.6; yaml 6.0; zipp NA; zmq 25.0.1; zoneinfo NA; -----; IPython 8.11.0; jupyter_client 8.0.3; jupyter_core 5.3.0; -----; Python 3.9.18 | packaged by conda-forge | (main, Aug 30 2023, 03:49:32) [GCC 12.3.0]; Linux-5.10.192-183.736.amzn2.x86_64-x86_64-with-glibc2.31; -----; Session information updated at 2024-04-03 19:50; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978:3673,update,updated,3673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978,1,['update'],['updated']
Deployability,"t__.py:80: in fn_compatible; return fn(*args_all, **kw); scanpy/preprocessing/_highly_variable_genes.py:651: in highly_variable_genes; df = _highly_variable_genes_single_batch(; scanpy/preprocessing/_highly_variable_genes.py:288: in _highly_variable_genes_single_batch; df[""highly_variable""] = _subset_genes(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . adata = AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score...'pca', 'rank_genes_groups', 'log1p'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'. def _subset_genes(; adata: AnnData,; *,; mean: NDArray[np.float64] | DaskArray,; dispersion_norm: NDArray[np.float64] | DaskArray,; cutoff: _Cutoffs | int,; ) -> NDArray[np.bool_] | DaskArray:; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; if isinstance(cutoff, _Cutoffs):; > dispersion_norm[np.isnan(dispersion_norm)] = 0 # similar to Seurat; E ValueError: assignment destination is read-only. scanpy/preprocessing/_highly_variable_genes.py:365: ValueError; ```. </details>. Dependencies are different, looks like a dask update and a pyarrow added dep. I suspect this has to do with the new dask-expr. ----. I can replicate locally by install the new dask, dask-expr, and pyarrow. ----. Importing dask.dataframe changes the settings for pandas somehow:. ```python; In [1]: import pandas as pd. In [2]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[2]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : True; ALIGNED : True; WRITEBACKIFCOPY : False. In [3]: import dask.dataframe as ddf. In [4]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[4]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : False; ALIGNED : True; WRITEBACKIFCOPY : False; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2902:6238,update,update,6238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2902,2,"['install', 'update']","['install', 'update']"
Deployability,"t_reduce_nodes.<locals>.lookup(var, varonly); 3624 def lookup(var, varonly=True):; -> 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; 3627 return lookup(val). RecursionError: Failed in nopython mode pipeline (step: convert to parfors); maximum recursion depth exceeded while calling a Python object. ```. #### Versions; <details>. -----; anndata 0.8.0; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 9.0.1; anndata 0.8.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 0.8.3; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.9; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.38.0; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.0; numpy 1.21.5; packaging 21.3; pandas 1.4.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.27; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2022.1; scanpy 1.8.2; scipy 1.8.0; seaborn 0.11.2; setuptools 60.10.0; sinfo 0.3.1; six 1.16.0; sklearn 1.0.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.13.2; tables 3.7.0; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.1; tqdm 4.63.1; traitlets 5.1.1; typing_extensions NA; umap 0.5.2; wcwidth 0.2.5; zmq 22.3.0; -----; IPython 8.1.1; jupyter_client 7.1.2; jupyter_core 4.9.2; notebook 6.4.10; -----; Python 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0]; Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31; 8 logical CPU cores, x86_64; -----; Session information updated at 2022-03-24 22:07. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:13585,update,updated,13585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['update'],['updated']
Deployability,"ta object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_cells_by_counts"". Number of cells this expression is ; measu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1485,update,updates,1485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['update'],['updates']
Deployability,ta-min-deps-test/lib/python...; FAILED scanpy/tests/test_paga.py::test_paga_plots[compare_pca-func6] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot3-fn2] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:42507,pipeline,pipeline,42507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ta.raw.X; > v = adata[:, 0 : adata.shape[1] // 2]; > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; > assert v.is_view; > with pytest.warns(Warning, match=""view""):; > > sc.pp.scale(v, flavor=flavor); > ; > scanpy/tests/test_preprocessing.py:127: ; > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1511,pipeline,pipeline,1511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['pipeline'],['pipeline']
Deployability,"ta/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:619) # Create the parser.; --> [620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:620) parser = TextFileReader(filepath_or_buffer, **kwds); [622](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:622) if chunksize or iterator:; [623](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:623) return parser. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds); [1617](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1617) self.options[""has_index_names""] = kwds[""has_index_names""]; [1619](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1619) self.handles: IOHandles | None = None; -> [1620](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1620) self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); [1878](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1878) if ""b"" not in mode:; [1879](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:11948,Pipeline,PipelineDevelope,11948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ta_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; creating build/lib/scanpy/preprocessing; copying scanpy/preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:3365,install,install,3365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['install'],['install']
Deployability,tails><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3058 +/- ##; ==========================================; - Coverage 76.27% 76.08% -0.20% ; ==========================================; Files 117 117 ; Lines 12803 12802 -1 ; ==========================================; - Hits 9766 9740 -26 ; - Misses 3037 3062 +25 ; ```. | [Files](https://app.codecov.io/gh/scverse/scanpy/pull/3058?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Coverage Δ | |; |---|---|---|; | [scanpy/preprocessing/\_scrublet/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L19faW5pdF9fLnB5) | `96.80% <100.00%> (+0.10%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/pipeline.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fpipeline.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3BpcGVsaW5lLnB5) | `94.59% <100.00%> (+0.30%)` | :arrow_up: |; | [scanpy/preprocessing/\_scrublet/sparse\_utils.py](https://app.codecov.io/gh/scverse/scanpy/pull/3058?src=pr&el=tree&filepath=scanpy%2Fpreprocessing%2F_scrublet%2Fsparse_utils.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c2NhbnB5L3ByZXByb2Nlc3NpbmcvX3NjcnVibGV0L3NwYXJzZV91dGlscy5weQ==) | `89.28% <71.42%> (-1.90%)` | :arrow_down: |. ... and [12 files with indirect coverage changes](https://app.codecov.io/gh/scverse/scanpy/pull/3058/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3058#issuecomment-2110286934:1824,pipeline,pipeline,1824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3058#issuecomment-2110286934,1,['pipeline'],['pipeline']
Deployability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:2207,configurat,configuration,2207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,2,['configurat'],['configuration']
Deployability,"talling collected packages: fa2; Running setup.py install for fa2 ... error. ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\xkalaj\AppData\Local\Temp\pip-record-y21s2idm\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\Include\fa2'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running install; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\lib.win-amd64-3.6\fa2; copying fa2\__init__.py -> build\lib.win-amd64-3.6\fa2; running egg_info; writing fa2.egg-info\PKG-INFO; writing dependency_links to fa2.egg-info\dependency_links.txt; writing requirements to fa2.egg-info\requires.txt; writing top-level names to fa2.egg-info\top_level.txt; reading manifest file 'fa2.egg-info\SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'fa2.egg-info\SOURCES.txt'; copying fa2\fa2util.c -> build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.pxd -> build\lib.win-amd64-3.6\fa2; running build_ext; skipping 'fa2\fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; error: Microsoft Visual C++ 14.0 is required. Get it with ""Build Tools",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:3930,install,installed,3930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['install'],['installed']
Deployability,"te-packages/pandas/io/parsers/readers.py:1879) mode += ""b""; -> [1880](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1880) self.handles = get_handle(; [1881](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1881) f,; [1882](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1887) errors=self.options.get(""encoding_errors"", ""strict""),; [1888](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1888) storage_opt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:14024,Pipeline,PipelineDevelope,14024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"te-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:7673,pipeline,pipeline,7673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['pipeline'],['pipeline']
Deployability,"te-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); [1013](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1013) kwds_defaults = _refine_defaults_read(; [1014](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1014) dialect,; [1015](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1015) delimiter,; (...); [1022](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1022) dtype_backend=dtype_backend,; [1023](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1023) ); [1024](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1024) kwds.update(kwds_defaults); -> [1026](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:9361,Pipeline,PipelineDevelope,9361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"te_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2991,integrat,integration,2991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['integrat'],['integration']
Deployability,tedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.off-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_data-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mod,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:35377,pipeline,pipeline,35377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,tep: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_heatmap - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot-fn5] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_var_dict-fn6] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[matrixplot_std_scale_group-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_std_scale_group-fn3] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin_std_scale_var_dict-fn10] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[dotplot_dict-fn4] - AssertionError: Error: Image files di,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:43552,pipeline,pipeline,43552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:24910,pipeline,pipeline,24910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"th k=1 and n=10, it works though n is small. The method is intended for extremely large n / k. The workaround was to rerun failed tests until the fixture randomly returned another solver, which isn’t great. I therefore simply `xfail` that test for now to not block PRs in https://github.com/scverse/scanpy/pull/2745. ### Minimal code sample. ```bash; # try one of the following to reproduce (if necessary multiple times); pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csr-zero_center-valid]'; pytest --runxfail 'scanpy/tests/test_pca.py::test_pca_warnings[scipy_csc-zero_center-valid]'; ```. ### Error output. (note that since the code is run with `warnings.simplefilter('error')`, the below is an error that fails the test). ```pytb; UserWarning: The problem size 5 minus the constraints size 0 is too small relative to the block size 4. Using a dense eigensolver instead of LOBPCG iterations.No output of the history of the iterations.; ```. ### Versions. <details>. ```; -----; anndata 0.10.2; scanpy 1.10.0.dev156+gd1a2c8f8.d20231110; -----; PIL 10.0.1; asciitree NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2023.10.0; dateutil 2.8.2; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jinja2 3.1.2; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.0; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.1; packaging 23.2; pandas 2.1.1; pluggy 1.3.0; psutil 5.9.6; py NA; pyparsing 3.1.1; pytest 7.4.3; pytz 2023.3.post1; scipy 1.11.3; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; six 1.16.0; sklearn 1.3.2; sparse 0.14.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.0; toolz 0.12.0; typing_extensions NA; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; Python 3.11.5 (main, Sep 2 2023, 14:16:33) [GCC 13.2.1 20230801]; Linux-6.6.1-zen1-1-zen-x86_64-with-glibc2.38; -----; Session information updated at 2023-11-10 09:29; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2744:2985,update,updated,2985,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2744,1,['update'],['updated']
Deployability,thank you! the bug was fixed in https://github.com/theislab/scanpy/commit/a4baaaf6c29b8da4f3d9026552719039d2600ca9 and release 0.4.1: `pip install scanpy --upgrade`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104:119,release,release,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63#issuecomment-355768104,3,"['install', 'release', 'upgrade']","['install', 'release', 'upgrade']"
Deployability,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163:24,release,release,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163,1,['release'],['release']
Deployability,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:147,install,install,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671,2,['install'],"['install', 'installed']"
Deployability,"the names in `sc.pl.dendrogram`. If I had more time i would try to track it down for you and maybe even send a PR. Sorry though, I dont. . Can anyone confirm that this is NOT the expected behavior?. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; OpenSSL 19.1.0; PIL 8.0.1; anndata 0.7.5; annoy NA; autoreload NA; backcall 0.2.0; botocore 1.19.22; brotli NA; certifi 2020.11.08; cffi 1.14.3; colorama 0.4.3; cryptography 3.2.1; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; defusedxml 0.6.0; fbpca NA; fsspec 0.8.4; get_version 2.1; h5py 3.1.0; igraph 0.8.3; intervaltree NA; invoke 1.4.1; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; jmespath 0.10.0; joblib 0.17.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.34.0; logzero 1.6.3; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.0.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; packaging 20.4; pandas 1.1.4; parso 0.7.1; pendulum 2.1.2; pexpect 4.8.0; pheno_tools 0.0.1; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pytz 2020.4; pytzdata NA; s3fs 0.4.2; scanorama 1.7; scanpy 1.6.0; scipy 1.5.3; seaborn 0.11.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sortedcontainers 2.3.0; sphinxcontrib NA; statsmodels 0.12.1; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.1; tqdm 4.52.0; traitlets 5.0.5; typing_extensions NA; umap 0.4.6; urllib3 1.25.11; wcwidth 0.2.5; xlrd 1.2.0; yaml 5.3.1; zmq 20.0.0; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.7.0; jupyterlab 2.2.9; notebook 6.1.5; -----; Python 3.8.6 | packaged by conda-forge | (default, Oct 7 2020, 19:08:05) [GCC 7.5.0]; Linux-4.4.0-1106-aws-x86_64-with-glibc2.10; 36 logical CPU cores, x86_64; -----; Session information updated at 2020-12-02 22:22. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1521:3568,update,updated,3568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1521,1,['update'],['updated']
Deployability,"the newest versions of leidenalg and igraph should be compatible with each other. update both, e.g. `pip install -U leidenalg igraph`. `python-igraph` is no longer necessary, you can remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1646862855:82,update,update,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1646862855,2,"['install', 'update']","['install', 'update']"
Deployability,theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.False-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:38051,pipeline,pipeline,38051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689; - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267:640,update,update,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267,1,['update'],['update']
Deployability,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/5:270,install,install,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5,1,['install'],['install']
Deployability,"this makes it possible to use `pip install` without installing numpy. it also includes automation for cython again, as currently the `python setup.py build_ext` command will never use cython, even if available. once the .pyx is changed and `build_ext` is executed, this now refreshes the `.c` file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/38:35,install,install,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/38,2,['install'],"['install', 'installing']"
Deployability,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666:2890,release,release,2890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666,1,['release'],['release']
Deployability,"thon; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; dill 0.3.7; gmpy2 2.1.2; google NA; h5py 3.9.0; igraph 0.11.3; joblib 1.3.2; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.40.1; louvain 0.8.2; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.3; pkg_resources NA; plotly 5.16.1; psutil 5.9.5; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; sympy 1.12; texttable 1.7.0; threadpoolctl 3.2.0; torch 2.0.1; tqdm 4.66.2; typing_extensions NA; wcwidth 0.2.6; yaml 6.0.1; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:41) [Clang 15.0.7 ]; macOS-14.3-arm64-arm-64bit; -----; Session information updated at 2024-06-22 00:24. # Failed case; -----; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; astunparse 1.6.3; cffi 1.16.0; colorama 0.4.6; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0; defusedxml 0.7.1; dill 0.3.8; google NA; h5py 3.11.0; igraph 0.11.5; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.42.0; louvain 0.8.2; matplotlib 3.8.4; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numexpr 2.10.0; numpy 1.26.4; optree 0.11.0; packaging 24.0; pandas 2.2.2; pkg_resources NA; plotly 5.22.0; psutil 5.9.8; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; six 1.16.0; sklearn 1.5.0; texttable 1.7.0; threadpoolctl 3.5.0; torch 2.2.2; torchgen NA; tqdm 4.66.4; typing_extensions NA; wcwidth 0.2.13; yaml 6.0.1; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:10:28) [Clang 15.0.7 ]; macOS-14.4.1-x86_64-i386-64bit; -----; Session information updated at 2024-06-22 00:26. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:16543,update,updated,16543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,2,['update'],['updated']
Deployability,"thor', xlabel='leiden_0.6', condition=None); print(relative_frequencies); sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None); ```. I am getting this output; ```. 0 1 2 3 4 5 \; Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 ; Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 ; Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 ; Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author ; Benitez 0.111111 0.133191 0.021368 0.000000 Benitez ; Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari ; Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari ; Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-44-a5b07a2bfb6d> in <module>; 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None); 7 print(relative_frequencies); ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save); 835 for i, typ in enumerate(reversed(cell_types)):; 836 fig = sb.barplot(; --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize; 838 ); 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643:2047,patch,patches,2047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643,2,"['Patch', 'patch']","['Patch', 'patches']"
Deployability,"throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1013,release,release,1013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['release'],['release']
Deployability,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,integrat,integration,1980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['integrat'],['integration']
Deployability,"to make formatting easier, you should do `pre-commit install` like mentioned in the contributor guide: https://scanpy.readthedocs.io/en/stable/dev/getting-set-up.html#pre-commit. Please also fill out the checklist:. - if there is an issue closed by this, please link it; - you can check the box for tests, since this function already has tests. I added the `benchmark` label so we see that it actually makes things faster; - what’s missing is a release note entry: just edit `1.10.2.md` and add a line there please",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949:53,install,install,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100#issuecomment-2173111949,2,"['install', 'release']","['install', 'release']"
Deployability,"to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14yf1xi/scanpy/setup.py';; running install; running build; running build_py; creating build; creating build/lib.linux-x86_64-3.6; creating build/lib.linux-x86_64-3.6/scanpy; copying scanpy/settings.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/_version.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__init__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/__main__.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/readwrite.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/utils.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/exporting.py -> build/lib.linux-x86_64-3.6/scanpy; copying scanpy/logging.py -> build/lib.linux-x86_64-3.6/scanpy; creating build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/top_genes_visual.py -> build/lib.linux-x86_64-3.6/scanpy/plotting copying scanpy/plotting/rcmod.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/ann_data.py -> build/lib.linux-x86_64-3.6/scanpy/plotting; copying scanpy/plotting/preprocessing.py -> build/lib.linux-x86_64-3.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:2985,install,install,2985,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['install'],['install']
Deployability,"toarray(), mat_neighbor10_pc19_with_pca.toarray())); ```. ```pytb; scanpy= 1.6.0; anndata= 0.7.4; numpy= 1.19.0; adata shape (20, 200); WARNING: You’re trying to run this on 200 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params. Results for `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15)` with and without precomputed pca are different; False. Results of `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=15)` without precomputed pca is same as results of `sc.pp.neighbors(adata, n_neighbors=10, n_pcs=19)` (19 is default `n_comps` for sc.pp.pca) with precomputed pca; True; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; igraph 0.8.3; importlib_metadata 1.7.0; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi 0.17.1; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; matplotlib 3.3.0; mpl_toolkits NA; natsort 7.0.1; numba 0.52.0; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.6.0; scipy 1.5.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; umap 0.4.6; wcwidth 0.2.5; yaml 5.4.1; zipp NA; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.0.3; -----; Python 3.7.2 (default, Jul 16 2019, 22:54:18) [GCC 8.2.0]; Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-centos-7.9.2009-Core; 48 logical CPU cores, x86_64; -----; Session information updated at 2021-12-24 13:42. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2093:4056,update,updated,4056,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2093,1,['update'],['updated']
Deployability,topep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdown 0.11; docutils 0.18.1; entrypoints 0.4; et-xmlfile 1.1.0; executing 0.8.3; fastjsonschema 2.16.2; filelock 3.9.0; flake8 6.0.0; Flask 2.2.2; fonttools 4.25.0; frozenlist 1.3.3; fsspec 2023.4.0; future 0.18.3; gensim 4.3.0; glob2 0.7; gmpy2 2.1.2; greenlet 2.0.1; h5py 3.9.0; HeapDict 1.0.1; holoviews 1.17.1; huggingface-hub 0.15.1; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.10.1; importlib-metadata 6.0.0; incremental 21.3.0; inflection 0.5.1; iniconfig 1.1.1; intake 0.6.8; intervaltree 3.1.0; ipykernel 6.25.0; ipython 8.15.0; ipython-genutils 0.2.0; ipywidgets 8.0.4; isort 5.9.3; itemadapter 0.3.0; itemloaders 1.0.4; itsdangerous 2.0.1; jaraco.classes 3.2.1; jedi 0.18.1; jeepney 0.7.1; jellyfish 1.0.1; Jinja2 3.1.2; jinja2-time 0.2.0; jmespath 0.10.0; joblib 1.2.0; json5 0.9.6; jsonpatch 1.32; jsonpointer 2.1; jsonschema 4.17.3; jupyter 1.0.0; jupy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:2817,patch,patch,2817,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['patch'],['patch']
Deployability,"ts that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/; Preparing metadata (setup.py) ... error; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:4258,install,install-,4258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['install'],['install-']
Deployability,"ts(1, 2); ----> 6 sc.pl.violin(adata2, keys=['CD8A', 'CD8B'], groupby=""group"", ax=axes). File /opt/conda/envs/analysis/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:835, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 833 axs = [ax]; 834 for ax, y, ylab in zip(axs, ys, ylabel):; --> 835 ax = sns.violinplot(; 836 x=x,; 837 y=y,; 838 data=obs_tidy,; 839 order=order,; 840 orient='vertical',; 841 scale=scale,; 842 ax=ax,; 843 **kwds,; 844 ); 845 if stripplot:; 846 ax = sns.stripplot(; 847 x=x,; 848 y=y,; (...); 854 ax=ax,; 855 ). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/_decorators.py:46, in _deprecate_positional_args.<locals>.inner_f(*args, **kwargs); 36 warnings.warn(; 37 ""Pass the following variable{} as {}keyword arg{}: {}. ""; 38 ""From version 0.12, the only valid positional argument ""; (...); 43 FutureWarning; 44 ); 45 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); ---> 46 return f(**kwargs). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:2408, in violinplot(x, y, hue, data, order, hue_order, bw, cut, scale, scale_hue, gridsize, width, inner, split, dodge, orient, linewidth, color, palette, saturation, ax, **kwargs); 2405 if ax is None:; 2406 ax = plt.gca(); -> 2408 plotter.plot(ax); 2409 return ax. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:1043, in _ViolinPlotter.plot(self, ax); 1041 def plot(self, ax):; 1042 """"""Make the violin plot.""""""; -> 1043 self.draw_violins(ax); 1044 self.annotate_axes(ax); 1045 if self.orient == ""h"":. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:761, in _ViolinPlotter.draw_violins(self, ax); 759 def draw_violins(self, ax):; 760 """"""Draw the violins onto `ax`.""""""; --> 761 fill_func = ax.fill_betweenx if self.orient == ""v"" else ax.fill_between; 762 for i, group_data in enumerate(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2136:1898,update,update,1898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2136,1,['update'],['update']
Deployability,"tsize=5, # Adjust font size if needed; frameon=True,; title=[""Provided cell type""],; ); ```. ### Error output. _No response_. ### Versions. -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 10.2.0; absl NA; array_api_compat 1.4.1; attr 23.2.0; chex 0.1.85; contextlib2 NA; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; docrep 0.3.2; etils 1.5.2; exceptiongroup 1.2.0; flax 0.8.1; fsspec 2024.2.0; h5py 3.10.0; igraph 0.10.8; importlib_resources NA; jax 0.4.25; jaxlib 0.4.25; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.2; lightning 2.1.4; lightning_utilities 0.10.1; llvmlite 0.42.0; louvain 0.8.1; markupsafe 2.1.5; matplotlib 3.8.3; ml_collections NA; ml_dtypes 0.3.2; mpl_toolkits NA; mpmath 1.3.0; msgpack 1.0.8; mudata 0.2.3; multipledispatch 0.6.0; natsort 8.4.0; numba 0.59.0; numpy 1.26.4; numpyro 0.14.0; opt_einsum v3.3.0; optax 0.2.1; packaging 23.2; pandas 2.2.1; pkg_resources NA; pygments 2.17.2; pynndescent 0.5.11; pyparsing 3.1.2; pyro 1.9.0; pytz 2024.1; rich NA; rpycall NA; rpytools NA; scipy 1.12.0; scvi 1.1.2; session_info 1.0.0; six 1.16.0; sklearn 1.4.1.post1; skmisc 0.3.1; sympy 1.12; texttable 1.7.0; threadpoolctl 3.3.0; toolz 0.12.1; torch 2.2.1+cu121; torchgen NA; torchmetrics 1.3.1; tqdm 4.66.2; typing_extensions NA; umap 0.5.5; yaml 6.0.1; zipp NA; zoneinfo NA; -----; Python 3.9.18 (main, Jan 4 2024, 00:00:00) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]; Linux-5.14.0-162.23.1.el9_1.x86_64-x86_64-with-glibc2.34; -----; Session information updated at 2024-03-25 15:35; <img width=""1407"" alt=""Screenshot 2024-03-25 at 2 29 35 PM"" src=""https://github.com/scverse/scanpy/assets/163191146/cbaf474f-fc62-4769-b1f8-7d9d10556169"">; <img width=""643"" alt=""Screenshot 2024-03-14 at 10 16 46 PM"" src=""https://github.com/scverse/scanpy/assets/163191146/835d5588-0235-4a9a-8def-4191275314c2"">; <img width=""973"" alt=""Screenshot 2024-03-22 at 2 14 30 AM"" src=""https://github.com/scverse/scanpy/assets/163191146/08fc300d-baef-40e7-a528-a1d4c3c957ce"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2955:2219,update,updated,2219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2955,1,['update'],['updated']
Deployability,"ttps://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1882) mode,; [1883](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1883) encoding=self.options.get(""encoding"", None),; [1884](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1884) compression=self.options.get(""compression"", None),; [1885](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1885) memory_map=self.options.get(""memory_map"", False),; [1886](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1886) is_text=is_text,; [1887](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1887) errors=self.options.get(""encoding_errors"", ""strict""),; [1888](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1888) storage_options=self.options.get(""storage_options"", None),; [1889](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1889) ); [1890](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1890) assert self.handles is not None; [1891](https://fil",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:14497,Pipeline,PipelineDevelope,14497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ttps://user-images.githubusercontent.com/84813314/153453748-b402e8f7-9ac1-4fd8-b8ce-682cd25ea082.png"">. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.4.0; anndata 0.7.8; asttokens NA; attr 21.2.0; backcall 0.2.0; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.0; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; django 4.0; executing 0.8.2; google NA; h5py 2.10.0; idna 3.1; igraph 0.9.8; importlib_resources NA; ipykernel 6.7.0; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jsonschema 4.4.0; jupyter_server 1.13.3; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.0.2; nbformat 5.1.3; numba 0.54.1; numexpr 2.8.0; numpy 1.19.5; packaging 21.3; pandas 1.1.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.24; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.1; pvectorc NA; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.10.0; pyparsing 3.0.6; pyrsistent NA; pytz 2021.3; scanpy 1.8.2; scipy 1.5.3; scprep 1.1.0; seaborn 0.11.2; send2trash NA; setuptools 58.0.4; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; stack_data 0.1.4; statsmodels 0.13.1; tables 3.6.1; terminado 0.12.1; texttable 1.6.4; tornado 6.1; tqdm 4.62.3; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zipp NA; zmq 22.3.0; -----; IPython 8.0.0; jupyter_client 6.1.12; jupyter_core 4.9.1; jupyterlab 3.2.8; notebook 6.4.7; -----; Python 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) [GCC 9.4.0]; Linux-5.4.0-1064-gcp-x86_64-with-glibc2.10; 16 logical CPU cores; -----; Session information updated at 2022-02-10 16:38; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2136:6027,update,updated,6027,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2136,1,['update'],['updated']
Deployability,"typo, `variabes->variables`. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [X] Tests included or not required because:; small change in documentation; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:; minor change in documentation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2793:479,release,release,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2793,2,"['Release', 'release']","['Release', 'release']"
Deployability,uals_general[toarray-float32-200n-100theta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); F,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:19908,pipeline,pipeline,19908,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,uals_general[toarray-int64-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FA,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:20139,pipeline,pipeline,20139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"uces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_reduced(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228:1402,update,updates,1402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228,2,['update'],"['update', 'updates']"
Deployability,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:39,install,install,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609,1,['install'],['install']
Deployability,"ues, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. The arguments I've passed there are whats in the documentation for the function, so I'd figured I'd give them a shot first. I've also tried `host=""www.ensembl.org/biomart""`, but had no such luck. This is after installing the `bioservices` module via `pip` and it creating some config file the first time I tried to run the query.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242:3268,install,installing,3268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242,1,['install'],['installing']
Deployability,"ull this out into the pipeline; --> 442 NativeLowering().run_pass(state); 443 lowered = state['cr']. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 369 metadata=metadata); --> 370 lower.lower(); 371 if not flags.no_cpython_wrapper:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 216 # Materialize LLVM Module; --> 217 self.library.add_ir_module(self.module); 218 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\codegen.py in add_ir_module(self, ir_module); 205 ir = cgutils.normalize_ir_text(str(ir_module)); --> 206 ll_module = ll.parse_assembly(ir); 207 ll_module.name = ir_module.name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\llvmlite\binding\module.py in parse_assembly(llvmir, context); 24 mod.close(); ---> 25 raise RuntimeError(""LLVM IR parsing error\n{0}"".format(errmsg)); 26 return mod. RuntimeError: Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4079:36: error: '%.2747' defined with type 'i64' but expected 'i32'; %"".2748"" = icmp eq i32 %"".2746"", %"".2747""; ^. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-10-a83dc5279093> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata, qc_vars=[""mt""], inplace=True). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 294 inplace=inplace,; 295 X=X,; --> 296 log1p=log1p,; 297 ); 298 var_metrics = describe_var(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 112 if percent_top:; 113 percent_top = sorted(percent_top); --> 114 proportions = top_segment_proportions(X, percent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:4754,pipeline,pipeline,4754,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['pipeline'],['pipeline']
Deployability,unable to install louvain on Windows,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/786:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786,1,['install'],['install']
Deployability,unable to install scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/276:10,install,install,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276,1,['install'],['install']
Deployability,unknown install issue,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190:8,install,install,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190,1,['install'],['install']
Deployability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Integrat,Integration,996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,"['Integrat', 'integrat']","['Integration', 'integrate']"
Deployability,update ValueError message in pca,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/858:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858,1,['update'],['update']
Deployability,update anndata-dev tests to install anndata test deps,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2420:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2420,2,"['install', 'update']","['install', 'update']"
Deployability,update bbknn arguments and docstring,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1868:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868,1,['update'],['update']
Deployability,update bbknn reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/861:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861,1,['update'],['update']
Deployability,update categorical annotation after subsetting,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/69:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/69,1,['update'],['update']
Deployability,update datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1473:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1473,1,['update'],['update']
Deployability,update doc images,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1314:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1314,1,['update'],['update']
Deployability,update documentation for scanpy.pl.rank_genes_group,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1292:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1292,1,['update'],['update']
Deployability,update ecosystem docs to contain scvi-tools,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1421:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421,1,['update'],['update']
Deployability,update ecosystem page,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2417:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2417,1,['update'],['update']
Deployability,"update on this, seems like this is an issue when the package is installed through conda,. reinstalled the package using pip and everything works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280,2,"['install', 'update']","['installed', 'update']"
Deployability,update scvi version to 0.6.5,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1212:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1212,1,['update'],['update']
Deployability,update source field on docs website,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3219:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3219,1,['update'],['update']
Deployability,update tutorials,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2502:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2502,1,['update'],['update']
Deployability,update with spatial functions,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1219:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1219,1,['update'],['update']
Deployability,"update:. managed to get a confidence thresholding with this type of logic:. ```py; def _knn_classify(self, labels):; # ensure it's categorical; cat_array: pd.Series = self._adata_ref.obs[labels].astype(""category""); values = []; confidences = []. for inds in self._indices:; mode_value = cat_array.iloc[inds].mode()[0]; mode_count = (cat_array.iloc[inds] == mode_value).sum(); confidence = mode_count / len(inds); values.append(mode_value); confidences.append(confidence); ; # Create a DataFrame for better readability; classification_df = pd.DataFrame({; ""Mode Values"": values,; ""Confidences"": confidences; }); print(classification_df). return pd.Categorical(values=values, categories=cat_array.cat.categories), np.array(confidences). def map_labels(self, labels, method, confidence_threshold: float = 0.5):; """"""\; Map labels of `adata` to `adata_new`. This function infers `labels` for `adata_new.obs`; from existing labels in `adata.obs`.; `method` can be only 'knn'.; """"""; if method == ""knn"":; classified_labels, confidences = self._knn_classify(labels); mask = confidences >= confidence_threshold; ; filtered_labels = [; label if mask[idx] else np.nan ; for idx, label in enumerate(classified_labels); ]; ; classified_labels = pd.Categorical(; filtered_labels,; categories=classified_labels.categories; ); ; self._adata_new.obs[labels] = classified_labels; self._adata_new.obs[labels + '_confidence'] = confidences; else:; raise NotImplementedError(""Ingest supports knn labeling for now.""); ```; . would love to get input on whether or not this makes sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3160#issuecomment-2270570908:0,update,update,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3160#issuecomment-2270570908,1,['update'],['update']
Deployability,updated Scanpy to support Weighted sampled data to perform clustering…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630:0,update,updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630,1,['update'],['updated']
Deployability,updated documentation to point to latest plotting tutorial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1319:0,update,updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1319,1,['update'],['updated']
Deployability,"updated it to the latest version, I'm still encountering the same error. -----; anndata 0.9.2; scanpy 1.10.2; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 2.1.0; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2024-09-04 17:28",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344:2061,update,updated,2061,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344,1,['update'],['updated']
Deployability,updated read_visium() to read in spaceranger 2.0 files,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2424:0,update,updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2424,1,['update'],['updated']
Deployability,updated test that works with pandas 1.2 #1562,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1584:0,update,updated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584,1,['update'],['updated']
Deployability,updates sparse scale,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942,1,['update'],['updates']
Deployability,updates the function link to `pp.neighbors` for rapids-singlecell,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2664:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2664,1,['update'],['updates']
Deployability,updates:; - [github.com/pre-commit/pre-commit-hooks: v4.0.1 → v4.1.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.0.1...v4.1.0),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2094:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2094,1,['update'],['updates']
Deployability,updates:; - [github.com/psf/black: 21.10b0 → 21.11b1](https://github.com/psf/black/compare/21.10b0...21.11b1),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2056:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2056,1,['update'],['updates']
Deployability,updates:; - [github.com/psf/black: 21.11b1 → 21.12b0](https://github.com/psf/black/compare/21.11b1...21.12b0),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2069:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2069,1,['update'],['updates']
Deployability,updates:; - [github.com/psf/black: 21.9b0 → 21.10b0](https://github.com/psf/black/compare/21.9b0...21.10b0),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2031:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2031,1,['update'],['updates']
Deployability,updates:; - https://github.com/python/black → https://github.com/psf/black; - [github.com/psf/black: 20.8b1 → 21.8b0](https://github.com/psf/black/compare/20.8b1...21.8b0); - https://gitlab.com/pycqa/flake8 → https://github.com/PyCQA/flake8; - [github.com/PyCQA/flake8: 3.8.4 → 3.9.2](https://github.com/PyCQA/flake8/compare/3.8.4...3.9.2); - [github.com/pre-commit/mirrors-autopep8: v1.5.5 → v1.5.7](https://github.com/pre-commit/mirrors-autopep8/compare/v1.5.5...v1.5.7),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1994:0,update,updates,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1994,1,['update'],['updates']
Deployability,upgrade your kali linux with the command ; sudo apt upgrade; pip3 install scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460:0,upgrade,upgrade,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460,3,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"urat_v3` gene selection and sorting by `highly_variable_nbatches` (which should then all be HVG, but they are not). <details>. ```python; import numpy as np; import scanpy as sc; import anndata . import sys; sys.path.append(""scanpy/preprocessing""); from _utils import _get_mean_var. np.random.seed(42); adata = anndata.AnnData(np.random.randint(0,5,(100,100))); adata.obs['batch'] = np.random.randint(0,5,(100)); adata.obs['batch'] = adata.obs['batch'].astype('category'); n_top_genes = 50. adata = adata.copy(); sc.pp.highly_variable_genes(adata,n_top_genes=n_top_genes,flavor='seurat_v3',inplace=True,batch_key='batch'). adata.var.sort_values(['highly_variable_nbatches'], ascending=[False]).iloc[:5,:]; ```. ```pytb; highly_variable highly_variable_rank means variances variances_norm highly_variable_nbatches; 87 True 8.0 1.76 2.446869 1.232373 5; 9 False 28.0 1.96 2.281212 1.159891 5; 78 True 24.0 1.95 2.209596 1.124666 5; 30 True 19.0 2.00 2.202020 1.134560 4; 14 False 25.0 2.14 2.162020 1.088266 4; ```; </details>. #### Versions. <details>. >>> sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.7.1.dev2+g8c469411; sinfo 0.3.1; -----; PIL 8.1.0; anndata 0.7.5; cffi 1.14.4; constants NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; get_version 2.1; google NA; h5py 2.10.0; highs_wrapper NA; joblib 1.0.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; numba 0.52.0; numexpr 2.7.2; numpy 1.19.5; packaging 20.8; pandas 1.2.0; pkg_resources NA; pyparsing 2.4.7; pytz 2020.5; scanpy 1.7.1.dev2+g8c469411; scipy 1.6.0; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.24.0; skmisc 0.1.3; tables 3.6.1; typing_extensions NA; yaml 5.3.1; -----; Python 3.8.0 (default, Oct 28 2019, 16:14:01) [GCC 8.3.0]; Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.27; 40 logical CPU cores, x86_64; -----; Session information updated at 2021-03-10 17:37. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733:3722,update,updated,3722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733,1,['update'],['updated']
Deployability,"url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filename, return_ext=True); --> 491 is_present = check_datafile_present_and_download(; 492 filename,; 493 backup_url=backup_url,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in check_datafile_present_and_download(path, backup_url); 745 path.parent.mkdir(parents=True); 746 ; --> 747 download(backup_url, path); 748 return True; 749 . ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in download(url, path); 722 ; 723 path.parent.mkdir(parents=True, exist_ok=True); --> 724 with tqdm(unit='B', unit_scale=True, miniters=1, desc=path.name) as t:; 725 def update_to(b=1, bsize=1, tsize=None):; 726 if tsize is not None:. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs); 206 unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1; 207 total = self.total * unit_scale if self.total else self.total; --> 208 self.container = self.status_printer(; 209 self.fp, total, self.desc, self.ncols); 210 self.sp = self.display. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols); 101 except NameError:; 102 # #187 #451 #558; --> 103 raise ImportError(; 104 ""FloatProgress not found. Please update jupyter and ipywidgets.""; 105 "" See https://ipywidgets.readthedocs.io/en/stable"". ImportError: FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:3269,update,update,3269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,2,['update'],['update']
Deployability,"use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 118 adata._init_as_actual(adata.copy()); 119 neighbors = Neighbors(adata); --> 120 neighbors.compute_neighbors(; 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 732 X = pairwise_distances(X, metric=metric, **metric_kwds); 733 metric = 'precomputed'; --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(; 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds); 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose); 273 # umap 0.5.0; 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 275 from umap.umap_ import nearest_neighbors; 276 ; 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 1 import pkg_resources; 2 import numba; ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer; 4 ; 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>; 19 import heapq; 20 ; ---> 21 import pynndescent.sparse as sparse; 22 import pynndescent.sparse_nndescent as sparse_nnd; 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>; 341 },; 342 ); --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:2040,install,installed,2040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['install'],['installed']
Deployability,"ustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2789,release,release,2789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['release'],['release']
Deployability,"ut your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | packaged by conda-forge | (default, Jun 1 2020, 18:33:30) ; [Clang 9.0.1 ]; >>> import platform; print(platform.python_implementation()); print(platform.platform()); CPython; Darwin-17.7.0-x86_64-i386-64bit; ```; ...; ```; $ sw_vers; ProductName:	Mac OS X; ProductVersion:	10.13.6; BuildVersion:	17G11023; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:1662,install,install,1662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,2,['install'],"['install', 'installed']"
Deployability,"utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in <module>; 198 ; 199 _ensure_llvm(); --> 200 _ensure_critical_deps(); 201 ; 202 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\.conda\envs\NewPy38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 138 raise ImportError(""Numba needs NumPy 1.18 or greater""); 139 elif numpy_version > (1, 21):; --> 140 raise ImportError(""Numba needs NumPy 1.21 or less""); 141 ; 142 try:. ImportError: Numba needs NumPy 1.21 or less; ```; Step5: I do` !pip uninstall Numpy`, then; ```python; !pip install numpy; Requirement already satisfied: numpy in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (1.21.5). import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). AttributeError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8308/1710492625.py in <module>; 1 import numpy as np; ----> 2 import pandas as pd; 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\pandas\__init__.py in <module>; 20 ; 21 # numpy compat; ---> 22 from pandas.compat import (; 23 np_version_under1p18 as _np_version_under1p18,; 24 is_numpy_dev as _is_numpy_dev,. ~\.conda\envs\NewPy38\lib\site-packages\pandas\compat\__init__.py in <module>; 12 import warnings; 13 ; ---> 14 from pandas._typing import F; 15 from pandas.compat.numpy import (; 16 is_numpy_dev,. ~\.conda\envs\NewPy38",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:7215,install,install,7215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['install'],['install']
Deployability,v.io/gh/scverse/scanpy/pull/3192?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `66.66667%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.61%. Comparing base [(`a60a96f`)](https://app.codecov.io/gh/scverse/scanpy/commit/a60a96fb7790e35abe8d007abd6f5a17b1573d7d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`fca69e6`)](https://app.codecov.io/gh/scverse/scanpy/commit/fca69e6d08e90f86c01a033fa4c8749cc2cb4ea3?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 43 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3192?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/get/get.py](https://app.codecov.io/gh/scverse/scanpy/pull/3192?src=pr&el=tree&filepath=src%2Fscanpy%2Fget%2Fget.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9nZXQvZ2V0LnB5) | 0.00% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3192?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3192 +/- ##; ==========================================; - Coverage 76.61% 76.61% -0.01% ; ==========================================; Files 109 109 ; Lines 12529 12532 +3 ; ==========================================; + Hits 9599 9601 +2 ; - Misses 2930 2931 +1 ; ```. | [Files with missing lines](https://app.codecov.io/gh/scverse/scan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3192#issuecomment-2263491216:1087,Patch,Patch,1087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3192#issuecomment-2263491216,1,['Patch'],['Patch']
Deployability,v.io/gh/scverse/scanpy/pull/3265?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `92.00000%` with `2 lines` in your changes missing coverage. Please review.; > Project coverage is 76.91%. Comparing base [(`7ccf96d`)](https://app.codecov.io/gh/scverse/scanpy/commit/7ccf96d4b6ac10f0a718010578f725e3de2902f7?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`bd99e6d`)](https://app.codecov.io/gh/scverse/scanpy/commit/bd99e6de4235131acaf426f62649a929db32c699?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on 1.10.x. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3265?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/\_compat.py](https://app.codecov.io/gh/scverse/scanpy/pull/3265?src=pr&el=tree&filepath=src%2Fscanpy%2F_compat.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9fY29tcGF0LnB5) | 81.81% | [2 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3265?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## 1.10.x #3265 +/- ##; ==========================================; - Coverage 76.93% 76.91% -0.02% ; ==========================================; Files 109 109 ; Lines 12454 12451 -3 ; ==========================================; - Hits 9581 9577 -4 ; - Misses 2873 2874 +1 ; ```. | [Flag](https://app.codecov.io/gh/scverse/scanpy/pull/3265/flags?s,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3265#issuecomment-2377150033:1086,Patch,Patch,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3265#issuecomment-2377150033,1,['Patch'],['Patch']
Deployability,"value_counts(); ```; ```console; 0 4268; 1 2132; 2 1691; 3 1662; 4 1659; 5 1563; ...; ```. On ; * Intel(R) Xeon(R) CPU E7- 4870 @ 2.40GHz. ![image](https://user-images.githubusercontent.com/7051479/137452439-7a094705-6473-4d22-8916-da3139273c6c.png); ```console; 0 3856; 1 2168; 2 2029; 3 1659; 4 1636; 5 1536; ...; ```. ### Minimal code sample (that we can copy&paste without having any data). A git repository with example data, notebook and a nextflow pipeline is available here:; https://github.com/grst/scanpy_reproducibility. A report of the analysis executed on four different CPU architectures is available here:; https://grst.github.io/scanpy_reproducibility/. #### Versions. <details>. ```; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; cairo 1.20.0; cffi 1.14.4; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.7; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.2; pyparsing 2.4.7; pytz 2020.4; scanpy 1.6.0; scipy 1.5.3; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; tornado 6.1; traitlets 5.0.5; umap 0.4.6; wcwidth 0.2.5; yaml 5.3.1; zmq 20.0.0; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.7.0; -----; Python 3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) [GCC 9.3.0]; Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-10-15 09:58; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014:2711,update,updated,2711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014,1,['update'],['updated']
Deployability,variable_genes_pearson_residuals_batch[toarray-int64-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-200n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:25378,pipeline,pipeline,25378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,verse/scanpy/pull/3267?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) Report; Attention: Patch coverage is `95.16129%` with `6 lines` in your changes missing coverage. Please review.; > Project coverage is 77.03%. Comparing base [(`bbcd4b1`)](https://app.codecov.io/gh/scverse/scanpy/commit/bbcd4b173aabebb8b4793cf2cdd6ea8b31e31005?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) to head [(`7516acc`)](https://app.codecov.io/gh/scverse/scanpy/commit/7516acc4474f54b86cc7a880e3508a20b8a40169?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse).; > Report is 1 commits behind head on main. | [Files with missing lines](https://app.codecov.io/gh/scverse/scanpy/pull/3267?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) | Patch % | Lines |; |---|---|---|; | [src/scanpy/preprocessing/\_pca/\_\_init\_\_.py](https://app.codecov.io/gh/scverse/scanpy/pull/3267?src=pr&el=tree&filepath=src%2Fscanpy%2Fpreprocessing%2F_pca%2F__init__.py&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse#diff-c3JjL3NjYW5weS9wcmVwcm9jZXNzaW5nL19wY2EvX19pbml0X18ucHk=) | 93.18% | [6 Missing :warning: ](https://app.codecov.io/gh/scverse/scanpy/pull/3267?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scverse) |. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## main #3267 +/- ##; ==========================================; + Coverage 76.95% 77.03% +0.08% ; ==========================================; Files 109 110 +1 ; Lines 12465 12492 +27 ; ==========================================; + Hits 9592 9623 +31 ; + Misses 2873 2869 -4 ; ```. ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267#issuecomment-2378904712:1084,Patch,Patch,1084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267#issuecomment-2378904712,1,['Patch'],['Patch']
Deployability,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928:200,update,update,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928,1,['update'],['update']
Deployability,we should update the tutorials and notebooks to use ` leiden` instead of `louvain`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-482127774:10,update,update,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-482127774,1,['update'],['update']
Deployability,"white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pytz 2023.3.post1; ruamel NA; scipy 1.11.3; session_info 1.0.0; setuptools 68.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.3.0; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.1; threadpoolctl 3.1.0; tornado 6.3.3; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; wcwidth 0.2.5; yaml 6.0.1; zmq 23.2.0; zoneinfo NA; zstandard 0.19.0; -----; IPython 8.12.0; jupyter_client 7.4.9; jupyter_core 5.5.0; notebook 6.5.4; -----; Python 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]; Linux-6.2.0-37-generic-x86_64-with-glibc2.35; -----; Session information updated at 2024-01-23 21:45. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:2663,update,updated,2663,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,1,['update'],['updated']
Deployability,why umap showing bbknn perfectively integrated but tsne,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370:36,integrat,integrated,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370,1,['integrat'],['integrated']
Deployability,"working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1313,install,install,1313,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['install'],['install']
Deployability,"would also make sense to have this as `colorbar_loc` as this only really applies for continuous coloring, right? I can definetly see the benefit of removing colorbar from figure if this is wanted by the user",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825:85,continuous,continuous,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825,1,['continuous'],['continuous']
Deployability,y-float32-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.3] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_data-groups.all] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-100n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-inftheta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_missing_values_categorical[pca-na_color.black_tup-na_in_legend.True-legend.on_right-groups.3] - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-100n-100theta-noclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-30clip-subset] - NotImplementedError: Failed in nopython mode p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:31031,pipeline,pipeline,31031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"y48/lib/site-packages/numba/core/compiler.py?line=495) assert self.state.func_ir is None; --> [497](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=496) return self._compile_core(). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:476, in CompilerBase._compile_core(self); [474](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=473) self.state.status.fail_reason = e; [475](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=474) if is_final_pipeline:; --> [476](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=475) raise e; [477](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=476) else:; [478](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=477) raise CompilerError(""All available pipelines exhausted""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler.py:463, in CompilerBase._compile_core(self); [461](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=460) res = None; [462](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=461) try:; --> [463](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=462) pm.run(self.state); [464](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=463) if self.state.cr is not None:; [465](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler.py?line=464) break. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:353, in PassManager.run(self, state); [350](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:21428,pipeline,pipelines,21428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['pipeline'],['pipelines']
Deployability,"y:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcore 0.2.0; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; frozendict 2.3.8; h5py 3.9.0; ikarus NA; importlib_resources NA; ipykernel 6.24.0; ipython_genutils 0.2.0; jedi 0.18.2; jinja2 3.1.2; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; packaging 23.1; pandas 2.0.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.1; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pyparsing 3.0.9; pyscenic 0.12.1; pytz 2023.3; scipy 1.11.1; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; stack_data 0.6.2; tblib 2.0.0; threadpoolctl 3.2.0; tlz 0.12.1; toolz 0.12.0; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; wcwidth 0.2.6; yaml 6.0; zipp NA; zmq 25.1.0; zoneinfo NA; -----; IPython 8.14.0; jupyter_client 8.3.0; jupyter_core 5.3.1; notebook 6.5.4; -----; Python 3.9.13 (main, Aug 25 2022, 18:29:29) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2023-07-21 20:08; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:4519,update,updated,4519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['update'],['updated']
Deployability,"y:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-pack",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:3495,Pipeline,PipelineDevelope,3495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"y:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Pytho",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:3693,Pipeline,PipelineDevelope,3693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,y::test_dotplot_matrixplot_stacked_violin[dotplot-fn0] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_plotting.py::test_violin_without_raw - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_paga.py::test_paga_path - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[pca] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_embedding_plots.py::test_enumerated_palettes[spatial] - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_dimension_broadcasting - TypeError: map() got an unexpected keyword argument 'na_action'; FAILED scanpy/tests/test_embedding_plots.py::test_marker_broadcasting - numpy.core._exceptions._UFuncNoLoopError: ufunc 'equal' did not contain a loop with signature matching types (<class 'n...; FAILED scanpy/tests/test_plotting.py::test_rank_genes_groups[ranked_genes_dotplot_gene_names-fn7] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_dotplot_matrixplot_stacked_violin[stacked_violin-fn9] - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_plotting.py::test_correlation - AssertionError: Error: Image files did not match.; FAILED scanpy/tests/test_paga.py::test_paga_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:45595,pipeline,pipeline,45595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,y_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-200n-inftheta-30clip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[toarray-int64-100n-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_batch[csr_matrix-float32-100n-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_inputchecks[toarray-float32] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[csr_matrix-int64-200n-100theta-infclip-full] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_highly_variable_genes.py::test_highly_variable_genes_pearson_residuals_general[toarray-float32-100n-100theta-noclip-subset] - NotImplementedError: Failed in nopython mode pipeline (step: analyzing bytecode); FAILED scanpy/tests/test_high,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:23120,pipeline,pipeline,23120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['pipeline'],['pipeline']
Deployability,"ycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; decoupler 1.4.0; defusedxml 0.7.1; dill 0.3.5.1; executing 1.2.0; fastjsonschema NA; fqdn NA; gseapy 1.0.5; h5py 3.9.0; idna 3.4; igraph 0.10.6; ipykernel 6.25.0; isoduration NA; jedi 0.19.0; jinja2 3.1.2; joblib 1.3.1; json5 NA; jsonpointer 2.4; jsonschema 4.18.4; jsonschema_specifications NA; jupyter_events 0.7.0; jupyter_server 2.7.0; jupyterlab_server 2.24.0; kiwisolver 1.4.4; leidenalg 0.10.1; liana 0.1.9; llvmlite 0.40.1; markupsafe 2.1.3; matplotlib 3.6.3; matplotlib_inline 0.1.6; mizani 0.9.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numpy 1.24.4; overrides NA; packaging 23.1; pandas 1.5.3; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotnine 0.12.2; prometheus_client NA; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pycparser 2.21; pydeseq2 0.3.5; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.1.1; pythonjsonlogger NA; pytz 2023.3; referencing NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rnaxplorer NA; rpds NA; scipy 1.11.1; seaborn 0.12.2; send2trash NA; session_info 1.0.0; sitecustomize NA; six 1.16.0; sklearn 1.3.0; sniffio 1.3.0; sphinxcontrib NA; stack_data 0.6.2; statsmodels 0.14.0; texttable 1.6.7; threadpoolctl 3.2.0; torch 1.13.1+cu117; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; uri_template NA; urllib3 2.0.4; wcwidth 0.2.6; webcolors 1.13; websocket 1.6.1; yaml 6.0.1; zmq 25.1.0; zoneinfo NA; -----; IPython 8.14.0; jupyter_client 8.3.0; jupyter_core 5.3.1; jupyterlab 4.0.3; notebook 7.0.1; -----; Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]; Linux-5.15.0-1033-gke-x86_64-with-glibc2.35; -----; Session information updated at 2023-08-21 12:14. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629:5615,update,updated,5615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629,1,['update'],['updated']
Deployability,"yeah, this was fixed inside of anndata, so you need to update anndata, not scanpy. `pip install anndata~=0.6.4` should get you a version where this is fixed! 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433:55,update,update,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/160#issuecomment-392088433,2,"['install', 'update']","['install', 'update']"
Deployability,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,integrat,integrate,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573,1,['integrat'],['integrate']
Deployability,"yes, apparently the problem is associated with these 'nan' values. I assumed this has to do with a package update, because it was working before. I rolled back to scanpy 1.7.1 and this block is working fine. The 'nan's are just omitted in the plots. I appreciate you offered to give a code example as my python skills are limited. But I figured a workaround so you don't have to bother with it. . I won't close the issue yet because I assume this behavior was not intended with the update.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437:107,update,update,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437,2,['update'],['update']
Deployability,"ynndescent==0.5.8; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.1.2; matplotlib 3.5.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.6; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.8; pyparsing 3.0.9; pythoncom NA; pytz 2022.1; pywintypes NA; ruamel NA; scipy 1.9.1; seaborn 0.11.2; session_info 1.0.0; setuptools 63.4.1; setuptools_scm NA; six 1.16.0; sklearn 1.0.2; snappy NA; sphinxcontrib NA; statsmodels 0.13.2; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.2; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; typing_extensions NA; umap 0.5.3; wcwidth 0.2.5; win32api NA; win32com NA; win32security NA; yaml 5.4.1; zipp NA; zmq 23.2.0; zope NA; -----; IPython 7.31.1; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.19045-SP0; -----; Session information updated at 2023-02-24 14:04; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2430:6219,update,updated,6219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430,1,['update'],['updated']
Deployability,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1592,install,installed,1592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,['install'],['installed']
Deployability,"ypi_0 pypi; torchmetrics 1.0.1 pypi_0 pypi; torchvision 0.15.2+cu118 pypi_0 pypi; tornado 6.3.2 py311h459d7ec_0 conda-forge; tqdm 4.65.0 pyhd8ed1ab_1 conda-forge; traitlets 5.9.0 pyhd8ed1ab_0 conda-forge; triton 2.0.0 pypi_0 pypi; typing-extensions 4.7.1 hd8ed1ab_0 conda-forge; typing_extensions 4.7.1 pyha770c72_0 conda-forge; tzdata 2023.3 pypi_0 pypi; umap-learn 0.5.3 pypi_0 pypi; urllib3 1.26.13 pypi_0 pypi; uvicorn 0.23.1 pypi_0 pypi; wcwidth 0.2.6 pyhd8ed1ab_0 conda-forge; websocket-client 1.6.1 pypi_0 pypi; websockets 11.0.3 pypi_0 pypi; wheel 0.41.0 pyhd8ed1ab_0 conda-forge; widgetsnbextension 4.0.8 pyhd8ed1ab_0 conda-forge; xarray 2023.7.0 pypi_0 pypi; xz 5.2.6 h166bdaf_0 conda-forge; yamlordereddictloader 0.4.0 pypi_0 pypi; yarl 1.9.2 pypi_0 pypi; zeromq 4.3.4 h9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; bro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:8418,install,install,8418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,3,['install'],['install']
Deployability,"ysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2332,pipeline,pipeline,2332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['pipeline'],['pipeline']
Deployability,"ython/Python312/site-packages/pandas/io/parsers/readers.py:1890) assert self.handles is not None; [1891](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/parsers/readers.py:1891) f = self.handles.handle. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:761) if compression == ""gzip"":; [762](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:16386,Pipeline,PipelineDevelope,16386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ython/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDeve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6811,Pipeline,PipelineDevelope,6811,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ython/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:596) header=None,; [597](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:597) sep=""\t"",; [598](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:598) ); [599](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:599) if var_names == ""gene_symbols"":; [600](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/App",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:7211,Pipeline,PipelineDevelope,7211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['Pipeline'],['PipelineDevelope']
Deployability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1930,release,release,1930,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,"['release', 'update']","['release', 'update']"
Deployability,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4886,install,install,4886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575,1,['install'],['install']
Deployability,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-517982303:215,update,updated,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-517982303,1,['update'],['updated']
Deployability,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1679,install,installing,1679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['install'],['installing']
Deployability,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:13,install,installs,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242,5,['install'],"['install', 'installs']"
Deployability,… if not installed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1524:9,install,installed,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1524,1,['install'],['installed']
Deployability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3796,pipeline,pipeline,3796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['pipeline'],['pipeline']
Energy Efficiency, 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdown 0.11; docutils 0.18.1; entrypoints 0.4; et-xmlfile 1.1.0; executing 0.8.3; fastjsonschema 2.16.2; filelock 3.9.0; flake8 6.0.0; Flask 2.2.2; fonttools 4.25.0; frozenlist 1.3.3; fsspec 2023.4.0; future 0.18.3; gensim 4.3.0; glob2 0.7; gmpy2 2.1.2; greenlet 2.0.1; h5py 3.9.0; HeapDict 1.0.1; holoviews 1.17.1; huggingface-hub 0.15.1; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.10.1; importlib-metadata 6.0.0; incremental 21.3.0; inflection 0.5.1; iniconfig 1.1.1; intake 0.6.8; intervaltree 3.1.0; ipykernel 6.25.0; ipython 8.15.0; ipython-genutils 0.2.0; ipywidgets 8.0.4; isort 5.9.3; itemadapter 0.3.0; itemloaders 1.0.4; itsdangerous 2.0.1; jaraco.classes 3.2.1; jedi 0.18.1; jeepney 0.7.1; jellyfish 1.0.1; Jinja2 3.1.2; jinja2-time 0.2.0; jmespath 0.10.0; joblib 1.2.0; json5 0.9.6; jsonpatch 1.32; jsonpointer 2.1; jsonschema 4.17.3; jupyter 1.0.0; jupyter_client 7.4.9; jupyter-console 6.6.3; jupyter_core 5.3.0; jupyter-events 0.6.3; jupyter-server 1.23.4; jupyter_server_fileid 0.9.0; jupyter_server_ydoc 0.8.0; jupyter-ydoc 0.2.4; jupyterlab 3.6.3; jupyterlab-pygments 0.1.2; jupyterlab_server 2.22.0; jupyterlab-widgets 3.0.5; kaleido 0.2.1; keyring 23.13.1; kiwisol,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:3136,green,greenlet,3136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['green'],['greenlet']
Energy Efficiency," :smile:. Re diffxpy: If you say that diffxpy has a good solution, why should we build a new one? Can't we just use their solution?. > I think there are also two separate problems here, which are ""what's a better way to store differential expression results"" and ""what's a good API for differential expression"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1180,efficient,efficient,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaptive']
Energy Efficiency," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaption']
Energy Efficiency," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1203,power,power,1203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['power'],['power']
Energy Efficiency," get a `Neighbors` object returned. Now, we can apply this logic to every single function that doesn't have a simple return value. Upon calling the function with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3639,efficient,efficient,3639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['efficient'],['efficient']
Energy Efficiency," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1159,efficient,efficient,1159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['efficient'],['efficient']
Energy Efficiency,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:640,Efficient,Efficient,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,1,['Efficient'],['Efficient']
Energy Efficiency,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi!. I am new to scanpy and I am facing some trouble reading my data in an appropriate way. I noticed that anndata objects in memory require roughly 4x the space they require on disk, so working with large datasets (>50GB on disk) is prohibitive in most scenarios. The USP of h5 files, however, is that you can index and slice them on disk as if they were in memory. This way I could greatly reduce the data size before loading it into memory. However, when I attempt to filter on a backed anndata object, I encounter a TypeError. The case of gene filtering should be just a column-sum, comparing it against a threshold and then saving it as a boolean index mask. It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:681,reduce,reduce,681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['reduce'],['reduce']
Energy Efficiency,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi there , I encountered a problem while using sc.read_h5ad to read the ""ReplogleWeissman2022_K562_essential.h5ad ""datasets . At first I supposed it is an AnnDataRead error: Above error raised while reading key '/X' of type <class 'h5py._hl.dataset.Dataset'> from /. ; But now I believe the main reason is MemoryError : Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32 . However, I am running this on a server with about 100 GB of memory , this problem really bothers me , could you please help me with that? Thank you very much !. ### Minimal code sample. ```python; import anndata; import pandas as pd; import scanpy as sc; annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""); ```. ### Error output. ```pytb; MemoryError Traceback (most recent call last); File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:202, in report_read_key_on_error.<locals>.func_wrapper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.regist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:621,allocate,allocate,621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['allocate'],['allocate']
Energy Efficiency,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When trying to compute neighbors with default settings, I'm getting errors in slurm jobs, as the code tries to allocate all available cores on a node. I tried to control the number of cores available by setting `scanpy._settings.ScanpyConfig.n_jobs`, however I found that this setting does not get used in the offending function `compute_neighbors_umap`, where the following gets called. https://github.com/scverse/scanpy/blob/9cab8cfa4033d3f47a36c7bb816b2c9fae5cfdc6/scanpy/neighbors/__init__.py#L314-L322. The `nearest_neighbors` function from the `umap` provides the parameter `n_jobs`, which is set to -1 (i.e. all cores) by default. https://github.com/lmcinnes/umap/blob/e0cff20b392192f2b62fcc1843c9cb9ca2c9ee27/umap/umap_.py#L255-L266. However, as a user I would expect the number of cores to be defined by the ScanpyConfig.; I assume this could also cause the kernel dying unintentionally in other work setups when users aren't aware of the number of cores the function is requesting. ### Minimal code sample. ```python; import scanpy as sc; sc._settings.ScanpyConfig(n_jobs=1). ad = sc.read('quite_large_h5ad_file'); sc.pp.pca(ad); sc.pp.neighbors(ad); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; RuntimeError Traceback (most recent call last); File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:212, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context); 211 try:; --> 212 self._repopulate_pool(); 213 except Exception:. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:303, in Pool._repopulate_pool(self); 302 def _repopulate_pool(self):; --> 303 return sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2506:402,allocate,allocate,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506,1,['allocate'],['allocate']
Energy Efficiency,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:467,efficient,efficiently,467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,2,"['Reduce', 'efficient']","['Reduced', 'efficiently']"
Energy Efficiency,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. @Intron7, found a use case 😆. It could be nice for `sc.get.aggregate` to be able to return sparse matrices where we don't expect the aggregation to return very dense data. Previously discussed in:. * https://github.com/scverse/scanpy/issues/2892. Usecases include:. * Taking the `max` for multiple reports of a genes (`sc.get.aggregate(adata, ""probe_target"", ""max"")`, e.g. https://discourse.scverse.org/t/merging-identical-genes-from-10x-fixed-scrna/2142); * *(note: max is not currently implemented)*; * Small aggregations, e.g. only summing neighbors. This would require both api design choices for what the argument is called, and efficient implementations for both dense and sparse results (`python-graphblas` could be useful here)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2898:796,efficient,efficient,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2898,1,['efficient'],['efficient']
Energy Efficiency,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. I am using `regress_out` and it is painfully slow. Even on a system where I set `n_jobs=36` and `sc.settings.n_jobs = 36`, each core of which has 36Gib of memory, I find that . ```; sc.pp.regress_out(adata, ['total_counts', ], n_jobs=n_jobs); ```. is practically unusable. At the moment that calculation is at `985` minutes. Looking at `htop` while the memory is certainly allocated (`191 Gib / 1.48Tb`), it _feels_ like setting `n_jobs` at all actually hinders performance.... I've checked the [source code](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L580) so I know that [`n_jobs`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L631C5-L631C55); should be set correctly. ```python; n_jobs = sett.n_jobs if n_jobs is None else n_jobs # NOTE: sett.n_jobs defaults to 1. # ... res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```. Looking at [`_regress_out_chunk`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L692), there really doesn't seem to be anything necessarily bottlenecking the performance except for either setting the [chunk length](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L662C5-L662C14) in `regress_out`, or just the size of the dataset... ```python; len_chunk = np.ceil(min(1000, X.shape[1]) / n_jobs).astype(int); ```. Am I missing something?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781:535,allocate,allocated,535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781,1,['allocate'],['allocated']
Energy Efficiency,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. It would be extremely helpful if the embedding manifold tools had scikit-learn style API. . For example, https://pydiffmap.readthedocs.io/en/master/reference/diffusion_map.html. Having the .fit, .transform, and .fit_transform would make the robust implementations in the backend of ScanPy a lot more accessible for users. Right now, the usage feels a bit restrictive and I'm having difficulty leveraging the power of the methods if it's not part of some similar workflow that is in the tutorials. . I'm trying to use the code in the backend of ScanPy implement this API myself but ScanPy is an extremely confusing package from an outside developer. There are nested functions and tests for even simple steps (many of which handle edge cases making the package robust). More specifically, I'm trying to use the ScanPy implementation of Diffusion Maps as I would use those from pyDiffMap or the spectral clustering from Sklearn. I would like to be able to fit a model with data. Pickle it. Then transform new samples based on the fitted model. This would provide a useful interface for users looking for a non linear alternative to pca.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054:570,power,power,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054,1,['power'],['power']
Energy Efficiency,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. currently `pp.scale` with a `mask_obs` with a sparse matrix and with `zero_center== False` takes a really long time to update the sparse matrix. This also takes up a lot of memory because of the parity calculations. I would suggest a numba kernel that just swaps out the data. This works really well for rapids-singlecell and greatly improves performance and reduces the memory overhead.; I would open a PR with this kernel. ------; Performance for 90k cells and 25k genes:; without mask:; CPU 645 ms | GPU 37 ms | 20x; with mask:; CPU 22 s | GPU 50 ms | 460x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2941:521,reduce,reduces,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2941,1,['reduce'],['reduces']
Energy Efficiency,### What kind of feature would you like to request?. Additional function parameters. ### Please describe your wishes. Hello:. Is it possible to borrow the `seaborn.objects.Jitter` function to reduce the overlapping dots in `scanpy.pl.umap`?; [https://seaborn.pydata.org/generated/seaborn.objects.Jitter.html](https://seaborn.pydata.org/generated/seaborn.objects.Jitter.html); Thanks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2768:192,reduce,reduce,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2768,1,['reduce'],['reduce']
Energy Efficiency,### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I'm currently implementing a function that takes in an anndata and then subsamples a given representation using https://github.com/jmschrei/apricot. This generally serves the purpose of semi-optimally picking a reduced number of points that's still representative of the latent space. . Is this sth within the scope of scanpy? When it's done it wouldn't be too much effort to polish it up for a PR. The dependency load seems fairly low.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:391,reduce,reduced,391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['reduce'],['reduced']
Energy Efficiency,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,adapt,adapted,937,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['adapt'],['adapted']
Energy Efficiency,"#2733 bumps the minimum version **up** to 3.6 and adds two good reasons why that exact minimum version requirement is chosen:. 1. matplotlib’s maintenance schedule drops support for 3.5 in a little bit; 2. we use a feature from 3.5. If some piece of infrastructure doesn’t allow bumping versions of client libraries, that piece of infrastructure needs improvement. Jupyter allows distinct environments for kernels and the server for a reason. So while colab should be able to pin and use whatever they need on the server, there’s zero reason why colab should dictate any version of anything other than `ipykernel` for the user environment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2413#issuecomment-1801466509:155,schedul,schedule,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2413#issuecomment-1801466509,1,['schedul'],['schedule']
Energy Efficiency,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622:204,reduce,reduce,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622,3,"['adapt', 'reduce']","['adapted', 'reduce', 'reduces']"
Energy Efficiency,"); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086688-66a3af40-ecc1-4b2e-a3e4-8038f7f207b0.png). ```; #make purple colormap; colors2 = plt.cm.Purples(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap4 = mymap(np.arange(mymap.N)); my_cmap4[:,-1] = np.linspace(0, 1, mymap.N); my_cmap4 = colors.ListedColormap(my_cmap4). sc.pl.umap(adata, color=['VWF'], use_raw=True, color_map=my_cmap4, show=False, frameon=False, vmax=5); ```; ![image](https://user-images.githubusercontent.com/56206488/126086713-d8b24873-f3ea-4067-8573-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:1635,Green,Greens,1635,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['Green'],['Greens']
Energy Efficiency,"**Most importantly**: Other than stated in the docs, the default for `zero_center` is `True` and has been `True` since I believe, July 2017 (very early Scanpy, maybe 0.2..). @VolkerBergen: I concur. One should obtain the same representation independent of the data type and that's what the default behavior of the function should give you. @Koncopd: One can also talk about a proper implementation of PCA for sparse data, which I thought would require quite some custom code. Your solution seems like a really good solution if randomiced_svd is able to treat that lazy evaluation object efficiently. @VolkerBergen: I've viewed `TruncatedSVD` as an alternative way of compressing the data. Of course, the first SVD component will then store all the information about the means. From component two on this alternative way should be similar to what you get from PCA, but yes, it's not equivalent... As I eventually didn't run into memory problems I never really investigated further... But I'm pretty sure that PCA is just one of 100 ways of compressing the data in a somewhat meaningful manner giving you somewhat meaningful results. That's already evident from the fact that all the autoencoder based latent space representations don't give you completely different results than PCA. My impression is that, in fact, the results are highly similar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446368749:587,efficient,efficiently,587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446368749,1,['efficient'],['efficiently']
Energy Efficiency,", values_key). ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_default_colors_for_categorical_obs(adata, value_to_plot); 468 ); 469 ; --> 470 _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length]); 471 ; 472 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_colors_for_categorical_obs(adata, value_to_plot, palette); 428 colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]; 429 ; --> 430 adata.uns[value_to_plot + '_colors'] = colors_list; 431 ; 432 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/compat/_overloaded_dict.py in __setitem__(self, key, value); 104 self.overloaded[key].set(value); 105 else:; --> 106 self.data[key] = value; 107 ; 108 def __delitem__(self, key):. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in __setitem__(self, idx, value); 32 stacklevel=2,; 33 ); ---> 34 with self._update() as container:; 35 container[idx] = value; 36 . ~/miniconda3/envs/csp/lib/python3.8/contextlib.py in __enter__(self); 111 del self.args, self.kwds, self.func; 112 try:; --> 113 return next(self.gen); 114 except StopIteration:; 115 raise RuntimeError(""generator didn't yield"") from None. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in _update(self); 38 def _update(self):; 39 adata_view, attr_name, keys = self._view_args; ---> 40 new = adata_view.copy(); 41 attr = getattr(new, attr_name); 42 container = reduce(lambda d, k: d[k], keys, attr). ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/anndata.py in copy(self, filename); 1526 ; 1527 if filename is None:; -> 1528 raise ValueError(; 1529 ""To copy an AnnData object in backed mode, ""; 1530 ""pass a filename: `.copy(filename='myfilename.h5ad')`. "". ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`. ```. #### Versions. Scanpy: 1.8.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2401:4061,reduce,reduce,4061,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2401,1,['reduce'],['reduce']
Energy Efficiency,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; ```. ```pytb; Illegal instruction (core dumped); ```. #### Versions. <details>. Python 3.9.7 and scanpy 1.8.2. CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold. I unfortunately cannot call scanpy.logging as I cannot import scanpy. I can, however, list the versions of all packages installed alongside scanpy. anndata 0.7.8; cycler 0.11.0; fonttools 4.28.2; h5py 3.6.0; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; matplotlib 3.5.0; natsort 8.0.0; networkx 2.6.0; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; pandas 1.3.4; patsy 0.5.2; pillow 8.4.0; pynndescent 0.5.5; python-dateutil 2.8.2; pytz 2021.3; scikit-learn 1.0.1; scipy 1.7.3; seaborn 0.11.2; setuptools-scm 6.3.2; sinfo 0.3.4; statsmodels 0.13.1; stdlib-list 0.8.0; tables 3.6.1; threadpoolctl 3.0.0; tqdm 4.62.3; umap-learn 0.5.2; xlrd-1.2.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062:914,monitor,monitor,914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062,1,['monitor'],['monitor']
Energy Efficiency,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Description; The issue is visible/reproducibe from the [plotting tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html#Heatmaps) in cell 23.; Here, as an example, Cluster 3/NK cells are associated with the marker genes GNLY and NKG7.; However, the cluster color code representing the rows and columns of the heatmap are mismatching. Cluster 3 is summarized using violet on the cell level (color bar below the heatmap) and using green for grouping Cluster 3 associated markers GNLY and NKG7 (color bar on the right). I would expect that these color codes match. ### Minimal code sample; Below are a few lines to reproduce this behaviour:; ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added='clusters', resolution=0.5). marker_genes_dict = {'NK': ['GNLY', 'NKG7'],; 'T-cell': ['CD3D'],; 'B-cell': ['CD79A', 'MS4A1'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}. sc.pl.heatmap(pbmc, marker_genes_dict, groupby='clusters', vmin=-2, vmax=2, cmap='RdBu_r', dendrogram=True, swap_axes=True); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479:686,green,green,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479,1,['green'],['green']
Energy Efficiency,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. To do some trajectory analysis, I wanted to first try some tutorials to adapt to the workflow of scanpy, after calling sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) i got the valueerror below. I also tried different Drosophila Datasets and it always happens, and its due to a gene called ""nan"" in the features.tsv or genes.tsv, if this gene is renamed, scanpy works as intended. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-6-455e630e3278> in <module>; 1 adata.var['mt'] = adata.var_names.str.startswith('mt:') # annotate the group of mitochondrial genes as 'mt'; ----> 2 sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 286 X.eliminate_zeros(); 287 ; --> 288 obs_metrics = describe_obs(; 289 adata,; 290 expr_type=expr_type,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 119 for qc_var in qc_vars:; 120 obs_metrics[f""total_{expr_type}_{qc_var}""] = (; --> 121 X[:, adata.var[qc_var].values].sum(axis=1); 122 ); 123 if log1p:. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\_index.py in __getitem__(self, key); 49 return self._get_sliceXslice(row, col); 50 elif col.ndim == 1:; ---> 51 return self._get_sliceXarray(row, col); 52 raise IndexError('index results in >2 dimensions'); 53 elif row.ndim == 1:. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1708:301,adapt,adapt,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708,1,['adapt'],['adapt']
Energy Efficiency,-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.1.3; executing 1.2.0; fastjsonschema 2.18.1; filelock 3.12.4; flake8 6.0.0; Flask 3.0.0; fonttools 4.43.1; fqdn 1.5.1; frozenlist 1.4.0; fsspec 2023.6.0; future 0.18.3; gensim 4.3.2; gitdb 4.0.10; GitPython 3.1.36; gmpy2 2.1.2; greenlet 3.0.0; h5py 3.9.0; holoviews 1.17.1; huggingface-hub 0.17.3; humanfriendly 10.0; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; igraph 0.10.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.11.0; importlib-metadata 6.8.0; importlib-resources 6.1.0; incremental 22.10.0; inflection 0.5.1; iniconfig 2.0.0; intake 0.7.0; intervaltree 3.1.0; ipykernel 6.25.2; ipython 8.16.1; ipython-genutils 0.2.0; ipywidgets 8.1.1; isoduration 20.11.0; isort 5.12.0; itemadapter 0.8.0; itemloaders 1.1.0; itsdangerous 2.1.2; jaraco.classes 3.3.0; jedi 0.18.1; jeepney 0.8.0; jellyfish 1.0.1; Jinja2 3.1.2; jmespath 1.0.1; joblib 1.3.2; json5 0.9.14; jsonpatch 1.33; jsonpointer 2.4; jsonschema 4.19.1; jsonschema-specifications 2023.7.1; jupyter 1.0.0; jupyter_client 8.3.1; jupyter-console 6.6.3; jupyter_core 5.3.2; jupyter-events 0.7.0; jupyter-lsp 2.2.0; jupyter_server 2.7.3; jupyter_server_terminals 0.4.4; jupyterlab 4.0.6; jupyterlab-pygments 0.2.2; jupyterlab_server 2.25.0; jupyt,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:5448,green,greenlet,5448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['green'],['greenlet']
Energy Efficiency,"/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 262 ; 263 def check(func, compiler_state):; --> 264 mangled = func(compiler_state); 265 if mangled not in (True, False):; 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 90 % (state.func_id.func_name,)):; 91 # Type inference; ---> 92 typemap, return_type, calltypes = type_inference_stage(; 93 state.typingctx,; 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors); 68 ; 69 infer.build_constraint(); ---> 70 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8103,reduce,reduce,8103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['reduce'],['reduce']
Energy Efficiency,"/a>;32m (...); 50 :raises InvalidVersion: When the version string is not a valid version.; 51 """"""; ---> 52 return Version(version). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:195, in Version.__init__(self, version); 184 """"""Initialize a Version object.; 185 ; 186 :param version:; ...; --> 195 match = self._regex.search(version); 196 if not match:; 197 raise InvalidVersion(f""Invalid version: '{version}'""). TypeError: expected string or bytes-like object; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cairo 1.23.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; colorlog NA; comm 0.1.4; cupy 12.2.0; cupy_backends NA; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.7.1; dateutil 2.8.2; debugpy 1.6.6; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.7; dot_parser NA; exceptiongroup 1.1.1; executing 1.2.0; fastrlock 0.8.2; google NA; greenlet 2.0.2; h5py 3.10.0; hypergeom_ufunc NA; igraph 0.10.8; importlib_resources NA; iniconfig NA; ipykernel 6.25.2; ipywidgets 8.1.1; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.9.1; llvmlite 0.42.0; markupsafe 2.1.2; matplotlib 3.7.1; mmh3 NA; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.59.1; numexpr 2.8.3; numpy 1.24.4; optuna 3.5.0; packaging 23.0; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.1.1; pluggy 1.4.0; prompt_toolkit 3.0.38; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pyarrow 11.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.14.0; pynvml 11.4.1; pyparsing 3.0.9; pytest 8.1.1; pytz 2022.7.1; scipy 1.9.1; session_info 1.0.0; setuptools 67.6.0; six 1.16.0; sklearn 1.2.2; sparse 0.14.0; sqlalchemy 1.4.46; stack_data 0.6.2; tblib 1.7.0; texttable 1.6.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978:2225,green,greenlet,2225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978,1,['green'],['greenlet']
Energy Efficiency,"2 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5593 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6412 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6662 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6039 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6036 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5923 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6767 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 7077 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6080 genes that are detected in less than 5 cells; Traceback (most recent call last):; File ""/mypath/runMulti_scanpy.py"", line 161, in main; adata = sc.concat([adata,adata_tmp],join='outer'); File ""/mypath/scanpy1.7/lib/python3.8/site-packages/anndata/_core/merge.py"", line 818, in concat; X = concat_arrays(; File ""/mypath/scanpy1.7/lib/python3.8/site-packages/anndata/_core/merge.py"", line 424, in concat_arrays; return np.concatenate(; File ""<__array_function__ internals>"", line 5, in concatenate; numpy.core._exceptions.MemoryError: Unable to allocate 15.9 GiB for an array with shape (180000, 23752) and data type float32; ```. Thanks !!!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2080:4291,allocate,allocate,4291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2080,1,['allocate'],['allocate']
Energy Efficiency,"<!-- What kind of feature would you like to request? -->; - [ X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. <!-- Please describe your wishes below: -->; It may be useful to adopt a PCA option similar to **`multiBatchPCA`** in the R batchelor package.; This is a useful approach where there are imbalances in batch size and PCA is conducted across a merged experiment.; It is pretty slow in R. From their documentation:. > ; > _""Our approach is to effectively weight the cells in each batch to mimic the situation where all batches; > have the same number of cells. This ensures that the low-dimensional space can distinguish subpopulations in smaller batches. Otherwise, batches with a large number of cells would dominate; > the PCA, i.e., the definition of the mean vector and covariance matrix. This may reduce resolution; > of unique subpopulations in smaller batches that differ in a different dimension to the subspace of; > the larger batches.""_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289:858,reduce,reduce,858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289,1,['reduce'],['reduce']
Energy Efficiency,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:477,adapt,adapting,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,1,['adapt'],['adapting']
Energy Efficiency,"<!-- What kind of feature would you like to request? -->; - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions; - [ ] New plotting function: Distribution of gene-set score across Conditions. ; <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition „green“ compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. ; 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? ; 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1629:829,green,green,829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629,1,['green'],['green']
Energy Efficiency,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:; ```PYTHON; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```; or . ```; sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'); .add_dendrogram(width=0.4,color='grey'); .swap_axes(); .dot_size_legend(title='fraction', location='left'); ```. Any comments? ; (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/956:472,reduce,reduce,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956,1,['reduce'],['reduce']
Energy Efficiency,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes # _no existing issue_; - [ ] Tests included or not required because: _No new tests_; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: _I did not write release notes_. Hi :). I am proposing a change that speeds up `filter_cells` (x1000 speedup) and `filter_genes` (x2 speedup) for CSR sparse matrices. On my personal machine for 1M cells, `sc.pp.filter_cells(adata, min_genes=xx)` runs in 1ms instead of 10s currently. The speedup should be even stronger on sparser modalities like ATAC. In spirit, this simply replaces `np.sum(X > 0, axis=axis)` with `X.getnnz(axis=axis)`, which is much more efficient. But the axis argument in `getnnz` in `csr_array` may be deprecated. I think it should still be fine with `csr_matrix`, but since I don't know for sure I manually implemented it for the CSR case as in https://github.com/scipy/scipy/issues/19405 . What do you think?. Regarding `getnnz`: Of course it would be nicer to be able to write `.getnnz(axis=axis)`, which extends beyond CSR to other sparse matrices. Can we assume that we're getting sparse matrices and not sparse arrays ?. Pinging @dschult from the Scipy issue liked above, who mentioned: . > I'm pretty sure that a reasonable and commonly occuring use-case would be enough to make the developers include this feature somehow. (edited because I confused `csr_array` and `csr_matrix`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2772:975,efficient,efficient,975,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2772,1,['efficient'],['efficient']
Energy Efficiency,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; colors_use = ['red', 'blue', 'green', 'purple', 'yellow', 'brown', 'black', 'peru', 'orange', 'olive',; 'darksage', 'cyan', 'lime', 'pink', 'teal', 'violet', 'darkblue', 'magenta', 'coral', 'gray']; adata = sc.AnnData(embedding); adata.obs[""category""] = label.astype(np.int); adata.obs[""domain""] = batch.astype(np.int); sc.tl.tsne(adata, use_rep = 'X', n_jobs = 10); adata.uns[""category_colors""] = list(colors_use[:len(np.unique(label))]); adata.uns[""domain_colors""] = list(colors_use[:len(np.unique(batch))]); sc.pl.tsne(adata, color = [""category"", ""domain""], title = [""class"", ""domain""],; show = False, size = 50000 / len(label), save = ""/{}.pdf"".format(file)). My scanpy version is 1.4.6, but when I run the above codes, I find that the color is continuous, why cannot set my specific color?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1963:212,green,green,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1963,1,['green'],['green']
Energy Efficiency,"= False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var = (1 / ((N - 1) * np.square(reg_std))) * (; (N * np.square(mean)); + squared_batch_counts_sum; - 2 * batch_counts_sum * mean; ); norm_gene_vars.append(norm_gene_var.reshape(1, -1)). norm_gene_vars = np.concatenate(norm_gene_vars, axis=0); # argsort twice gives ranks; ranked_norm_gene_vars = np.argsort(np.argsort(norm_gene_vars, axis=1), axis=1); median_norm_gene_vars = np.median(norm_gene_vars, axis=0); median_ranked = np.median(ranked_norm_gene_vars, axis=0). num_batches_high_var = np.sum(; ranked_norm_gene_vars >= (adata.X.shape[1] - n_top_genes), axis=0; ); df = pd.DataFrame(index=np.array(adata.var_names)); df[""highly_variable_nbatches""] = num_batches_high_var; df[""highly_variable_median_rank""] = median_ranked. df[""highly_variable_median_variance""] = median_norm_gene_vars; df.sort_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:1794,power,power,1794,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,1,['power'],['power']
Energy Efficiency,"=========; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1205,reduce,reducer,1205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"========================= test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1107,reduce,reducer,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"=================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1168,reduce,reducer,1168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['reduce'],['reducer']
Energy Efficiency,"> (1) Any user doing data processing or interactive analysis could benefit from multithreading here. Consider the two big for-loops which through all of the genes between compared in the samples, and the for loop which automatically does this for each ""group"" in the ScanPy object. My concern is that there will be issues if you keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:452,allocate,allocated,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['allocate'],['allocated']
Energy Efficiency,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here?; > ; > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ?; > ; > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?!. I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464:497,efficient,efficient,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464,1,['efficient'],['efficient']
Energy Efficiency,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:73,efficient,efficient,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531,2,['efficient'],['efficient']
Energy Efficiency,"> @gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?. Honestly, I don't know. But @ivirshup also brought it up here https://github.com/theislab/scanpy/pull/1180#discussion_r412871280 and here https://github.com/theislab/scanpy/pull/1180#discussion_r413445806, I just didn't have time to address it. I am fine with making them not None.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822:186,reduce,reduce,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822,1,['reduce'],['reduce']
Energy Efficiency,"> Also the code in Scanpy solely started off Laleh's Matlab version. In which I introduced that convention when helping Laleh to make it more efficient :wink: . I don’t know if others came up with it independently before I did in early 2015, but it wouldn’t surprise me :laughing:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-457914722:142,efficient,efficient,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-457914722,1,['efficient'],['efficient']
Energy Efficiency,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:1185,reduce,reducedDims,1185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,1,['reduce'],['reducedDims']
Energy Efficiency,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {‘umap’, ‘gauss’, ‘rapids’}, None (default: 'umap'). Use ‘umap’ [McInnes18] or ‘gauss’ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use ‘rapids’ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,adapt,adaptive,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742,1,['adapt'],['adaptive']
Energy Efficiency,"> I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different.; > ; > If we only switch the green or red color with another, most people won’t even notice. how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310:204,green,green,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444420310,2,['green'],['green']
Energy Efficiency,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-514006827:50,monitor,monitor,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-514006827,1,['monitor'],['monitor']
Energy Efficiency,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['adapt'],['adapted']
Energy Efficiency,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:34,efficient,efficiently,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265,1,['efficient'],['efficiently']
Energy Efficiency,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:147,efficient,efficiently,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['efficient'],['efficiently']
Energy Efficiency,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n × k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455:75,efficient,efficient,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455,2,['efficient'],['efficient']
Energy Efficiency,"> I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:; > ; > * a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes?; > * our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code.; > * some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?. Thank you so much for all your assistance and detailed suggestion, I really appreciate it. I tracked the memory usage and found out that the free system memory is only about 5 GB, so apparently I have misunderstood the concept of ""memory usage"" before......Now I am going to turn to people who have encountered the same machine problem on stackoverflow and see if there is some advice. Thank you again for your time and attention!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551#issuecomment-1640426447:398,allocate,allocated,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551#issuecomment-1640426447,3,['allocate'],"['allocate', 'allocated', 'allocates']"
Energy Efficiency,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:184,schedul,scheduling,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240,1,['schedul'],['scheduling']
Energy Efficiency,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:796,reduce,reduce,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,1,['reduce'],['reduce']
Energy Efficiency,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:507,reduce,reduced,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599,1,['reduce'],['reduced']
Energy Efficiency,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > […](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:102,reduce,reduce,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['reduce'],['reduce']
Energy Efficiency,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589:322,reduce,reduced,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589,2,"['efficient', 'reduce']","['efficient', 'reduced']"
Energy Efficiency,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:247,power,power-method,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937,2,['power'],"['power-method', 'powering']"
Energy Efficiency,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['adapt'],['adapt']
Energy Efficiency,"> `batch1.X.mean(1)` should give you the desired result.; > Or maybe i don't get what you actually need. Thanks for the reply. Sometimes I got index error when I used the subset data view for further analysis, saying the dimension was not matched. ; It could be solved by assigning .copy() when subsetting the adata. So I was just wondering if any memory-efficient way to do this also?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520:355,efficient,efficient,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520,1,['efficient'],['efficient']
Energy Efficiency,"> how about making green just a bit brighter/less bright to make it discernible from red for color-blind people? should not break much. I guess that would be best done by switching the green with another green somewhere down the list to not mess with the color map entirely. Not sure this is a general purpose fix for all types of colorblindness though, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868:19,green,green,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444463868,3,['green'],['green']
Energy Efficiency,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:227,reduce,reduce,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,1,['reduce'],['reduce']
Energy Efficiency,"> obs-like structure with clusters in rows. Completely agreed!; 1. agreed with @ivirshup that there should be a more comprehensive object (which can possibly simply be stored as a dataframe and params in `.uns['rank_genes_groups']`, that clarify what the reference for the test was, but that might be not powerful enough)... your latest suggestion, @ivirshup, representing things as in 3d array sounds very promising, too... how to make an intuitive object? represent the 3d array in a long-form dataframe where two axes are accessible from one multi-index? or store an actual 3d array in AnnData, which can be cast into a convenient object, through a casting namespace... the logic being `sc.tl....` computes some complicated annotation, `sc.pl...` visualizes this annotation and `sc.ex....` extracts and casts annotation into more easily manageable objects. One example is `sc.Neigbors` (which should go into `sc.ex...`) which takes the weird annotation that `sc.pp.neighbors` writes and casts them into an object that allows accessing things... ; 2. Related, but really independent of `rank_genes_groups`: I had implemented a [draft of a `.collapse()` function](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/zebrafish/zebrafish.ipynb#Collapsing-the-AnnData-object), which is very similar to the [`.groupby()` function](https://github.com/ivirshup/mantis#group-by) that @ivirshup suggests, but much less elegant (I would also never have put it into the main repo...). You take a summary metric like `.mean()` or `.std()` and collapse the object by that (in pandas, would be `df.groupby('louvain').mean()`. > Why is it that .obs, .var, and .uns don't have data frames in them? np.recarray don't seem like a very popular data structure elsewhere. We just did only allow rec arrays in `.uns` as they are natively supported by hdf5 and dataframes aren't. It was really just that reason, nothing else. As mentioned in anndata, I'd love to completely move away from rec arrays as a means o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241:305,power,powerful,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487279241,1,['power'],['powerful']
Energy Efficiency,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3067,Power,Powered,3067,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892,1,['Power'],['Powered']
Energy Efficiency,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124:152,power,powers,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124,1,['power'],['powers']
Energy Efficiency,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261:132,adapt,adaption,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261,1,['adapt'],['adaption']
Energy Efficiency,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327:45,efficient,efficient,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327,2,['efficient'],['efficient']
Energy Efficiency,"@LuckyMD Thanks for sharing your code, I will try it. As for a dataset with very similar cells, I think the pbmc68k has T-cells that are very similar to each other. You can take a quick look at a reduced datase by doing:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); sc.pl.umap(adata, color='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/290#issuecomment-428935816:196,reduce,reduced,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290#issuecomment-428935816,1,['reduce'],['reduced']
Energy Efficiency,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147:71,efficient,efficient,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147,1,['efficient'],['efficient']
Energy Efficiency,"@VolkerBergen ; Also appropriately implemented power method (last section of [this](http://www.cs.yale.edu/homes/el327/datamining2013aFiles/07_singular_value_decomposition.pdf), for example) for svd should be fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446612573:47,power,power,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446612573,1,['power'],['power']
Energy Efficiency,"@falexwolf , I used '0.4.2' version, I will update to the latest.; Thanks, scanpy is powerful tool for single cell RNASeq data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754:85,power,powerful,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370292754,1,['power'],['powerful']
Energy Efficiency,"@falexwolf Do you know a more efficient way to get the value of a single column given the gene name. Currently, I am using:. ```; adata[:, 'gene_name'].X; ```. This is easy but not optimal. Any idea?. I will start updating the test once we are happy we the new results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458:30,efficient,efficient,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-421258458,1,['efficient'],['efficient']
Energy Efficiency,"@falexwolf, I think it would be worth going over what kind of interactivity would be most useful. I find linked selection and summary statistics on selected groups is pretty powerful. For QC plots, it's nice to know other properties of cells which look like outliers. It can also be useful for figuring out what's up with the classification that's not agreeing with your reduced dimension plot. Being able to interactively search and select genes to view would also be nice. It would also be good if it were easy to share this kind of visualization with non-technical collaborators easily. I think there's also a question of scale, and whether it would be nice to use libraries like [datashader](http://datashader.org) to avoid the over plotting problems that are so common in this field. I'm working on a few prototypes at the moment, but I'm not sure how well they fit into the api of adding an `interact` flag. Once I have things a little more formalized I'll set up a repo, but am open to suggestions for other plot types. I'd be interested in getting opinions on the usefulness of `datashader`'s edge bundling for graph plots ([examples here](http://holoviews.org/user_guide/Network_Graphs.html#) under the header ""Bundling graphs""). There's also the issue of libraries. Currently, I'm frustrated with every python plotting library, but am leaning towards the `holoviews`, `bokeh`, `datashader` stack for this. @gokceneraslan If you're asking about what usage of bokeh looks like, [they have a bunch of notebooks](https://github.com/bokeh/bokeh-notebooks) in a repo that'll run on [Binder](https://mybinder.org).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651:174,power,powerful,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-418597651,2,"['power', 'reduce']","['powerful', 'reduced']"
Energy Efficiency,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:495,reduce,reduce,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062,1,['reduce'],['reduce']
Energy Efficiency,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:164,sustainab,sustainable,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,2,"['adapt', 'sustainab']","['adapting', 'sustainable']"
Energy Efficiency,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:160,reduce,reduce,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120,1,['reduce'],['reduce']
Energy Efficiency,"@flying-sheep I'm thinking we should switch to using conda-forge for distributing scanpy. Right now, we're having trouble maintaining the bioconda builds. We already use conda-forge for anndata, so I think it would reduce the maintenance burden to consolidate our build systems. Also, I find conda-forge's tooling and documentation much easier to follow. I've started setting up a conda-forge recipe, which has gone pretty well. However I'm running into one issue with the tests. It looks like we don't bundle the `conftest.py` file at our project root with the rest of scanpy. This leads to the flaky internet tests being run when the conda package is built. Not sure how to deal with this at the moment. Update: Also `test_regress_out_ordinal` seems to hang forever when testing the build.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1169:215,reduce,reduce,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1169,1,['reduce'],['reduce']
Energy Efficiency,"@flying-sheep `normalize_total` is newer and more efficient. And we should use it instead of `normalize_per_cell`, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/816#issuecomment-531900088:50,efficient,efficient,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531900088,1,['efficient'],['efficient']
Energy Efficiency,@flying-sheep do you want to close this in favor of your future much more powerful plotting revamp?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681:74,power,powerful,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681,1,['power'],['powerful']
Energy Efficiency,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,adapt,adapt,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951,1,['adapt'],['adapt']
Energy Efficiency,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,adapt,adapted,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250,1,['adapt'],['adapted']
Energy Efficiency,"@gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141:184,reduce,reduce,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141,1,['reduce'],['reduce']
Energy Efficiency,"@ivirshup Looks great! I like the new spatial test image ;) well done!. I just give it a try and didn't find any problem. One little change: can you add to the legend of `na_color` that this is also the color used when the parameter for `color` is not given. . I noticed two parameters in the embedding that I think belong only to the spatial.. Those are `bw` and `alpha_img`. In embeddings they do nothing. . Other issue, that I don't expect to address at the moment, is the increase in parameters because is becoming difficult to go through the list of parameters when browsing through the documentation. To help on this we can start using alphabetical order for all optional parameters. Other suggestion is to add to the documentation in which version a parameter was added. Thus, power users can easily track changes and try new options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918:784,power,power,784,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918,1,['power'],['power']
Energy Efficiency,"@jlause I think I caught the remaining bug for the docs, now they should pass. ; EDIT: they do!. I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:; - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042:563,green,green,563,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042,1,['green'],['green']
Energy Efficiency,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:427,allocate,allocated,427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727,2,"['allocate', 'schedul']","['allocated', 'scheduled']"
Energy Efficiency,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:17,power,powers,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872,2,['power'],"['power', 'powers']"
Energy Efficiency,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526:851,reduce,reduced,851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526,1,['reduce'],['reduced']
Energy Efficiency,Adapt name of UMAP coordinates according to used representation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1403:0,Adapt,Adapt,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1403,1,['Adapt'],['Adapt']
Energy Efficiency,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495:0,Adapt,Adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495,1,['Adapt'],['Adapted']
Energy Efficiency,Add a batch_key option to HVG function to reduce the batch effects,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614:42,reduce,reduce,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614,2,['reduce'],['reduce']
Energy Efficiency,"Added p-values output to Wilcoxon tests, as discussed in #270. Left most of the code the same (including the chunking). Wilcoxon test now outputs pvals, adjusted pvals (Bonferroni correction), and log fold changes. The change in speed should be much smaller than before:; Original time:; <img width=""468"" alt=""screen shot 2018-10-08 at 4 04 02 pm"" src=""https://user-images.githubusercontent.com/37122760/46632615-43fbda80-cb19-11e8-9a6b-9f6e06dd656c.png"">. New time:; <img width=""481"" alt=""screen shot 2018-10-08 at 4 02 47 pm"" src=""https://user-images.githubusercontent.com/37122760/46632624-4bbb7f00-cb19-11e8-916a-db518ff60791.png"">. I also fixed a bug where the Wilcoxon test would output the absolute values of the scores when rankby_abs=True, which shouldn't happen according to the function description. . As a side note, I was unable to find a quick/efficient way to rank the full sparse matrix for the wilcoxon test, which is why I kept the chunking along genes as suggested. Hope this helps!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289:858,efficient,efficient,858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289,1,['efficient'],['efficient']
Energy Efficiency,Adds a numba kernel for `seurat_v3` for sparse matrices. This kernel is a lot faster and more memory efficient. I doesn't copy nor promotes the matrix to 64-bit floats.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3017:101,efficient,efficient,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3017,1,['efficient'],['efficient']
Energy Efficiency,"Ah I see. I did produce the results on the same machine with the same package version and number of CPUs. . The clustering seems to be hanged which becomes visible from these plots:; ![image](https://github.com/scverse/scanpy/assets/127406679/5f0513c8-a3b0-48b2-ac5e-03ed3642dfd5); This is the unclustered map, where you can see that the bottom group in cluster 1 (orange) actually is pulled toward the group in cluster 2 (green) when I clustered them initially: . ![clustered_UMAP](https://github.com/scverse/scanpy/assets/127406679/9eca16a4-c8ca-4a66-b567-2beb4bc87ae2); So here you see that part of cluster 1 is actually added to cluster 2 (which also make sense when looking at the expression profiles of those groups).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2020653583:423,green,green,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2020653583,1,['green'],['green']
Energy Efficiency,"Ah sorry, I should have told you that. All of the preprocessing functions can be migrated to only work with AnnData; there is no important setting in which you want to pass an array or a sparse matrix. That's also remniscient from the early days when I thought people might not like to use AnnData. But that's of course stupid, they wouldn't use Scanpy in that case, anyway. I'm merging this for now so that we have something working for 1.4.1, but if you want to simplify and reduce this to AnnData-only, happy to merge a PR on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016:477,reduce,reduce,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551#issuecomment-476000016,1,['reduce'],['reduce']
Energy Efficiency,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857:683,adapt,adapt,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857,1,['adapt'],['adapt']
Energy Efficiency,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546:180,reduce,reduce,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546,1,['reduce'],['reduce']
Energy Efficiency,"As a possible alternate/ supplement, what about a discourse forum? I think threaded conversations are useful, especially when you can't expect everyone who has input to be online at the same time. It also looks like google indexes discourse, which could reduce repeated questions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476053717:254,reduce,reduce,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476053717,1,['reduce'],['reduce']
Energy Efficiency,"As discussed in issue #313, score_genes function returns different values on various machines. This is due to using float32 dtype in the `np.nanmean` calls. This PR fixes this behaviour by changing the dtype to float64 in the relevant sections of code ie. functions `gene_score()` and `_sparse_nanmean`. Following the suggestion of @ivirshup the returned value is now also float64. I also adapted the tests `test_add_score` and `test_npnanmean_vs_sparsemean` to use and expect float64.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1890:389,adapt,adapted,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890,1,['adapt'],['adapted']
Energy Efficiency,As it's scheduled to be removed in anndata 0.10.0. This will require a quick bug fix release. It may also require bumping the anndata dependency to >0.8.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2658:8,schedul,scheduled,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2658,1,['schedul'],['scheduled']
Energy Efficiency,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,adapt,adapting,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001,1,['adapt'],['adapting']
Energy Efficiency,"Background: ~40k cells, umap, tsne, ... all had no problem. When running diffmap:; `sc.tl.diffmap(adata,n_comps=13)`. >/fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:793: RuntimeWarning: divide by zero encountered in true_divide; Q = scipy.sparse.spdiags(1.0/q, 0, W.shape[0], W.shape[0]); /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/neighbors/__init__.py:803: RuntimeWarning: divide by zero encountered in true_divide; self.Z = scipy.sparse.spdiags(1.0/z, 0, K.shape[0], K.shape[0]); ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-6472f1ef45f7> in <module>(); ----> 1 sc.tl.diffmap(adata,n_comps=13). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/scanpy/tools/diffmap.py in diffmap(adata, n_comps, copy); 49 dmap.compute_transitions(); 50 dmap.compute_eigen(n_comps=n_comps); ---> 51 adata.obsm['X_diffmap'] = dmap.eigen_basis; 52 adata.uns['diffmap_evals'] = dmap.eigen_values; 53 logg.info(' finished', time=True, end=' ' if settings.verbosity > 2 else '\n'). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, key, arr); 104 raise ValueError('Can only assign an array of same length ({}), '; 105 'not of length {}.'; --> 106 .format(self.shape[0], arr.shape[0])); 107 # the following always allocates a new array; 108 # even if the key already exists and dimensions match. ValueError: Can only assign an array of same length (43570), not of length 2056. What might be the problem here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/123:1405,allocate,allocates,1405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/123,1,['allocate'],['allocates']
Energy Efficiency,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/658#issuecomment-495248091:28,efficient,efficient,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658#issuecomment-495248091,3,"['efficient', 'reduce']","['efficient', 'reduce']"
Energy Efficiency,"Change default leiden clustering backend to `igraph`, and reduce default number of iterations",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2865:58,reduce,reduce,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2865,1,['reduce'],['reduce']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2901""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2901#issuecomment-1994813104,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2962""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2962#issuecomment-2020403203,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2974""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2974#issuecomment-2031851380,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/2984""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2984#issuecomment-2042330743,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3120""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3120#issuecomment-2188428017,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3216""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3216#issuecomment-2321426331,1,['Power'],['Powered']
Energy Efficiency,"Check out this pull request on&nbsp; <a href=""https://app.reviewnb.com/scverse/scanpy/pull/3222""><img align=""absmiddle"" alt=""ReviewNB"" height=""28"" class=""BotMessageButtonImage"" src=""https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png""/></a> . See visual diffs & provide feedback on Jupyter Notebooks. . ---. <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303:344,Power,Powered,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222#issuecomment-2324014303,1,['Power'],['Powered']
Energy Efficiency,"Could we change colors for groups from being defined by an array of colors to being a dict mapping from the relevant key/ category to the color? I would find this more intuitive, and much easier to modify. Plus tools like `seaborn` can accept `dict`s directly as a palette:. ```python; import seaborn as sns. iris = sns.load_dataset(""iris""); g = sns.FacetGrid(; iris, ; row=""species"", ; hue=""species"", ; palette={""setosa"": ""red"", ""versicolor"": ""green"", ""virginica"": ""blue""}; ); g.map(sns.kdeplot, ""sepal_width"").show(); ```. ![example](https://user-images.githubusercontent.com/8238804/55700859-48477880-5a14-11e9-921c-612c3387b7cb.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596:445,green,green,445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596,1,['green'],['green']
Energy Efficiency,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639:207,reduce,reduced,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639,1,['reduce'],['reduced']
Energy Efficiency,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364:324,Adapt,Adapt,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364,1,['Adapt'],['Adapt']
Energy Efficiency,"Dear all,; I am very interested to set my own set of markers and see the expression of those markers in my umap. So I basically want to see expression of multiple signature genes in one plot. since I am new to python and scanpy I am not sure how can it be done and if there is already a function for that.; In R I had a similar situation in which I just took the avg expression of those cells and treated them as on feature and plotted them but here I am not how can it be efficiently done!. Any help would be very appreciated thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/638:473,efficient,efficiently,473,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638,1,['efficient'],['efficiently']
Energy Efficiency,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529:96,reduce,reduce,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529,1,['reduce'],['reduce']
Energy Efficiency,"Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/477:64,reduce,reduce,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477,1,['reduce'],['reduce']
Energy Efficiency,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:90,efficient,efficient,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,2,"['adapt', 'efficient']","['adapted', 'efficient']"
Energy Efficiency,Green light on my side to merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441549419:0,Green,Green,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441549419,1,['Green'],['Green']
Energy Efficiency,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:110,Power,Powershell,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['Power'],['Powershell']
Energy Efficiency,"Hello @davidhbrann,; Thanks for the response.; I did pip install --user scikit-misc --force in the anaconda powershell, but this bug kept the same. Not solved.; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473:108,power,powershell,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473,1,['power'],['powershell']
Energy Efficiency,"Hello scanpy team,. this is my first pull request, I hope I at least vaguely adhered to your contribution guidelines. I did not find an issue tackling this exact situation, but if I overlooked it, feel free to point that out. This PR changes few lines of code involved in getting dimensionality reductions like X_pca. The current code skips subsetting the respective array to the n_pcs requested, if the representation name is not X_pca. I do not think this is optimal. This change allows to only take the dimensions needed also for non X_pca representations. This can be useful when using harmony for example or storing different PCA embeddings in the same object. Then it is not needed to first store them as X_pca to be able to properly use the n_pcs parameter in e.g. pp.neighbors. It might make sense then to adapt the documentation respectively as well. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. ----------. Fixes #1846",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179:814,adapt,adapt,814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179,1,['adapt'],['adapt']
Energy Efficiency,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156:651,green,green,651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156,1,['green'],['green']
Energy Efficiency,"Hey @buesra-oezmen! I don't think this is a UMAP issue, but instead an issue of the MultiVI parameterization or the data. But as I know the data quite well in this case, I'm pretty sure it's maybe just a MultiVI model that is not sufficiently trained. Maybe try for a few more epochs? Or otherwise maybe the network architecture needs to be adapted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471:341,adapt,adapted,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471,1,['adapt'],['adapted']
Energy Efficiency,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467,1,['adapt'],['adapted']
Energy Efficiency,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:245,efficient,efficient,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395,1,['efficient'],['efficient']
Energy Efficiency,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:150,efficient,efficient,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207,1,['efficient'],['efficient']
Energy Efficiency,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,adapt,adapt,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,1,['adapt'],['adapt']
Energy Efficiency,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:138,reduce,reduce,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943,1,['reduce'],['reduce']
Energy Efficiency,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:113,efficient,efficient,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764,4,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:411,reduce,reduce,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661,1,['reduce'],['reduce']
Energy Efficiency,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:213,reduce,reduced,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817,3,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:159,reduce,reduce,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221,1,['reduce'],['reduce']
Energy Efficiency,"Hi Dan, about 1.: I’m asking you what the semantic meaning is :smile: . about 2.: there are two functions called `dendrogram`, and they have compatible signatures. Each computed dendrogram can be plotted. So what I’m saying is that the plotting version hasn’t been adapted. Also an important question: in `tl.dendrogram`, we call `_choose_representation`, which will compute a PCA for the .obs axis. When specifying `axis='var'`, should it compute a PCA for the `var` axis instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869:265,adapt,adapted,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869,1,['adapt'],['adapted']
Energy Efficiency,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693:510,reduce,reduce,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693,1,['reduce'],['reduce']
Energy Efficiency,"Hi Joshua,. ok, first of all, you should store the group id as a field in the annotation of observations/cells (`.obs`). Taking your AnnData, you'd do the following:; ```; adata.obs['mygroups'] = [name.split('--')[0] for name in adata.obs_names]; ```; You can then do ; ```; sc.pl.violin(adata, 'mygene', group_by='mygroups'); ```; as in the [standard example](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb), box [32]. Instead of 'mygene', you can use any annotation key; for instance gene scores, as produced by `sc.pp.score_genes`. Or averages over genes `adata.obs['my_gene_set_avg'] = adata[:, gene_set].X.mean(axis=1)`. Does this help or did I misunderstand something?. PS: As a principle rule for writing efficient python code: never loop over more than a 100 items... absolutely never use nested loops. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370141359:762,efficient,efficient,762,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370141359,1,['efficient'],['efficient']
Energy Efficiency,"Hi Scanpy devs. Sorry, this isn't an enhancement request, just wan't sure where this fitted. . Just a quick one- when's the next Scanpy release (1.6.0?) scheduled for?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1365:153,schedul,scheduled,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1365,1,['schedul'],['scheduled']
Energy Efficiency,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,adapt,adapt,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798,1,['adapt'],['adapt']
Energy Efficiency,"Hi all, there are three packages I would like to suggest adding to the [scanpy ecosystem](https://scanpy.readthedocs.io/en/stable/ecosystem.html):. - [CellRank](https://cellrank.readthedocs.io/en/latest/): computing root, final and intermediate metastable states as well as lineage probabilities bases on RNA velocity. Fully compatible with scanpy and scvelo, well documented and scales to large cell numbers; - [scachepy](https://github.com/theislab/scachepy): caching the output of expensive scanpy/scvelo computations. Works with different backends (pickle etc). This is handy in a typical data analysis workflow where I have a couple of commands in my jupyter notebook which take very long to compute. This is more convenient then storing each attribute manually (many scanpy commands write to different `AnnData` attributes`) and more memory efficient than saving the entire `AnnData` object; - [interactive plotting](https://github.com/theislab/interactive_plotting): offers a set of interactive plotting functions based on bokeh and holoviews. Fully compatible with scanpy and scales to large cell numbers thanks to holoviews. Basically, it's a way to interactively work with your data from within a jupyter notebook. . Involved in the development of these three projects are @VolkerBergen , @michalk8 and myself.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1282:847,efficient,efficient,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1282,1,['efficient'],['efficient']
Energy Efficiency,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:489,reduce,reduce,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,3,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"Hi all,. this is a re-implementation of the ComBat function in python for batch effect removal by Brent Pedersen at the University of Utah which I slightly modified to work with AnnData objects. I asked Brent for permission and he would be happy with us using this. Originally, the code was written in R for the SVA package:; Jeffrey T. Leek, W. Evan Johnson, Hilary S. Parker, Andrew E. Jaffe; and John D. Storey (). sva: Surrogate Variable Analysis. R package; version 3.4.0. The idea is taken from this paper:; Johnson WE, Rabinovic A, Li C (2007). Adjusting batch effects in microarray; expression data using Empirical Bayes methods. Biostatistics 8:118-127. . Originally, the method was developed to adjust for batch effects in microarray data, however, it is commonly applied to scRNA-seq data nowadays. The method fits linear models to the genes and pools statistical power by means of EB to estimate per gene correction factors. . I understand that @mbuttner also has an implementation of this - maybe we can combine and get the best of both approaches?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398:875,power,power,875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398,1,['power'],['power']
Energy Efficiency,"Hi everyone,. a while back, @giovp asked me to create a pull request that integrates analytical Pearson residuals in scanpy. We already discussed a bit with @LuckyMD and @ivirshup over at berenslab/umi-normalization#1 how to structure it, and now I made a version that should be ready for review. As discussed earlier, this pull request implements two core methods:; - `sc.pp.normalize_pearson_residuals()`, which applies the method to `adata.X`. Overall, the function is very similar in structure to `sc.pp.normalize_total()` (support for layers, inplace operation etc).; - `sc.pp.highly_variable_genes(flavor='pearson_residuals')`, which selects genes based on Pearson residual variance. The ""inner"" function `_highly_variable_pearson_residuals()` is structured similarly to `_highly_variable_seurat_v3()` (support for multiple batches, median ranks for tie breaking). It includes the `chunksize` argument to allow for memory-efficient computation of the residual variance. We discussed quite a lot how to implement a third function that would bundle gene selection, normalization by analytical residuals and PCA. This PR includes the two options that emerged at the end of that discussion, so now we have to choose ;). - `sc.pp.recipe_pearson_residuals()` which does HVG selection and normalization both via Pearson residuals prior to PCA; - `sc.pp.normalize_pearson_residuals_pca()` which applies any HVG selection if the user previously added one to the `adata` object, and then normalizes via Pearson residuals and does PCA. Both functions retain the raw input counts as `adata.X` and add fields for PCA/Normalization/HVG selection results (or return them) as applicable, most importantly the `X_pca` in `adata.obsm['pearson_residuals_X_pca']`. I hope this addresses some of the issues we discussed over at the other repo in a scanpy-y way. Let me know what you think and where you think improvements are needed!. Cheers, Jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715:928,efficient,efficient,928,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715,1,['efficient'],['efficient']
Energy Efficiency,"Hi thanks for your excellent work!. I noticed for a subset adata object, say; batch1 = adata[adata.obs[""batch""] == ""batch1"", :]; it will be a view of the original adata; ; For further analysis, if I just wanted to calculate the mean of selected rows (it will give me the mean of all the rows of original data as it is a view),; I was wondering if there is any memory-efficient way to do it instead of using copy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1940:367,efficient,efficient,367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1940,1,['efficient'],['efficient']
Energy Efficiency,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:371,power,powerful,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,1,['power'],['powerful']
Energy Efficiency,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,adapt,adapt,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,1,['adapt'],['adapt']
Energy Efficiency,"Hi, we’re currently planning scanpy 2.0. We want to reduce the number of ways people can make heatmaps, but maybe marsilea would be a good base for the new version. We’ll keep an eye on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1594388914:52,reduce,reduce,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1594388914,1,['reduce'],['reduce']
Energy Efficiency,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:167,monitor,monitor,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['monitor'],['monitor']
Energy Efficiency,"Hm, I adapted your reproducer to use scanpy 1.10.3’s code and it doesn’t seem to be an issue: https://gist.github.com/flying-sheep/b2ae449ab70a9358e07a82f284de5dca#file-score_genes_diagnostics_tests2-ipynb. I’m going to assume that this is fixed in 1.10.3. If you can reproduce it with 1.10.3, we can reopen it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983:6,adapt,adapted,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983,1,['adapt'],['adapted']
Energy Efficiency,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Energy Efficiency,"How can one get a DEG table with a pts column for each cluster? So that for each group there would be 4 columns: 'names', 'logfoldchanges', 'pvals_adj' and 'pts'?. Manual sorting from 2 files is not quite optimal:; ```; sc.tl.rank_genes_groups(adata, 'cell_types', method='wilcoxon', pts=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; degs_by_cluster = pd.DataFrame({group + '_' + key[:14]: result[key][group]; for group in groups for key in ['names', 'logfoldchanges', 'pvals_adj']}); degs_by_cluster.to_csv(""DEG_adata_cell_types_pct_to_sort.csv""); pts=pd.DataFrame(adata.uns['rank_genes_groups']['pts']); pts.to_csv(""pts_adata.csv""); ```. Could you help with a more efficient way to do that? ; @fidelram @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407:765,efficient,efficient,765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407,1,['efficient'],['efficient']
Energy Efficiency,"I added a new `batch_key` option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614:153,reduce,reduce,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614,2,['reduce'],"['reduce', 'reduces']"
Energy Efficiency,I agree but I'm not in charge of the PRs. Probably a better solution would be to provide a backend parameter in the PR to choose which module to use. In my opinion bioservices looks more mature and maintained.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457056054:23,charge,charge,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457056054,1,['charge'],['charge']
Energy Efficiency,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358:158,power,power,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358,2,['power'],['power']
Energy Efficiency,"I agree, I've been trying to reduce the dataset and save it as 10x_h5 format again but it seems to be more complicated than I thought. I think it would be best to have an 10x_h5 format so to also use it for the read function test. Also, yes will only keep the lowres image",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692:29,reduce,reduce,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692,1,['reduce'],['reduce']
Energy Efficiency,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt); [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt); [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt); [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048:15,reduce,reduced,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427822048,1,['reduce'],['reduced']
Energy Efficiency,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,adapt,adapted,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688,1,['adapt'],['adapted']
Energy Efficiency,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:557,charge,charge,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['charge'],['charge']
Energy Efficiency,"I am partial color blind as well. So I second any initiative in this; direction. On Tue, Dec 4, 2018 at 7:03 PM Alex Wolf <notifications@github.com> wrote:. > We're using a custom color map in scanpy by default, anyways:; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22; > .; >; > It would, of course, be easy to change this, but then everything changes; > for everyone and many people will wonder why everything looks different now; > (""where is my green cluster?""). If we do it, we only exchange green with; > another color, so that at least all other colors will be unaffected...; >; > I would have liked to wait until a major update, because I consider this; > breaking backward consistency, though...; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/387#issuecomment-444197487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1aBQoQxEiqx5gNfgpj2-tJvQZ2Ssks5u1rjXgaJpZM4ZA5qf>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795:483,green,green,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444388795,2,['green'],['green']
Energy Efficiency,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:241,efficient,efficient,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,1,['efficient'],['efficient']
Energy Efficiency,"I don’t consider this breaking backwards compatibility. Everything is still in the same place, still has the same labels. Only the default color of the labels is different. If we only switch the green or red color with another, most people won’t even notice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444418923:195,green,green,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444418923,1,['green'],['green']
Energy Efficiency,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-513822396:48,monitor,monitor,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513822396,1,['monitor'],['monitor']
Energy Efficiency,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,adapt,adapted,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069,1,['adapt'],['adapted']
Energy Efficiency,"I just changed one color in each of the named pairs (i.e. the green, the purple, and the kakhi). Are the swatches in the lower pictures distinguishable for you? Then the mathematical model for color closeness matches your vision and we should adopt something like it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444805820:62,green,green,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805820,1,['green'],['green']
Energy Efficiency,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:143,efficient,efficient,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['efficient'],['efficient']
Energy Efficiency,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316:341,efficient,efficient,341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316,1,['efficient'],['efficient']
Energy Efficiency,I realized that this is an issue originated from coercing the sparse matrix to be of np.int8 type to reduce the size the ann object and the mtx file to be written for another application. Making it np.int32 fixes the issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547#issuecomment-1624533422:101,reduce,reduce,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547#issuecomment-1624533422,1,['reduce'],['reduce']
Energy Efficiency,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Energy Efficiency,"I suppose to do this properly one ought to scan the code base for uses of igraph, check which among them require the RNG and then add the seeding to those modules?. Since I'm still very new to scanpy and rather swamped at the moment, I won't have the time to do this anytime soon. Maybe add a help wanted tag? If nobody takes it up by the time my schedule lightens I'll see what I can do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544:347,schedul,schedule,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544,1,['schedul'],['schedule']
Energy Efficiency,"I think that if you store a categorical value in a pandas dataframe (like; .obs) the storage of this redundant information is quite efficient. In a; quick search I found this:; https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>; wrote:. > To clarify a bit... I think it would be good to enable something like:; > sc.pl.umap(adata, color=(uns_dict_key, obs_column)); >; > Where the sc.pl.umap() function then does:; >; > if isinstance(color, tuple):; > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]; > sc.pl.plot_scatter(adata, color=color_vector, ...); >; > It might need to be a pandas dataframe rather than a dictionary with the; > above setup.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/658#issuecomment-495177880:132,efficient,efficient,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658#issuecomment-495177880,1,['efficient'],['efficient']
Energy Efficiency,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:371,efficient,efficient,371,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255,1,['efficient'],['efficient']
Energy Efficiency,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975:34,efficient,efficient,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975,1,['efficient'],['efficient']
Energy Efficiency,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Energy Efficiency,"I think there's definitely room for more plotting libraries in the ecosystem, but have some doubts about whether all needs can be met by one library. I personally use `seaborn`/ `matplotlib`, `bokeh`, `datashader`, and `altair` for different cases. I also think making a good plotting API is exceedingly difficult, especially if you target both high and low level use cases. I would note that the plotting code in scanpy feels like some of the most maintenance intensive code in the library. > provides helper functions for handling colors, saving figures, etc. We can do a bit more of this here. But of course, much of it would end up being `matplotlib` specific. > encourages a consistent plotting API (e.g. by defining abstract base classes). I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on `seaborn` plotting classes more heavily here, potentially contributing features upstream. Here's one example https://github.com/mwaskom/seaborn/issues/2487 of a feature which could fit the `AnnData` data model nicely. > there is quite some duplicated code in the plotting section. We'd definitely like to reduce the amount of duplicated code, which is what drove the addition of `sc.get`. This seems to be working out internally, if slowly. > All the scanpy helper functions for plotting (e.g. savefig_or_show, _set_color_for_categorical_obs etc.) are private scanpy functions. I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is `_set_color_for_categorical_obs`, which I'd also like to make accessible through `sc.get`. Adding `groupby` support to `anndata` would help a lot here too (https://github.com/theislab/anndata/issues/556). `save_fig_or_show` is something that I don't think we should export, and may need a rework (#1508).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749:1165,reduce,reduce,1165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749,1,['reduce'],['reduce']
Energy Efficiency,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:287,reduce,reduces,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760,1,['reduce'],['reduces']
Energy Efficiency,"I think we should do that in a way that doesn’t tempt the user to use literal colors like you just did. The color names people think of have a lot of bad properties for colorblind people and contrast. Maybe if we redefine common color names? When users specify “red” there, they only want some kind of red, not `#ff0000`. In one of my analyses, there’s predefined clusters which I recolor like this:. ![grafik](https://user-images.githubusercontent.com/291575/55707300-9001dc00-59e3-11e9-93b5-2dfac3814edf.png). It’s in R and what I do is that I give names to a lot of colorbrewer colors:. ```r; prettier_colors <- c(; setNames(brewer.pal(8, 'Dark2')[c(1,4,6,8)], c('turquoise', 'magenta', 'yellow', 'black')),; setNames(brewer.pal(9, 'Set1')[-c(4,6)], c('red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey')); ); ```. In python that would be:. ```py; import numpy as np; from matplotlib import cm. prettier_colors = dict(zip(; [; 'turquoise', 'magenta', 'yellow', 'black',; 'red', 'blue', 'green', 'darkorange', 'brown', 'pink', 'grey',; ], np.concatenate([; np.array(cm.Dark2.colors)[[0,3,5,7]],; np.array(cm.Set1.colors)[np.setdiff1d(np.arange(9), [3, 5])],; ]); )); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480730857:771,green,green,771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480730857,2,['green'],['green']
Energy Efficiency,"I think we should introduce a standardized “mask” argument to scanpy functions. This would be a boolean array (or reference to a boolean array in `obs`/ `var`) which masks out certain data entries. This can be thought of as a generalization of how highly variable genes is handled. As an example:. ```python; sc.pp.pca(adata, use_highly_variable=True); ```. Would be equivalent to:. ```python; sc.pp.pca(adata, mask=""highly_variable""); # or; sc.pp.pca(adata, mask=adata.obs[""highly_variable""]); ```. One of the big advantages of making this more widespread is that tasks which previously required using `.raw` or creating new anndata objects will be much easier. Some uses for this change:. ### Plotting. A big one is plotting. Right now if you want to show gene expression for a subset of cells, you have to manually work with the Matplotlib Axes:. ```python; ax = sc.pl.umap(pbmc, show=False); sc.pl.umap(; pbmc[pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells',])],; color=""LDHB"",; ax=ax,; ); ```. If a user could provide a mask, this could be reduced, and would make plotting more than one value possible:. ```python; sc.pl.umap(; pbmc,; color=['LDHB', 'LYZ', 'CD79A’],; mask=pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells’,]),; ); ```. ### Other uses. This has come up before in a few contexts:. * Performing normalization on just some variables https://github.com/scverse/scanpy/issues/2142#issuecomment-1046729522; * Selecting a subset of variables for DE tests: https://github.com/scverse/scanpy/issues/1744; * See also https://github.com/scverse/scanpy/issues/748; * Changing use_raw https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988. ## Implementation. I think this could fit quite well into the `sc.get` getter/ validation functions (https://github.com/scverse/scanpy/issues/828#issuecomment-560072919).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234:1063,reduce,reduced,1063,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234,1,['reduce'],['reduced']
Energy Efficiency,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,adapt,adaptation,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214,1,['adapt'],['adaptation']
Energy Efficiency,"I was looking over normalize_total and saw some strange behaviour. Since it's such a common function, I think it's important that it has standard scanpy behaviour. To this end, this PR looks at cleanup up it's code. ### Addition. `layer` argument. A specific layer can now be normalized by itself. ### Deprecations. I've deprecated the `layers` and `layer_norm` argument. Normalizing multiple layers at once seems less useful than normalizing a specific layer. These seem like very specific use cases that are easy for user's to implement themselves, and are not common patterns in scanpy functions. ### TODO:. - [x] Tests for deprecations ; - [x] Scheduling of deprecations (deprecate in 1.8, remove in 1.9)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1667:648,Schedul,Scheduling,648,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1667,1,['Schedul'],['Scheduling']
Energy Efficiency,"I was wondering if plotting could be facilitated and made more consistent across ; the Scanpy ecosystem. I envisage a library (""scanpyplot"" or whatever) that . * provides building blocks for single-cell-related plots which can be re-used across the ecosystem; * provides helper functions for handling colors, saving figures, etc. ; * encourages a consistent plotting API (e.g. by defining abstract base classes); * See also: https://github.com/theislab/scanpy/issues/956. ## Motivation:; * Even within scanpy, there is quite some duplicated code in the plotting section. Reducing duplicated code reduces the maintenance burden in the long run. Across the ecosystem there's even more duplicated plotting code:; - While implementing the [`scirpy.pl.clonotype_network`](https://icbi-lab.github.io/scirpy/generated/scirpy.pl.clonotype_network.html#scirpy.pl.clonotype_network) function, I found myself copying over a lot of code from the `scanpy.pl.paga` and `scanpy.pl.dotplot` functions. ; - [scvelo also copied most of the paga code](https://github.com/theislab/scvelo/commit/082be06940267917118988ca0b9d18ba86c026db); - scvelo has its own `scatter`; * All the scanpy helper functions for plotting (e.g. `savefig_or_show`, `_set_color_for_categorical_obs` etc.) are private scanpy functions. Implementing plotting functions with consistent bahaviour requires either to duplicate a lot of code, or to rely on a potentially unstable API. In fact, e.g. `scvelo` has duplicates of most of these scanpy helper functions. Squidpy has [similar functions, too](https://github.com/theislab/squidpy/blob/04ba5e887f4b6cd80fd45989e599d926a0f6efb9/squidpy/pl/_utils.py#L51-L93). ; * I know at least three ecosystem libraries that implement some variant of a stacked barplot based on `AnnData.obs` ([`dandelion.pl.stackedbarplot`](https://sc-dandelion.readthedocs.io/en/latest/modules/dandelion.plotting.stackedbarplot.html#dandelion.plotting.stackedbarplot) by @zktuong, [`sc_toolbox.api.plot.cluster_composition_st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1832:596,reduce,reduces,596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832,1,['reduce'],['reduces']
Energy Efficiency,"I'm confused by the figure above. In Cluster 2 (shown in green below the heatmap) the marker gene that is coming up most prominently is CD3D, which was defined as a marker for Cluster 0, though.; The color code on the right seems correct, but why is CD3D most prominently expressed in Cluster 2 and not in Cluster 0?; The same seems to be the case for the other clusters.; Or were the marker genes perhaps just mislabelled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006:57,green,green,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006,1,['green'],['green']
Energy Efficiency,I'm fine with such rather small changes. This should indeed not bother people. Tell me when you decided on a green that satisfies @ftheis. We don't want to make this change multiple times... :wink:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444729710:109,green,green,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444729710,1,['green'],['green']
Energy Efficiency,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:1156,power,powershell,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,1,['power'],['powershell']
Energy Efficiency,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,1,['adapt'],['adapted']
Energy Efficiency,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,adapt,adapting,1718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,1,['adapt'],['adapting']
Energy Efficiency,I'm having the same issue using pp.neighbors for a slightly different clustering purpose. It seems to handle ~200k data points just fine but above ~300k gives `Segmentation fault (core dumped)` no matter how much memory I allocate to the job. ; The quick fix with threads above did not work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1426965264:222,allocate,allocate,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1426965264,1,['allocate'],['allocate']
Energy Efficiency,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Energy Efficiency,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:349,adapt,adapt,349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358,1,['adapt'],['adapt']
Energy Efficiency,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Energy Efficiency,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:145,efficient,efficiently,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['efficient'],['efficiently']
Energy Efficiency,"Indeed, ~3-7x faster for me & of course quite a bit more memory efficient (quickly checked with scalene). Tests keep working (for `seurat_v3`/`seurat_v3_paper` somewhat tight numeric comparison with Seurat results), nice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3017#issuecomment-2122484190:64,efficient,efficient,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3017#issuecomment-2122484190,1,['efficient'],['efficient']
Energy Efficiency,Is there a `Jitter` parameter in `scanpy.pl.umap` to reduce overlap?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2768:53,reduce,reduce,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2768,1,['reduce'],['reduce']
Energy Efficiency,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171:199,schedul,scheduling,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171,1,['schedul'],['scheduling']
Energy Efficiency,"Is there any way to estimate the number of branching points? It seems that this number has to be explicitly given before running the algorithm, which might be difficult if it's not clear from simply looking at the dimensionally reduced data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/11:228,reduce,reduced,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/11,1,['reduce'],['reduced']
Energy Efficiency,"Isaac says that lobpcg seems much less precise and is probably not worth it, so we should probably just adapt the warning to mention the imprecision and call it a day",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465:104,adapt,adapt,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465,1,['adapt'],['adapt']
Energy Efficiency,"It is not much trouble to get something closer to a more square aspect, but the issue is whether the rcParams['figure.figsize'] is respected or not. To make room for the colormap on the right, the plot area is shrinked a bit but the figsize continues to be a square. . In my view, the solutions are:; * respect the rcParams['figure.figsize'] but make the colormap thinner, thus aiming towards a more squared image.; * enlarge the width of the figure or reduce the height to make it more square. I will experiment a bit to see what is better. As for the panels_per_row, I think is better to use some standard naming. I will change that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425866204:453,reduce,reduce,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425866204,1,['reduce'],['reduce']
Energy Efficiency,"It would also be nice to add some stats for vars. Let's say we have aggregated cells from control and stim samples. ; For example, the percentage of cells expressing gene A in clusterX for every group sample would allow us to filter genes expressed in so few cells but with relatively high counts. It will reduce the false positives in pseudobulk differential gene expression analysis caused by these genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3063#issuecomment-2234253911:306,reduce,reduce,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3063#issuecomment-2234253911,1,['reduce'],['reduce']
Energy Efficiency,"It'll take a little doing, but it's certainly do-able. Something like this should do it:. ```python; import numpy as np; from functools import reduce. def concat(arrays: ""list[np.recarray]""):; names = arrays[0].dtype.names; dtypes = [dict(a.dtype.descr) for a in arrays]; assert all(arrays[0].dtype.names == a.dtype.names for a in arrays[1:]), ""All arrays should have same names""; ; offset = 0; out_dtypes = {}; for k in names:; out_dtype = reduce(np.result_type, (dtype[k] for dtype in dtypes)); out_dtypes[k] = (out_dtype, offset); offset += out_dtype.alignment. out_recarray = np.recarray(sum(map(len, arrays)), dtype=out_dtypes) ; np.concatenate(arrays, out=out_recarray); ; return out_recarray; ```. Maybe the solution should happen upstream though. . Do we concatenate recarrays often?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652:143,reduce,reduce,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652,2,['reduce'],['reduce']
Energy Efficiency,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:626,efficient,efficient,626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['efficient'],['efficient']
Energy Efficiency,"I’m unsure if we can help you with this, as it’s your system creating his error and we don’t have access to the data. There’s multiple possible causes and solutions:. - a misunderstanding: The arrays should require `310385 * 8563 * 8B = 21.3 GB`, no idea why it says 10. But of course that’s still not close to 100GB unless other people/processes are using the machine, or other arrays are allocated. Can you track the memory usage and see if it spikes close to the maximum before the code crashes?; - our implementation allocates memory it shouldn’t: this is the only one we can fix, but we’d have to figure out under which circumstances that happens, and where in our code.; - some machine problem, [like many people on stackoverflow](https://www.google.com/search?q=site%3Astackoverflow.com+%22Unable+to+allocate%22+%22GiB+for+an+array+with+shape%22+%22and+data+type%22): maybe your sysadmin can help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551#issuecomment-1630946384:390,allocate,allocated,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551#issuecomment-1630946384,3,['allocate'],"['allocate', 'allocated', 'allocates']"
Energy Efficiency,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:117,power,powers,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140,2,['power'],['powers']
Energy Efficiency,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>; <summary>Script </summary>. ```python; import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:272,reduce,reduce,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['reduce'],['reduce']
Energy Efficiency,"Just wanted to mention that this would add much convenience. Since Fidel rewrote the plotting API, I saw that it was the right thing to do to reduce all the code in the almost-identical function headers, but I was missing the autocomplete, which had obviously disappeared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474279197:142,reduce,reduce,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474279197,1,['reduce'],['reduce']
Energy Efficiency,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494:104,efficient,efficient,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494,1,['efficient'],['efficient']
Energy Efficiency,"Louvain is being difficult to build since a new setuptools release dropped any python2 compatibility https://github.com/vtraag/louvain-igraph/issues/57. We've largely worked around this in #2063, by making louvain dependent tests optional. However, the paul15 PAGA test is difficult to extract louvian from. It checks hardcoded values based on the results of a louvain clustering. To adapt this test to use leiden, we would have to redo the tutorial and create new results. Or louvain building could be fixed, but the package is deprecated anyways.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2065:384,adapt,adapt,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2065,1,['adapt'],['adapt']
Energy Efficiency,"MAGIC API modelled off the DCA API. A couple of comments:. - I changed `threads` to `n_jobs` to match `sc.tl` API; - I use `sc.settings.verbosity` rather than `False` for `verbose` default; - I updated the PHATE API while I was here; - I'm open to suggestions on how to handle `name_list`. Re: `name_list`: MAGIC accepts both gene names and column indices. I've set it up currently to return to adata.obsm['X_magic'] if genes are specified as the number of columns is reduced from the original data, and to adata.X if all genes are returned. However, adata.obsm['X_magic'] now has no information about the order of the genes, unless I return it from MAGIC as a `pandas.DataFrame`, or as an AnnData object in its own right. What do you think?. P.S. MAGIC handling `genes` on AnnData input is currently on the dev branch, but will be merged to master once we settle on an API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/193:468,reduce,reduced,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/193,1,['reduce'],['reduced']
Energy Efficiency,Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542:262,power,powerusers,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542,1,['power'],['powerusers']
Energy Efficiency,"Matplotlib 3 came and went without https://github.com/matplotlib/matplotlib/issues/9460 being fixed. @ftheis still can’t properly distinguish clusters made by scanpy. The two designated colorblind-friendly colormaps in matplotlib look [like this](https://gist.github.com/flying-sheep/5172a1f1f9757d4ece239e742ba41b08). An alternative would be to push red or green to the end of the current color cycle, so it only appears for 10+ clusters. @mpetroff created a [neat tool](https://colorcyclepicker.mpetroff.net/) that could help to design a custom one if we decide the available ones are too ugly.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387:358,green,green,358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387,1,['green'],['green']
Energy Efficiency,"Mmh, very strange. Graph abstraction will be in the next Scanpy release and is not stable yet... Are you simply running the [minimal example](https://github.com/theislab/graph_abstraction/blob/master/minimal_examples/minimal_examples.ipynb)? Maybe reread and reload your data? At some point a few months ago, the format for AnnData files changed. Also, the master branch on Github doesn't have all tests on all notebooks yet, I'd recommend to wait until the release that is scheduled for the next week. Cheers,; alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844:474,schedul,scheduled,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/40#issuecomment-333528844,1,['schedul'],['scheduled']
Energy Efficiency,"Most of the time? There is an issue with fairly old CPUs (no AVX2, so like >5 years), but that was the last I saw. My guess is that there are more reproducibility issues on windows than linux, likely because it is tested less. I would like to confirm that it's UMAP and not the PCA though. After that could be worth checking the threading (e.g. reduce to one thread, though I thought UMAP should be as reproducible as possible w.r.t. threading by default).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016712689:345,reduce,reduce,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114#issuecomment-1016712689,1,['reduce'],['reduce']
Energy Efficiency,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver it’s the third, for calendar ver, it’s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` → `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what you’re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, there’s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what I’m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:1054,power,powerful,1054,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,1,['power'],['powerful']
Energy Efficiency,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:130,reduce,reduced,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,1,['reduce'],['reduced']
Energy Efficiency,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:106,efficient,efficiently,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466,1,['efficient'],['efficiently']
Energy Efficiency,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:867,reduce,reduce,867,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879,1,['reduce'],['reduce']
Energy Efficiency,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,1,['adapt'],['adapt']
Energy Efficiency,"Not currently, but since the scope of that PR got reduced, it shouldn’t be too much work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2578#issuecomment-1945679038:50,reduce,reduced,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2578#issuecomment-1945679038,1,['reduce'],['reduced']
Energy Efficiency,"Not the developer of scanpy. ; The reason for this is that scanpy will try to initialize the palette automatically if you do not provide it. ; Then this palette will be saved in `adata.uns` . When the number of clusters is large enough, scanpy cannot find a suitable palette, so all colors are initialized to grey.; After the number of clusters is reduced, you can just delete `adata.uns['leiden_colors']` to force the scanpy to reinitialize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620:348,reduce,reduced,348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620,1,['reduce'],['reduced']
Energy Efficiency,"Numpy has dropped support for python 3.6 in 1.20.0, and have recommended that packages in the pydata ecosystem adopt a similar schedule for python versions ([NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html)). Should we follow this?. Some cool stuff we get by dropping 3.6:. * [Postponed evaluation of type annotations](https://docs.python.org/3/whatsnew/3.7.html#pep-563-postponed-evaluation-of-annotations); * [`singledispatch` can register from type annotations](https://docs.python.org/3/whatsnew/3.7.html#functools); * [Customized access to module attributes](https://docs.python.org/3/whatsnew/3.7.html#pep-562-customization-of-access-to-module-attributes). Also, users will be pinned to old versions of numpy if they're using 3.6, and maybe we don't want to deal with that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1697:127,schedul,schedule,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697,1,['schedul'],['schedule']
Energy Efficiency,"OK, done, please check out https://github.com/scverse/scanpy/issues/2828. The sister API `sc.pp.louvain` uses `flavor` for this exact use case. It also says about the `'vtraag'` implementation: “Much more powerful than 'igraph', and the default”. I’m not sure why we consider it that much more “powerful” …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1910351580:205,power,powerful,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1910351580,2,['power'],['powerful']
Energy Efficiency,"Ok, so the first problem is normalization for some reason.; @falexwolf , it seems we should just use new normalize_total when the pull request is accepted, it is more memory efficient.; ![image](https://user-images.githubusercontent.com/3065736/54003760-9afeed80-4153-11e9-9187-8e474a2fa03e.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470782494:174,efficient,efficient,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470782494,1,['efficient'],['efficient']
Energy Efficiency,"PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1114,reduce,reduce,1114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['reduce'],['reduce']
Energy Efficiency,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:7,adapt,adapt,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['adapt'],['adapt']
Energy Efficiency,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:397,monitor,monitor,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937,1,['monitor'],['monitor']
Energy Efficiency,Reading h5ad with backed='r' does not reduce memory usage,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434:38,reduce,reduce,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434,1,['reduce'],['reduce']
Energy Efficiency,Reduce error potential from networkx (e.g. #1227),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1323:0,Reduce,Reduce,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1323,1,['Reduce'],['Reduce']
Energy Efficiency,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/724#issuecomment-510394228:59,reduce,reduced,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-510394228,1,['reduce'],['reduced']
Energy Efficiency,"So I've been sitting offline with @VolkerBergen trying to get to the bottom of this. It seems that the precision of `adata.X` is reduced after subsetting. This is the case when using either of:. ```; adata_hvg = adata_hvg[:, disp_filter['gene_subset']]; adata_hvg._inplace_subset_var(disp_filter['gene_subset']); ```. Either way `adata[:,disp_filter['gene_subset']].X` gives a higher precision than `adata_hvg.X`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/324#issuecomment-433010405:129,reduce,reduced,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324#issuecomment-433010405,1,['reduce'],['reduced']
Energy Efficiency,"So what’s the difference between normalize_per_cell and normalize_total?. If they’re replacable, why don’t we just use the more efficient one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/816#issuecomment-531486992:128,efficient,efficient,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531486992,1,['efficient'],['efficient']
Energy Efficiency,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:592,power,power,592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['power'],['power']
Energy Efficiency,"Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):. `#1f77b4` `#ff7f0e` `#2ca02c` `#d62728` `#9467bd` `#8c564b` `#e377c2` `#7f7f7f` `#bcbd22` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49573631-6605b380-f936-11e8-9629-68b177e59043.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49573600-52f2e380-f936-11e8-9729-67ec0dde0aaf.png). - red and green; - blue and purple; - orange and kakhi. Playing around a bit, it’s easy to get a version that works, e.g. `#1f77b4` `#ff7f0e` `#279e68` `#d62728` `#aa40fc` `#8c564b` `#e377c2` `#7f7f7f` `#b5bd61` `#17becf`. Normal | Deuteranomaly; --- | ---; ![Normal](https://user-images.githubusercontent.com/291575/49574129-926dff80-f937-11e8-8fad-58e89cceebf0.png) | ![Deuteranomaly](https://user-images.githubusercontent.com/291575/49574155-a285df00-f937-11e8-84ba-c1b1ae28ee99.png). I think the changes are subtle enough that we *can* adopt it now and change it a little later if we want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444803441:478,green,green,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444803441,1,['green'],['green']
Energy Efficiency,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:107,reduce,reduce,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,1,['reduce'],['reduce']
Energy Efficiency,"Sorry about the late response here... I can imagine that we'd have more than the color attributes for each category. Just think of a `.groupby` followed by a summary statistics. This gives rise to dataframes that are ordered as `.cat.categories`. The array-way of storing color attributes is inspired by the way that `pd.Categorical` manages the string attributes of categories. Also, seaborn accepts this array-way of passing palettes. You can just do `palette=adata.uns['..._colors']` in any of the seaborn functions. At least I think I've done that many times already. I think that the current way of storing colors is ugly and bad. One improvement would be to have one dict for colors only; `adata.uns['colors'] = {'cat_var1': [...], 'cat_var2': ...}`; or one dict for all attributes of each categorical; `adata.uns['louvain'] = {'colors': [...], 'annotations': [...], 'mean_expr': [...], 'markers': [...]}`. Evidently, in the latter case, one would rather like to have a more powerful `pd.Categorical` class that can do more than just attach a string label to an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-487178978:981,power,powerful,981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-487178978,1,['power'],['powerful']
Energy Efficiency,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,adapt,adapted,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289,1,['adapt'],['adapted']
Energy Efficiency,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Energy Efficiency,Thank you both. @Zethson I've rebased off master. Please merge when this goes green.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637:78,green,green,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637,1,['green'],['green']
Energy Efficiency,"Thank you for the reminder, Joshua! :smile:. How about doing this?; ```; import scanpy.api as sc; import pandas as pd; adata = sc.datasets.toggleswitch(); adata.obs['replicate'] = 0; adata.obs['replicate'].loc[100:] = 1; df = pd.DataFrame(adata.X) # does not allocate new memory if X is an array, so this efficient; df['replicate'] = adata.obs['replicate'].values # if not using assign, no copy is made; df_grouped = df.groupby('replicate'); print(df_grouped.mean()); print(df_grouped.std()); ```; outputs; ```; 0 1; replicate ; 0 0.510177 0.135317; 1 0.152043 0.439836; 0 1; replicate ; 0 0.293965 0.162549; 1 0.153663 0.271669; ```; Of course, you can add this stuff as unstructured annotation to an AnnData... Does it answer your question?. PS: Visualize this is using ideas e.g. from https://stackoverflow.com/questions/46186784/handling-replicate-data-in-pandas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055:259,allocate,allocate,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-378912055,2,"['allocate', 'efficient']","['allocate', 'efficient']"
Energy Efficiency,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:653,power,powerful,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['power'],['powerful']
Energy Efficiency,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:; ```; preprocessing/_combat.py:150: in combat; s_data, design, var_pooled, stand_mean = stand_data(model, data); preprocessing/_combat.py:78: in stand_data; design = design_mat(model, batch_levels); preprocessing/_combat.py:32: in design_mat; model, return_type=""dataframe""); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix; NA_action, return_type); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design; NA_action); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders; formula_like = ModelDesc.from_formula(formula_like); ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula; tree = parse_formula(tree_or_string); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula; _atomic_token_types); ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse; for token in token_source:; ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula; yield _read_python_expr(it, end_tokens); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr; for pytype, token_string, origin in it:; ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next; return six.advance_iterator(self._it); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):; # Since formulas can only contain Python expressions, and Python; # expressions cannot meaningfully c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530:62,adapt,adapting,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530,1,['adapt'],['adapting']
Energy Efficiency,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,adapt,adapt,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444,1,['adapt'],['adapt']
Energy Efficiency,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following?. > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211:358,reduce,reduce,358,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211,1,['reduce'],['reduce']
Energy Efficiency,"Thank you! So you say it doesn’t work, but I see a green checkmark. Would you mind adding a test that exposes the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382:51,green,green,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364153382,1,['green'],['green']
Energy Efficiency,"Thank you!. One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980:261,reduce,reduce,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321#issuecomment-432347980,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for merging. I did some test and the memory usage was not very high with 1 or 20 regressors. Nevertheless, I tried more memory efficient methods but the gain was minimal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079:134,efficient,efficient,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164#issuecomment-394292079,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Energy Efficiency,"Thanks for the example! Could we maybe expand the documentation, asking the users to think about this step and select an appropriate strategy? Also, what is the most efficient way to aggregate the features with the same names? Would be great to have a function for that in scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363151960:166,efficient,efficient,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363151960,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:127,green,green,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387,1,['green'],['green']
Energy Efficiency,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:35,reduce,reduce,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,3,['reduce'],['reduce']
Energy Efficiency,"Thanks for your great package, it is really powerful, but I am encountered with some mistakes in installing.; My platform is Linux login-0-1.local 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux, and Python 2.7.10。; I also try to install in win10 , neither Python 2.7.10 or Python 3.6 makes errors again. firstly, it says versioneer.py has errors, so I reinstall the package,; but when I try to install scanpy again, errors comes out again. Following picture clearly show my errors.; Thanks a lot, if you have time to see my question. ![image](https://user-images.githubusercontent.com/25199946/46243375-34d4a880-c406-11e8-9bb5-4392664fb3cc.png). ![image](https://user-images.githubusercontent.com/25199946/46243461-13c08780-c407-11e8-96c9-bfbb0d863b2f.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/276:44,power,powerful,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/276,1,['power'],['powerful']
Energy Efficiency,"Thanks for your information. I am surprised that this step is taking too; long as is was supposed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:101,reduce,reduce,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for your thorough response! And I apologize for the late reply, I've been away at a conference. I agree with what you mentioned - in my experience I also don't see a 1,000-10,000 fold change in size factors, even when you're comparing drastically different cells with dramatic differences in RNA content (e.g. regulatory T-cells with large cells like macrophages). In this scenario, however, any sensitive differential testing should be done within similar cell types to account for these massive differences. For differential testing, I agree with you that their suggestion of increasing the pseudo-count reduces power and is not ideal. Your suggestion of checking size-factor distributions is definitely an interesting idea, but beyond the scope of the package I think. However, I do like the idea for further downstream verification of differentially regulated genes, particularly those with non-obvious distribution changes. In general, it seems that the authors of the paper are particularly worried about spurious architecture in the embeddings, but some of these can be tested and discarded with diff expression testing. Of note, I usually also see a count depth effect in embedding as well, even with normalization methods other than CPM normalization. I've never been satisfied with attributing all count-number variation as technical noise, which like you said is definitely incorrect, particularly when comparing conditions that could include drastic changes in transcription. All of this is to say, it seems that effect of log-mean vs mean-log should be manageable for embeddings, and like you mentioned, so much of the downstream processing depends on approximation of normal distributions that it seems impossible to get rid of the log-transformation. Overall, I haven't been able to find a satisfying solution either. In general, I tend to say that any obvious expression differences should be minimally affected by this problem, however, when you're trying to distinguish more c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823:613,reduce,reduces,613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823,2,"['power', 'reduce']","['power', 'reduces']"
Energy Efficiency,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:131,efficient,efficient,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['efficient'],['efficient']
Energy Efficiency,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702:109,power,power,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-428210702,3,['power'],['power']
Energy Efficiency,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:48,reduce,reducer,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365,3,['reduce'],"['reduced', 'reducer']"
Energy Efficiency,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:167,efficient,efficiently,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975,4,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,adapt,adapting,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891,1,['adapt'],['adapting']
Energy Efficiency,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:404,sustainab,sustainable,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,1,['sustainab'],['sustainable']
Energy Efficiency,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:25,power,powerful,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['power'],['powerful']
Energy Efficiency,"These t-SNE optimizations are mentioned in the following paper. Adding it here for reference.; https://arxiv.org/abs/2212.11506; Accelerating Barnes-Hut t-SNE Algorithm by Efficient Parallelization on Multi-Core CPUs; N Chaudhary, A Pivovar, P Yakovlev, A Gorshkov… - arXiv preprint arXiv:2212.11506, 2022",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798:172,Efficient,Efficient,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798,1,['Efficient'],['Efficient']
Energy Efficiency,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369:763,reduce,reduced,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369,1,['reduce'],['reduced']
Energy Efficiency,"This field is developing very fast, more and more advanced DE test methos are emerging, it's better to adopt these powerful methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-551411475:115,power,powerful,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-551411475,1,['power'],['powerful']
Energy Efficiency,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:420,power,powerful,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['power'],['powerful']
Energy Efficiency,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462:271,reduce,reduces,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462,1,['reduce'],['reduces']
Energy Efficiency,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:161,power,powerful,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['power'],['powerful']
Energy Efficiency,This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543,1,['adapt'],['adapted']
Energy Efficiency,Use matplotlib 3.1 and adapt tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1020:23,adapt,adapt,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020,1,['adapt'],['adapt']
Energy Efficiency,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780:259,reduce,reduce,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780,1,['reduce'],['reduce']
Energy Efficiency,"We had a journal club about this recently and couldn't really come up with a good solution tbh. Mean-log is so much easier for a lot of applications. Log-transforming data also has a variance-stabilizing effect and it reduces skewness so that the data at least better approximates a normal distribution than before, which many downstream methods assume (although data is often still far from normal). So I don't see how we can forgo log transformation without modeling count data for everything directly. The effect outlined in the paper is the most pronounced for differential expression tests between groups for which size factor distributions differ... So you could check size factor distributions before performing the test to estimate whether the log-mean vs mean-log difference will affect the test. If yes, try without log transforming the data and see if the test can deal with the outliers. Especially for the t-test, the poorer approximation of normality may not have as strong an effect as the log-mean vs mean-log difference. However, in our experience size factors tend to have a range of ~100-fold difference, and not 1,000-10,000 fold as was shown in the paper. We weren't so taken with the suggestion in the paper of increasing the pseudocount as that essentially removes fold-change effects... and also removes zeros (making all matrices dense). As for embeddings... you could remove size factor outliers for the PCA calculation and do it without log-transformation. Although in practice we found it gives very similar results. Thus, our solution was to visualize size factor distributions on embeddings to see whether there is an effect. Usually you do see a count depth effect in the embedding though... and that's not that surprising for CPM normalization, as you assume that all cells are of similar molecule count, which is incorrect. With other normalization methods it shouldn't be as bad. But yeah... overall, it's a complicated problem without a good solution. I imagine it i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918:218,reduce,reduces,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918,1,['reduce'],['reduces']
Energy Efficiency,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191:124,efficient,efficient,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191,2,['efficient'],['efficient']
Energy Efficiency,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:88,reduce,reduced,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['reduce'],['reduced']
Energy Efficiency,"We're using a custom color map in scanpy by default, anyways: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py#L22. It would, of course, be easy to change this, but then everything changes for everyone and many people will wonder why everything looks different now (""where is my green cluster?""). If we do it, we only exchange green with another color, so that at least all other colors will be unaffected... I would have liked to wait until a major update, because I consider this breaking backward consistency, though...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487:305,green,green,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444197487,2,['green'],['green']
Energy Efficiency,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and there’s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095:207,adapt,adapted,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095,1,['adapt'],['adapted']
Energy Efficiency,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306:158,power,power,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306,2,['power'],['power']
Energy Efficiency,"Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1423,reduce,reduce,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['reduce'],['reduce']
Energy Efficiency,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:292,drain,draining,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,1,['drain'],['draining']
Energy Efficiency,Yes I think traversing a minimum spanning tree is good enough since you can never perfectly order cell states in one dimension while capturing all the key features.; ArchR's code to do clustering (addClusters) is here:; https://github.com/GreenleafLab/ArchR/blob/master/R/Clustering.R. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275:239,Green,GreenleafLab,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275,1,['Green'],['GreenleafLab']
Energy Efficiency,Yes I would also be keen to know the most efficient way to aggregate the features,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363174209:42,efficient,efficient,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2363174209,1,['efficient'],['efficient']
Energy Efficiency,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,adapt,adapting,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139,1,['adapt'],['adapting']
Energy Efficiency,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:701,efficient,efficient,701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,1,['efficient'],['efficient']
Energy Efficiency,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you 😄). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,adapt,adapt,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468,1,['adapt'],['adapt']
Energy Efficiency,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-433267386:30,reduce,reduce,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-433267386,1,['reduce'],['reduce']
Energy Efficiency,"You could just add a `sparse` argument to `pca`. If True, just call this function instead of scikit-learn's PCA:. ```; def sparse_pca(X,npcs,mu = None):; # X -- scipy sparse data matrix; # npcs -- number of principal components; # mu -- precomputed feature means. if None, calculates them from X. # compute mean of data features; if mu is None: ; mu = X.mean(0).A.flatten()[None,:]. # dot product operator for the means; mmat = mdot = mu.dot ; # dot product operator for the transposed means; mhmat = mhdot = mu.T.dot ; # dot product operator for the data; Xmat = Xdot = X.dot ; # dot product operator for the transposed data; XHmat = XHdot = X.T.conj().dot ; # dot product operator for a vector of ones; ones = np.ones(X.shape[0])[None,:].dot . # modify the matrix/vector dot products to subtract the means; def matvec(x): ; return Xdot(x) - mdot(x); def matmat(x): ; return Xmat(x) - mmat(x); def rmatvec(x): ; return XHdot(x) - mhdot(ones(x)); def rmatmat(x): ; return XHmat(x) - mhmat(ones(x)); ; # construct the LinearOperator; XL = sp.sparse.linalg.LinearOperator(matvec = matvec, dtype = X.dtype,; matmat = matmat,; shape = X.shape,; rmatvec = rmatvec, rmatmat = rmatmat); ; u,s,v = sp.sparse.linalg.svds(XL,solver='arpack',k=npcs); ; # i like my eigenvalues sorted in decreasing order; idx = np.argsort(-s); S = np.diag(s[idx]); # principal components; pcs = u[:,idx].dot(S) ; # equivalent to PCA.components_ in sklearn ; components_ = v[idx,:] ; return pcs,components_; ```. This only works for the `arpack` solver. It's a bit slower than PCA on dense matrices (since arpack is slower than randomized), but it's super memory efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727:1634,efficient,efficient,1634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-581727727,1,['efficient'],['efficient']
Energy Efficiency,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:76,Green,GreenleafLab,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348,2,"['Green', 'reduce']","['GreenleafLab', 'reduced']"
Energy Efficiency,"_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:2509,reduce,reduce,2509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['reduce'],['reduce']
Energy Efficiency,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854:242,reduce,reduce,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854,1,['reduce'],['reduce']
Energy Efficiency,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:138,reduce,reduced,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296,1,['reduce'],['reduced']
Energy Efficiency,"`log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929:502,adapt,adapt,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929,1,['adapt'],['adapt']
Energy Efficiency,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754:219,adapt,adapted,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754,1,['adapt'],['adapted']
Energy Efficiency,"`n_genes` was calculated when you filtered cells. I agree that this implicit behavior can be confusing. btw, if your expression is in a sparse matrix, this would be a more efficient way to count the number of expressed genes:. ```python; adata.X.eliminate_zeros() # Removes explicit zeros; n_genes = adata.X.getnnz(axis=1) # Counts explicit values. # or, slightly less efficient but still more efficient than making a dense array; np.ravel((adata.X != 0).sum(axis=1)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/488#issuecomment-464412167:172,efficient,efficient,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/488#issuecomment-464412167,3,['efficient'],['efficient']
Energy Efficiency,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:295,power,powerful,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['power'],['powerful']
Energy Efficiency,"aconda3/envs/scRNA/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py"", line 188, in pca; X_pca = pca_.fit_transform(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/utils/_set_output.py"", line 140, in wrapped; data_to_wrap = f(self, X, *args, **kwargs); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 462, in fit_transform; U, S, Vt = self._fit(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 514, in _fit; return self._fit_truncated(X, n_components, self._fit_svd_solver); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 609, in _fit_truncated; U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/_svds.py"", line 532, in svds; _, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,; File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 1697, in eigsh; params.iterate(); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 573, in iterate; raise ArpackError(self.info, infodict=self.iterate_infodict); scipy.sparse.linalg._eigen.arpack.arpack.ArpackError: ARPACK error -9999: Could not build an Arnoldi factorization. IPARAM(5) returns the size of the current Arnoldi factorization. The user is advised to check that enough workspace and array storage has been allocated. ```. #### Versions:. <!-- Output of scvi.__version__ -->. > scvi-tools==0.20.3 python==3.9.16 scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==1.5.3 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.4 pynndescent==0.5.10. and macOS 13.2 (intel). > ; <!-- Relevant screenshots -->. Thanks; Patrick",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2473:2498,allocate,allocated,2498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2473,1,['allocate'],['allocated']
Energy Efficiency,adapted from https://github.com/scverse/anndata/blob/main/.azure-pipelines.yml. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:0,adapt,adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['adapt'],['adapted']
Energy Efficiency,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:2009,efficient,efficient,2009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178,1,['efficient'],['efficient']
Energy Efficiency,"atching, some corner cases where numerics might have turned narrow outcomes, so actually 1991/2000. ```py; pmbc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; pbmc.obs[""dummy_tech""] = ""source_1""; pbmc.obs.loc[pbmc.obs.index[500:1000], ""dummy_tech""] = ""source_2""; pbmc.obs.loc[pbmc.obs.index[1000:1500], ""dummy_tech""] = ""source_3""; pbmc.obs.loc[pbmc.obs.index[1500:2000], ""dummy_tech""] = ""source_4""; pbmc.obs.loc[pbmc.obs.index[2000:], ""dummy_tech""] = ""source_5"". # default settings in scanpy are the same as for Seurat; seurat_v3_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3"", batch_key=""dummy_tech"", inplace=False); seurat_v3_paper_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_paper"", batch_key=""dummy_tech"", inplace=False); seurat_v3_implementation_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_implementation"", batch_key=""dummy_tech"", inplace=False). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat_batch = pd.read_csv(""scanpy/scanpy/tests/_scripts/seurat_hvg_v3_batch.csv"", index_col=0). seu = pd.Index(pbmc3k_tutorial_FindVariableGenes_seurat_batch[""x""].values); ```. Matching genes `'seurat_v3'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_hvg[seurat_v3_hvg.highly_variable].index)); ```; `764`. Matching genes `'seurat_v3_paper'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_paper_hvg[seurat_v3_paper_hvg.highly_variable].index)); ```; `1990`. Matching genes `'seurat_v3_implementation'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_implementation_hvg[seurat_v3_implementation_hvg.highly_variable].index)); ```; `1990`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:5772,adapt,adapted,5772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Energy Efficiency,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3172,consumption,consumption,3172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['consumption'],['consumption']
Energy Efficiency,"be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`; - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:2658,reduce,reduced,2658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['reduce'],['reduced']
Energy Efficiency,"been added.; * **update**: method chaining was added to avoid explosion of parameters as suggested in #956. The caveat is that a different name is needed to keep the current functionality of `sc.pl.dotplot`. Now is possible to do `sc.pl.DotPlot(adata, ....).add_dendrogram(size=1.2).swap_axes().show()`. ![image](https://user-images.githubusercontent.com/4964309/77526545-13ce0600-6e8b-11ea-996a-3d548f52f200.png). ![image](https://user-images.githubusercontent.com/4964309/77529251-8c36c600-6e8f-11ea-8446-c1dd18374cd4.png). Open issues:; * we need a better way to filter the output from rank_genes_groups (and I would also consider a better name). One option is to filter while plotting.; * to correctly plot the results from `sc.tl.rank_genes_groups` is better to report the results for all genes (`n_genes`). The default is to keep the top 100 genes per group. Thus, for many genes, the pvalue, logfoldchange etc in other groups is not saved.; * added new parameters, for some I would be happy to have some feedback. One is `style` that has two options `square color` and `dot color`. **Update**; The following is now possible (notice the method chaining):. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}; sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \; .add_totals(sort='descending', size=1.2) \; .legend(color_title='log(UMI count+1)', width=1.6)\; .style(color_map='RdBu_r', dot_min=0.3)\; .show(); ```; ![image](https://user-images.githubusercontent.com/4964309/77777939-889e6d00-7050-11ea-842e-cf2a11739df6.png). TODO:; - [x] Remove small secondary ticks; - [x] Adapt matrixplot to new object model; - [ ] Adapt heatmap to object model; - [x] Adapt stackedviolin to object model; - [ ] Incorporate modifications from #1116 ; - [x] Fix call from rank_genes_groups; - [x] 'black' code; - [x] Update docstrings; - [ ] Update tests; - [ ] Update tutorial and readthedocs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127:2398,Adapt,Adapt,2398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127,3,['Adapt'],['Adapt']
Energy Efficiency,"ccurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:2410,reduce,reduce,2410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['reduce'],['reduce']
Energy Efficiency,"ce with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=source, color='color',; alpha=0.8, size=5)#'msize', ). # hover tools; hover = plot_lda.select(dict(type=HoverTool)); hover.tooltips = {""content"": ""Sequence: @seq, CCS: @ccs, Charge: @charge ""}; plot_lda.legend.location = ""top_left"". show(plot_lda); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253:1433,Charge,Charge,1433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253,6,"['Charge', 'charge']","['Charge', 'charge']"
Energy Efficiency,"ces,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:3334,Reduce,Reduced,3334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['Reduce'],['Reduced']
Energy Efficiency,"check:. <details>; <summary> Code </summary>. ```python; import scanpy as sc; from sparse_wrapper._core.coordinate_ops import difference_indices; import numpy as np; from scipy import sparse. def find_steps_val(adj, n_steps):; """"""Finding positions with value of 1.""""""; cur = adj.astype(int); diffs = [adj.copy()]; for i in range(n_steps):; cur.data[:] = 1; cur = adj @ cur; diffs.append(cur == 1); return diffs. def find_steps(adj, n_steps):; """"""Finding differences in sparsity patterns.""""""; diffs = [adj.copy()]; prev = adj + sparse.eye(adj.shape[0]); for i in range(n_steps):; cur = prev @ adj; diffs.append(difference_indices(cur, prev)); prev = cur; return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""); sc.pp.visium_connectivity(adata); adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1); val_res = find_steps_val(adj, N-1). for i in range(N):; adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int); adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(; adata,; color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],; ncols=N; ); ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>; <summary> </summary>. ```python; from operator import add; from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False); adata.obs[""fireworks""] = (; reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))); .max(axis=0); .toarray(); .ravel(); ); sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614:2001,reduce,reduce,2001,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614,2,['reduce'],['reduce']
Energy Efficiency,"da3/envs/py48/lib/site-packages/umap/layouts.py?line=31) cache=True,; [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=32) locals={; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=33) ""result"": numba.types.float32,; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=34) ""diff"": numba.types.float32,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=35) ""dim"": numba.types.int32,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=36) },; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=37) ); ---> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=40) ; [42](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=41) Parameters; (...); [49](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=48) The squared euclidean distance between x and y; [50](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=49) """"""; [51](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=50) result = 0.0. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\decorators.py:219, in _jit.<locals>.wrapper(func); [217](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=216) with typeinfer.register_dispatcher(disp):; [218](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=217) for s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:12555,Reduce,Reduced,12555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['Reduce'],['Reduced']
Energy Efficiency,"def find_mutual_nn(data1, data2, k1, k2, n_jobs):; <source elided>; mutual_2 = []; for index_2 in range(data2.shape[0]):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 94:; def find_mutual_nn(data1, data2, k1, k2, n_jobs):; <source elided>; mutual_2 = []; for index_2 in range(data2.shape[0]):; ^. state.func_ir.loc)); Computing correction vectors...; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning: ; Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'mnn2' of function 'compute_correction'. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function compute_correction failed at nopython mode lowering due to: iterating over 2D array. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:22624,schedul,scheduled,22624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['schedul'],['scheduled']
Energy Efficiency,"don’t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,adapt,adapting,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361,1,['adapt'],['adapting']
Energy Efficiency,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2827,schedul,scheduler,2827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,1,['schedul'],['scheduler']
Energy Efficiency,"e values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values. /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce; return ufunc.reduce(obj, axis, dtype, out, **passkwargs). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 339 pass; 340 else:; --> 341 return printer(obj); 342 # Finally look for special method names; 343 method = get_real_method(obj, self.print_method). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/pylabtools.py in <lambda>(fig); 241 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)); 242 if 'retina' in formats or 'png2x' in formats:; --> 243 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)); 244 if 'jpg' in formats or 'jpeg' in formats:; 245 jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', **kwargs)). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264:4148,reduce,reduce,4148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264,1,['reduce'],['reduce']
Energy Efficiency,"e); ```; I get the following traceback; ```; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>; adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function; ```. When i run the `python-igraph` leiden implementation like so...; ```; obs_name = f""leiden {leiden_suffix}""; g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(; objective_function=""modularity"", ; resolution_parameter=0.1, ; weights = 'weight',; n_iterations=2; ); adata.obs[obs_name] = (; pd.Series(; clustering.membership, ; dtype='category', ; index=adata.obs.index; ); ); ```; I get the following, similar traceback; ```; UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473; running Leiden clustering; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>; obs_name = f""leiden {leiden_suffix}""; File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden; g = _utils.get_igraph_from_adjacency(adjacency, directed=directed); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285:2332,power,power,2332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285,1,['power'],['power']
Energy Efficiency,"e**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards favoring the style of `seurat_v3_paper`. Better naming suggestions more than welcome. **Examples**; - Good when no `batch_key` used:; ```py; import numpy as np; import pandas as pd; import scanpy as sc. # load exactly the data from seurat tutorial. pbmc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). print(pbmc). # default settings in scanpy are the same as for Seurat; sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3""). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat = pd.read_csv(""scanpy/scanpy/scanpy/tests/_scripts/seurat_hvg_v3.csv.gz"", index_col=0). # This is used to order and rank the hvg when no batch information used; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance.standardized""],; pbmc.var[""variances_norm""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""mean""],; pbmc.var[""means""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance""],; pbmc.var[""variances""]),; ); ```. - Discrepancy when `batch_key` is used: The flavor `'seurat_v3'` shows 764/2000 genes overlap with Seurat. The new suggested flavors `'seurat_v3_paper'` and `'seurat_v3_implementation'` show a 1990/2000 selected genes overlap with Seurat. The 10 non-matching genes contain 1 gene renamed by Seurat and therefore not found matching, some corner ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:3561,adapt,adapted,3561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Energy Efficiency,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328:2123,adapt,adapted,2123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328,1,['adapt'],['adapted']
Energy Efficiency,"erating = self._original_iterator is not None; 1088 while self.dispatch_one_batch(iterator):. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:901, in Parallel.dispatch_one_batch(self, iterator); 899 return False; 900 else:; --> 901 self._dispatch(tasks); 902 return True. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/parallel.py:819, in Parallel._dispatch(self, batch); 817 with self._lock:; 818 job_idx = len(self._jobs); --> 819 job = self._backend.apply_async(batch, callback=cb); 820 # A job can complete so quickly than its callback is; 821 # called before we get here, causing self._jobs to; 822 # grow. To ensure correct results ordering, .insert is; 823 # used (rather than .append) in the following line; 824 self._jobs.insert(job_idx, job). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:252, in PoolManagerMixin.apply_async(self, func, callback); 250 def apply_async(self, func, callback=None):; 251 """"""Schedule a func to be run""""""; --> 252 return self._get_pool().apply_async(; 253 SafeFunction(func), callback=callback). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/site-packages/joblib/_parallel_backends.py:407, in ThreadingBackend._get_pool(self); 401 """"""Lazily initialize the thread pool; 402 ; 403 The actual pool of worker threads is only initialized at the first; 404 call to apply_async.; 405 """"""; 406 if self._pool is None:; --> 407 self._pool = ThreadPool(self._n_jobs); 408 return self._pool. File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:927, in ThreadPool.__init__(self, processes, initializer, initargs); 926 def __init__(self, processes=None, initializer=None, initargs=()):; --> 927 Pool.__init__(self, processes, initializer, initargs). File ~/tools/mambaforge/envs/scanpy/lib/python3.9/multiprocessing/pool.py:216, in Pool.__init__(self, processes, initializer, initargs, maxtasksperchild, context); 214 for p in self._pool:; 215 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2506:8511,Schedul,Schedule,8511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2506,1,['Schedul'],['Schedule']
Energy Efficiency,fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7841,green,greenlet,7841,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['green'],['greenlet']
Energy Efficiency,"haha with ease. you can observe the inhomogeneous contrast distribution with C2 and C10 there: the colors are indistinguishable dark blue while C7 and C8 go from snot green all the way to orange. that would be horrible for continuous data, but merely makes C2 and C10 indistinguishable for categorical colors and unnecessarily reduces contrast there (as it’s a color map and no palette).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544:167,green,green,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278339544,2,"['green', 'reduce']","['green', 'reduces']"
Energy Efficiency,"hon-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyparsing 3.0.7; pytz 2022.1; scipy 1.8.0; session_info 1.0.0; sitecustomize NA; six 1.16.0; sklearn 1.1.2; threadpoolctl 3.1.0; -----; Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]; Windows-10-10.0.22000-SP0; -----; Session information updated at 2022-10-26 15:35. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:3896,allocate,allocate,3896,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['allocate'],['allocate']
Energy Efficiency,"https://github.com/theislab/anndata/pull/85; https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/memory_issue_huge.ipynb. I wasn't right about recursion, almost no effect. `read_direct` is efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600:202,efficient,efficient,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-443483600,1,['efficient'],['efficient']
Energy Efficiency,ial-gene-expression-ffpe/probe-sets/overview)) currently has an average of 3 probes per gene. Additionally around 1/3-rd of our probes span splice junctions and can be treated differently from other probes for various downstream analyses like gDNA characterization and RNA-velocity. This matrix will enable users to use the more fine grained probe-barcode matrix for downstream analyses. This PR does the following:; - Adds an example `raw_probe_barcode_matrix.h5` to `tests/_data/visium_data/2.1.0/raw_probe_bc_matrix.h5` (this is a probe barcode matrix downsampled to 1000 features to reduce the size); - The structure of a probe barcode h5 file is; ```; /matrix Group; /matrix/barcodes Dataset {4987}; /matrix/data Dataset {17581240/Inf}; /matrix/features Group; /matrix/features/feature_type Dataset {21178}; /matrix/features/filtered_probes Dataset {21178}; /matrix/features/gene_id Dataset {21178}; /matrix/features/gene_name Dataset {21178}; /matrix/features/genome Dataset {21178}; /matrix/features/id Dataset {21178}; /matrix/features/name Dataset {21178}; /matrix/features/probe_region Dataset {21178}; /matrix/features/target_sets Group; /matrix/features/target_sets/Visium\ Mouse\ Transcriptome\ Probe\ Set Dataset {19779}; /matrix/filtered_barcodes Dataset {4987}; /matrix/indices Dataset {17581240/Inf}; /matrix/indptr Dataset {4988}; /matrix/shape Dataset {2}; ```; - Enables `_read_v3_10x_h5` to read all the metadata that is in the output H5 files into an anndata. This is done by reading all the metadata in the `h5` files into the anndata. This changes the default behaviour of while reading 10x H5 files to read in all the metadata (the code currently reads all the metadata we usually put in - this will read any additional fields if we put them in too). ; - Adds a test to make sure the reader works correctly.; <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) t,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2470:870,reduce,reduce,870,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2470,1,['reduce'],['reduce']
Energy Efficiency,"ib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")); 322 def read_array(elem, _reader):; --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype); 767 try:; --> 768 return self._fast_reader.read(args); 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last); Cell In[2], line 4; 2 import pandas as pd; 3 import scanpy as sc; ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 240 return read_dataframe(elem); 241 return func(elem); --> 243 adata = read_dispatched(f, callback=callback); 245 # Backwards compat (should figure out which version); 246 if ""raw.X"" in f:. File D:\Python3.10.9\lib\site-packages\anndata\experimental\__init__.py:58, in read_dispatched(elem, callback); 54 from anndata._io.specs import Reader, _REGISTRY; 56 reader = Reader(_REGISTRY, callback=callback); ---> 58 return reader.read_elem(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\utils.py:204, in repo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:2587,allocate,allocate,2587,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['allocate'],['allocate']
Energy Efficiency,"import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__; filename=filename, filemode=filemode); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual; X = X.as",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:1150,reduce,reduce,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['reduce'],['reduce']
Energy Efficiency,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:245,Power,Powershell,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692,1,['Power'],['Powershell']
Energy Efficiency,"irst concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614:1457,adapt,adapted,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614,1,['adapt'],['adapted']
Energy Efficiency,"make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Plotting with scanpy.pl gives attribute error. . ### Minimal code sample. ```python; sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[47], line 1; ----> 1 sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True). File ~/anaconda3/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:795, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 790 if multi_panel and groupby is None and len(ys) == 1:; 791 # This is a quick and dirty way for adapting scales across several; 792 # keys if groupby is None.; 793 y = ys[0]; --> 795 g = sns.catplot(; 796 y=y,; 797 data=obs_tidy,; 798 kind=""violin"",; 799 scale=scale,; 800 col=x,; 801 col_order=keys,; 802 sharey=False,; 803 order=keys,; 804 cut=0,; 805 inner=None,; 806 **kwds,; 807 ); 809 if stripplot:; 810 grouped_df = obs_tidy.groupby(x). File ~/anaconda3/lib/python3.11/site-packages/seaborn/categorical.py:2932, in catplot(data, x, y, hue, row, col, kind, estimator, errorbar, n_boot, units, seed, order, hue_order, row_order, col_order, col_wrap, height, aspect, log_scale, native_scale, formatter, orient, color, palette, hue_norm, legend, legend_out, sharex, sharey, margin_titles, facet_kws, ci, **kwargs); 2929 linecolor = plot_kws.pop(""linecolor"", ""auto""); 2930 linecolor = p._complement_color(linecolor, color, p._hue_map); -> 2932 p.plot_violins(; 2933 width=width,; 2934 dodge=dod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:1111,adapt,adapting,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['adapt'],['adapting']
Energy Efficiency,"mn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.arra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:2355,reduce,reduce,2355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,2,['reduce'],['reduce']
Energy Efficiency,"mn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:2026,reduce,reduce,2026,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,5,['reduce'],['reduce']
Energy Efficiency,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1365,efficient,efficiently,1365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['efficient'],['efficiently']
Energy Efficiency,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,adapt,adapt,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['adapt'],['adapt']
Energy Efficiency,"numpy.core._exceptions.MemoryError: Unable to allocate 15.9 GiB for an array with shape (180000, 23752) and data type float32",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2080:46,allocate,allocate,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2080,1,['allocate'],['allocate']
Energy Efficiency,"ompile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: cannot pickle 'weakref' object; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:4909,reduce,reduce,4909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,1,['reduce'],['reduce']
Energy Efficiency,"orsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086688-66a3af40-ecc1-4b2e-a3e4-8038f7f207b0.png). ```; #make purple colormap; colors2 = plt.cm.Purples(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap4 = mymap(np.arange(mymap.N)); my_cmap4[:,-1] = np.linspace(0, 1, mymap.N); my_cmap4 = colors.ListedColormap(my_cmap4). sc.pl.umap(adata, color=['VWF'], use_raw=True, color_map=my_cmap4, show=False, frameon=False, vmax=5); ```; ![image](https://user-images.githubusercontent.com/56206488/12608",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:1602,green,green,1602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['green'],['green']
Energy Efficiency,"ot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wondering if there's a good way to show ""colors cannot be trusted in this region"". This could be done like how camera's do zebra stripes – where a texture is overlaid on the viewfinder for the sensor pixels which are saturated with light.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:2190,sensor,sensor,2190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155,1,['sensor'],['sensor']
Energy Efficiency,"p(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3475,Reduce,Reduced,3475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['Reduce'],['Reduced']
Energy Efficiency,"posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values; posx and posy should be finite values. /Users/user/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce; return ufunc.reduce(obj, axis, dtype, out, **passkwargs). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 339 pass; 340 else:; --> 341 return printer(obj); 342 # Finally look for special method names; 343 method = get_real_method(obj, self.print_method). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/pylabtools.py in <lambda>(fig); 241 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)); 242 if 'retina' in formats or 'png2x' in formats:; --> 243 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)); 244 if 'jpg' in formats or 'jpeg' in formats:; 245 jpg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'jpg', **kwargs)). ~/.pyenv/versions/3.6.5/Python.framewor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/264:4127,reduce,reduce,4127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264,1,['reduce'],['reduce']
Energy Efficiency,"precation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError(""minvalue must be positive""); 1182 ; 1183 def __call__(self, value, clip=None):; ValueError: minvalue must be positive; ```. #### Versions. <details>. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3868,Power,PowerNorm,3868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['Power'],['PowerNorm']
Energy Efficiency,"pytables 3.7.0 is incompatible with Windows. Installing pytables 3.6.1 solves the problem.; For the UMAP, it's quite weird. Whether I restart anaconda between installing individual packages plays the trick.; It should be done in this way.; ```python; !pip install tables==3.6.1. !pip install scanpy[leiden]; !pip install -U scvelo; !pip install tqdm; !pip install ipywidgets; !pip install cellrank. close Edge. run anaconda powershell prompt (anaconda3) as admin; conda activate HYJ_py38; conda install -y -c anaconda cytoolz; pip install pyscenic. restart anaconda3. !pip install adjustText openpyxl; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391:424,power,powershell,424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1013605391,1,['power'],['powershell']
Energy Efficiency,"quick practical comment on this very interesting discussion.; @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well.; Sorry to put pressure but we are on a tight schedule 😅 . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491:314,schedul,schedule,314,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491,1,['schedul'],['schedule']
Energy Efficiency,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,adapt,adapted,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['adapt'],['adapted']
Energy Efficiency,"s!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warning to notify the user if the weights were computed in a non-standard way; * There should be a function for computing a perplexity weighted nearest neighbor graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:1324,efficient,efficient,1324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['efficient'],['efficient']
Energy Efficiency,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:34,reduce,reduced,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158,1,['reduce'],['reduced']
Energy Efficiency,"sure! in short: alex said he didn’t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,adapt,adapt,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977,1,['adapt'],['adapt']
Energy Efficiency,"t']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 C106_N; ... ; TTTGCGCAGACACGAC-1-SMC25T SMC25T; TTTGCGCCATGGAATA-1-SMC25T SMC25T; TTTGCGCCATGTTCCC-1-SMC25T SMC25T; TTTGGTTGTAGGGTAC-1-SMC25T SMC25T; TTTGTCAAGAGGGATA-1-SMC25T SMC25T; Name: patient, Length: 44245, dtype: category; Categories (39, object): ['C106_N', 'C126_N', 'C130_N', 'C133_N', ..., 'C138_T', 'C168_T', 'SMC17T', 'SMC20T']; ```; ```python; adata_sub2.obs['type']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 Normal; ... ; TTTGCGCAGACACGAC-1-SMC25T NMCAD; TTTGCGCCATGGAATA-1-SMC25T NMCAD; TTTGCGCCATGTTCCC-1-SMC25T NMCAD; TTTGGTTGTAGGGTAC-1-SMC25T NMCAD; TTTGTCAAGAGGGATA-1-SMC25T NMCAD; Name: type, Length: 44245, dtype: category; Categories (3, object): ['Normal', 'NMCAD', 'MCAD']; ```. I know Scanpy has `sc.pl.matrixplot()`, and we can make matrix plot for gene set score, like; ```python; sc.pl.matrixplot(adata_sub2, var_names=['Normal_signature'], groupby='patient', use_raw=True, dendrogram=False, cmap='inferno', standard_scale='var', swap_axes=True, save='27.pdf'); ```; <img width=""1052"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/75048821/ccde5e13-ae61-4a04-b098-e693e9543103"">. Could you please let me know whether Scanpy can make a matrixplot-like plot showing the 'type' grouped by 'patient'? like:; ```python; sc.pl.matrixplot(adata_sub2, var_names=['type'], groupby='patient', palette={'Normal': 'blue', 'NMCAD': 'green', 'MCAD': 'red'}, dendrogram=False, swap_axes=True, save='27.pdf'); ```; <img width=""824"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/75048821/7fc4f8eb-c53c-49a2-ac91-ccdbacee81e2"">. Thank you!; Best,; Yuanjian",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2797:2071,green,green,2071,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2797,1,['green'],['green']
Energy Efficiency,"thanks Phil - as discussed before can you just change the red and green i.e. #ff7f0e #2ca02c which look virtually the same to people with deuteranomaly (incl me) by the two colors you suggested?. > Am 06.12.2018 um 10:17 schrieb Philipp A. <notifications@github.com>:; > ; > Some colors of that cycle are too close for people with deuteranomaly (by far the most common type):; > ; > #1f77b4 #ff7f0e #2ca02c #d62728 #9467bd #8c564b #e377c2 #7f7f7f #bcbd22 #17becf; > ; > Normal	Deuteranomaly; > 	; > 	• red and green; > 	• blue and purple; > 	• orange and kakhi; > Playing around a bit, it’s easy to get a version that works, e.g. #1f77b4 #ff7f0e #279e68 #d62728 #aa40fc #8c564b #e377c2 #7f7f7f #b5bd61 #17becf; > ; > Normal	Deuteranomaly; > 	; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; > . ---; Fabian Theis; Institute of Computational Biology - http://icb.helmholtz-muenchen.de; Helmholtz Zentrum München and Depts. Mathematics&Life Sciences, TU München. Helmholtz Zentrum Muenchen; Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH); Ingolstaedter Landstr. 1; 85764 Neuherberg; www.helmholtz-muenchen.de; Aufsichtsratsvorsitzende: MinDirig.in Petra Steiner-Hoffmann; Stellv.Aufsichtsratsvorsitzender: MinDirig. Dr. Manfred Wolter; Geschaeftsfuehrer: Prof. Dr. med. Dr. h.c. Matthias Tschoep, Heinrich Bassler, Dr. rer. nat. Alfons Enhsen; Registergericht: Amtsgericht Muenchen HRB 6466; USt-IdNr: DE 129521671",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210:66,green,green,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444805210,2,['green'],['green']
Energy Efficiency,"tivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5567,Reduce,Reduced,5567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['Reduce'],['Reduced']
Energy Efficiency,"types.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. /usr/local/lib/python3.7/dist-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: can't pickle weakref objects. How to solve this problem? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951:4170,reduce,reduce,4170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951,1,['reduce'],['reduce']
Energy Efficiency,"u keep the current calculations, but parallelize over the groups. Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting – since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` – so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently bei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1333,power,power,1333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,1,['power'],['power']
Energy Efficiency,"when you use the firefox extension I quoted above, you’ll see that the orange and green are very similar for people with *protanomaly*, not deuteranomaly. so i guess fabian has the former, more rare thing. confirmed by fabian and the simulation, my changed colors seem to work well for both types. so I assume the webtool can be used to design this. the only change from default in the parameters I made was that i removed the minimum lightness distance (making the colors not suited for completely color blind people, but having no cone cells, their vision is probably too bad to see our plots anyway.). PS: please use backticks around colors, like `#fe57a1` so we can see little swatches on github! I edited your post for this @LuckyMD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/387#issuecomment-444852474:82,green,green,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/387#issuecomment-444852474,1,['green'],['green']
Energy Efficiency,"wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:1423,adapt,adapting,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,2,"['adapt', 'power']","['adapting', 'power']"
Integrability," 'expression'); ```. ```pytb; [/Users/dan/opt/anaconda3/lib/python3.9/site-packages/openpyxl/reader/workbook.py:88: UserWarning: File contains an invalid specification for expression. This will be removed; warn(msg). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); /var/folders/f1/7pq3lkn50cb7mtfkxrk9fnfh0000gn/T/ipykernel_3372/1224090119.py in <module>; ----> 1 adata = sc.read_excel(filename = '/Users/dan/Documents/Graduate_School/Bioinformatics_MS/BF550/Project/muscle_expression_.xlsx', sheet = 'expression'). ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_io/read.py in read_excel(filename, sheet, dtype); 73 from pandas import read_excel; 74 ; ---> 75 df = read_excel(fspath(filename), sheet); 76 X = df.values[:, 1:]; 77 row = dict(row_names=df.iloc[:, 0].values.astype(str)). ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 309 stacklevel=stacklevel,; 310 ); --> 311 return func(*args, **kwargs); 312 ; 313 return wrapper. ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in read_excel(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options); 463 ; 464 try:; --> 465 data = io.parse(; 466 sheet_name=sheet_name,; 467 header=header,. ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 1456 DataFrame from the passed in Excel file.; 1457 """"""; -> 1458 return self._reader.parse(; 1459 sheet_name=sheet_name,; 1460 header=header,. ~/opt/anaconda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:1576,wrap,wrapper,1576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,2,['wrap'],['wrapper']
Integrability," (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2408,wrap,wrapper,2408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability," 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-vi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15132,wrap,wrapt,15132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability," 20.2.0; backcall 0.2.0; brotli NA; cellrank 1.0.0-rc.10; certifi 2020.06.20; cffi 1.14.3; chardet 3.0.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; datacache 1.1.5; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; gtfparse 1.2.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; importlib_metadata 0.23; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.14.0; jsonschema 3.2.0; kiwisolver 1.1.0; legacy_api_wrap 1.2; leidenalg 0.8.1; libpetsc4py NA; llvmlite 0.34.0; louvain 0.6.1+2.g8073db7; markupsafe 1.1.1; matplotlib 3.3.2; memoized_property NA; more_itertools NA; mpl_toolkits NA; natsort 6.0.0; nbformat 5.0.7; networkx 2.3; numba 0.51.2; numexpr 2.7.0; numpy 1.19.1; packaging 20.4; palantir 1.0.0; pandas 1.1.2; parso 0.7.1; petsc4py 3.13.0; pexpect 4.8.0; phenograph 1.5.6; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.0; progressbar 3.53.1; prometheus_client NA; prompt_toolkit 3.0.7; psutil 5.7.2; ptyprocess 0.6.0; pvectorc NA; py 1.8.0; pyensembl 1.8.7; pygam 0.8.0; pygments 2.7.1; pyparsing 2.4.2; pyrsistent NA; pytest 5.2.1; python_utils NA; pytz 2019.2; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; scvelo 0.2.2; seaborn 0.11.0; send2trash NA; serializable 0.2.1; setuptools_scm NA; simplejson 3.17.2; sinfo 0.3.1; six 1.15.0; sklearn 0.21.3; slepc4py 3.13.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.1; storemagic NA; tables 3.5.2; terminado 0.9.1; texttable 1.6.3; tornado 6.0.4; traitlets 4.3.3; typechecks NA; typing_extensions NA; tzlocal NA; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.1.2; zipp NA; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) [GCC 7.5.0]; Linux-4.4.0-189-generic-x86_64-with-debian-buster-sid; 24 logical CPU cores, x86_64; -----; Session information updated at 2020-09-30 12:20. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:3710,wrap,wrapt,3710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['wrap'],['wrapt']
Integrability," I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; We ran across a case where score genes does not work for one specific gene, but works for others; the gene is in anndata. The same code works on other datasets preprocessed in the same way. ```pytb; genes=['INS']; score_name='ins'; 'INS' in adata_rawnorm.var_names; True; IndexError Traceback (most recent call last); Input In [109], in <module>; ----> 1 sc.tl.score_genes(adata_rawnorm, gene_list=genes, score_name=score_name+'_score', use_raw=False). File ~/miniconda3/envs/block/lib/python3.8/site-packages/scanpy/tools/_score_genes.py:167, in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 164 else:; 165 X_list = np.nanmean(X_list, axis=1, dtype='float64'); --> 167 X_control = _adata[:, control_genes].X; 168 if issparse(X_control):; 169 X_control = np.array(_sparse_nanmean(X_control, axis=1)).flatten(). File ~/miniconda3/envs/block/lib/python3.8/site-packages/anndata/_core/anndata.py:625, in AnnData.X(self); 622 X = _subset(X, (self._oidx, self._vidx)); 623 elif self.is_view:; 624 X = as_view(; --> 625 _subset(self._adata_ref.X, (self._oidx, self._vidx)),; 626 ElementRef(self, ""X""),; 627 ); 628 else:; 629 X = self._X. File ~/miniconda3/envs/block/lib/python3.8/functools.py:875, in singledispatch.<locals>.wrapper(*args, **kw); 871 if not args:; 872 raise TypeError(f'{funcname} requires at least '; 873 '1 positional argument'); --> 875 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/block/lib/python3.8/site-packages/anndata/_core/index.py:127, in _subset(a, subset_idx); 125 if all(isinstance(x, cabc.Iterable) for x in subset_idx):; 126 subset_idx = np.ix_(*subset_idx); --> 127 return a[subset_idx]. IndexError: arrays used as indices must be of integer (or boolean) type; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2153:1505,wrap,wrapper,1505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2153,1,['wrap'],['wrapper']
Integrability," Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10416,wrap,wrap,10416,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:2219,message,messages,2219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['message'],['messages']
Integrability," a) numba was failing inference unless I was explicit about integers and b) downsampling counts only makes sense for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>; <summary> 🍝 </summary>. ```python; def log1p(data, copy=False, chunked=False, chunk_size=None):; """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters; ----------; data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; copy : `bool`, optional (default: `False`); If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; if copy:; if not isinstance(data, AnnData):; data = data.astype(np.floating); data = data.copy(); elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):; raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):; if issparse(X):; np.log1p(X.data, out=X.data); else:; np.log1p(X, out=X). return X. if isinstance(data, AnnData):; if not np.issubdtype(data.X.dtype, np.floating):; data.X = data.X.astype(np.floating, copy=False); if chunked:; for chunk, start, end in data.chunked_X(chunk_size):; data.X[start:end] = _log1p(chunk); else:; _log1p(data.X); else:; _log1p(data). return data if copy else None; ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239:1225,depend,depending,1225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239,1,['depend'],['depending']
Integrability," about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_visium(path, schema=""v1""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:1076,depend,depend,1076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['depend'],['depend']
Integrability," brotli NA; celltypist 1.6.2; certifi 2023.11.17; cffi 1.16.0; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2022.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; dill 0.3.7; docrep 0.3.2; entrypoints 0.4; exceptiongroup 1.2.0; executing 0.8.3; fsspec 2023.10.0; h5py 3.7.0; idna 3.4; igraph 0.10.8; inflect NA; ipykernel 6.28.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.3; joblib 1.3.2; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.7; numpy 1.26.3; omnipath 1.0.8; packaging 23.1; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prompt_toolkit 3.0.43; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydantic 1.10.12; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pytz 2023.3.post1; requests 2.31.0; ruamel NA; scipy 1.11.4; seaborn 0.12.2; session_info 1.0.0; setuptools 65.6.3; six 1.16.0; sklearn 1.4.1.post1; snappy NA; socks 1.7.1; sparse 0.14.0; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.1; tblib 1.7.0; texttable 1.7.0; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.12.0; torch 1.12.1; tornado 6.1; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; urllib3 1.26.18; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0.1; zipp NA; zmq 25.1.2; zoneinfo NA; zope NA; zstandard 0.19.0; -----; IPython 8.20.0; jupyter_client 7.3.4; jupyter_core 5.5.0; jupyterlab 3.5.3; notebook 6.5.2; -----; Python 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]; macOS-14.1.2-arm64-arm-64bit; -----; Session information updated at 2024-03-12 14:52; ​. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906:2622,wrap,wrapt,2622,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906,1,['wrap'],['wrapt']
Integrability," conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Both https://github.com/scverse/scanpy/commit/23c20bc4dd3b1a1e5432ad4e23f56cf5efcf7ebe and https://dev.azure.com/scverse/scanpy/_build/results?buildId=6733&view=logs&j=cb4d9293-b492-5d67-02b0-e6a595893958&t=99aeec2e-a40e-57fc-1ab3-27c1a626c3e0&s=96ac2280-8cb4-5df5-99de-dd2da759617d show errors from scrublet which I cannot reproduce locally. ### Minimal code sample. ```python; See `test_scrublet_data` under `anndata_dev`; ```. ### Error output. ```pytb; E AssertionError: ; E Not equal to tolerance rtol=1e-15, atol=1e-15; E ; E Mismatched elements: 1 / 200 (0.5%); E Max absolute difference: 0.0126501664; E Max relative difference: 0.1823112224; E x: array([0.033079, 0.039326, 0.047022, 0.069388, 0.047022, 0.033079,; E 0.069388, 0.019841, 0.069388, 0.069388, 0.069388, 0.039326,; E 0.033079, 0.149254, 0.047022, 0.056738, 0.047022, 0.023555,...; E y: array([0.033079, 0.039326, 0.047022, 0.069388, 0.047022, 0.033079,; E 0.069388, 0.019841, 0.069388, 0.069388, 0.069388, 0.039326,; E 0.033079, 0.149254, 0.047022, 0.056738, 0.047022, 0.023555,...; ```. ### Versions. <details>. ```; Package Version; ----------------- -------------------------; anndata 0.11.0.dev114+g105f354; annoy 1.17.3; scipy 1.13.0; scprep 1.1.0; seaborn 0.13.2; session-info 1.0.0; setuptools 69.5.1; setuptools-scm 8.1.0; six 1.16.0; sniffio 1.3.1; sortedcontainers 2.4.0; sparse 0.16.0a6; statsmodels 0.14.2; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; textual 0.60.1; threadpoolctl 3.5.0; tifffile 2024.5.10; toolz 0.12.1; tornado 6.4; tqdm 4.66.4; typing-extensions 4.12.0rc1; tzdata 2024.1; uc-micro-py 1.0.3; umap-learn 0.5.6; urllib3 2.2.1; uv 0.1.44; virtualenv 20.26.2; wrapt 1.16.0; zarr 2.18.1; zict 3.0.0. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3068:1972,wrap,wrapt,1972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3068,1,['wrap'],['wrapt']
Integrability," dataset). At all times there seems to be enough free RAM / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1312,wrap,wrapper,1312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability," dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as ad; import dask.array as da; import scanpy as sc. # write data to zarr file; rel_zarr_path = 'data/pbmc3k_processed.zarr'; adata = sc.datasets.pbmc3k_processed(); adata.write_zarr(f'./{rel_zarr_path}', chunks=[adata.shape[0], 5]); zarr.consolidate_metadata(f'./{rel_zarr_path}'). # read data from zarr file with X as a dask array; def read_dask(store):; f = zarr.open(store, mode=""r""). def callback(func, elem_name: str, elem, iospec):; if iospec.encoding_type in (; ""dataframe"",; ""csr_matrix"",; ""csc_matrix"",; ""awkward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.fr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:1717,wrap,wrapped,1717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapped']
Integrability," even when you're comparing drastically different cells with dramatic differences in RNA content (e.g. regulatory T-cells with large cells like macrophages). In this scenario, however, any sensitive differential testing should be done within similar cell types to account for these massive differences. For differential testing, I agree with you that their suggestion of increasing the pseudo-count reduces power and is not ideal. Your suggestion of checking size-factor distributions is definitely an interesting idea, but beyond the scope of the package I think. However, I do like the idea for further downstream verification of differentially regulated genes, particularly those with non-obvious distribution changes. In general, it seems that the authors of the paper are particularly worried about spurious architecture in the embeddings, but some of these can be tested and discarded with diff expression testing. Of note, I usually also see a count depth effect in embedding as well, even with normalization methods other than CPM normalization. I've never been satisfied with attributing all count-number variation as technical noise, which like you said is definitely incorrect, particularly when comparing conditions that could include drastic changes in transcription. All of this is to say, it seems that effect of log-mean vs mean-log should be manageable for embeddings, and like you mentioned, so much of the downstream processing depends on approximation of normal distributions that it seems impossible to get rid of the log-transformation. Overall, I haven't been able to find a satisfying solution either. In general, I tend to say that any obvious expression differences should be minimally affected by this problem, however, when you're trying to distinguish more closely-related subpopulations, mean-log might have an undesirable effect. However, at this stage, no test or transformation is ideal, and we should be validating these subpopulations using several methods anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823:1661,depend,depends,1661,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-474080823,1,['depend'],['depends']
Integrability," filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20453,wrap,wraps,20453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wraps']
Integrability," igraph 0.10.4; importlib_resources NA; ipykernel 6.23.1; ipython_genutils 0.2.0; ipywidgets 8.0.6; jax 0.4.12; jaxlib 0.4.12; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; lightning 2.0.3; lightning_cloud NA; lightning_fabric 2.0.3; lightning_utilities 0.8.0; llvmlite 0.40.0; louvain 0.8.0; matplotlib 3.7.1; matplotlib_inline 0.1.6; ml_collections NA; ml_dtypes 0.2.0; mpl_toolkits NA; mpmath 1.3.0; msgpack 1.0.5; mudata 0.2.3; multidict 6.0.4; multipart 0.0.6; multipledispatch 0.6.0; natsort 8.3.1; numba 0.57.0; numpy 1.24.3; numpyro 0.12.1; nvfuser NA; opt_einsum v3.3.0; optax 0.1.5; ordered_set 4.1.0; packaging 23.1; pandas 2.0.2; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.3; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydantic 1.10.9; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.15.1; pyparsing 3.0.9; pyro 1.8.5; pytorch_lightning 2.0.3; pytz 2023.3; requests 2.31.0; rich NA; scipy 1.10.1; scvi 1.0.0; seaborn 0.12.2; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; sklearn 1.2.2; sniffio 1.3.0; socks 1.7.1; soupsieve 2.3.2.post1; sparse 0.14.0; sphinxcontrib NA; stack_data 0.6.2; starlette 0.22.0; statsmodels 0.14.0; sympy 1.12; texttable 1.6.7; threadpoolctl 3.1.0; toml 0.10.2; toolz 0.12.0; torch 2.0.1+cu117; torchmetrics 0.11.4; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; tree 0.1.8; typing_extensions NA; urllib3 2.0.3; uvicorn 0.22.0; wcwidth 0.2.6; websocket 1.5.3; websockets 11.0.3; wrapt 1.15.0; xarray 2023.5.0; yaml 6.0; yarl 1.9.2; zmq 25.1.0; zoneinfo NA; -----; IPython 8.14.0; jupyter_client 8.2.0; jupyter_core 5.3.1; notebook 6.5.4; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]; Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27; -----; Session information updated at 2023-07-06 03:56; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547:7144,wrap,wrapt,7144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547,1,['wrap'],['wrapt']
Integrability," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1991,wrap,wrap,1991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['wrap'],['wrap']
Integrability," is much more complicated to maintain - at least didn't feel too bad for me when I added a few recipes there in the past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1024,depend,depend,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['depend']
Integrability," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1425,depend,depends,1425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852,1,['depend'],['depends']
Integrability," k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pyd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:3503,wrap,wrapper,3503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['wrap'],['wrapper']
Integrability," methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1438,integrat,integration,1438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643,1,['integrat'],['integration']
Integrability," more predictability at the expense of backwards compatibility*? Especially the “euclidean” condition makes not much sense IMHO; - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'` … ; https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617; - I also changed `method` to only mean “connectivity method”. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR; - [x] figure out what the `_more_tags` methods are ; - [x] allow specifying algorithm and/or backend; - [x] revert 75c6670, move connectivities code out of backends; - [x] switch our stuff to KNeighborsTransformer; - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity); - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually tes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:2149,depend,depend,2149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['depend'],['depend']
Integrability," past. My concerns are mostly fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too – this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels ❓ . And thanks all for the ongoing discussion, still learning things here and also getting new perspective",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1142,depend,dependents,1142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,1,['depend'],['dependents']
Integrability," run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: must be str, not list; ```. </details>. I have never had this error message before and Google can't find anything. . Debugging this a bit, it happens because in this line in build_py.py:. ```py; globs = (self.package_data.get('', []); + self.package_data.get(package, [])); ```. package is ""scanpy"" and the first part before the + is `""*.txt""` and the second part after the + is `[]`. This is Python 3.6.0a1 and pip 9.0.1 on Centos 6.8",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:8475,message,message,8475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['message'],['message']
Integrability," scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When I try to plot a scatter plot color coded by categorical data, it still output the image, but the legend does not include colors. Additionally, if i use continuous data as the key for plotting, the code executes as expected. Thanks!. ![image](https://github.com/scverse/scanpy/assets/43973217/8c1b0a03-3c0b-4452-b759-9cf588b45c53). ### Minimal code sample. ```python; sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1175,wrap,wrapper,1175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['wrap'],['wrapper']
Integrability," such a method in its. ValueError: provided out is the wrong size for the reduction; ```. #### Versions. <details>. ```; -----; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 7.2.0; anndata 0.7.4; appdirs 1.4.4; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; colorama 0.4.3; colorlog NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; fa2 NA; fcsparser 0.2.1; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; markupsafe 1.1.1; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; palantir 1.0.0; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.6; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; rpy2 3.4.2; sca NA; scanpy 1.7.0; scipy 1.4.1; scprep 1.0.5.post2; seaborn 0.10.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; texttable 1.6.2; threadpoolctl 2.1.0; tornado 6.0.4; tqdm 4.48.2; traitlets 4.3.3; tzlocal NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-19 11:23; ```; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1670:4337,wrap,wrapt,4337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670,1,['wrap'],['wrapt']
Integrability," the IPython notebooks), the scanpy ""set_figure_params(...)"" command just outputs an ""In :"", and I need to exit using [Ctrl][D]: . ### Minimal code sample (that we can copy&paste without having any data); ```; import scanpy as sc; sc.set_figure_params(figsize=(4, 4)); ```; The output in console is:; ```; In :. In :; ```; and need to press [Crtrl][D] to exit, as otherwise this just loops ""In :"" repeatedly. If I add ""ipython_format=None"":; ```; sc.set_figure_params(figsize=(4, 4), ipython_format=None); ```; then this continues without any ""In :"", and I can continue with script. . I think the reason for this is in the scanpy ""_settings.py"" file:; ```; def set_figure_params(; ......etc.....; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; from matplotlib import rcParams; .....etc......; ```; where the:; ```; IPython.display.set_matplotlib_formats(*ipython_format); ```; produces this ""In :"" message loop. I think this bug could be fixed by adding ""if self._is_run_from_ipython():"" before the ""try: import IPython"": ; ```; def set_figure_params(; ......etc.....; if self._is_run_from_ipython():; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; ```; The ""try:"" ; ```; >>> import IPython; ```; doesn't throw an exception, as IPython is installed:; ```; $ pip show IPython; Name: ipython; Version: 7.18.1; Summary: IPython: Productive Interactive Computing; Home-page: https://ipython.org; Author: The IPython Development Team; Author-email: ipython-dev@python.org; License: BSD; Location: /slipstream/home/sbridgett/miniconda3/lib/python3.8/site-packages; Requires: pygments, jedi, prompt-toolkit, backcall, pexpect, setuptools, decorator, traitlets, pickleshare; Required-by: ipywidgets, ipykernel; ```. whereas the _is_run_from_ipython()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477:1344,message,message,1344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477,1,['message'],['message']
Integrability," the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3553,Depend,Depending,3553,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['Depend'],['Depending']
Integrability," this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(; ncase,; n_top_genes=3000,; # subset=True, # to automatically subset to the 4000 genes; layer=""counts"",; flavor=""seurat""; ); ```. ```pytb; ValueError: cannot specify integer `bins` when input data contains infinity; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; marku",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:1115,message,message,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['message'],['message']
Integrability," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1091,wrap,wrap,1091,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021,1,['wrap'],['wrap']
Integrability," with `sc.pp.pca` and default params.; computing PCA; with n_comps=50; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-65-c244be664e51> in <module>(); ----> 1 sc.tl.tsne(adata, n_pcs = 50); 2 # UMAP, first with neighbor calculation; 3 sc.pp.neighbors(adata, n_pcs = 50, n_neighbors = 20); 4 sc.tl.umap(adata). ~/.local/lib/python3.6/site-packages/scanpy/tools/_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 78 start = logg.info('computing tSNE'); 79 adata = adata.copy() if copy else adata; ---> 80 X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs); 81 # params for sklearn; 82 params_sklearn = dict(. ~/.local/lib/python3.6/site-packages/scanpy/tools/_utils.py in _choose_representation(adata, use_rep, n_pcs, silent); 41 'Falling back to preprocessing with `sc.pp.pca` and default params.'; 42 ); ---> 43 X = pca(adata.X); 44 adata.obsm['X_pca'] = X[:, :n_pcs]; 45 else:. ~/.local/lib/python3.6/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 199 ; 200 output = _pca_with_sparse(; --> 201 X, n_comps, solver=svd_solver, random_state=random_state; 202 ); 203 # this is just a wrapper for the results. ~/.local/lib/python3.6/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 298 shape=X.shape,; 299 rmatvec=rmatvec,; --> 300 rmatmat=rmatmat,; 301 ); 302 . /usr/local/anaconda/lib/python3.6/site-packages/scipy/sparse/linalg/interface.py in __new__(cls, *args, **kwargs); 131 if cls is LinearOperator:; 132 # Operate as _CustomLinearOperator factory.; --> 133 return _CustomLinearOperator(*args, **kwargs); 134 else:; 135 obj = super(LinearOperator, cls).__new__(cls). TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252:2336,wrap,wrapper,2336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252,2,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"""""""; 341 if isinstance(moduleOrReq, Requirement):; --> 342 return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]; 343 try:; 344 module = sys.modules[moduleOrReq]. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in require(self, *requirements); 884 included, even if they were already activated in this working set.; 885 """"""; --> 886 needed = self.resolve(parse_requirements(requirements)); 887 ; 888 for dist in needed:. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras); 770 if dist is None:; 771 requirers = required_by.get(req, None); --> 772 raise DistributionNotFound(req, requirers); 773 to_activate.append(dist); 774 if dist not in req:. DistributionNotFound: The 'pynndescent' distribution was not found and is required by the application; ```. #### Versions; latest version of scipy as well as pynndescent. strangely enough, if I try to print the versions I get this error as well:; ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /tmp/955352.tmpdir/ipykernel_5496/3375110566.py in <module>; ----> 1 print(sc.logging.print_versions()). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/logging.py in print_versions(file); 167 try:; 168 buf = sys.stdout = io.StringIO(); --> 169 sinfo(; 170 dependencies=True,; 171 excludes=[. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/sinfo/main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 208 for mod_name in clean_modules:; 209 mod_names.append(mod_name); --> 210 mod = sys.modules[mod_name]; 211 # Since modules use different attribute names to store version info,; 212 # try the most common ones. KeyError: 'umap'; ```. I should note that I uninstalled and reinstalled the latest version of umap-learn.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:4822,depend,dependencies,4822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,2,['depend'],['dependencies']
Integrability,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,inject,injecting,2178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530,1,['inject'],['injecting']
Integrability,"## Description. Running the scanpy tests locally (see system information and package versions below), I noticed the test `scanpy/tests/test_plotting.py::test_paga` sometimes failing due to `AssertionError: Error: Image files did not match` but passing on a consecutive run. Did anyone encounter similar problems or does anyone know why this is happening?. ### Failure message. ```bash; >>> pytest scanpy/tests/test_plotting.py; scanpy/tests/test_plotting.py .............................................x............F.... [100%]. =========================================================================================================== FAILURES ============================================================================================================; ___________________________________________________________________________________________________________ test_paga ___________________________________________________________________________________________________________. image_comparer = <function make_comparer at 0x7fbce4ade8c0>. def test_paga(image_comparer):; # Sometimes things shift a pixel or so, resulting in diffs up to ~27; # The 1px-edges aren’t that good actually as they’re ignored at this tol …; save_and_compare_images = image_comparer(ROOT, FIGS, tol=30); ; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.paga(pbmc, groups='bulk_labels'); ; common = dict(threshold=0.5, max_edge_width=1.0, random_state=0, show=False); ; # delete bulk_labels_colors to test the creation of color list by paga; del pbmc.uns['bulk_labels_colors']; sc.pl.paga(pbmc, **common); save_and_compare_images('master_paga'); ; sc.pl.paga(pbmc, color='CST3', **common); save_and_compare_images('master_paga_continuous'); ; pbmc.obs['cool_feature'] = pbmc[:, 'CST3'].X.squeeze(); sc.pl.paga(pbmc, color='cool_feature', **common); save_and_compare_images('master_paga_continuous_obs'); ; sc.pl.paga(pbmc, color=['CST3', 'GATA2'], **common); save_and_compare_images('master_paga_continuous_multiple'); ; sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1418:368,message,message,368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418,1,['message'],['message']
Integrability,"## Description. The documentation in `CONTRIBUTING.md` states that all dependencies required for testing can be installed via `pip install scanpy[test]`. It should, however, be `pip install ""scanpy[test]""`. ### Minimal code sample. ```zsh; pip install scanpy[test]; zsh: no matches found: scanpy[test]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441:71,depend,dependencies,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441,1,['depend'],['dependencies']
Integrability,"## Test failures on 141eb6a315542317ddab2f7a413a24559c84492f. 252 failed, 650 passed, 59 skipped, 5 xfailed, 1038 warnings, 128 errors in 451.20s (0:07:31). Noting some of the big causes:. ### Expected warnings not thrown. A few times. ### Older versions of pandas do not support `na_action`. Likely caused during [Fix more pandas warnings by flying-sheep · Pull Request #2789 · scverse/scanpy](https://github.com/scverse/scanpy/pull/2789). Which did also bump up the required pandas version to 2.1.3. However, I think we'll want to revert that bump because:. * According to the scientific python versioning specification we're still meant to be on 1.4 ; * More than half of pandas users are still on 1.*; * Bumping pandas up to 2.1.3 actually requires bumping the versions on a number of other dependencies whose current minimums do not work with pandas 2.1.3. ### ufunc equal. Something is happening in a lot of plotting functions with the `equal` ufunc. ### Numba NotImplementedError. During `test_highly_variable_genes_pearson_residuals_general`. ### AnnData private methods used in tests. A lot of private anndata methods are used at test time. But these didn't exist at the time. Not totally sure what the best solution here is. * Vendoring anndata test helpers over here.; * Literally pulling in the file is probably not so bad; * I will investigate to see how many functions are really needed, possible it's just a few one liners (`as_dense_dask_array` is getting hit often); * Make a new package with just the test helpers? Probably too much of a pain. ### ImportError: cannot import name 'check_is_fitted' from 'sklearn.base'. <details>; <summary> Raw test output </summary>. ```python; FAILED scanpy/tests/test_datasets.py::test_krumsiek11 - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.; FAILED scanpy/tests/test_datasets.py::test_toggleswitch - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.; FAILED scanpy/get/get.py::",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515:795,depend,dependencies,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895708515,1,['depend'],['dependencies']
Integrability,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:840,interface,interface,840,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154,2,"['integrat', 'interface']","['integrating', 'interface']"
Integrability,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:354,wrap,wrapping,354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['wrap'],['wrapping']
Integrability,"### Please describe your wishes and possible alternatives to achieve the desired result. No key inside 'use_rep' can mean 'X' or 'X_pca' was used for nearest neighbor calculation depending on the number of vars. It's storing things correctly if 'use_rep' is called explicitly. ```py; sc.pp.neighbors(combined_emb, n_neighbors=50); combined_emb.uns[""neighbors""][""params""][""use_rep""] -> KeyError. sc.pp.neighbors(combined_emb, n_neighbors=50, use_rep='X'); combined_emb.uns[""neighbors""][""params""][""use_rep""] -> 'X'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2736:179,depend,depending,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2736,1,['depend'],['depending']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Attempted label transfer using `ingest`, using the example code on the [docs page](https://scanpy.readthedocs.io/en/1.10.x/generated/scanpy.tl.ingest.html). The fix is trivial (you must run `sc.pp.pca(adata_ref)` as well), but the error message is cryptic so I got stuck for a bit! I'll create the PR too. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd. sc.settings.verbosity = 1 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:526,message,message,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I was running a basic workflow with 3k PBMC and at the step sc.pl.stacked_violin got Attribute error below. ### Minimal code sample. ```python; sc.pl.stacked_violin(adata, marker_genes, groupby='leiden', rotation=90); ```. ### Error output. ```pytb; --------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[51], line 1; ----> 1 sc.pl.stacked_violin(adata, marker_genes, groupby='leiden', rotation=90);. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/scanpy/plotting/_stacked_violin.py:825, in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, title, colorbar_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, standard_scale, var_group_rotation, layer, stripplot, jitter, size, scale, yticklabels, order, swap_axes, show, save, return_fig, row_palette, cmap, ax, vmin, vmax, vcenter, norm, **kwds); 823 if return_fig:; 824 return vp; --> 825 vp.make_figure(); 826 savefig_or_show(StackedViolin.DEFAULT_SAVE_PREFIX, show=show, save=save); 827 show = settings.autoshow if show is None else show. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/scanpy/plotting/_baseplot_class.py:792, in BasePlot.make_figure(self); 789 return_ax_dict[""gene_gr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:886,wrap,wrapper,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi all. I was having issues generating rank gene groups. The error is as below. When I used the ""Manuscript_Identity"" group, I got such error message, but when I used another group ""CellType_Category"", it worked. These two groups are in the same type. Could you tell me how to fix it?; Look forward to your response, thanks a lot!. ### Minimal code sample. ```python; sc.tl.rank_genes_groups(adata_sc, groupby=""Manuscript_Identity"", use_raw=False). adata_sc.obs['CellType_Category'].cat.categories; Index(['Endothelial', 'Epithelial', 'Lymphoid', 'Multiplet', 'Myeloid',; 'Stromal'],; dtype='object'); adata_sc.obs['Manuscript_Identity'].cat.categories; Index(['ATI', 'ATII', 'Aberrant_Basaloid', 'B', 'B_Plasma', 'Basal',; 'Ciliated', 'Club', 'DC_Langerhans', 'DC_Mature', 'Fibroblast',; 'Goblet', 'ILC_A', 'ILC_B', 'Ionocyte', 'Lymphatic', 'Macrophage',; 'Macrophage_Alveolar', 'Mast', 'Mesothelial', 'Multiplet',; 'Myofibroblast', 'NK', 'PNEC', 'Pericyte', 'SMC', 'T', 'T_Cytotoxic',; 'T_Regulatory', 'VE_Arterial', 'VE_Capillary_A', 'VE_Capillary_B',; 'VE_Peribronchial', 'VE_Venous', 'cDC1', 'cDC2', 'cMonocyte',; 'ncMonocyte', 'pDC'],; dtype='object'); ```. ### Error output. ```pytb; WARNING: Default of the method has been changed to 't-test' from 't-test_overestim_var'; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/udd/rekso/.conda/envs/rekso_tangram_env/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 592, in rank_genes_groups; test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts); File ""/udd/rekso/.conda/envs/rekso_tangram_env/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 106, in __i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2821:433,message,message,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2821,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am trying to run ingest in order to transfer labels from a reference dataset to a query dataset. . I have subsetted both datasets to the same features and re-run basic processing on both datasets, including pca, umap, and generating neighbor graphs. . When I run ingest I receive the following error message:; `ValueError: all input arrays must have the same shape`. ### Minimal code sample. ```python; var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. sc.tl.pca(adata_ref, svd_solver='arpack'); sc.pp.neighbors(adata_ref, n_neighbors=10, n_pcs=40); sc.tl.paga(adata_ref, groups = 'cell_type'); sc.pl.paga(adata_ref, plot=False) # remove `plot=False` if you want to see the coarse-grained graph; sc.tl.umap(adata_ref, init_pos='paga'). adata.obs['seurat_clusters'] = adata.obs['seurat_clusters'].astype('category'); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.paga(adata, groups = 'seurat_clusters'); sc.pl.paga(adata, plot=False) # remove `plot=False` if you want to see the coarse-grained graph; sc.tl.umap(adata, init_pos='paga'). sc.tl.ingest(adata, adata_ref, obs='cell_type'); ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); <ipython-input-18-6b34a6250614> in <module>; 1 # we map our tabula sapiens cell type labels onto our data; ----> 2 sc.tl.ingest(adata, adata_ref, obs='cell_type'). ~/.local/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 124 labeling_method = labeling_method * len(obs); 125 ; --> 126 ing = Ingest(adata_ref, n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:593,message,message,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I'm encountering an error when running the sc.pl.rank_genes_groups_heatmap function in the scanpy package. The error message is ""Linkage 'Z' contains negative distances."" What could be causing this error and how can I fix it?. ### Minimal code sample. ```python; sc.pl.rank_genes_groups_heatmap(adata, n_genes=10, groupby='clusters',show_gene_labels=True,save='cluster.markers.heatmap.svg'); ```. ### Error output. ```pytb; sc.pl.rank_genes_groups_heatmap(adata, n_genes=10, groupby=cluster,show_gene_labels=True,save=(id+'_processed.top10.cluster.markers.heatmap.svg')); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 673, in rank_genes_groups_heatmap; return _rank_genes_groups_plot(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 592, in _rank_genes_groups_plot; return heatmap(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 1087, in heatmap; dendro_data = _reorder_categories_after_dendrogram(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 2134, in _reorder_categories_after_dendrogram; key = _get_dendrogram_key(adata, dendrogram, groupby); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 2236, in _get_dendrogram_key; dendrogram(adata, groupby, key_added=dendrogram_key); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/tools/_dendrogram.py"", line 143, in dendrogram; dendro_info = sch.dendrogram(z_var, labels=list(categories), no_plot=True); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scipy/cluster/hierarchy.py"", line 3301, in dendrogram; is_valid_linkage(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2804:408,message,message,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2804,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistribut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:446,depend,dependencies,446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['depend'],['dependencies']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Since the most recent matplotlib release (3.8.1), the `_get_signature` function seems to have issues. I am not sure why this is the case. I am opening the issue for now and will dig a bit. Could be on my end. ### Minimal code sample. ```python; `pip install ehrapy`. `>>> import ehrapy as ep`; ```. ### Error output. ```pytb; tests/tools/causal/test_dowhy.py:10: in <module>; import ehrapy as ep; ehrapy/__init__.py:14: in <module>; from ehrapy import plot as pl; ehrapy/plot/__init__.py:3: in <module>; from ehrapy.plot._scanpy_pl_api import * # noqa: F403; ehrapy/plot/_scanpy_pl_api.py:1134: in <module>; @_wraps_plot_scatter; ../../miniconda3/envs/ehrapy/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:605: in _wraps_plot_scatter; wrapper_sig = _get_signature(wrapper, eval_str=True); ../../miniconda3/envs/ehrapy/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:590: in _get_signature; lambda: inspect.Signature.empty, get_annotations(obj, eval_str=eval_str); ../../miniconda3/envs/ehrapy/lib/python3.11/inspect.py:276: in get_annotations; return_value = {key:; ../../miniconda3/envs/ehrapy/lib/python3.11/inspect.py:277: in <dictcomp>; value if not isinstance(value, str) else eval(value, globals, locals); <string>:1: in <module>; ???; E NameError: name 'Axes' is not defined; ```. ### Versions. <details>. ```; matplotlib 3.8.1; scanpy 1.9.6; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2747:1076,wrap,wrapper,1076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2747,1,['wrap'],['wrapper']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. While I am using `sc.pp.calculate_qc_metrics(ad, inplace=True)` to get QC metrics, its reported that a error occoured. ; Error message as below. ; ﻿﻿; It might just be because there's something wrong with my data. Does anyone else have a similar situation?. ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; import anndata. # ad = anndata.read_h5ad('mypath'). def scrublet_by_sample(ad, key='samplename'):; """""" do doublet prediction by batch/sample """"""; """""" ad = anndata object """"""; """""" key = sample or batch in ad.obs""""""; sc.pp.calculate_qc_metrics(ad, inplace=True); ads = []; samplenames = ad.obs[key].unique(); for i in samplenames:; adx = ad[ad.obs[key].isin([i])].copy(); print(i,adx.n_obs); sc.external.pp.scrublet(adx,n_prin_comps=min(30,adx.shape[0]-1)); ads.append(adx); adata = ads[0].concatenate(tuple(ads[1:]), join='outer'); return adata. if np.array_equal(arr, np.round(arr)):; ad = scrublet_by_sample(ad, 'sample_ID'); ad.write(qc_h5); qc_md5 = generate_file_md5(qc_h5); print(""QC MD5 Hash:"", qc_md5); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""<stdin>"", line 5, in scrublet_by_sample; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 306, in calculate_qc_metrics; obs_metrics = describe_obs(; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 401, in top_segment_proportions; return top_segment_proportions_sparse_csr(mtx.d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2758:418,message,message,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2758,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. aggregate throws error when aggregating `obsm` or `varm`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:866,wrap,wrapper,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['wrap'],['wrapper']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was trying to install `scanpy=1.9.6` using conda in a `python=3.9` environment that had 1.9.5 working with `seaborn=0.13`.; Conda raised a solving issue due to ; `package scanpy-1.9.6-pyhd8ed1ab_0 requires seaborn !=0.13.0, but none of the providers can be installed`; I tried the second build (1ab_1) and the error stayed:; ` package scanpy-1.9.6-pyhd8ed1ab_1 requires seaborn !=0.13.0, but none of the providers can be installed`. I checked github and saw that the dependency in `pyproject.toml` is `""seaborn>=0.13.0""` but when i checked the conda package's `index.json` i saw `""seaborn !=0.13.0""`. The discrepancy between the dependencies is unclear. the full `index.json`:; ```json; {; ""arch"": null,; ""build"": ""pyhd8ed1ab_1"",; ""build_number"": 1,; ""depends"": [; ""anndata >=0.7.4"",; ""get-annotations"",; ""h5py >=3"",; ""joblib"",; ""matplotlib-base >=3.6"",; ""natsort"",; ""networkx >=2.3"",; ""numba >=0.41"",; ""numpy >=1.17"",; ""packaging"",; ""pandas >=1.1.1,!=2.1.2"",; ""patsy"",; ""python >=3.8"",; ""scikit-learn >=0.24"",; ""scipy >=1.4"",; ""seaborn !=0.13.0"",; ""session-info"",; ""statsmodels >=0.11"",; ""tqdm"",; ""umap-learn >=0.3.10""; ],; ""license"": ""BSD-3-Clause"",; ""license_family"": ""BSD"",; ""name"": ""scanpy"",; ""noarch"": ""python"",; ""platform"": null,; ""subdir"": ""noarch"",; ""timestamp"": 1699376683854,; ""version"": ""1.9.6""; }; ```. ### Minimal code sample. ```python; conda create -n test ""python=3.9"" ""scanpy=1.9.6"" ""seaborn=0.13"" -c conda-forge; ```. ### Error output. _No response_. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2791:760,depend,dependency,760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2791,3,['depend'],"['dependencies', 'dependency', 'depends']"
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python; N/A; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.9.0; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; packaging 23.1; pandas 2.0.3; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.1; session_info 1.0.0; sitecustomize NA; six 1.16.0; sklearn 1.3.0; threadpoolctl 3.2.0; -----; Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]; macOS-13.2.1-arm64-arm-64bit; -----; Session information updated at 2023-07-19 13:34; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2562:593,depend,dependency,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562,1,['depend'],['dependency']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Many different calls in scanpy emit warnings that are currently suppressed by our testing framework (I think). . ### Minimal code sample. I discovered this unrelatedly by editing the notebooks, see for example: https://github.com/scverse/scanpy-tutorials/blob/master/spatial/integration-scanorama.ipynb. @flying-sheep mentioned that the scanpy tests filter out warnings and indeed you can reproduce these by e.g.,:; ```sh; pytest -W error::FutureWarning -n auto scanpy/tests/test_plotting.py; ```. ### Error output. - [x] `…/scanpy/plotting/_tools/scatterplots.py:401:`. > UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored. - [x] `…/scanpy/plotting/_tools/__init__.py:1269:`. > FutureWarning: The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect. ; > `_ax = sns.violinplot(`. - [x] `…/scanpy/preprocessing/_simple.py:274:`. > ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; > `adata.var[""n_cells""] = number`. - [x] `…/scanpy/plotting/_stacked_violin.py:503: FutureWarning:`. > Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect. ; > `row_ax = sns.violinplot(`. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.10.0.dev191+gf7f5d5c6; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jedi 0.19.1; jinja2 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839:566,integrat,integration-scanorama,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839,1,['integrat'],['integration-scanorama']
